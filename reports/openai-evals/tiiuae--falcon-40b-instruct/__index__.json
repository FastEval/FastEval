{
    "aba_mrpc_true_false.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "aba_mrpc_true_false.dev.v0",
            "base_eval": "aba_mrpc_true_false",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "aba_mrpc_true_false/samples.jsonl"
                    },
                    "key": "aba_mrpc_true_false.dev.v0",
                    "group": "aba-mrpc-true-false"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528153401B44SQRWG",
            "created_at": "2023-05-28 15:34:01.771867"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "algebra-word-problems.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "algebra-word-problems.s1.simple-v0",
            "base_eval": "algebra-word-problems",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "algebra_word_problems/samples.jsonl"
                    },
                    "key": "algebra-word-problems.s1.simple-v0",
                    "group": "algebra-word-problems"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527193514ZGVGI2DQ",
            "created_at": "2023-05-27 19:35:14.154676"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "anagrams.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "anagrams.test.v1",
            "base_eval": "anagrams",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "few_shot_jsonl": "anagrams/fewshot.jsonl",
                        "num_few_shot": 5,
                        "samples_jsonl": "anagrams/samples.jsonl"
                    },
                    "key": "anagrams.test.v1",
                    "group": "anagrams"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527193537SSIL54LZ",
            "created_at": "2023-05-27 19:35:37.147689"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "balance-chemical-equation.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "balance-chemical-equation.dev.v0",
            "base_eval": "balance-chemical-equation",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "balance_chemical_equation/samples.jsonl"
                    },
                    "key": "balance-chemical-equation.dev.v0",
                    "group": "balance-chemical-equation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527193946X22CAIBV",
            "created_at": "2023-05-27 19:39:46.122878"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "belarusian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "belarusian-lexicon.dev.v0",
            "base_eval": "belarusian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "belarusian_lexicon/samples.jsonl"
                    },
                    "key": "belarusian-lexicon.dev.v0",
                    "group": "belarusian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527194549PCXSF6LC",
            "created_at": "2023-05-27 19:45:49.067026"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "bigrams.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "bigrams.dev.v0",
            "base_eval": "bigrams",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bigrams/samples.jsonl"
                    },
                    "key": "bigrams.dev.v0",
                    "group": "bigrams"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527194812CWTMX2DT",
            "created_at": "2023-05-27 19:48:12.283712"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "bitwise.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "bitwise.dev.v0",
            "base_eval": "bitwise",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bitwise/samples.jsonl"
                    },
                    "key": "bitwise.dev.v0",
                    "group": "bitwise"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305271950005P2X6QT6",
            "created_at": "2023-05-27 19:50:00.812842"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "born-first.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "born-first.dev.v0",
            "base_eval": "born-first",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "born_first/born_first.jsonl"
                    },
                    "key": "born-first.dev.v0",
                    "group": "born-first"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527195507LSFYZKVP",
            "created_at": "2023-05-27 19:55:07.987778"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "brazilian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "brazilian-lexicon.dev.v0",
            "base_eval": "brazilian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "brazilian-lexicon/samples.jsonl"
                    },
                    "key": "brazilian-lexicon.dev.v0",
                    "group": "brazilian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527195657TXAUKUQ6",
            "created_at": "2023-05-27 19:56:57.013952"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "bulgarian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "bulgarian-lexicon.dev.v0",
            "base_eval": "bulgarian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bulgarian-lexicon/samples.jsonl"
                    },
                    "key": "bulgarian-lexicon.dev.v0",
                    "group": "bulgarian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527195857JOJ25FVI",
            "created_at": "2023-05-27 19:58:57.819043"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "categorize-with-distractors.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "categorize-with-distractors.dev.v0",
            "base_eval": "categorize-with-distractors",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "categorize_with_distractors/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "categorize-with-distractors.dev.v0",
                    "group": "categorize_with_distractors"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527200118EMIFKOWM",
            "created_at": "2023-05-27 20:01:18.830601"
        },
        "final_report": {
            "counts/__invalid__": 9,
            "counts/B": 3,
            "counts/A": 6,
            "counts/C": 1,
            "counts/E": 1
        }
    },
    "chess-piece-count.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "chess-piece-count.s1.simple-v0",
            "base_eval": "chess-piece-count",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "chess_piece_count/fuzzy_match.jsonl"
                    },
                    "key": "chess-piece-count.s1.simple-v0",
                    "group": "chess-piece-count"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527201405W3EVCVOL",
            "created_at": "2023-05-27 20:14:05.242334"
        },
        "final_report": {
            "accuracy": 0.0,
            "f1_score": 0.0
        }
    },
    "chess.match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "chess.match.dev.v0",
            "base_eval": "chess",
            "split": "match",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "chess/match.jsonl"
                    },
                    "key": "chess.match.dev.v0",
                    "group": "chess"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527201652LB3TKDNB",
            "created_at": "2023-05-27 20:16:52.383760"
        },
        "final_report": {
            "accuracy": 0.35,
            "f1_score": 0.0
        }
    },
    "compare-countries-area.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "compare-countries-area.dev.v0",
            "base_eval": "compare-countries-area",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "compare-countries-area/samples.jsonl"
                    },
                    "key": "compare-countries-area.dev.v0",
                    "group": "compare-countries-area"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527201940YILA5SMJ",
            "created_at": "2023-05-27 20:19:40.914774"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "computer-science-problems.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "computer-science-problems.s1.simple-v0",
            "base_eval": "computer-science-problems",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "test_comp_sci/questions.jsonl"
                    },
                    "key": "computer-science-problems.s1.simple-v0",
                    "group": "test-comp-sci"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305281144407J5ZUUMK",
            "created_at": "2023-05-28 11:44:40.491856"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "connect4.s1.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "connect4.s1.v1",
            "base_eval": "connect4",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "connect4/samples.jsonl"
                    },
                    "key": "connect4.s1.v1",
                    "group": "connect-4"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527202430QXAHDYCA",
            "created_at": "2023-05-27 20:24:30.991447"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "convert-hex-hsl-lightness.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "convert-hex-hsl-lightness.dev.v0",
            "base_eval": "convert-hex-hsl-lightness",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "convert-hex-hsl-lightness/samples.jsonl"
                    },
                    "key": "convert-hex-hsl-lightness.dev.v0",
                    "group": "convert-hex-hsl-lightness"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527203130L454OX2U",
            "created_at": "2023-05-27 20:31:30.191036"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "coqa-closedqa-conciseness.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "coqa-closedqa-conciseness.dev.v0",
            "base_eval": "coqa-closedqa-conciseness",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "conciseness: Is the answer concise and to the point?"
                        }
                    },
                    "key": "coqa-closedqa-conciseness.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527221329LBJA5Y5V",
            "created_at": "2023-05-27 22:13:29.571902"
        },
        "final_report": {
            "counts/__invalid__": 6,
            "counts/Y": 3,
            "score": 0.3333333333333333
        }
    },
    "coqa-closedqa-correct.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "coqa-closedqa-correct.dev.v0",
            "base_eval": "coqa-closedqa-correct",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "correctness: Is the answer correct?"
                        }
                    },
                    "key": "coqa-closedqa-correct.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527212754SAD2XW3S",
            "created_at": "2023-05-27 21:27:54.051200"
        },
        "final_report": {
            "counts/__invalid__": 5,
            "counts/Y": 3,
            "counts/N": 1,
            "score": 0.3333333333333333
        }
    },
    "coqa-closedqa-relevance.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "coqa-closedqa-relevance.dev.v0",
            "base_eval": "coqa-closedqa-relevance",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "relevance: Is the submission referring to a real quote from the text?"
                        }
                    },
                    "key": "coqa-closedqa-relevance.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527215648XCQBMBZJ",
            "created_at": "2023-05-27 21:56:48.751336"
        },
        "final_report": {
            "counts/__invalid__": 7,
            "counts/N": 2,
            "score": 0.0
        }
    },
    "coqa-fact-expl.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "coqa-fact-expl.dev.v0",
            "base_eval": "coqa-fact-expl",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "eval_type": "classify_cot",
                        "modelgraded_spec": "fact"
                    },
                    "key": "coqa-fact-expl.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527210248UXCL4BEA",
            "created_at": "2023-05-27 21:02:48.877326"
        },
        "final_report": {
            "counts/C": 1,
            "counts/A": 4,
            "counts/__invalid__": 3,
            "counts/D": 1
        }
    },
    "coqa-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "coqa-fact.dev.v0",
            "base_eval": "coqa-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "coqa-fact.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527204418TIJDLWHF",
            "created_at": "2023-05-27 20:44:18.916030"
        },
        "final_report": {
            "counts/__invalid__": 5,
            "counts/A": 3,
            "counts/E": 1
        }
    },
    "coqa-match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "coqa-match.dev.v0",
            "base_eval": "coqa-match",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "coqa/match.jsonl"
                    },
                    "key": "coqa-match.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527203640E3OPILDY",
            "created_at": "2023-05-27 20:36:40.854779"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "crepe.dev.v2.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "crepe.dev.v2",
            "base_eval": "crepe",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "crepe/samples.jsonl"
                    },
                    "key": "crepe.dev.v2",
                    "group": "crepe"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527223348DVKZ2XQ3",
            "created_at": "2023-05-27 22:33:48.285288"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "cube-pack.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "cube-pack.dev.v0",
            "base_eval": "cube-pack",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "cube-pack/samples.jsonl"
                    },
                    "key": "cube-pack.dev.v0",
                    "group": "cube-pack"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527223618SOX5KD2P",
            "created_at": "2023-05-27 22:36:18.019136"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "decrypt-caesar-cipher.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "decrypt-caesar-cipher.dev.v0",
            "base_eval": "decrypt-caesar-cipher",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "decrypt_caesar_cipher/samples.jsonl"
                    },
                    "key": "decrypt-caesar-cipher.dev.v0",
                    "group": "decrypt-caesar-cipher"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527224015PO4OQ6CM",
            "created_at": "2023-05-27 22:40:15.988581"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "determinant.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "determinant.test.v1",
            "base_eval": "determinant",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "determinant/samples.jsonl"
                    },
                    "key": "determinant.test.v1",
                    "group": "determinant"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527224437OF2LEHCW",
            "created_at": "2023-05-27 22:44:37.363954"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "diagrammatic_logic.dev.v2.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "diagrammatic_logic.dev.v2",
            "base_eval": "diagrammatic_logic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "diagrammatic_logic/samples.jsonl"
                    },
                    "key": "diagrammatic_logic.dev.v2",
                    "group": "diagrammatic_logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527224716FDVZAMQG",
            "created_at": "2023-05-27 22:47:16.189238"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "dice-rotation-sequence.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "dice-rotation-sequence.dev.v0",
            "base_eval": "dice-rotation-sequence",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "dice-rotation-sequence/samples.jsonl"
                    },
                    "key": "dice-rotation-sequence.dev.v0",
                    "group": "dice-rotation-sequence"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527225334WHLSEBWK",
            "created_at": "2023-05-27 22:53:34.822913"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "diversity.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "diversity.dev.v0",
            "base_eval": "diversity",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/joke_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "diversity",
                        "multicomp_n": 4,
                        "sample_kwargs": {
                            "temperature": 0.4
                        }
                    },
                    "key": "diversity.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528133622RSPXLQ4X",
            "created_at": "2023-05-28 13:36:22.919452"
        },
        "final_report": {
            "counts/Yes": 5,
            "counts/__invalid__": 5,
            "score": 0.5
        }
    },
    "dutch-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "dutch-lexicon.dev.v0",
            "base_eval": "dutch-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "dutch-lexicon/samples.jsonl"
                    },
                    "key": "dutch-lexicon.dev.v0",
                    "group": "dutch-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527225925VHKKFD3X",
            "created_at": "2023-05-27 22:59:25.438972"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "emoji-riddle.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "emoji-riddle.s1.simple-v0",
            "base_eval": "emoji-riddle",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "emoji_riddle/fuzzy_match.jsonl"
                    },
                    "key": "emoji-riddle.s1.simple-v0",
                    "group": "emoji-riddle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527230202FCCESOMQ",
            "created_at": "2023-05-27 23:02:02.003937"
        },
        "final_report": {
            "accuracy": 0.1,
            "f1_score": 0.05032679738562092
        }
    },
    "emotional-intelligence.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "emotional-intelligence.dev.v0",
            "base_eval": "emotional-intelligence",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "emotional-intelligence/samples.jsonl"
                    },
                    "key": "emotional-intelligence.dev.v0",
                    "group": "emotional-intelligence"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527230416EQYQK3YK",
            "created_at": "2023-05-27 23:04:16.054490"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "escher-sentences.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "escher-sentences.dev.v0",
            "base_eval": "escher-sentences",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "escher_sentences/samples.jsonl"
                    },
                    "key": "escher-sentences.dev.v0",
                    "group": "escher-sentences"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527231024SKOCBZHY",
            "created_at": "2023-05-27 23:10:24.542655"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "fcc_amateur_extra.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "fcc_amateur_extra.dev.v0",
            "base_eval": "fcc_amateur_extra",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "fcc_amateur_extra/samples.jsonl"
                    },
                    "key": "fcc_amateur_extra.dev.v0",
                    "group": "fcc_amateur_extra"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527231415ILDLDPFM",
            "created_at": "2023-05-27 23:14:15.369005"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "first-letters.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "first-letters.dev.v0",
            "base_eval": "first-letters",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "first-letters/samples.jsonl"
                    },
                    "key": "first-letters.dev.v0",
                    "group": "first-letters"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527232048VP6DV3QG",
            "created_at": "2023-05-27 23:20:48.817324"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "formal-logic.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "formal-logic.dev.v0",
            "base_eval": "formal-logic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "formal_logic/formal_logic_expressions.jsonl"
                    },
                    "key": "formal-logic.dev.v0",
                    "group": "formal_logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527232140TYJGSBJU",
            "created_at": "2023-05-27 23:21:40.723289"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "forth-stack-sim-basic.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "forth-stack-sim-basic.dev.v0",
            "base_eval": "forth-stack-sim-basic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/basic_samples.jsonl"
                    },
                    "key": "forth-stack-sim-basic.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527233405FEGRHW6N",
            "created_at": "2023-05-27 23:34:05.770241"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "forth-stack-sim-detailed.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "forth-stack-sim-detailed.dev.v0",
            "base_eval": "forth-stack-sim-detailed",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/detailed_samples.jsonl"
                    },
                    "key": "forth-stack-sim-detailed.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527234143YBABITTM",
            "created_at": "2023-05-27 23:41:43.994905"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "forth-stack-sim.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "forth-stack-sim.dev.v0",
            "base_eval": "forth-stack-sim",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/samples.jsonl"
                    },
                    "key": "forth-stack-sim.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527232801U6YB2BQY",
            "created_at": "2023-05-27 23:28:01.811570"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "greek-vocabulary.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "greek-vocabulary.dev.v0",
            "base_eval": "greek-vocabulary",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "greek_vocabulary/samples.jsonl"
                    },
                    "key": "greek-vocabulary.dev.v0",
                    "group": "greek-vocabulary"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230527235639LFB5TULD",
            "created_at": "2023-05-27 23:56:39.961909"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "hand_ranks.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "hand_ranks.test.v1",
            "base_eval": "hand_ranks",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "poker_hand_ranks/full_samples.jsonl"
                    },
                    "key": "hand_ranks.test.v1",
                    "group": "poker_hand_ranks"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528095553SGBPATMQ",
            "created_at": "2023-05-28 09:55:53.502841"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "heart-disease.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "heart-disease.v0",
            "base_eval": "heart-disease",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "heart-disease/samples.jsonl"
                    },
                    "key": "heart-disease.v0",
                    "group": "heart-disease"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528000002JIFP54ZH",
            "created_at": "2023-05-28 00:00:02.820425"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "hebrew-rhyme.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "hebrew-rhyme.v0",
            "base_eval": "hebrew-rhyme",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "hebrew_rhyme/samples.jsonl"
                    },
                    "key": "hebrew-rhyme.v0",
                    "group": "hebrew-rhyme"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280002067IZND24C",
            "created_at": "2023-05-28 00:02:06.809343"
        },
        "final_report": {
            "accuracy": 0.1,
            "f1_score": 0.003999999999999999
        }
    },
    "hellaswag.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "hellaswag.val.ab-v1",
            "base_eval": "hellaswag",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://hellaswag?split=validation",
                        "instructions": "Choose the most plausible continuation for the story."
                    },
                    "key": "hellaswag.val.ab-v1",
                    "group": "language"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280125123VPJPWBQ",
            "created_at": "2023-05-28 01:25:12.428854"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "hindi_upsc.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "hindi_upsc.dev.v0",
            "base_eval": "hindi_upsc",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "hindi_upsc/samples.jsonl"
                    },
                    "key": "hindi_upsc.dev.v0",
                    "group": "hindi_upsc"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528000512LUBWIOUS",
            "created_at": "2023-05-28 00:05:12.092651"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "illinois-law.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "illinois-law.v0",
            "base_eval": "illinois-law",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "illinois-law/samples.jsonl"
                    },
                    "key": "illinois-law.v0",
                    "group": "illinois-law"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528001111OOZQ2HBJ",
            "created_at": "2023-05-28 00:11:11.076362"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "imperial_date_to_string.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "imperial_date_to_string.dev.v0",
            "base_eval": "imperial_date_to_string",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "imperial_date_to_string/samples.jsonl"
                    },
                    "key": "imperial_date_to_string.dev.v0",
                    "group": "imperial_date_to_string"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528004548JDSKCNWT",
            "created_at": "2023-05-28 00:45:48.922752"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "infiniteloop-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "infiniteloop-match.s1.simple-v0",
            "base_eval": "infiniteloop-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "infiniteloop-match/infiniteloop-match.jsonl"
                    },
                    "key": "infiniteloop-match.s1.simple-v0",
                    "group": "infiniteloop-match"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528004645DHL2IHFA",
            "created_at": "2023-05-28 00:46:45.001084"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "invoices.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "invoices.dev.v0",
            "base_eval": "invoices",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "invoices/match.jsonl"
                    },
                    "key": "invoices.dev.v0",
                    "group": "invoices"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528005026FFNQSJWF",
            "created_at": "2023-05-28 00:50:26.806753"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "japanese-national-medical-exam01.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "japanese-national-medical-exam01.dev.v0",
            "base_eval": "japanese-national-medical-exam01",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "japanese-national-medical-exam01/japanese-national-medical-exam01.jsonl"
                    },
                    "key": "japanese-national-medical-exam01.dev.v0",
                    "group": "japanese-national-medical-exam01"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528005202TBA3K776",
            "created_at": "2023-05-28 00:52:02.748243"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "japanese_driving_license.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "japanese_driving_license.s1.simple-v0",
            "base_eval": "japanese_driving_license",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "japanese_driving_license/samples.jsonl"
                    },
                    "key": "japanese_driving_license.s1.simple-v0",
                    "group": "japanese_driving_license"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528010017TAAG7HOM",
            "created_at": "2023-05-28 01:00:17.002821"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "job_listing_title_for_a_caregiver_in_japan.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "job_listing_title_for_a_caregiver_in_japan.test.v1",
            "base_eval": "job_listing_title_for_a_caregiver_in_japan",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "job_listing_title_for_a_caregiver_in_japan/samples.jsonl"
                    },
                    "key": "job_listing_title_for_a_caregiver_in_japan.test.v1",
                    "group": "job_listing_title_for_a_caregiver_in_japan"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528010348Q6T226Y4",
            "created_at": "2023-05-28 01:03:48.016931"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "joke-animals-vs-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "joke-animals-vs-fruits.dev.v0",
            "base_eval": "joke-animals-vs-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/joke_animals_vs_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "joke-animals-vs-fruits.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528114613JWJVCM3I",
            "created_at": "2023-05-28 11:46:13.457681"
        },
        "final_report": {
            "counts/__invalid__": 9,
            "score": 0.0
        }
    },
    "joke-fruits-ans-meta.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "joke-fruits-ans-meta.dev.v0",
            "base_eval": "joke-fruits-ans-meta",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_metaeval/joke_fruits_labeled.jsonl",
                        "eval_type": "classify",
                        "modelgraded_spec": "humor",
                        "metaeval": true
                    },
                    "key": "joke-fruits-ans-meta.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528133554TRDKHYNG",
            "created_at": "2023-05-28 13:35:54.894682"
        },
        "final_report": {
            "counts/__invalid__": 4,
            "counts/Unsure": 3,
            "counts/Yes": 1,
            "counts/No": 2,
            "score": 0.25,
            "metascore": 0.2
        }
    },
    "joke-fruits-expl-meta.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "joke-fruits-expl-meta.dev.v0",
            "base_eval": "joke-fruits-expl-meta",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_metaeval/joke_fruits_labeled.jsonl",
                        "eval_type": "classify_cot",
                        "modelgraded_spec": "humor",
                        "metaeval": true
                    },
                    "key": "joke-fruits-expl-meta.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528133504FLRRP2OA",
            "created_at": "2023-05-28 13:35:04.991705"
        },
        "final_report": {
            "counts/__invalid__": 4,
            "counts/No": 2,
            "counts/Yes": 4,
            "score": 0.4,
            "metascore": 0.6
        }
    },
    "joke-fruits-likert.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "joke-fruits-likert.dev.v0",
            "base_eval": "joke-fruits-likert",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/joke_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "humor_likert"
                    },
                    "key": "joke-fruits-likert.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528132859ZC2J4YDB",
            "created_at": "2023-05-28 13:28:59.494504"
        },
        "final_report": {
            "counts/__invalid__": 9,
            "counts/1": 1,
            "score": 1.0
        }
    },
    "joke-fruits-meta.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "joke-fruits-meta.dev.v0",
            "base_eval": "joke-fruits-meta",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_metaeval/joke_fruits_labeled.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "humor",
                        "metaeval": true
                    },
                    "key": "joke-fruits-meta.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528133239OHZKA5JL",
            "created_at": "2023-05-28 13:32:39.716562"
        },
        "final_report": {
            "counts/__invalid__": 8,
            "counts/Yes": 2,
            "score": 0.2,
            "metascore": 0.1
        }
    },
    "joke-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "joke-fruits.dev.v0",
            "base_eval": "joke-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/joke_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "humor"
                    },
                    "key": "joke-fruits.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528132406DQLO247E",
            "created_at": "2023-05-28 13:24:06.293654"
        },
        "final_report": {
            "counts/__invalid__": 7,
            "counts/Yes": 2,
            "counts/No": 1,
            "score": 0.2
        }
    },
    "knot-theory-unknotting-number.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "knot-theory-unknotting-number.dev.v0",
            "base_eval": "knot-theory-unknotting-number",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "knot-theory/knot-theory-unknotting-numbers.jsonl"
                    },
                    "key": "knot-theory-unknotting-number.dev.v0",
                    "group": "knot-theory"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280121006UDLISLS",
            "created_at": "2023-05-28 01:21:00.773122"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "knot-theory-unknotting-problem.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "knot-theory-unknotting-problem.dev.v0",
            "base_eval": "knot-theory-unknotting-problem",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "knot-theory/knot-theory-unknotting-problems.jsonl"
                    },
                    "key": "knot-theory-unknotting-problem.dev.v0",
                    "group": "knot-theory"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528011129JA44ZIMY",
            "created_at": "2023-05-28 01:11:29.754566"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "last-word-nth.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "last-word-nth.s1.simple-v0",
            "base_eval": "last-word-nth",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "last_word_nth/samples.jsonl"
                    },
                    "key": "last-word-nth.s1.simple-v0",
                    "group": "last-word-nth"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528012703WCHHT3U3",
            "created_at": "2023-05-28 01:27:03.762013"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "lat_long_identify.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "lat_long_identify.dev.v0",
            "base_eval": "lat_long_identify",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "lat_long_identify/samples.jsonl"
                    },
                    "key": "lat_long_identify.dev.v0",
                    "group": "lat_long_identify"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528012831Z4C5NSUZ",
            "created_at": "2023-05-28 01:28:31.018471"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "logic-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "logic-fact.dev.v0",
            "base_eval": "logic-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "logic/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "logic-fact.dev.v0",
                    "group": "logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528014854MXFZSXRA",
            "created_at": "2023-05-28 01:48:54.356577"
        },
        "final_report": {
            "counts/A": 4,
            "counts/C": 2,
            "counts/__invalid__": 4
        }
    },
    "logic-liar-paradox.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "logic-liar-paradox.dev.v0",
            "base_eval": "logic-liar-paradox",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "logic-liar-paradox/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "logic-liar-paradox.dev.v0",
                    "group": "logic-liar-paradox"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280131446UWGFGQC",
            "created_at": "2023-05-28 01:31:44.443744"
        },
        "final_report": {
            "counts/A": 5,
            "counts/C": 2,
            "counts/__invalid__": 7,
            "counts/B": 1
        }
    },
    "logic-statements.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "logic-statements.dev.v0",
            "base_eval": "logic-statements",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "logic-statements/logic-statements.jsonl"
                    },
                    "key": "logic-statements.dev.v0",
                    "group": "logic-statements"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528014254XSMYPPXH",
            "created_at": "2023-05-28 01:42:54.167244"
        },
        "final_report": {
            "accuracy": 0.05,
            "f1_score": 0.0
        }
    },
    "logiqa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "logiqa.dev.v0",
            "base_eval": "logiqa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "logiqa/logiqa.jsonl"
                    },
                    "key": "logiqa.dev.v0",
                    "group": "logiqa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280201556MUN5DPX",
            "created_at": "2023-05-28 02:01:55.829704"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "loss-logic-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "loss-logic-fact.dev.v0",
            "base_eval": "loss-logic-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "loss_logic/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "loss-logic-fact.dev.v0",
                    "group": "loss-logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528020704QCN2I2P2",
            "created_at": "2023-05-28 02:07:04.313809"
        },
        "final_report": {
            "counts/__invalid__": 2,
            "counts/B": 1,
            "counts/C": 1,
            "counts/D": 1
        }
    },
    "manga-translation-bubble.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "manga-translation-bubble.dev.v0",
            "base_eval": "manga-translation-bubble",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.translate:Translate",
                    "args": {
                        "samples_jsonl": "manga-translation/bubbles.jsonl"
                    },
                    "key": "manga-translation-bubble.dev.v0",
                    "group": "manga-translation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528063909IEMW4NZQ",
            "created_at": "2023-05-28 06:39:09.020766"
        },
        "final_report": {
            "accuracy": 0.0,
            "sacrebleu_score": 0.607854661159236
        }
    },
    "manga-translation-page.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "manga-translation-page.dev.v0",
            "base_eval": "manga-translation-page",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.translate:Translate",
                    "args": {
                        "samples_jsonl": "manga-translation/pages.jsonl"
                    },
                    "key": "manga-translation-page.dev.v0",
                    "group": "manga-translation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528062536PGUB5N35",
            "created_at": "2023-05-28 06:25:36.205517"
        },
        "final_report": {
            "accuracy": 0.0,
            "sacrebleu_score": 2.382246328433287
        }
    },
    "manga-translation-panel.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "manga-translation-panel.dev.v0",
            "base_eval": "manga-translation-panel",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.translate:Translate",
                    "args": {
                        "samples_jsonl": "manga-translation/panels.jsonl"
                    },
                    "key": "manga-translation-panel.dev.v0",
                    "group": "manga-translation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528063528H4CZGH4U",
            "created_at": "2023-05-28 06:35:28.068231"
        },
        "final_report": {
            "accuracy": 0.0,
            "sacrebleu_score": 0.8347311560865032
        }
    },
    "map-electronic-component-part-to-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "map-electronic-component-part-to-fact.dev.v0",
            "base_eval": "map-electronic-component-part-to-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "map-electronic-component-part-to-fact/samples.jsonl"
                    },
                    "key": "map-electronic-component-part-to-fact.dev.v0",
                    "group": "map-electronic-component-part-to-fact"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528064316F3ZXDAZA",
            "created_at": "2023-05-28 06:43:16.110582"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "match_banking77.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "match_banking77.test.v1",
            "base_eval": "match_banking77",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "banking77/samples.jsonl"
                    },
                    "key": "match_banking77.test.v1",
                    "group": "banking77"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305271943062YGLLMZP",
            "created_at": "2023-05-27 19:43:06.077435"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "medmcqa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "medmcqa.dev.v0",
            "base_eval": "medmcqa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "medmcqa/samples.jsonl"
                    },
                    "key": "medmcqa.dev.v0",
                    "group": "medmcqa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528064606C62E5UQ7",
            "created_at": "2023-05-28 06:46:06.913864"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mendelian_inheritance.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mendelian_inheritance.dev.v0",
            "base_eval": "mendelian_inheritance",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "mendelian_inheritance/samples.jsonl"
                    },
                    "key": "mendelian_inheritance.dev.v0",
                    "group": "mendelian_inheritance"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528064839GQNUTXOB",
            "created_at": "2023-05-28 06:48:39.542805"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mg-humor-people_jp.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mg-humor-people_jp.dev.v0",
            "base_eval": "mg-humor-people_jp",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/humor_people_jp.jsonl",
                        "eval_type": "cot_classify_jp",
                        "modelgraded_spec": "humor_jp"
                    },
                    "key": "mg-humor-people_jp.dev.v0",
                    "group": "test-modelgraded-generated"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23052812354432SWG5VB",
            "created_at": "2023-05-28 12:35:44.520748"
        },
        "final_report": {
            "counts/4": 1,
            "counts/__invalid__": 16,
            "counts/3": 1,
            "counts/1": 2,
            "score": 1.25
        }
    },
    "mmlu-abstract-algebra.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-abstract-algebra.val.ab-v1",
            "base_eval": "mmlu-abstract-algebra",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=abstract_algebra&split=validation"
                    },
                    "key": "mmlu-abstract-algebra.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23052806560974QTCQA7",
            "created_at": "2023-05-28 06:56:09.106222"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-anatomy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-anatomy.val.ab-v1",
            "base_eval": "mmlu-anatomy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=anatomy&split=validation"
                    },
                    "key": "mmlu-anatomy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528065859EAKUJEI6",
            "created_at": "2023-05-28 06:58:59.195105"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-astronomy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-astronomy.val.ab-v1",
            "base_eval": "mmlu-astronomy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=astronomy&split=validation"
                    },
                    "key": "mmlu-astronomy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528065945V6EMBDGU",
            "created_at": "2023-05-28 06:59:45.757686"
        },
        "final_report": {
            "accuracy": 0.0625
        }
    },
    "mmlu-business-ethics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-business-ethics.val.ab-v1",
            "base_eval": "mmlu-business-ethics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=business_ethics&split=validation"
                    },
                    "key": "mmlu-business-ethics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528070132Z624Q4TI",
            "created_at": "2023-05-28 07:01:32.277465"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-clinical-knowledge.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-clinical-knowledge.val.ab-v1",
            "base_eval": "mmlu-clinical-knowledge",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=clinical_knowledge&split=validation"
                    },
                    "key": "mmlu-clinical-knowledge.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528070223QTFNAASV",
            "created_at": "2023-05-28 07:02:23.803638"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-college-biology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-college-biology.val.ab-v1",
            "base_eval": "mmlu-college-biology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_biology&split=validation"
                    },
                    "key": "mmlu-college-biology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528070412OCTGPG4U",
            "created_at": "2023-05-28 07:04:12.121814"
        },
        "final_report": {
            "accuracy": 0.0625
        }
    },
    "mmlu-college-chemistry.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-college-chemistry.val.ab-v1",
            "base_eval": "mmlu-college-chemistry",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_chemistry&split=validation"
                    },
                    "key": "mmlu-college-chemistry.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280705434EUYHLGK",
            "created_at": "2023-05-28 07:05:43.835662"
        },
        "final_report": {
            "accuracy": 0.125
        }
    },
    "mmlu-college-computer-science.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-college-computer-science.val.ab-v1",
            "base_eval": "mmlu-college-computer-science",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_computer_science&split=validation"
                    },
                    "key": "mmlu-college-computer-science.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528070631OTQSJIM2",
            "created_at": "2023-05-28 07:06:31.271754"
        },
        "final_report": {
            "accuracy": 0.09090909090909091
        }
    },
    "mmlu-college-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-college-mathematics.val.ab-v1",
            "base_eval": "mmlu-college-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_mathematics&split=validation"
                    },
                    "key": "mmlu-college-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528070826WAITURRV",
            "created_at": "2023-05-28 07:08:26.547265"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-college-medicine.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-college-medicine.val.ab-v1",
            "base_eval": "mmlu-college-medicine",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_medicine&split=validation"
                    },
                    "key": "mmlu-college-medicine.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528071034X5RW6SNX",
            "created_at": "2023-05-28 07:10:34.293436"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "mmlu-college-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-college-physics.val.ab-v1",
            "base_eval": "mmlu-college-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_physics&split=validation"
                    },
                    "key": "mmlu-college-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528071316NK3OTOSE",
            "created_at": "2023-05-28 07:13:16.250420"
        },
        "final_report": {
            "accuracy": 0.09090909090909091
        }
    },
    "mmlu-computer-security.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-computer-security.val.ab-v1",
            "base_eval": "mmlu-computer-security",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=computer_security&split=validation"
                    },
                    "key": "mmlu-computer-security.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528071533JL2E4MIM",
            "created_at": "2023-05-28 07:15:33.346160"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-conceptual-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-conceptual-physics.val.ab-v1",
            "base_eval": "mmlu-conceptual-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=conceptual_physics&split=validation"
                    },
                    "key": "mmlu-conceptual-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528071655ADD2MFLX",
            "created_at": "2023-05-28 07:16:55.271330"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-econometrics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-econometrics.val.ab-v1",
            "base_eval": "mmlu-econometrics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=econometrics&split=validation"
                    },
                    "key": "mmlu-econometrics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528071842BHSRGEEZ",
            "created_at": "2023-05-28 07:18:42.756040"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-electrical-engineering.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-electrical-engineering.val.ab-v1",
            "base_eval": "mmlu-electrical-engineering",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=electrical_engineering&split=validation"
                    },
                    "key": "mmlu-electrical-engineering.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23052807195062NP3Y4U",
            "created_at": "2023-05-28 07:19:50.391237"
        },
        "final_report": {
            "accuracy": 0.125
        }
    },
    "mmlu-elementary-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-elementary-mathematics.val.ab-v1",
            "base_eval": "mmlu-elementary-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=elementary_mathematics&split=validation"
                    },
                    "key": "mmlu-elementary-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528072115ZEKVHNRC",
            "created_at": "2023-05-28 07:21:15.785893"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-formal-logic.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-formal-logic.val.ab-v1",
            "base_eval": "mmlu-formal-logic",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=formal_logic&split=validation"
                    },
                    "key": "mmlu-formal-logic.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528072423L4OOEF34",
            "created_at": "2023-05-28 07:24:23.176951"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-global-facts.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-global-facts.val.ab-v1",
            "base_eval": "mmlu-global-facts",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=global_facts&split=validation"
                    },
                    "key": "mmlu-global-facts.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528072732J6MMP6Z2",
            "created_at": "2023-05-28 07:27:32.153569"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-biology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-biology.val.ab-v1",
            "base_eval": "mmlu-high-school-biology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_biology&split=validation"
                    },
                    "key": "mmlu-high-school-biology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280728112B2FM2UI",
            "created_at": "2023-05-28 07:28:11.409543"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-chemistry.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-chemistry.val.ab-v1",
            "base_eval": "mmlu-high-school-chemistry",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_chemistry&split=validation"
                    },
                    "key": "mmlu-high-school-chemistry.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528072955KZE7I4ZC",
            "created_at": "2023-05-28 07:29:55.702018"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-high-school-computer-science.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-computer-science.val.ab-v1",
            "base_eval": "mmlu-high-school-computer-science",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_computer_science&split=validation"
                    },
                    "key": "mmlu-high-school-computer-science.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528073236AH44LUWT",
            "created_at": "2023-05-28 07:32:36.891765"
        },
        "final_report": {
            "accuracy": 0.2222222222222222
        }
    },
    "mmlu-high-school-european-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-european-history.val.ab-v1",
            "base_eval": "mmlu-high-school-european-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_european_history&split=validation"
                    },
                    "key": "mmlu-high-school-european-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528073328B2MXWY6S",
            "created_at": "2023-05-28 07:33:28.099262"
        },
        "final_report": {
            "accuracy": 0.05555555555555555
        }
    },
    "mmlu-high-school-geography.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-geography.val.ab-v1",
            "base_eval": "mmlu-high-school-geography",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_geography&split=validation"
                    },
                    "key": "mmlu-high-school-geography.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528073946IEX47CLD",
            "created_at": "2023-05-28 07:39:46.286666"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-high-school-government-and-politics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-government-and-politics.val.ab-v1",
            "base_eval": "mmlu-high-school-government-and-politics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_government_and_politics&split=validation"
                    },
                    "key": "mmlu-high-school-government-and-politics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528074133TSRN4CU2",
            "created_at": "2023-05-28 07:41:33.884051"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-high-school-macroeconomics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-macroeconomics.val.ab-v1",
            "base_eval": "mmlu-high-school-macroeconomics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_macroeconomics&split=validation"
                    },
                    "key": "mmlu-high-school-macroeconomics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528074447VFE2UXIJ",
            "created_at": "2023-05-28 07:44:47.833213"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-mathematics.val.ab-v1",
            "base_eval": "mmlu-high-school-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_mathematics&split=validation"
                    },
                    "key": "mmlu-high-school-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528074907ZBU4YHQT",
            "created_at": "2023-05-28 07:49:07.405508"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-microeconomics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-microeconomics.val.ab-v1",
            "base_eval": "mmlu-high-school-microeconomics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_microeconomics&split=validation"
                    },
                    "key": "mmlu-high-school-microeconomics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528075413T4YYGNJV",
            "created_at": "2023-05-28 07:54:13.844481"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-physics.val.ab-v1",
            "base_eval": "mmlu-high-school-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_physics&split=validation"
                    },
                    "key": "mmlu-high-school-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528075645PIO3R6IC",
            "created_at": "2023-05-28 07:56:45.253136"
        },
        "final_report": {
            "accuracy": 0.17647058823529413
        }
    },
    "mmlu-high-school-psychology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-psychology.val.ab-v1",
            "base_eval": "mmlu-high-school-psychology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_psychology&split=validation"
                    },
                    "key": "mmlu-high-school-psychology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528080022STQHYI7K",
            "created_at": "2023-05-28 08:00:22.228167"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-statistics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-statistics.val.ab-v1",
            "base_eval": "mmlu-high-school-statistics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_statistics&split=validation"
                    },
                    "key": "mmlu-high-school-statistics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528080306DU3BYUFQ",
            "created_at": "2023-05-28 08:03:06.051223"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-us-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-us-history.val.ab-v1",
            "base_eval": "mmlu-high-school-us-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_us_history&split=validation"
                    },
                    "key": "mmlu-high-school-us-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528080551IWOIVPSH",
            "created_at": "2023-05-28 08:05:51.622000"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-world-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-high-school-world-history.val.ab-v1",
            "base_eval": "mmlu-high-school-world-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_world_history&split=validation"
                    },
                    "key": "mmlu-high-school-world-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528081336JLEAFUTB",
            "created_at": "2023-05-28 08:13:36.000341"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-human-aging.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-human-aging.val.ab-v1",
            "base_eval": "mmlu-human-aging",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=human_aging&split=validation"
                    },
                    "key": "mmlu-human-aging.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528081702AWEFYS5W",
            "created_at": "2023-05-28 08:17:02.961938"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-human-sexuality.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-human-sexuality.val.ab-v1",
            "base_eval": "mmlu-human-sexuality",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=human_sexuality&split=validation"
                    },
                    "key": "mmlu-human-sexuality.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528081829ASG4OHAV",
            "created_at": "2023-05-28 08:18:29.574409"
        },
        "final_report": {
            "accuracy": 0.08333333333333333
        }
    },
    "mmlu-international-law.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-international-law.val.ab-v1",
            "base_eval": "mmlu-international-law",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=international_law&split=validation"
                    },
                    "key": "mmlu-international-law.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528081942L5C4FCMP",
            "created_at": "2023-05-28 08:19:42.148537"
        },
        "final_report": {
            "accuracy": 0.07692307692307693
        }
    },
    "mmlu-jurisprudence.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-jurisprudence.val.ab-v1",
            "base_eval": "mmlu-jurisprudence",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=jurisprudence&split=validation"
                    },
                    "key": "mmlu-jurisprudence.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23052808225242Q6KX2X",
            "created_at": "2023-05-28 08:22:52.226917"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-logical-fallacies.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-logical-fallacies.val.ab-v1",
            "base_eval": "mmlu-logical-fallacies",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=logical_fallacies&split=validation"
                    },
                    "key": "mmlu-logical-fallacies.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528082423YXUZLVLK",
            "created_at": "2023-05-28 08:24:23.795551"
        },
        "final_report": {
            "accuracy": 0.16666666666666666
        }
    },
    "mmlu-machine-learning.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-machine-learning.val.ab-v1",
            "base_eval": "mmlu-machine-learning",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=machine_learning&split=validation"
                    },
                    "key": "mmlu-machine-learning.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528082544FNIXODBF",
            "created_at": "2023-05-28 08:25:44.133895"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-management.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-management.val.ab-v1",
            "base_eval": "mmlu-management",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=management&split=validation"
                    },
                    "key": "mmlu-management.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528082712EEHSSCWJ",
            "created_at": "2023-05-28 08:27:12.860690"
        },
        "final_report": {
            "accuracy": 0.09090909090909091
        }
    },
    "mmlu-marketing.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-marketing.val.ab-v1",
            "base_eval": "mmlu-marketing",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=marketing&split=validation"
                    },
                    "key": "mmlu-marketing.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528082753P7ZZ4SYI",
            "created_at": "2023-05-28 08:27:53.063229"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "mmlu-medical-genetics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-medical-genetics.val.ab-v1",
            "base_eval": "mmlu-medical-genetics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=medical_genetics&split=validation"
                    },
                    "key": "mmlu-medical-genetics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280829484552UPIQ",
            "created_at": "2023-05-28 08:29:48.901246"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-miscellaneous.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-miscellaneous.val.ab-v1",
            "base_eval": "mmlu-miscellaneous",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=miscellaneous&split=validation"
                    },
                    "key": "mmlu-miscellaneous.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528083055WDQSVHFM",
            "created_at": "2023-05-28 08:30:55.084680"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-moral-disputes.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-moral-disputes.val.ab-v1",
            "base_eval": "mmlu-moral-disputes",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=moral_disputes&split=validation"
                    },
                    "key": "mmlu-moral-disputes.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528083234XV2E6XT6",
            "created_at": "2023-05-28 08:32:34.632335"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "mmlu-moral-scenarios.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-moral-scenarios.val.ab-v1",
            "base_eval": "mmlu-moral-scenarios",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=moral_scenarios&split=validation"
                    },
                    "key": "mmlu-moral-scenarios.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280834374NB2UJZA",
            "created_at": "2023-05-28 08:34:37.248954"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-nutrition.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-nutrition.val.ab-v1",
            "base_eval": "mmlu-nutrition",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=nutrition&split=validation"
                    },
                    "key": "mmlu-nutrition.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528083739KOLTOOWD",
            "created_at": "2023-05-28 08:37:39.017988"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-philosophy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-philosophy.val.ab-v1",
            "base_eval": "mmlu-philosophy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=philosophy&split=validation"
                    },
                    "key": "mmlu-philosophy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528083935KT7TWC4T",
            "created_at": "2023-05-28 08:39:35.955651"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-prehistory.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-prehistory.val.ab-v1",
            "base_eval": "mmlu-prehistory",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=prehistory&split=validation"
                    },
                    "key": "mmlu-prehistory.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528084133266ERT6T",
            "created_at": "2023-05-28 08:41:33.009115"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-professional-accounting.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-professional-accounting.val.ab-v1",
            "base_eval": "mmlu-professional-accounting",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_accounting&split=validation"
                    },
                    "key": "mmlu-professional-accounting.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528084305F2MFRL6L",
            "created_at": "2023-05-28 08:43:05.792846"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-professional-law.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-professional-law.val.ab-v1",
            "base_eval": "mmlu-professional-law",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_law&split=validation"
                    },
                    "key": "mmlu-professional-law.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528084729AVARMUBZ",
            "created_at": "2023-05-28 08:47:29.327785"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-professional-medicine.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-professional-medicine.val.ab-v1",
            "base_eval": "mmlu-professional-medicine",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_medicine&split=validation"
                    },
                    "key": "mmlu-professional-medicine.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528085010XPWUJIBW",
            "created_at": "2023-05-28 08:50:10.830653"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "mmlu-professional-psychology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-professional-psychology.val.ab-v1",
            "base_eval": "mmlu-professional-psychology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_psychology&split=validation"
                    },
                    "key": "mmlu-professional-psychology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528085422PEGUHU2F",
            "created_at": "2023-05-28 08:54:22.071409"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "mmlu-public-relations.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-public-relations.val.ab-v1",
            "base_eval": "mmlu-public-relations",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=public_relations&split=validation"
                    },
                    "key": "mmlu-public-relations.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528085644NBIU7ODR",
            "created_at": "2023-05-28 08:56:44.310467"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-security-studies.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-security-studies.val.ab-v1",
            "base_eval": "mmlu-security-studies",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=security_studies&split=validation"
                    },
                    "key": "mmlu-security-studies.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528085732PIJJ3ZDF",
            "created_at": "2023-05-28 08:57:32.947581"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-sociology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-sociology.val.ab-v1",
            "base_eval": "mmlu-sociology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=sociology&split=validation"
                    },
                    "key": "mmlu-sociology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280900596PXAU3G2",
            "created_at": "2023-05-28 09:00:59.179508"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-us-foreign-policy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-us-foreign-policy.val.ab-v1",
            "base_eval": "mmlu-us-foreign-policy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=us_foreign_policy&split=validation"
                    },
                    "key": "mmlu-us-foreign-policy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528090245VKFMV3VX",
            "created_at": "2023-05-28 09:02:45.755712"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-virology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-virology.val.ab-v1",
            "base_eval": "mmlu-virology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=virology&split=validation"
                    },
                    "key": "mmlu-virology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528090353Q34MS2DP",
            "created_at": "2023-05-28 09:03:53.745932"
        },
        "final_report": {
            "accuracy": 0.1111111111111111
        }
    },
    "mmlu-world-religions.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "mmlu-world-religions.val.ab-v1",
            "base_eval": "mmlu-world-religions",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=world_religions&split=validation"
                    },
                    "key": "mmlu-world-religions.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528090648FDG6EC3P",
            "created_at": "2023-05-28 09:06:48.069410"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "moral_exceptQA.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "moral_exceptQA.test.v1",
            "base_eval": "moral_exceptQA",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "moral_exceptQA/samples.jsonl"
                    },
                    "key": "moral_exceptQA.test.v1",
                    "group": "moral_exceptQA"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23052809082564JJTIEV",
            "created_at": "2023-05-28 09:08:25.069453"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "multi-step-equations.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "multi-step-equations.dev.v0",
            "base_eval": "multi-step-equations",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "multi-step-equations/samples.jsonl"
                    },
                    "key": "multi-step-equations.dev.v0",
                    "group": "multi-step-equations"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280917174AHDIJR5",
            "created_at": "2023-05-28 09:17:17.442736"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "naughty_strings_fuzzy.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "naughty_strings_fuzzy.test.v1",
            "base_eval": "naughty_strings_fuzzy",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "naughty_strings/samples.jsonl"
                    },
                    "key": "naughty_strings_fuzzy.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280923063A6X7DEG",
            "created_at": "2023-05-28 09:23:06.776925"
        },
        "final_report": {
            "accuracy": 0.25,
            "f1_score": 0.058823529411764705
        }
    },
    "naughty_strings_graded.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "naughty_strings_graded.test.v1",
            "base_eval": "naughty_strings_graded",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "naughty_strings/security.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "security"
                    },
                    "key": "naughty_strings_graded.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528092504CQKVHBEB",
            "created_at": "2023-05-28 09:25:04.439851"
        },
        "final_report": {
            "counts/Yes": 8,
            "counts/__invalid__": 11,
            "counts/Unsure": 1,
            "score": 0.425
        }
    },
    "naughty_strings_graded_meta.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "naughty_strings_graded_meta.test.v1",
            "base_eval": "naughty_strings_graded_meta",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "naughty_strings/security.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "security",
                        "metaeval": true
                    },
                    "key": "naughty_strings_graded_meta.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528093010FIU7NACD",
            "created_at": "2023-05-28 09:30:10.676550"
        },
        "final_report": {
            "counts/__invalid__": 17,
            "counts/Yes": 3,
            "score": 0.15,
            "metascore": 0.0
        }
    },
    "number-pattern.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "number-pattern.dev.v0",
            "base_eval": "number-pattern",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "number_pattern/samples.jsonl"
                    },
                    "key": "number-pattern.dev.v0",
                    "group": "number-pattern"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528093352CT7VBYMB",
            "created_at": "2023-05-28 09:33:52.437716"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "number-reading.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "number-reading.dev.v0",
            "base_eval": "number-reading",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "number_reading/number_reading.jsonl"
                    },
                    "key": "number-reading.dev.v0",
                    "group": "number-reading"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280937276HUWPQDG",
            "created_at": "2023-05-28 09:37:27.680701"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "partially_solved_crossword_clues.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "partially_solved_crossword_clues.dev.v0",
            "base_eval": "partially_solved_crossword_clues",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "partially_solved_crossword_clues/samples.jsonl"
                    },
                    "key": "partially_solved_crossword_clues.dev.v0",
                    "group": "partially_solved_crossword_clues"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528093852V6OUT2Y6",
            "created_at": "2023-05-28 09:38:52.777557"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "pattern_identification.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "pattern_identification.dev.v0",
            "base_eval": "pattern_identification",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "pattern_identification/samples.v0.jsonl"
                    },
                    "key": "pattern_identification.dev.v0",
                    "group": "pattern_identification"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305280940332VRAM3VN",
            "created_at": "2023-05-28 09:40:33.576345"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "ph-calculation.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "ph-calculation.dev.v0",
            "base_eval": "ph-calculation",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "ph_calculation/samples.jsonl"
                    },
                    "key": "ph-calculation.dev.v0",
                    "group": "ph_calculation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528094913WDWJ6VHW",
            "created_at": "2023-05-28 09:49:13.560167"
        },
        "final_report": {
            "accuracy": 0.15,
            "f1_score": 0.0034482758620689646
        }
    },
    "qa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "qa.dev.v0",
            "base_eval": "qa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "qa/q_and_a.jsonl"
                    },
                    "key": "qa.dev.v0",
                    "group": "qa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528100510YYQICHBL",
            "created_at": "2023-05-28 10:05:10.178015"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "rap-animals-vs-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "rap-animals-vs-fruits.dev.v0",
            "base_eval": "rap-animals-vs-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_animals_vs_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-animals-vs-fruits.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528120742GXT65SU2",
            "created_at": "2023-05-28 12:07:42.346461"
        },
        "final_report": {
            "counts/__invalid__": 8,
            "counts/Yes": 1,
            "score": 0.1111111111111111
        }
    },
    "rap-people-vs-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "rap-people-vs-fruits.dev.v0",
            "base_eval": "rap-people-vs-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_people_vs_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-people-vs-fruits.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528121937NY6HPHMM",
            "created_at": "2023-05-28 12:19:37.249212"
        },
        "final_report": {
            "counts/__invalid__": 6,
            "counts/Yes": 3,
            "score": 0.3333333333333333
        }
    },
    "rap-people-vs-people.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "rap-people-vs-people.dev.v0",
            "base_eval": "rap-people-vs-people",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_people_vs_people.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-people-vs-people.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528114921GQN2SK6P",
            "created_at": "2023-05-28 11:49:21.937056"
        },
        "final_report": {
            "counts/__invalid__": 8,
            "counts/No": 1,
            "score": 0.0
        }
    },
    "regex.match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "regex.match.dev.v0",
            "base_eval": "regex",
            "split": "match",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "regex-match/samples.jsonl"
                    },
                    "key": "regex.match.dev.v0",
                    "group": "regex-match"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528101220EX277MAF",
            "created_at": "2023-05-28 10:12:20.834059"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "rot13.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "rot13.s1.simple-v0",
            "base_eval": "rot13",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "rot13/rot13.jsonl"
                    },
                    "key": "rot13.s1.simple-v0",
                    "group": "rot13"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305281018533KIOQWTO",
            "created_at": "2023-05-28 10:18:53.863616"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "rucola.test.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "rucola.test.v0",
            "base_eval": "rucola",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "rucola/samples.jsonl",
                        "few_shot_jsonl": "rucola/few_shot.jsonl",
                        "num_few_shot": 4
                    },
                    "key": "rucola.test.v0",
                    "group": "rucola"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528102145ZHMXMANY",
            "created_at": "2023-05-28 10:21:45.175210"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "russe.test.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "russe.test.v0",
            "base_eval": "russe",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russe/samples.jsonl",
                        "few_shot_jsonl": "russe/few_shot.jsonl",
                        "num_few_shot": 2
                    },
                    "key": "russe.test.v0",
                    "group": "russe"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528102850CYE7WHKE",
            "created_at": "2023-05-28 10:28:50.786188"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "russian-nlp-tasks.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "russian-nlp-tasks.dev.v0",
            "base_eval": "russian-nlp-tasks",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russian-nlp-tasks/samples.jsonl"
                    },
                    "key": "russian-nlp-tasks.dev.v0",
                    "group": "russian-nlp-tasks"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528103708ZUWGI463",
            "created_at": "2023-05-28 10:37:08.314214"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "russian-rhyme.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "russian-rhyme.v0",
            "base_eval": "russian-rhyme",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "russian-rhyme/samples.jsonl"
                    },
                    "key": "russian-rhyme.v0",
                    "group": "russian-rhyme"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23052810401677PMVZ5R",
            "created_at": "2023-05-28 10:40:16.354677"
        },
        "final_report": {
            "accuracy": 0.0,
            "f1_score": 0.08823529411764706
        }
    },
    "russian_medical.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "russian_medical.dev.v0",
            "base_eval": "russian_medical",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russian_medical/samples.jsonl"
                    },
                    "key": "russian_medical.dev.v0",
                    "group": "russian_medical"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528104531LEQNPK7K",
            "created_at": "2023-05-28 10:45:31.065117"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "sarcasm.test.v1.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "sarcasm.test.v1",
            "base_eval": "sarcasm",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "sarcasm/samples.jsonl",
                        "few_shot_jsonl": "sarcasm/few_shot.jsonl",
                        "num_few_shot": 5
                    },
                    "key": "sarcasm.test.v1",
                    "group": "sarcasm"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528104833KNDNRJSA",
            "created_at": "2023-05-28 10:48:33.065698"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "simple-knowledge-mongolian.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "simple-knowledge-mongolian.dev.v0",
            "base_eval": "simple-knowledge-mongolian",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "simple-knowledge-mongolian/samples.v0.jsonl"
                    },
                    "key": "simple-knowledge-mongolian.dev.v0",
                    "group": "simple-knowledge-mongolian"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528105435I5VKYABZ",
            "created_at": "2023-05-28 10:54:35.185744"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "sort-numbers.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "sort-numbers.s1.simple-v0",
            "base_eval": "sort-numbers",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "sort_numeric/samples.jsonl"
                    },
                    "key": "sort-numbers.s1.simple-v0",
                    "group": "sort-numeric"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528105559GM5KSC5K",
            "created_at": "2023-05-28 10:55:59.860120"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "stock-option-terms-bear-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-option-terms-bear-call-spread.dev.v0",
            "base_eval": "stock-option-terms-bear-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_bear_call_spread.jsonl"
                    },
                    "key": "stock-option-terms-bear-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528112120T5VREF76",
            "created_at": "2023-05-28 11:21:20.310056"
        },
        "final_report": {
            "accuracy": 0.08333333333333333
        }
    },
    "stock-option-terms-bull-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-option-terms-bull-call-spread.dev.v0",
            "base_eval": "stock-option-terms-bull-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_bull_call_spread.jsonl"
                    },
                    "key": "stock-option-terms-bull-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528112234V2IQJM5Z",
            "created_at": "2023-05-28 11:22:34.527988"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-option-terms-inverse-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-option-terms-inverse-iron-condor-spread.dev.v0",
            "base_eval": "stock-option-terms-inverse-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_inverse_iron_condor_spread.jsonl"
                    },
                    "key": "stock-option-terms-inverse-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305281129334OJJV2TI",
            "created_at": "2023-05-28 11:29:33.093842"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "swedish-spelling.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "swedish-spelling.dev.v0",
            "base_eval": "swedish-spelling",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "swedish-spelling/samples.jsonl"
                    },
                    "key": "swedish-spelling.dev.v0",
                    "group": "swedish-spelling"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528113157SHVYBNUH",
            "created_at": "2023-05-28 11:31:57.570197"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-option-terms-iron-butterfly-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-option-terms-iron-butterfly-spread.dev.v0",
            "base_eval": "stock-option-terms-iron-butterfly-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_iron_butterfly_spread.jsonl"
                    },
                    "key": "stock-option-terms-iron-butterfly-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528112341VOLP5C2B",
            "created_at": "2023-05-28 11:23:41.026087"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-option-terms-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-option-terms-iron-condor-spread.dev.v0",
            "base_eval": "stock-option-terms-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_iron_condor_spread.jsonl"
                    },
                    "key": "stock-option-terms-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528112620BGDVLTFW",
            "created_at": "2023-05-28 11:26:20.554584"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "stock-options-bear-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-options-bear-call-spread.dev.v0",
            "base_eval": "stock-options-bear-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_bear_call_spread.jsonl"
                    },
                    "key": "stock-options-bear-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528105859JZZC2KUH",
            "created_at": "2023-05-28 10:58:59.989662"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-bull-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-options-bull-call-spread.dev.v0",
            "base_eval": "stock-options-bull-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_bull_call_spread.jsonl"
                    },
                    "key": "stock-options-bull-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528110000LMUWBGHT",
            "created_at": "2023-05-28 11:00:00.055016"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-inverse-iron-butterfly-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-options-inverse-iron-butterfly-spread.dev.v0",
            "base_eval": "stock-options-inverse-iron-butterfly-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_inverse_iron_butterfly_spread.jsonl"
                    },
                    "key": "stock-options-inverse-iron-butterfly-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528110146U7L6UBYP",
            "created_at": "2023-05-28 11:01:46.310136"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-inverse-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-options-inverse-iron-condor-spread.dev.v0",
            "base_eval": "stock-options-inverse-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_inverse_iron_condor_spread.jsonl"
                    },
                    "key": "stock-options-inverse-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528111627NAHIZLR4",
            "created_at": "2023-05-28 11:16:27.202274"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "stock-options-iron-condor-spread.dev.v0",
            "base_eval": "stock-options-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_iron_condor_spread.jsonl"
                    },
                    "key": "stock-options-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528110831LUS3E4YV",
            "created_at": "2023-05-28 11:08:31.806886"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "taxes.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "taxes.dev.v0",
            "base_eval": "taxes",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "taxes/samples.jsonl"
                    },
                    "key": "taxes.dev.v0",
                    "group": "taxes"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528113444HZNXM2BS",
            "created_at": "2023-05-28 11:34:44.726386"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "tempo_to_measure_count.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "tempo_to_measure_count.dev.v0",
            "base_eval": "tempo_to_measure_count",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "tempo_to_measure_count/samples.jsonl"
                    },
                    "key": "tempo_to_measure_count.dev.v0",
                    "group": "tempo_to_measure_count"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528113717X4HNH2SM",
            "created_at": "2023-05-28 11:37:17.935990"
        },
        "final_report": {
            "accuracy": 0.15,
            "f1_score": 0.06666666666666667
        }
    },
    "test-fuzzy-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "test-fuzzy-match.s1.simple-v0",
            "base_eval": "test-fuzzy-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl"
                    },
                    "key": "test-fuzzy-match.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528114306VOYX43Z3",
            "created_at": "2023-05-28 11:43:06.950902"
        },
        "final_report": {
            "accuracy": 0.3333333333333333,
            "f1_score": 0.11764705882352942
        }
    },
    "test-includes-ignore-case.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "test-includes-ignore-case.s1.simple-v0",
            "base_eval": "test-includes-ignore-case",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl",
                        "ignore_case": true
                    },
                    "key": "test-includes-ignore-case.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528114410XFGY6YAF",
            "created_at": "2023-05-28 11:44:10.565673"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "test-includes.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "test-includes.s1.simple-v0",
            "base_eval": "test-includes",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl"
                    },
                    "key": "test-includes.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528114331F7IJ7U6C",
            "created_at": "2023-05-28 11:43:31.957852"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "test-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "test-match.s1.simple-v0",
            "base_eval": "test-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "test_match/samples.jsonl"
                    },
                    "key": "test-match.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528114302RKS2RMTX",
            "created_at": "2023-05-28 11:43:02.810941"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "three-pt-mapping.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "three-pt-mapping.dev.v0",
            "base_eval": "three-pt-mapping",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "three-pt-mapping/three_pt_mapping.jsonl"
                    },
                    "key": "three-pt-mapping.dev.v0",
                    "group": "three-pt-mapping"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528134314JSAGH3GD",
            "created_at": "2023-05-28 13:43:14.469314"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "ukraine-eit.val.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "ukraine-eit.val.v0",
            "base_eval": "ukraine-eit",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "ukraine_eit/samples.jsonl"
                    },
                    "key": "ukraine-eit.val.v0",
                    "group": "ukraine-eit"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305281349147TL6QKHK",
            "created_at": "2023-05-28 13:49:14.537753"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "unified-patch.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "unified-patch.dev.v0",
            "base_eval": "unified-patch",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "unified_patch/samples.jsonl"
                    },
                    "key": "unified-patch.dev.v0",
                    "group": "unified-patch"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305281409595DFLZZXX",
            "created_at": "2023-05-28 14:09:59.168580"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "us-tort-law.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "us-tort-law.dev.v0",
            "base_eval": "us-tort-law",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "few_shot_jsonl": "us_tort_law/few_shot.jsonl",
                        "num_few_shot": 4,
                        "samples_jsonl": "us_tort_law/samples.jsonl"
                    },
                    "key": "us-tort-law.dev.v0",
                    "group": "us-tort-law"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528142216XDLD2GUF",
            "created_at": "2023-05-28 14:22:16.305204"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "utility_price_parsing.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "utility_price_parsing.dev.v0",
            "base_eval": "utility_price_parsing",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "utility_price_parsing/samples.jsonl"
                    },
                    "key": "utility_price_parsing.dev.v0",
                    "group": "utility_price_parsing"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305281514497TFIHHIA",
            "created_at": "2023-05-28 15:14:49.501071"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "which-is-heavier.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "falcon:tiiuae/falcon-40b-instruct"
            ],
            "eval_name": "which-is-heavier.dev.v0",
            "base_eval": "which-is-heavier",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "falcon:tiiuae/falcon-40b-instruct"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "which_is_heavier/which_is_heavier.jsonl"
                    },
                    "key": "which-is-heavier.dev.v0",
                    "group": "which-is-heavier"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -b openai-evals -m falcon:tiiuae/falcon-40b-instruct",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230528152150RP2OCQ7S",
            "created_at": "2023-05-28 15:21:50.985105"
        },
        "final_report": {
            "accuracy": 0.2
        }
    }
}