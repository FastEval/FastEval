{
    "score": 0.2,
    "model_outputs": [
        {
            "id": 0,
            "question": "Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 3,
            "model_answer": "(B) False, False.\n\nExplanation:\n\nStatement 1: Linear regression estimator has the smallest variance among all unbiased estimators.\n\n* This statement is True. In linear regression, the estimator is the mean of the observed values. The variance of the linear regression estimator is proportional to the sampling variance and inversely proportional to the sample size. Therefore, when the sample size is large, the variance of the linear regression estimator is lower than those of other unbiased estimators.\n\nStatement 2: The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\n\n* This statement is False. AdaBoost is an algorithm that iteratively models weak classifiers in a sequence. Each weak classifier is expected to perform better against the error of the previous classifier, and its output is multiplied by a weight \u03b1. If a weak classifier performs poorly against the error, its weight \u03b1 will be negative so that it has less effect on the final prediction. Therefore, the coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are not always non-negative.",
            "correct": false
        },
        {
            "id": 1,
            "question": "Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 2,
            "model_answer": "The answer is (B) False, False.\n\nThe first statement is talking about the pretraining corpus size of RoBERTa compared to BERT. It does not mention anything about the activation functions used in ResNeXts. Therefore, the first part of the answer is False.\n\nThe second statement is also talking about the activation functions used in ResNeXts in 2018. It does not mention anything about the pretraining corpus size of RoBERTa or BERT. Therefore, the second part of the answer is False.",
            "correct": false
        },
        {
            "id": 2,
            "question": "Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "(A) True, True",
            "correct": false
        },
        {
            "id": 3,
            "question": "A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?\n\n(A) 12\n(B) 24\n(C) 48\n(D) 72",
            "correct_answer": 3,
            "model_answer": "A machine learning problem involves four attributes plus a class. The total number of possible values for the class is 3, and since the number of possible values for each attribute is 3, 2, 2, and 2, respectively, then the total number of possible values for the attributes is 3 * 2 * 2 * 2 = 48. Therefore, the maximum number of possible examples is 48.\n\nThe answer is (C) 48.",
            "correct": false
        },
        {
            "id": 4,
            "question": "As of 2020, which architecture is best for classifying high-resolution images?\n\n(A) convolutional networks\n(B) graph networks\n(C) fully connected networks\n(D) RBF networks",
            "correct_answer": 0,
            "model_answer": "(A) convolutional networks",
            "correct": true
        },
        {
            "id": 5,
            "question": "Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "The answer is (A) True, True.\n\nExplanation:\n\n* Statement 1 is true because the expectation maximization algorithm is a method for finding the maximum likelihood estimate of parameters in a statistical model, and at each iteration, the log-likelihood of the data is evaluated and updated. As the algorithm progresses, the log-likelihood generally increases towards a maximum value.\n* Statement 2 is also true because Q-learning is a type of reinforcement learning algorithm that requires the learner to have a model of how its actions affect the environment. Specifically, Q-learning requires a model of the transition dynamics of the environment, which describes how the environment transitions from one state to another in response to the learner's actions. Without knowledge of this transition model, Q-learning cannot be used effectively.\n\nTherefore, both statements are true and the answer is (A) True, True.",
            "correct": false
        },
        {
            "id": 6,
            "question": "Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient?\n\n(A) O(D)\n(B) O(N)\n(C) O(ND)\n(D) O(ND^2)",
            "correct_answer": 0,
            "model_answer": "(C) O(ND)",
            "correct": false
        },
        {
            "id": 7,
            "question": "Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x. Statement 2| Decision tree is learned by minimizing information gain.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "The answer to the question is (D) False, True.\n\nStatement 1: For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x.\n\n* The statement is True.\n* If the variable x is continuous, and hence its probability distribution function p(x) must have the property that 0 \u2264 p(x) \u2264 1 for all x.\n\nStatement 2: Decision tree is learned by minimizing information gain.\n\n* The statement is False.\n* A decision tree is constructed by dividing the input data into regions of either homogeneous or changing class label and this division is done in such a way that the information gain is maximized.\n\nTherefore the answer is (D) False, True.",
            "correct": false
        },
        {
            "id": 8,
            "question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?\n\n(A) 2\n(B) 4\n(C) 8\n(D) 16",
            "correct_answer": 2,
            "model_answer": "The Bayesian network consists of directed arrows, and based on the direction, we can determine the dependence of the variables:\n\nH -> U <- P <- W implies that H, U, P, and W are dependent on each other.\n\nTo find the number of independent parameters, we need to determine the number of degrees of freedom in the system. In this case, since all variables are dependent on each other, we cannot construct a joint probability distribution by adding up the product of individual marginal probabilities. Instead, we need to use the concept of conditional independence, which is represented by the empty arrows in the diagram ( No Edge).\n\nFor example, W is conditionally independent of H given P, which means that P acts as an intermediary (or mediator) between H and W.\n\nTo determine the number of independent parameters, we need to identify the number of edges in the Bayesian network, which represent conditional dependencies among the variables. In this case, there are three edges: H -> U, U <- P, and P <- W. Each edge represents one independent parameter.\n\nTherefore, the answer is (C) 8, as each edge represents one independent parameter.\n\n(C) 8",
            "correct": true
        },
        {
            "id": 9,
            "question": "As the number of training examples goes to infinity, your model trained on that data will have:\n\n(A) Lower variance\n(B) Higher variance\n(C) Same variance\n(D) None of the above",
            "correct_answer": 0,
            "model_answer": "(D) None of the above.\n\nThe variance of a model trained on a dataset is a measure of how much the model's predictions deviate from the mean. It does not depend solely on the number of training examples, but also on the distribution of the training data, the model architecture, and other factors.\n\nIn general, the variance of a model can decrease (become more accurate) or increase (become less accurate) as the number of training examples increases. However, it is not accurate to make a general statement that the variance of a model trained on a dataset will always have a certain behavior as the number of training examples increases. Therefore, (A), (B), and (C) are incorrect.",
            "correct": false
        }
    ]
}