{
    "score": 0.0,
    "model_outputs": [
        {
            "id": 0,
            "question": "Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 3,
            "model_answer": "Explanation:\n\nStatement 1: Linear regression estimator has the smallest variance among all unbiased estimators.\n\nAns: D, False.\n\nFor linear regression problems, the variance of the estimators can be calculated easily using a few statistical formulas. And for this case, the linear regression estimator is unbiased as it can be calculated using a least square approach. However, in some cases, such as with higher-order polynomial regression, the variance of the estimator can be smaller and can be even negative. \r\n\r\nThus, the linear regression estimator has the smallest variance among all unbiased estimators.\r\n\r\nStatement 2: The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\r\n\r\nAns: D, False.\r\n\r\nThe AdaBoost algorithm assigns positive weights to the classifiers during the training phase. It can happen that the coefficients get negative values, especially in some cases when the dataset has a different scale. However, the AdaBoost algorithm is able to correct such cases by introducing an epsilon term to the weights assigned to each classifier. \r\n\r\nThus, the coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.",
            "correct": false
        },
        {
            "id": 1,
            "question": "Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 2,
            "model_answer": "Explanation:\r\n\r\nAnswer: D \r\n\r\nExplanation: \r\n\r\nStatement 1 Statement 2 Reasoning:\r\n\r\nBased on the statement, RoBERTa pretrains on a corpus that is approximate 10x larger than BERT pretrained on. Since RoBERTa is an improvement over BERT, which was not shown, I assumed that it pretrains on a larger corpus. \r\n\r\nResNeXts in 2018 usually used tanh activation functions is based on the assumption that tanh is a commonly used activation function in the earlier ResNeXts generation. Again, based on this assumption, I assumed that ResNeXts also used tanh activation functions.",
            "correct": false
        },
        {
            "id": 2,
            "question": "Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Statement 1: False, True\r\nReasoning: SVMs give a probability distribution over the possible labels only for the examples that are closest to the decision boundary and give high probability to the class with the largest margin between the hyperplanes. In the other cases, the decision boundary is highly nonlinear and may not be able to capture the probability distribution accurately.\r\n\r\nStatement 2: True, False\r\nReasoning: The support vectors do not necessarily remain constant in general as we move from a linear kernel to higher order polynomial kernels. This is because the conditions for defining support vectors are different in each case, and some examples may become support vectors in one kernel but not the other. The support vectors tend to increase with kernel complexity, but there may be some cases where they decrease, especially for non-linear kernel functions. However, in general, the support vectors do not change in a significant way as we increase the complexity of the model.",
            "correct": false
        },
        {
            "id": 3,
            "question": "A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?\n\n(A) 12\n(B) 24\n(C) 48\n(D) 72",
            "correct_answer": 3,
            "model_answer": "Explanation:\r\n\r\nTo count the maximum possible different examples, we need to multiply the number of possible values for each attribute together. In this case, there are 3, 2, 2, and 2 possible values for the attributes respectively. We can therefore multiply these numbers together to get:\r\n\r\n3 x 2 x 2 x 2 =\r\n24\r\n\r\nSo the maximum possible different examples in this case would be 24.",
            "correct": false
        },
        {
            "id": 4,
            "question": "As of 2020, which architecture is best for classifying high-resolution images?\n\n(A) convolutional networks\n(B) graph networks\n(C) fully connected networks\n(D) RBF networks",
            "correct_answer": 0,
            "model_answer": "Reasoning:\r\n\r\n1. Given the increase in the spatial resolution of digital images, there must be an increase in the number of parameters required to classify them accurately.\r\n2. Convolutional neural networks (CNNs) are well-suited for image classification tasks, as they are able to process high-dimensional inputs while using fewer parameters than fully connected networks. \r\n3. CNNs can learn spatial features that can be used to construct more complex features in subsequent layers, which helps to prevent over-fitting.\r\n4. RBF networks are also suitable for image classification tasks, but they focus on the local features of the input pixels, rather than global features that span multiple pixels.\r\n5. Graph networks are also suitable for image classification tasks, but they are applied to a graph representation of the input. This graph may include pixels as vertices and their connections as edges.\r\n6. Fully connected networks (FCNs) are not useful for image classification tasks, as they do not process spatial information and rely on multiple fully connected layers to classify the input image.\r\n\r\nConclusion:\r\n\r\nBased on the given reasons and the general approach for image classification, the best architecture for classifying high-resolution images is convolutional neural networks (CNNs).",
            "correct": false
        },
        {
            "id": 5,
            "question": "Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "1. The log-likelihood of the data will always increase because the model will keep improving its predictions based on the available data.\r\n\r\n2. One disadvantage of Q-learning is that it cannot infer the effect of actions on the environment since it does not have any prior information. This means that it cannot choose actions that are likely to lead to better outcomes.",
            "correct": false
        },
        {
            "id": 6,
            "question": "Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient?\n\n(A) O(D)\n(B) O(N)\n(C) O(ND)\n(D) O(ND^2)",
            "correct_answer": 0,
            "model_answer": "Explanation:\r\n\r\nTo compute the cost of one gradient descent update, we need to know the dimensionality of the input feature space (D), the size of the mini-batch (N), and the gradient update step size (alpha). The cost is given by:\r\n\r\n$$\r\n\\text{cost} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{d=1}^D (y_i-\\hat{y}_i)^2\r\n$$\r\n\r\nwhere $\\hat{y}_i$ is the predicted output of the model for the input $x_i$. We can compute the cost for each mini-batch of size N and add them up if we have multiple mini-batches, but to keep things simple, let's assume we have just one mini-batch.\r\n\r\nThe gradient update step size is given by:\r\n\r\n$$\r\n\\text{update} = -\\alpha \\nabla_{\\theta} \\text{cost}\r\n$$\r\n\r\nwhere $\\alpha$ is the learning rate. Since we are updating the parameters $\\theta$ using the negative gradient, the cost decays with each update. Therefore, the algorithm will eventually converge to a local minimum of the cost function, which has a diminishing gradient as we approach the bottom of the curve.\r\n\r\nTherefore, the cost of one gradient descent update is O(ND), where N is the size of the mini-batch and D is the dimensionality of the input feature space.",
            "correct": false
        },
        {
            "id": 7,
            "question": "Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x. Statement 2| Decision tree is learned by minimizing information gain.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Closed-form solution:\n\nThe variable x can take on certain values which are given by the support of the probability distribution function p(x). A decision tree is a graph-like model that partitions the input space into discrete regions. Let's assume the probability distribution function is piecewise constant and the ranges of x are given by the intervals $[a_i, b_i]$, where $a_i$ and $b_i$ are the boundaries of the i-th interval. Let $A_1, A_2, \\ldots, A_k$ be the regions with support $[a_i, b_i]$. The probability mass function $p(x)$ is then given by:\n\n$$p(x) = \\sum_{i=1}^k p(x)_{A_i} = \\sum_{i=1}^k \\int_{a_i}^{b_i} dx' f(x')$$\n\nwhere the integral $\\int_{a_i}^{b_i} dx' f(x')$ is the probability mass assigned to the region $A_i$. The decision tree can be grown by recursively partitioning the input space into regions, with the objective of minimizing the expected information gain. The expected information gain is given by:\n\n$$\\text{EIG}(S) = \\sum_{i=1}^k \\frac{|S_i|}{|S|} \\text{EIG}(S_i)$$\n\nwhere $|S|$ is the size of the dataset and $S_i$ is the subset of the training data within the i-th region. If a region $A_i$ is pure, then the information gain is zero and we have reached the base case. Otherwise, we split $A_i$ into $m$ subregions $A_{i1}, A_{i2}, \\ldots, A_{im}$ and aim to minimize:\n\n$$\\text{EIG}(S) = \\text{EIG}(S_i) + \\text{EIG}(S_j) - \\text{EIG}(S_{ij})$$\n\nwhere $S_{ij}$ is the dataset containing elements from $A_{i}$ and $A_{j}$. The optimal decision tree can be grown by selecting the feature whose splitting results in the best expected information gain, and then recursively partitioning the remaining space into regions using the selected feature. The algorithm can be summarized as follows:\n\n1. Create a leaf node containing a constant value as the decision output.\n2. Repeat until all training instances have been classified:\n   - Compute the information gain for each feature and recursively split each node based on the feature with the largest information gain.\n   - Create a leaf node for any remaining instances that cannot be classified using the selected feature.\n3. Return the decision tree.",
            "correct": false
        },
        {
            "id": 8,
            "question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?\n\n(A) 2\n(B) 4\n(C) 8\n(D) 16",
            "correct_answer": 2,
            "model_answer": "Here's the reasoning step-by-step:\r\n\r\nFirst we need to identify the variables in the network. We have H, U, P, and W. So, this gives us 4 variables.\r\n\r\nNow we need to count the number of possible states each variable can take on. We have:\r\n\r\n- H: Two states - On and Off\r\n- U: Two states - Less than or equal to 5 and greater than 5\r\n- P: Three states - Low, Medium, High\r\n- W: Three states - Weak, Medium, Strong\r\n\r\nSo, we have 2 \\* 2 \\* 3 \\* 3 = 24 possible states for each variable. Can we assume that all the variables are binary? No, it depends on the data we have.\r\n\r\nNext, we need to consider the conditional dependence between the variables. Based on the data we have, we know that the probability of W given H is 0.6. We also know that the probability of P given U is given by a [multiplicative rule](https://en.wikipedia.org/wiki/Multiplication_rule).\r\n\r\nWe can represent the given data using a [Directed Acyclic Graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) as follows:\r\n\r\n```\r\n         \u2193\r\n  H    \u2193   \u2193\r\n\u2193     \u2193   \u2193   \u2193\r\nU   P \u2194   W   \u2194    \r\n     \u2193 \u2191     \u2191\r\n       \u2193   \u2193   \u2193\r\n      Less 5,> 5 \r\n```\r\n\r\nBased on this DAG, we can calculate the joint probability distribution for any two variables. For example, we can calculate the joint probability distribution for P and U separately. The joint probability distribution is the probability of P and U together occuring together. We can use the [conditional probability table](https://en.wikipedia.org/wiki/Conditional_probability_table) for this. For P, we have:\r\n\r\n|H|P(H=0,P) P(H=1,P)|\r\n|---|---|\r\n|0|0.5 0.5|\r\n|1|0.2 0.8|\r\n\r\nWe can apply the same rule to U to get:\r\n\r\n|H|Less 5|> 5|Less 5|> 5|\r\n|---|---|---|---|\r\n|0|0.95 0.05|0.8 0.2|0.6 0.4|\r\n|1|0.9 0.1|0.7 0.3|0.6 0.4|\r\n\r\nSo, we have the following joint probability distributions:\r\n\r\n- P and U together: G(0,0): 0.392 (probability of (H=0,U=0) and (P=Low)) * 0.6 (probability of P=Low given U=0) * 0.1 (probability of H=0), G(0,1): 0.108 (probability of (H=0,U=0) and (P=Medium)) * 0.6 (probability of P=Medium given U=0) * 0.1 (probability of H=0), G(0,2): 0.055 (probability of (H=0,U=0) and (P=High)) * 0.6 (probability of P=High given U=0) * 0.1 (probability of H=0) +\r\n                      0.443 (probability of (H=0,U=1) and (P=Low)) * 0.3 (probability of P=Low given U=1) * 0.9 (probability of H=0), G(1,0): 0.08 (probability of (H=1,U=0) and (P=Low)) * 0.6 (probability of P=Low given U=0) * 0.9 (probability of H=1), G(1,1): 0.14 (probability of (H=1,U=0) and (P=Medium)) * 0.6 (probability of P=Medium given U=0) * 0.9 (probability of H=1), G(1,2): 0.46 (probability of (H=1,U=0) and (P=High)) * 0.6 (probability of P=High given U=0) * 0.9 (probability of H=1)\r\n- U and P: G(0,0): 0.6 * 0.5 * 0.1 = 0.05, G(0,1): 0.6 * 0.3 * 0.1 = 0.02, G(0,2): 0.6 * 0.2 * 0.1 = 0.01, G(1,0): 0.3 * 0.5 * 0.9 = 0.45, G(1,1): 0.3 * 0.3 * 0.9 = 0.27, G(1,2): 0.3 * 0.2 * 0.9 = 0.18\r\n\r\nNow, let's sum up these joint probability distributions. We have 4 variables (H, U, P, W) with 8 * 8 = 64 possible states each. So, we have 4 * 64 = 256 total possible states.\r\n\r\nWe have the following four joint probability distributions:\r\n\r\n1. P and U together: G(0,0): 0.05, G(0,1): 0.02, G(0,2): 0.01, G(1,0): 0.45, G(1,1): 0.27, G(1,2): 0.18\r\n2. U and P: G(0,0): 0.6 * 0.5 * 0.1 = 0.05, G(0,1): 0.6 * 0.3 * 0.1 = 0.02, G(0,2): 0.6 * 0.2 * 0.1 = 0.01, G(1,0): 0.3 * 0.5 * 0.9 = 0.45, G(1,1): 0.3 * 0.3 * 0.9 = 0.27, G(1,2): 0.3 * 0.2 * 0.9 = 0.18\r\n\r\nWe can use these four joint probability distributions to calculate the marginal probability distribution of each variable separately. For example, we can calculate the marginal probability distribution of P: 0.05 + 0.02 + 0.01 + 0.45 + 0.27 + 0.18 = 2.26.\r\n\r\nWe can calculate the marginal probability distribution of U: 1*( 0.05 + 0.02 + 0.01 + 0.45 + 0.27 + 0.18 ) + 0*( 0.6 * 0.5 * 0.1 + 0.6 * 0.3 * 0.1 + 0.6 * 0.2 * 0.1 + 0.3 * 0.5 * 0.9 + 0.3 * 0.3 * 0.9 + 0.3 * 0.2 * 0.9 ) = 1.20.\r\n\r\nWe can calculate the marginal probability distribution of H: 1*( 0.95 + 0.1 + 0.9 + 0.8 + 0.7 + 0.6 ) + 0*( 0.05 + 0.8 + 0.6 + 0.95 + 0.7 + 0.6 ) = 3.8.\r\n\r\nWe can calculate the marginal probability distribution of W: 1*( 0.05 + 0.45 + 0.18 + ( 0.6 * 0.5 + 0.6 * 0.3 + 0.6 * 0.2 + 0.3 * 0.5 + 0.3 * 0.3 + 0.3 * 0.2 ) ) + 0*( 0.05 + 0.45 + 0.18 + ( 0.6 * 0.5 + 0.6 * 0.3 + 0.6 * 0.2 + 0.3 * 0.5 + 0.3 * 0.3 + 0.3 * 0.2 ) ) = 3.16.\r\n\r\nSo, we have the following marginal probability distributions:\r\n\r\n- P: 2.26\r\n- U: 1.20\r\n- H: 3.8\r\n- W: 3.16",
            "correct": false
        },
        {
            "id": 9,
            "question": "As the number of training examples goes to infinity, your model trained on that data will have:\n\n(A) Lower variance\n(B) Higher variance\n(C) Same variance\n(D) None of the above",
            "correct_answer": 0,
            "model_answer": "Your reasoning for question 2 is correct. Based on this question, the higher the number of training examples, the lower the model's variance is likely to be. This is because having a larger number of training examples provides a wider range of data to learn from, which helps the model generalize better and reduce overfitting. The model's training error will be less likely to be overly optimistic, as there is more data to base its predictions off of.",
            "correct": false
        }
    ]
}