{
    "score": 0.1,
    "model_outputs": [
        {
            "id": 0,
            "question": "Which one of the following is the most appropriate definition of a 99% confidence interval?\n\n(A) 99% of the time in repeated samples, the interval would contain the true value of the parameter\n(B) 99% of the time in repeated samples, the interval would contain the estimated value of the parameter\n(C) 99% of the time in repeated samples, the null hypothesis will be rejected\n(D) 99% of the time in repeated samples, the null hypothesis will not be rejected when it was false",
            "correct_answer": 0,
            "model_answer": "A - Correct answer. Essentially, this is the definition of a 99% confidence interval, which states that in repeated samples of indipendent measurements, 99% of the time the interval will contain the true value of the parameter.",
            "correct": false
        },
        {
            "id": 1,
            "question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?\n\n(A) ADF is a single equation approach to unit root testing while PP is a systems approach\n(B) PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test\n(C) The PP test incorporates an automatic correction for autocorrelated residuals in the test regression\n(D) PP tests have good power in small samples whereas DF tests do not.",
            "correct_answer": 2,
            "model_answer": "Reasoning:The main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing is that they both involve testing for stationarity and sign of the test statistic. \n\nDickey Fuller (DF) is a single equation approach to unit root testing that tests the null and alternative hypotheses:\n\nH0: There is a unit root in the series\nHA: There is no unit root in the series\n\nUsing the DF approach, a unit root test needs to be applied to a stationary series, i.e., a series that is stationary around a trend. However, for many real-world time-series problems, it may not be possible to identify or model the trend component and hence the DF approach may not be useful. In such cases, PP is a better option as it automatically reverses the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test. \n\nPhillips-Perron (PP) is a systems approach that tests the null hypothesis based on the stationarity of the entire system. The test involves reversing the DF null and alternative hypotheses:\n\nH0: In a system with a unit root, the series exhibits a stationary process\nHA: In a system with a unit root, the series is non-stationary\n\nUnlike the DF approach, PP tests only the null hypothesis of stationarity of the entire system and does not assume stability under the null. As a result, PP tests have good power in small samples as they can test multiple lags of the test hypothesis, whereas DF tests do not. PP tests incorporate an automatic correction for autocorrelated residuals in the test regression. \n\nConclusion: (B) PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test. This is because the PP null hypothesis tests the stabilization of the entire system rather than the partial correlation of individual lags. This leads to a better test of the unit root hypothesis in smaller samples. PP tests also incorporate an automatic correction for autocorrelated residuals in the test regression. (B) is the correct answer.",
            "correct": false
        },
        {
            "id": 2,
            "question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?\n\n(A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude\n(B) It would be symmetrical about zero\n(C) It would rise less quickly for negative disturbances than for positive ones of the same magnitude\n(D) It would be zero for all positive disturbances",
            "correct_answer": 0,
            "model_answer": "Explanation:\r\n\r\nIf there were a leverage effect in practice, the news impact curve shape would look different depending on the direction of the disturbance.\r\n\r\nIf the disturbance was negative and loud, say a decrease in the stock price by 5%, it would cause a high news impact, which means the news coverage would reach a peak and then decrease slowly for a while afterwards. This would look like a leveraged news curve.\r\n\r\nOn the other hand, if the disturbance was positive and subtle, say a slight increase in the stock price by 1%, the news impact would be low and the curve would be flat. This could also be considered a leveraged news curve since it is consistent with the idea of a pure news release.\r\n\r\nOverall, the news impact curve shape depends on the type of disturbance.",
            "correct": false
        },
        {
            "id": 3,
            "question": "Which of the following statements is false concerning the linear probability model?\n\n(A) There is nothing in the model to ensure that the estimated probabilities lie between zero and one\n(B) Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one\n(C) The error terms will be heteroscedastic and not normally distributed\n(D) The model is much harder to estimate than a standard regression model with a continuous dependent variable",
            "correct_answer": 3,
            "model_answer": "At first glance, statement (C) seems to be the answer as it appears in the question. However, we need to consider the context before making a final decision. The wording of the question could be improved and the explanation of the answer might be more clear. For example:\n\nStatement (C) is false because linear probability models assume that the error terms are normally distributed. However, it is possible for the error terms to be heteroscedastic in some cases, where the variance of the error is different among the different cases. This means that the error terms may not follow a normal distribution, but still be approximately normally distributed. In such cases, the linear probabilistic model may still work well and give meaningful estimates of the probabilities.",
            "correct": false
        },
        {
            "id": 4,
            "question": "Which of the following statements concerning the regression population and sample is FALSE?\n\n(A) The population is the total collection of all items of interest\n(B) The population can be infinite\n(C) In theory, the sample could be larger than the population\n(D) A random sample is one where each individual item from the population is equally likely to be drawn.",
            "correct_answer": 2,
            "model_answer": "False. The population can be infinite and a random sample is one where each individual item from the population is equally likely to be drawn, which means that all items will be equally likely to be in the sample. Therefore, the sample can have any number of items in it, as long as it is a random sample.",
            "correct": false
        },
        {
            "id": 5,
            "question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?\n\n(A) Asymptotically, the values of the two test statistics will be equal\n(B) The Q test has better small-sample properties than the Q*\n(C) The Q test is sometimes over-sized for small samples\n(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients.",
            "correct_answer": 1,
            "model_answer": "Reasoning:\r\n\r\n1. Asymptotically, the values of the two test statistics will be equal: False, because the asymptotic properties of the two test statistics are different.\r\n\r\n2. The Q test has better small-sample properties than the Q*: True, because of the central limit theorem and the fact that the Q test can be used for comparison of univariate processes as well as for multiple linear regression. Using the Q test instead of a standardized version of Ljung-Box test makes it more powerful in detecting temporal trends even when there are non-stationary disturbances and heteroscedasticity in the data.\r\n\r\n3. The Q test is sometimes over-sized for small samples: True, because the test hinges on the assumption that the autocorrelation of the time series is zero or a MA(1) process. In practice, the test may be over-sized especially when the sample size is low, and requires multiple testing to correct for the high number of rejections when the process is actually of zero autocorrelation.\r\n\r\n4. As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients: False, because the null hypothesis assumes that the process has true zero autocorrelation coefficients, whereas the tests only establish that the autocorrelation coefficients may not be zero.",
            "correct": false
        },
        {
            "id": 6,
            "question": "A parsimonious model is one that\n\n(A) Includes too many variables\n(B) Includes as few variables as possible to explain the data\n(C) Is a well-specified model\n(D) Is a mis-specified model",
            "correct_answer": 0,
            "model_answer": "Model (A) is incorrect because it is too aggressive, including unimportant variables. The goal of a parsimonious model is to include only the relevant variables to create a simple model that can explain all the data.\n\nModel (B) is also incorrect because it does not always include all the relevant variables. A parsimonious model may include only a few variables to explain the data, but it should also balance the importance of each variable.\n\nModel (C) is also incorrect because a well-specified model does not always work well in practice. The model may be too complex or unnecessarily detailed, which can lead to a lack of interpretability and unreliability of the model.\n\nModel (D) is correct because a mis-specified model can produce unrealistic or incorrect results. It can result in a model that does not accurately reflect the actual relationships or behaviors of the underlying process. This can be problematic if the model is supposed to be used in scientific or industrial settings.\n\nTherefore, the answers are (C) and (D). The final letter is (D) because the model is mis-specified.",
            "correct": false
        },
        {
            "id": 7,
            "question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?\n\n(A) They can be interpreted as continuously compounded changes in the prices\n(B) They can be added over time to give returns for longer time periods\n(C) They can be added across a portfolio of assets to give portfolio returns\n(D) They are usually fat-tailed",
            "correct_answer": 2,
            "model_answer": "Explanation Step 1: Continuously compounded returns log(1+r)\r\n\r\nLog-returns are the natural logarithm of continuously compounded returns, i.e. log(1+r) where r is the return. Continuously compounded returns are the product of the compounding factor (e.g. daily, monthly, annually) and the growth over time.\r\n\r\nExplanation Step 2: Product of compounding factor and growth over time\r\n\r\nFor example, if a stock's price grows by 10% in one day then the compounding factor for the day is 1 (daily) and the growth over time is 10% (10% of initial investment). The continuously compounded return for this day is therefore 10% of the initial investment, which is equivalent to log(1+10%) or log(1.1).\r\n\r\nIf the price continues to grow then the compounding factor increases (e.g. to 1/2 and 1/4 if the time interval is twice and four times a year respectively) and the compounding growth continues. Thus, the total growth over the entire time period is the sum of the individual growths multiplied by the appropriate compounding factor.\r\n\r\nExplanation Step 3: They can be added over time to give returns for longer time periods\r\n\r\nIf the growth over time is constant, then the total growth is proportional to the compounding growth (e.g. 1/2 for daily to monthly and 1/4 for daily to quarterly), and the log-return over time between consecutive time periods is simply the difference in the log-returns over those time periods:\r\n\r\nlog(1+20%) - log(1+10%) = 1/2 per time period\r\n\r\nExplanation Step 4: They can be interpreted as continuously compounded changes in the prices\r\n\r\nAn annual return of 10% can be interpreted as an increase in the initial investment of the stock by 10% per annum. This means that for every dollar of the initial investment invested in the stock, you would have gained another dollar of the current value on the stock, at the end of the year. Therefore, the change in value invested annually is the product of the initial investment and the annual growth:\r\n\r\ndPV = P x A = 100 x (1+10%) = 110$\r\n\r\nLikewise, a daily change of 10% can be interpreted as an increase in the initial investment of the stock by 10% per day, i.e. after one day, you would have invested another 10% of the current value on the stock. Therefore, the change in value invested daily is the product of the initial investment and the daily growth:\r\n\r\ndPV = P x r/365 = 100 x (1+10%/365) = 109.4887$\r\n\r\nExplanation Step 5: They can be added across a portfolio of assets to give portfolio returns\r\n\r\nA portfolio can be thought of as consisting of several different investments, each with its own initial investment and growth rate. The portfolio log-return is then the sum of the log-returns for each component of the portfolio.\r\n\r\nMidterm exam question:\r\n\r\nThe stock market is open for trading. Suppose the daily growth of the stock (measured as a percentage) is approximately normally distributed with a mean of 10% and a standard deviation of 1%. Also, suppose the distribution is fat-tailed, with a tails curve centered at the mean (i.e. a peak greater than the mean but less than or equal to twice the mean).\r\n\r\nQuestion 1: What is the probability that the log-return on the stock will be greater than 2%?\r\n\r\nSolution:\r\n\r\nThe log-return on the stock is normally distributed with a mean of 0 and a standard deviation of 1. Therefore, to find the probability that the log-return is greater than 2%, we need to find the z-score of 2% on the normal distribution (which we can do using a calculator or a standard table). The z-score of 2% on a normal distribution with mean 0 and standard deviation 1 is equal to the number of standard deviations from the mean that 2% is (which is also equal to 1.96).\r\n\r\nThe probability that the log-return is less than or equal to 2% is equal to the area under the standard normal curve (i.e. 1/2) to the left of the value of 1.96 (i.e. 1-0.975=0.025",
            "correct": false
        },
        {
            "id": 8,
            "question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns\n\n(A) (ii) and (iv) only\n(B) (i) and (iii) only\n(C) (i), (ii), and (iii) only\n(D) (i), (ii), (iii), and (iv)",
            "correct_answer": 1,
            "model_answer": "Reasoning:\r\n\r\nGARCH(1,1) model is a commonly used model for analyzing financial return time-series. It assumes that the returns are a combination of two stochastic processes:\r\n- Normal random walk: the return at time t-h and t-2h effects the return at time t\r\n- GARCH process: the return at time t-h and t-2h influences the variance at time t\r\n\r\nAccording to the GARCH model, the mean return at a given time is a linear combination of returns from the previous period, weighted by a constant called alpha. The variance is then modeled as a function of the variance at the previous period, and the return variance can be modeled as a function of the past two variances.\r\n\r\nIn a GARCH(1,1) model:\r\n- Alpha is equal to 0.5, because the normal random walk component is effectively assumed to be half that of the variance of the constant variance component of the GARCH process (which we are not considering in our analysis).\r\n- V(h) is the variance at time t-h, V(2h) is the variance at time t-2h.\r\n- The variance at time t is then determined by: V(t) = c + \u03b3(V(t-h)^2) + \u03b5(t), where c is a constant, \u03b3 is the GARCH coefficient, and \u03b5(t) is the error term.\r\n\r\nThe key features that can be captured by the GARCH model include:\r\n- Fat tails in the return distribution: because the variance is assumed to be a function of the variance at the previous period, it can capture fat tails in the return distribution.\r\n- Leverage effects: because the model takes into account the variance at the previous period and the variance at the previous two periods, it can capture leverage effects.\r\n- Volatility clustering: because the GARCH process can capture volatility clustering, it can capture the volatility clustering in the returns.\r\n- Volatility affecting returns: because the variance is a function of the variance at the previous period, it can capture the influence of volatility on returns.\r\n\r\nFrom the above analysis, it can be seen that the key features we are looking for are (ii) and (iv).\r\n\r\nAnswer: (B)/(i) and (iii) only",
            "correct": true
        },
        {
            "id": 9,
            "question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?\n\n(A) Less than -1\n(B) Between -1 and 0\n(C) Between 0 and 1\n(D) Bigger than 1",
            "correct_answer": 2,
            "model_answer": "Reasoning:\r\n- GARCH-M is a hybrid model which combines the GARCH component and the ARMA component. \r\n- GARCH-M is a model that estimates the mean reversion and volatility parameters of a GARCH(1,1) model. \r\n- In the presence of an ARMA component, GARCH-M becomes a GARCH model. \r\n- GARCH(1,1) is a simple model which assumes the volatility is constant over time. \r\n- The mean reversion parameter is the fraction of variance that is attributable to a shock in the mean process. \r\n- A value of zero or negative denotes an unstable mean process, while a value of one or positive denotes a stable mean process. \r\n- In presence of an ARMA component, GARCH-M can be used to infer long-run variance dynamics of the time-series. \r\n- The ARMA component models the covariance structure between the innovations of the time-series. \r\n- Given that the data employed are daily corporate bond percentage returns, the mean reversion rate of variation in the time-series may be high in such a case. Therefore, the answer to the question is not clearly clear. \r\n\r\nTherefore, we cannot make a definitive answer to the question. The range of expected values for the GARCH-in-mean parameter estimate is not relevant for a small sample size. However, we can suggest assuming a value of -1 as this is found to be a commonly used default choice for this parameter.",
            "correct": false
        }
    ]
}