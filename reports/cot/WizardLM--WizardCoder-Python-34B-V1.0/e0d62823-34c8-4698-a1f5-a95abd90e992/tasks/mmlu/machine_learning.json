{
    "score": 0.0,
    "model_outputs": [
        {
            "id": 17,
            "question": "For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model):\n\n(A) The number of hidden nodes\n(B) The learning rate\n(C) The initial choice of weights\n(D) The use of a constant-term unit input",
            "correct_answer": 0,
            "model_answer": "The structural assumption that most affects the trade-off between underfitting and overfitting in a neural network is (C) The initial choice of weights. \r\n\r\nExplanation:\r\n\r\nUnderfitting occurs when the model is too simple and cannot capture the complexity of the data. This is typically caused by a high bias in the model, which means that the model is too constrained and cannot adapt to the training data. Overfitting occurs when the model is too complex and starts to memorize the noise in the training data instead of capturing the underlying pattern. This is typically caused by a high variance in the model, which means that the model is too flexible and can fit the training data too closely.\r\n\r\nThe initial choice of weights in a neural network can have a significant impact on the model's ability to generalize to new data. If the weights are initialized too high or too low, the model may not be able to converge to a good solution. If the weights are initialized randomly, the model may converge to a local minimum and not be able to find the global minimum. Therefore, it is important to choose an appropriate initialization method that can help the model converge to a good solution.\r\n\r\nIn contrast, the number of hidden nodes and the learning rate can also affect the model's ability to generalize, but they are not as significant as the initial choice of weights. The number of hidden nodes determines the complexity of the model, while the learning rate determines the step size during gradient descent. However, the choice of the learning rate and the number of hidden nodes can be adjusted after the model has been trained, while the initial choice of weights cannot be changed without retraining the model.\r\n\r\nFinally, the use of a constant-term unit input can also affect the model's ability to generalize, but it is not as significant as the other three assumptions. The constant-term unit input is a bias unit that adds a constant value to the input, which can help the model fit the data better. However, it is not always necessary to include a constant-term unit input, and it can be added or removed during the training process.",
            "correct": false
        },
        {
            "id": 72,
            "question": "Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Explanation:\r\n\r\nStatement 1: Industrial-scale neural networks are normally trained on CPUs, not GPUs.\r\n\r\nThis statement is true. Industrial-scale neural networks are trained on CPUs because GPUs have limited memory and cannot handle the large amount of data required for training these networks. However, GPUs are used for inference, which is the process of using a trained neural network to make predictions on new data.\r\n\r\nStatement 2: The ResNet-50 model has over 1 billion parameters.\r\n\r\nThis statement is also true. The ResNet-50 model is a convolutional neural network (CNN) architecture that has over 1 billion parameters. It is one of the most popular CNN architectures used for image classification tasks.\r\n\r\nTherefore, the correct answer is (A) True, True.",
            "correct": false
        },
        {
            "id": 108,
            "question": "Which of the following is false?\n\n(A) Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image.\n(B) A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive.\n(C) When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive.\n(D) A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative.",
            "correct_answer": 3,
            "model_answer": "First, let's clarify some terminology:\n\n- Semantic segmentation: A task where the goal is to classify each pixel in an image into one of the predefined classes.\n- Multiclass image classification: A task where the goal is to classify an entire image into one of the predefined classes.\n- Bounding box: A rectangle drawn around an object in an image, defining its location and size.\n- IoU (intersection over union): A metric used to measure the overlap between two bounding boxes. It is calculated as the area of their intersection divided by the area of their union.\n\nNow, let's answer the question step-by-step:\n\n1. Which of the following is false?\n   - (A) Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image.\n   - (B) A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive.\n   - (C) When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive.\n   - (D) A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative.\n\n2. Let's eliminate option (A) as it is a statement of fact, not a question.\n\n3. Options (B) and (C) are statements of fact, not questions.\n\n4. Option (D) is a question, but it is not the question we are looking for.\n\n5. The question we are looking for is: \"Which of the following is false?\"\n\n6. Therefore, the answer is (A).",
            "correct": false
        },
        {
            "id": 102,
            "question": "You are training a linear regression model for a simple estimation task, and notice that the model is overfitting to the data. You decide to add in $\\ell_2$ regularization to penalize the weights. As you increase the $\\ell_2$ regularization coefficient, what will happen to the bias and variance of the model?\n\n(A) Bias increase ; Variance increase\n(B) Bias increase ; Variance decrease\n(C) Bias decrease ; Variance increase\n(D) Bias decrease ; Variance decrease",
            "correct_answer": 1,
            "model_answer": "As you increase the $\\ell_2$ regularization coefficient, the bias and variance of the model will both decrease. Here's why:\n\n1. Bias: The bias of a model is the error caused by its simplifying assumptions. Increasing the $\\ell_2$ regularization coefficient adds a penalty term to the loss function, which discourages the model from overfitting to the training data. This means that the model will be less likely to make strong assumptions about the data, which can lead to a reduction in bias.\n\n2. Variance: The variance of a model is the measure of how much the model's predictions vary across different samples from the training data. Increasing the $\\ell_2$ regularization coefficient also adds a penalty term to the loss function, which discourages the model from assigning too much weight to any single feature. This means that the model will be less likely to overfit to any one feature, which can lead to a reduction in variance.\n\nTherefore, the correct answer is (C) Bias decrease ; Variance decrease.",
            "correct": false
        },
        {
            "id": 97,
            "question": "Which one of the following is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them?\n\n(A) P(A | B) * P(B | C) * P(C | A)\n(B) P(C | A, B) * P(A) * P(B)\n(C) P(A, B | C) * P(C)\n(D) P(A | B, C) * P(B | A, C) * P(C | A, B)",
            "correct_answer": 2,
            "model_answer": "To find P(A, B, C), we can use the law of total probability:\n\nP(A, B, C) = P(A, B, C | A)P(A) + P(A, B, C | A')P(A')\n\nNow, we can use the law of total probability again to find P(A, B, C | A) and P(A, B, C | A'):\n\nP(A, B, C | A) = P(B, C | A)P(B | A) + P(B', C | A)P(B' | A)\nP(A, B, C | A') = P(B, C | A')P(B | A') + P(B', C | A')P(B' | A')\n\nNow, we can substitute these expressions into the first equation:\n\nP(A, B, C) = P(B, C | A)P(B | A)P(A) + P(B', C | A)P(B' | A)P(A) + P(B, C | A')P(B | A')P(A') + P(B', C | A')P(B' | A')P(A')\n\nNow, we can use the law of total probability again to find P(B, C | A) and P(B', C | A):\n\nP(B, C | A) = P(B, C, A) + P(B, C, A')\nP(B', C | A) = P(B', C, A) + P(B', C, A')\n\nNow, we can substitute these expressions into the second equation:\n\nP(A, B, C) = (P(B, C, A) + P(B, C, A'))P(B | A)P(A) + (P(B', C, A) + P(B', C, A'))P(B' | A)P(A) + (P(B, C, A') + P(B, C, A'))P(B | A')P(A') + (P(B', C, A') + P(B', C, A'))P(B' | A')P(A')\n\nNow, we can use the law of total probability again to find P(B | A) and P(B' | A):\n\nP(B | A) = P(B, A) / P(A)\nP(B' | A) = P(B', A) / P(A)\n\nNow, we can substitute these expressions into the third equation:\n\nP(A, B, C) = (P(B, C, A) + P(B, C, A'))(P(B, A) / P(A))P(A) + (P(B', C, A) + P(B', C, A'))(P(B', A) / P(A))P(A) + (P(B, C, A') + P(B, C, A'))(P(B, A') / P(A'))P(A') + (P(B', C, A') + P(B', C, A'))(P(B', A') / P(A'))P(A')\n\nNow, we can use the law of total probability again to find P(B, C, A) and P(B, C, A'):\n\nP(B, C, A) = P(B, C, A | A)P(A) + P(B, C, A | A')P(A')\nP(B', C, A) = P(B', C, A | A)P(A) + P(B', C, A | A')P(A')\n\nNow, we can substitute these expressions into the fourth equation:\n\nP(A, B, C) = (P(B, C, A | A)P(A) + P(B, C, A | A')P(A'))(P(B, A) / P(A))P(A) + (P(B', C, A | A)P(A) + P(B', C, A | A')P(A'))(P(B', A) / P(A))P(A) + (P(B, C, A' | A')P(A') + P(B, C, A' | A)P(A))(P(B, A') / P(A'))P(A') + (P(B', C, A' | A')P(A') + P(B', C, A' | A)P(A))(P(B', A') / P(A'))P(A')\n\nNow, we can use the law of total probability again to find P(B, C, A | A) and P(B, C, A | A'):\n\nP(B, C, A | A) = P(B, C, A, A) / P(A)\nP(B, C, A | A') = P(B, C, A, A') / P(A')\n\nNow, we can substitute these expressions into the fifth equation:\n\nP(A, B, C) = (P(B, C, A, A) / P(A)P(A) + P(B, C, A, A') / P(A')P(A'))(P(B, A) / P(A))P(A) + (P(B', C, A, A) / P(A)P(A) + P(B', C, A, A') / P(A')P(A'))(P(B', A) / P(A))P(A) + (P(B, C, A', A') / P(A')P(A') + P(B, C, A', A) / P(A)P(A))(P(B, A') / P(A'))P(A') + (P(B', C, A', A') / P(A')P(A') + P(B', C, A', A) / P(A)P(A))(P(B', A') / P(A'))P(A')\n\nNow, we can use the law of total probability again to find P(B, C, A, A) and P(B, C, A, A'):\n\nP(B, C, A, A) = P(B, C, A, A | A)P(A) + P(B, C, A, A | A')P(A')\nP(B, C, A, A') = P(B, C, A, A' | A)P(A) + P(B, C, A, A' | A')P(A')\n\nNow, we can substitute these expressions into the sixth equation:\n\nP(A, B, C) = (P(B, C, A, A | A)P(A) + P(B, C, A, A | A')P(A'))(P(B, A) / P(A))P(A) + (P(B', C, A, A | A)P(A) + P(B', C, A, A | A')P(A'))(P(B', A) / P(A))P(A) + (P(B, C, A', A' | A')P(A') + P(B, C, A', A | A)P(A))(P(B, A') / P(A'))P(A') + (P(B', C, A', A' | A')P(A') + P(B', C, A', A | A')P(A))(P(B', A') / P(A'))P(A')\n\nNow, we can use the law of total probability again to find P(B, C, A, A | A) and P(B, C, A, A | A'):\n\nP(B, C, A, A | A) = P(B, C, A, A, A) / P(A)\nP(B, C, A, A | A') = P(B, C, A, A, A') / P(A')\n\nNow, we can substitute these expressions into the seventh equation:\n\nP(A, B, C) = (P(B, C, A, A, A) / P(A)P(A) + P(B, C, A, A, A') / P(A'))(P(B, A) / P(A))P(A) + (P(B', C, A, A, A) / P(A)P(A) + P(B', C, A, A, A') / P(A'))(P(B', A) / P(A))P(A) + (P(B, C, A', A', A') / P(A')P(A') + P(B, C, A', A",
            "correct": false
        },
        {
            "id": 8,
            "question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?\n\n(A) 2\n(B) 4\n(C) 8\n(D) 16",
            "correct_answer": 2,
            "model_answer": "To determine the number of independent parameters needed for this Bayesian Network, we need to count the number of edges that have a directional arrow pointing to the child node. In this case, there are 3 nodes (H, U, P) and 3 edges (H -> U, U <- P, P <- W). Therefore, there are 3 independent parameters needed for this Bayesian Network.",
            "correct": false
        },
        {
            "id": 32,
            "question": "Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can\u2019t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Explanation:\r\n\r\nStatement 1: True\r\n\r\nReasoning: Mapping into a higher-dimensional feature space Q using a radial basis kernel function can often improve the performance of a 1-NN classifier by reducing the amount of noise in the data and capturing non-linear relationships. This is because the kernel function can generate a set of features that are more complex than a linearly separable hyperplane, allowing the classifier to make more accurate predictions.\r\n\r\nStatement 2: False\r\n\r\nReasoning: The VC dimension of a Perceptron is 2, which means it can shatter any set of 2 points in the input space. However, a simple linear SVM can only shatter a set of points that are co-linear, which means its VC dimension is at most 2. Therefore, the VC dimension of a Perceptron is greater than that of a simple linear SVM.\r\n\r\nTherefore, the correct answer is (C).",
            "correct": false
        },
        {
            "id": 15,
            "question": "Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True\n\nExplanation:\n\nStatement 1 is true because Layer Normalization was introduced in the original ResNet paper, while Batch Normalization was introduced in a separate paper.\n\nStatement 2 is false because DCGANs do not use self-attention to stabilize training. Instead, they use a technique called spectral normalization to stabilize training.",
            "correct": false
        },
        {
            "id": 63,
            "question": "What are support vectors?\n\n(A) The examples farthest from the decision boundary.\n(B) The only examples necessary to compute f(x) in an SVM.\n(C) The data centroid.\n(D) All the examples that have a non-zero weight \u03b1k in a SVM.",
            "correct_answer": 1,
            "model_answer": "Support vectors are the examples that are closest to the decision boundary in a Support Vector Machine (SVM). They are the most important examples in determining the shape of the decision boundary. The SVM algorithm tries to find the hyperplane that maximizes the margin around the data points, which is the distance between the hyperplane and the nearest data point. The support vectors are the data points that are closest to the hyperplane and furthest from the decision boundary. They are the ones that determine the shape of the decision boundary and are the most important in the SVM algorithm. Therefore, the correct answer is (A).",
            "correct": false
        },
        {
            "id": 57,
            "question": "Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 3,
            "model_answer": "Explanation:\r\n\r\nStatement 1: L2 regularization of linear models tends to make models more sparse than L1 regularization.\r\n\r\nL1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the model parameters. This means that it tends to force the model parameters to be smaller, which can result in a more sparse model. L2 regularization, on the other hand, adds a penalty term that is proportional to the squared values of the model parameters. This means that it tends to force the model parameters to be smaller in magnitude, but not necessarily zero. Therefore, L2 regularization tends to make models more smooth and less sparse than L1 regularization.\r\n\r\nStatement 2: Residual connections can be found in ResNets and Transformers.\r\n\r\nResidual connections are a technique used in deep neural networks to help them learn more easily. They allow the output of a layer to be added to the input of the next layer, effectively bypassing the activation function of the next layer. This technique is used in ResNets and Transformers to help them learn more effectively and avoid the vanishing gradient problem that can occur in deep networks.\r\n\r\nBased on the above explanation, the correct answer is:\r\n\r\n(C) True, False",
            "correct": false
        }
    ]
}