{
    "score": 0.2,
    "model_outputs": [
        {
            "id": 0,
            "question": "Which one of the following is the most appropriate definition of a 99% confidence interval?\n\n(A) 99% of the time in repeated samples, the interval would contain the true value of the parameter\n(B) 99% of the time in repeated samples, the interval would contain the estimated value of the parameter\n(C) 99% of the time in repeated samples, the null hypothesis will be rejected\n(D) 99% of the time in repeated samples, the null hypothesis will not be rejected when it was false",
            "correct_answer": 0,
            "model_answer": "The most appropriate definition of a 99% confidence interval is (A) 99% of the time in repeated samples, the interval would contain the true value of the parameter.\n\nExplanation:\nA confidence interval is a range of values within which we can be confident (i.e., have a high level of belief) that the true value of the parameter lies. \n\nA 99% confidence interval means that in 99% of the cases, if we were to conduct a large number of experiments and measure the parameter each time, the interval would contain the true value of the parameter. \n\nThis definition (A) satisfies this requirement. The other definitions are not as appropriate as they do not speak to the accuracy of the interval in containing the true value of the parameter.",
            "correct": true
        },
        {
            "id": 1,
            "question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?\n\n(A) ADF is a single equation approach to unit root testing while PP is a systems approach\n(B) PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test\n(C) The PP test incorporates an automatic correction for autocorrelated residuals in the test regression\n(D) PP tests have good power in small samples whereas DF tests do not.",
            "correct_answer": 2,
            "model_answer": "The main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing is (B) - PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test. While both tests aim to determine if a time series is stationary, DF tests assume that a given series is non-stationary and attempts to reject this assumption while PP tests assume series is stationary and aims to reject this assumption.",
            "correct": false
        },
        {
            "id": 2,
            "question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?\n\n(A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude\n(B) It would be symmetrical about zero\n(C) It would rise less quickly for negative disturbances than for positive ones of the same magnitude\n(D) It would be zero for all positive disturbances",
            "correct_answer": 0,
            "model_answer": "The answer is (A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude.\n\nExplanation: \nIf there were a leverage effect in practice, it means that the impact of a disturbance would be more significant for one variable than for another, given their joint dependence. This is typically true for variables that are positively correlated. \n\nFor a model that accounts for this leverage, the news impact curve would rise more quickly for negative disturbances, which are less likely to occur but have a more significant impact on the joint outcome. Conversely, the curve would rise less quickly, or not at all, for positive disturbances, which are more likely to occur and have a less significant impact on the joint outcome. \n\nTherefore, option (A) is the correct answer.",
            "correct": true
        },
        {
            "id": 3,
            "question": "Which of the following statements is false concerning the linear probability model?\n\n(A) There is nothing in the model to ensure that the estimated probabilities lie between zero and one\n(B) Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one\n(C) The error terms will be heteroscedastic and not normally distributed\n(D) The model is much harder to estimate than a standard regression model with a continuous dependent variable",
            "correct_answer": 3,
            "model_answer": "To determine which statement is false, we need to look at each statement individually and assess whether they accurately describe the linear probability model.\n\nStatement (A): There is nothing in the model to ensure that the estimated probabilities lie between zero and one. This statement is not false. In fact, it is true that the linear probability model does not inherently ensure that the estimated probabilities will lie between zero and one. This is because the linear probability model estimates probabilities using a linear regression model, which can produce values outside of the zero-to-one range.\n\nStatement (B): Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one. This statement is not false either. It is true that, in practice, a significant proportion of observations may have either a zero or a one probability, even if the model has not been fitted to output probabilities outside of this range.\n\nStatement (C): The error terms will be heteroscedastic and not normally distributed. This statement is false. The linear probability model assumes that the errors are normally distributed, which means that they follow a Gaussian distribution. This is an important assumption because it allows for the use of maximum likelihood estimation to estimate the coefficients of the model.\n\nStatement (D): The model is much harder to estimate than a standard regression model with a continuous dependent variable. This statement is not false either. It is true that the linear probability model, which involves estimating probabilities, can be more challenging to estimate than a standard regression model with a continuous dependent variable. This is because the estimation of probabilities can be sensitive to outliers and other issues that are not as prevalent in regression modelling.\n\nBased on the assessment of each statement, the false statement is (C). Therefore, the answer is (C).",
            "correct": false
        },
        {
            "id": 4,
            "question": "Which of the following statements concerning the regression population and sample is FALSE?\n\n(A) The population is the total collection of all items of interest\n(B) The population can be infinite\n(C) In theory, the sample could be larger than the population\n(D) A random sample is one where each individual item from the population is equally likely to be drawn.",
            "correct_answer": 2,
            "model_answer": "B\n(A) The population is the total collection of all items of interest - This statement is true since a population refers to the entire group, species or collection of individuals or objects of interest to a research study.\n\n(B) The population can be infinite - This statement is true since a population can have an unlimited number of individuals.\n\n(C) In theory, the sample could be larger than the population - This statement is true since there is no theoretical limitation to the size of a sample compared to the population.\n\n(D) A random sample is one where each individual item from the population is equally likely to be drawn. - This statement is true since a random sample is a subset of the population in which every member has an equal chance of being selected.",
            "correct": false
        },
        {
            "id": 5,
            "question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?\n\n(A) Asymptotically, the values of the two test statistics will be equal\n(B) The Q test has better small-sample properties than the Q*\n(C) The Q test is sometimes over-sized for small samples\n(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients.",
            "correct_answer": 1,
            "model_answer": "To answer this question, we need to understand the differences between the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series. \n\nThe Box-Pierce Q test is used to test the null hypothesis of no linear trend in a time series. On the other hand, the Ljung-Box Q* test is used to test the null hypothesis of no autocorrelation in a time series. \n\nLet's now analyze each statement and see which one is incorrect:\n\n(A) Asymptotically, the values of the two test statistics will be equal: This statement is incorrect. The Q test statistic is based on the residuals of a linear regression model, while the Q* test statistic is based on the autocorrelation of the errors in a regression model. Therefore, their values will not be equal in general.\n\n(B) The Q test has better small-sample properties than the Q*: This statement is correct. The Q test is preferred over the Q* test when the sample size is small. This is because the Q test uses the standard errors of the coefficients in a regression model, while the\nQ* test uses the standard errors of the errors in a regression model. As the sample size increases, the two test methods converge, and the Q* test performs better.\n\n(C) The Q test is sometimes over-sized for small samples: This statement is incorrect. It is a restatement of statement (B), which we have already determined is correct.\n\n(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients: This statement is incorrect. It is true that as the sample size increases, the Q and Q* tests will have more power to detect linear trends and autocorrelation. However, they will not always reject the null hypothesis of zero autocorrelation coefficients. The tests will only reject the null hypothesis when the sample size is large enough to provide sufficient evidence of linear trends or autocorrelation.\n\nIn conclusion, the incorrect statement is (D).",
            "correct": false
        },
        {
            "id": 6,
            "question": "A parsimonious model is one that\n\n(A) Includes too many variables\n(B) Includes as few variables as possible to explain the data\n(C) Is a well-specified model\n(D) Is a mis-specified model",
            "correct_answer": 0,
            "model_answer": "B. Includes as few variables as possible to explain the data.\n\nReasoning: \nA parsimonious model is one that includes the least number of variables necessary to explain the data. This means that it avoids including unnecessary variables that may complicate the model and not significantly contribute to the explanation of the data. So, the answer is (B).",
            "correct": false
        },
        {
            "id": 7,
            "question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?\n\n(A) They can be interpreted as continuously compounded changes in the prices\n(B) They can be added over time to give returns for longer time periods\n(C) They can be added across a portfolio of assets to give portfolio returns\n(D) They are usually fat-tailed",
            "correct_answer": 2,
            "model_answer": "First, we need to understand the concept of continuously compounded returns. Continuously compounded returns measure the change in an investment over time, taking into account the impact of compounding.\n\nNow, let's analyze each option:\n\n(A) They can be interpreted as continuously compounded changes in the prices\n  - This statement is correct as continuously compounded returns are designed to measure the change in an investment over time.\n\n(B) They can be added over time to give returns for longer time periods\n  - This statement is correct as continuously compounded returns can be calculated by adding up the rates of return for each period over the entire holding period.\n\n(C) They can be added across a portfolio of assets to give portfolio returns\n  - This statement is correct as continuously compounded returns can be used to calculate the overall return of a portfolio by combining the returns of each individual asset within the portfolio.\n\n(D) They are usually fat-tailed\n  - This statement is incorrect as continuously compounded returns, by nature, are calculated over a continuous period of time. Fat-tailed distributions refer to distributions with a higher than expected probability of extreme values, which is not a feature of continuously compounded returns.\n\nBased on the above analysis, the answer is (D) They are usually fat-tailed.",
            "correct": false
        },
        {
            "id": 8,
            "question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns\n\n(A) (ii) and (iv) only\n(B) (i) and (iii) only\n(C) (i), (ii), and (iii) only\n(D) (i), (ii), (iii), and (iv)",
            "correct_answer": 1,
            "model_answer": "The GARCH(1,1) model is specifically designed to capture the following features of financial asset return time-series:\n\ni) Fat tails in the return distribution: GARCH(1,1) model is an extension of ARCH(1) model which captures the high frequency volatility movements through a time varying volatility component. As a result, it can capture fat tails in the return distribution by allowing larger increases in volatility during market stress events.\n\nii) Leverage effects: GARCH(1,1) model captures the leverage effects by introducing a time-varying conditional volatility coefficient that depends on past squared returns. This allows the model to capture the risk-reward relationship where large returns coincide with higher risk, as measured by the conditional volatility.\n\nHowever, GARCH(1,1) model does not capture the following features of financial asset return time-series:\n\niii) Volatility clustering: GARCH(1,1) model does not capture the volatility clustering or the propensity of high volatility periods to occur in clusters. This feature can be captured using other models such as EGARCH(1,1) or TGARCH(1,1) models with innovations having a multivariate Student-t or T partially managed distribution.\n\niv) Volatility affecting returns: GARCH(1,1) model captures the contemporaneous relationship between volatility and returns through the convenience of similar conditional volatility parameter estimates. However, this model does not capture the predictable component of the return-volatility relationship. Models such as GJR-GARCH or DCC-GARCH can capture this feature by including specified linear or non-linear predictable components in the model.\n\nTherefore, the correct answer is:\n\n(A) (ii) and (iv) only",
            "correct": false
        },
        {
            "id": 9,
            "question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?\n\n(A) Less than -1\n(B) Between -1 and 0\n(C) Between 0 and 1\n(D) Bigger than 1",
            "correct_answer": 2,
            "model_answer": "To estimate a GARCH-M model, we need to consider the asymmetric effect of the shocks on the return. When the shocks are positively autocorrelated, the GARCH-in-mean parameter estimate will be negative. This is because the post-shock volatility is expected to be higher than the pre-shock volatility. Therefore, the value of the GARCH-in-mean parameter estimate would be:\n\n(A) Less than -1\n\nFinal answer: A",
            "correct": false
        }
    ]
}