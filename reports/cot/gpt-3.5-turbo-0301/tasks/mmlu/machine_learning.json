{
    "score": 0.6,
    "model_outputs": [
        {
            "id": 0,
            "question": "Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 3,
            "model_answer": "For Statement 1, we know that linear regression is an unbiased estimator. Therefore, we only need to show that its variance is smallest among all unbiased estimators. This is indeed the case, as it can be mathematically proven that the variance of the linear regression estimator is equal to the variance of the error term (assuming homoscedasticity) which cannot be reduced any further. Therefore, Statement 1 is true.\n\nFor Statement 2, we can prove counterexamples. AdaBoost is an algorithm that selects the best weak classifier at each iteration and assigns a coefficient \u03b1 to it. While it is true that \u03b1 is designed to give more weight to better performing weak classifiers, it is not always non-negative. In fact, if the weak classifier performs worse than chance, it is possible for its corresponding \u03b1 to become negative. Therefore, Statement 2 is false.\n\nAnswer: (C) True, False",
            "correct": false
        },
        {
            "id": 1,
            "question": "Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 2,
            "model_answer": "For statement 1, we know that RoBERTa is a refinement of BERT and one of the improvements is the pretraining corpus. According to the RoBERTa paper, it is pre-trained on a corpus of 160GB of text which is 10 times larger than the corpus BERT was trained on. Therefore, statement 1 is True.\n\nFor statement 2, we cannot say for sure whether ResNeXts in 2018 usually used tanh activation functions based on this statement alone. ResNeXts are a type of deep neural network architecture used for image classification. While tanh activation functions were commonly used in neural networks in the past, the choice of activation function can vary depending on the specific architecture and the problem it is trying to solve. Therefore, we cannot confidently say that statement 2 is True or False.\n\nThe answer is (C) True, False.",
            "correct": true
        },
        {
            "id": 2,
            "question": "Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Statement 1 states that support vector machines (SVMs) give a probability distribution over the possible labels given an input example, which is true. This is because SVMs use a kernel function to transform the input data into a higher-dimensional space where it is easier to separate the classes. The probability of a certain class is then computed based on the distance to the decision boundary.\n\nStatement 2 states that we would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels. This is not necessarily true. The choice of kernel function can affect which examples are selected as support vectors and how the decision boundary is shaped. In some cases, a higher order polynomial kernel may result in a different set of support vectors compared to a linear kernel.\n\nTherefore, the answer is (C) True, False.",
            "correct": false
        },
        {
            "id": 3,
            "question": "A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?\n\n(A) 12\n(B) 24\n(C) 48\n(D) 72",
            "correct_answer": 3,
            "model_answer": "To calculate the maximum possible different examples, we need to multiply the number of possible values for each attribute and the class. \n\nFor the first attribute, there are 3 possible values. \nFor the second attribute, there are 2 possible values. \nFor the third attribute, there are 2 possible values. \nFor the fourth attribute, there are 2 possible values. \nFor the class, there are 3 possible values.\n\nMultiplying all these together, we get: 3 x 2 x 2 x 2 x 3 = 72.\n\nTherefore, the answer is (D) 72.",
            "correct": true
        },
        {
            "id": 4,
            "question": "As of 2020, which architecture is best for classifying high-resolution images?\n\n(A) convolutional networks\n(B) graph networks\n(C) fully connected networks\n(D) RBF networks",
            "correct_answer": 0,
            "model_answer": "When it comes to classifying high-resolution images, convolutional neural networks (CNNs) have been shown to be the most effective architecture. This is because CNNs are specifically designed to handle spatial data, which is exactly what images are. They are able to effectively capture features at various scales and learn representations that are highly discriminative for image classification tasks. So, the answer to the question is (A) convolutional networks.",
            "correct": true
        },
        {
            "id": 5,
            "question": "Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "For statement 1, we need to understand what the expectation maximization (EM) algorithm does. It is often used for estimating parameters of probabilistic models when some of the variables are not observed. The algorithm consists of two steps - an expectation (E) step and a maximization (M) step. In the E-step, the algorithm calculates the expected value of the unobserved variables given the current estimate of the parameters. In the M-step, it updates the parameters to maximize the log-likelihood of the data given the expected values computed in the E-step. \n\nSince the EM algorithm updates its estimate of the parameters to maximize the log-likelihood of the data at each iteration, we can say that statement 1 is true.\n\nFor statement 2, we need to understand what Q-learning is. Q-learning is a model-free reinforcement learning algorithm that learns the value of an action in a given state by updating the Q-value of that action based on the reward received in the current state and the estimated value of the next state. \n\nThe statement that Q-learning can only be used when the learner has prior knowledge of how its actions affect its environment is not true. Q-learning is a model-free algorithm, which means that it does not need prior knowledge of the environment to learn the optimal policy. Instead, it learns through trial-and-error by interacting with the environment. \n\nTherefore, statement 2 is false.\n\nThe answer is (C) True, False.",
            "correct": false
        },
        {
            "id": 6,
            "question": "Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient?\n\n(A) O(D)\n(B) O(N)\n(C) O(ND)\n(D) O(ND^2)",
            "correct_answer": 0,
            "model_answer": "The cost of one gradient descent update depends on the size of the gradient vector, which is equal to the number of parameters in our model. Let D be the number of parameters. During each update, we need to multiply the gradient vector g by a scalar learning rate, which has a constant cost of O(D) operations. Therefore, the cost of one gradient descent update is O(D). \n\nThe answer is (A).",
            "correct": true
        },
        {
            "id": 7,
            "question": "Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x. Statement 2| Decision tree is learned by minimizing information gain.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "For statement 1, this is true because the probability of any event cannot be negative and cannot be greater than 1. Therefore, the probability distribution function for a continuous random variable must also follow this rule. \n\nFor statement 2, this is false. Decision trees are learned by minimizing impurity, not information gain. Information gain is used in the process of selecting the best attribute to split on, but the actual tree construction is based on impurity. \n\nTherefore, the answer is (C) True, False.",
            "correct": false
        },
        {
            "id": 8,
            "question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?\n\n(A) 2\n(B) 4\n(C) 8\n(D) 16",
            "correct_answer": 2,
            "model_answer": "There are 4 nodes in the Bayesian network: H, U, P and W. \n\nNode H has no parents, so it requires only one parameter.\n\nNode W has only one parent, P, so it requires 2 parameters (one for each possible value of P).\n\nNode P has two parents, U and W, so it requires 4 parameters (one for each possible combination of values for U and W).\n\nNode U has one parent, H, so it requires 2 parameters (one for each possible value of H).\n\nIn total, we need 1 + 2 + 4 + 2 = 9 independent parameters.\n\nTherefore, the answer is (C) 8.",
            "correct": true
        },
        {
            "id": 9,
            "question": "As the number of training examples goes to infinity, your model trained on that data will have:\n\n(A) Lower variance\n(B) Higher variance\n(C) Same variance\n(D) None of the above",
            "correct_answer": 0,
            "model_answer": "As the number of training examples goes to infinity, the model trained on that data will have lower variance. \n\nThis is because as the number of training examples increases, the model becomes more generalized and is less likely to overfit the training data, thus resulting in lower variance. The model will have seen more examples and patterns in the data, therefore establishing better generalization. \n\nTherefore, the answer is (A).",
            "correct": true
        }
    ]
}