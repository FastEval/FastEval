{
    "score": 0.3,
    "model_outputs": [
        {
            "id": 17,
            "question": "For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model):\n\n(A) The number of hidden nodes\n(B) The learning rate\n(C) The initial choice of weights\n(D) The use of a constant-term unit input",
            "correct_answer": 0,
            "model_answer": "The structural assumption that most affects the trade-off between underfitting and overfitting in a neural network is (A) the number of hidden nodes.\n\nExplanation:\n\nThe number of hidden nodes determines the complexity of the model. A larger number of hidden nodes allows for more complex relationships to be learned, but also increases the risk of overfitting. On the other hand, a smaller number of hidden nodes reduces the complexity of the model, but also increases the risk of underfitting.\n\nTherefore, the number of hidden nodes is the most important structural assumption in determining the trade-off between underfitting and overfitting in a neural network.\n\nThe answer is (A).",
            "correct": true
        },
        {
            "id": 72,
            "question": "Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Statement 1: True, Statement 2: True\n\nReasoning:\n\nStatement 1: Industrial-scale neural networks are normally trained on CPUs, not GPUs.\n\nThis statement is true because training neural networks on GPUs can be computationally expensive and can lead to overheating of the GPU. Therefore, it is more efficient to train neural networks on CPUs.\n\nStatement 2: The ResNet-50 model has over 1 billion parameters.\n\nThis statement is true because ResNet-50 is a deep convolutional neural network that uses a combination of residual blocks and identity mappings to reduce the number of parameters required for the model. The ResNet-50 model has 25.5 million parameters in the first residual block alone.\n\nTherefore, the answer is (A) True, True.",
            "correct": false
        },
        {
            "id": 108,
            "question": "Which of the following is false?\n\n(A) Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image.\n(B) A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive.\n(C) When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive.\n(D) A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative.",
            "correct_answer": 3,
            "model_answer": "Explanation:\n\n(A) False. Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image.\n\n(B) True. A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive.\n\n(C) False. When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive.\n\n(D) False. A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative.\n\nAnswer: (B)",
            "correct": false
        },
        {
            "id": 102,
            "question": "You are training a linear regression model for a simple estimation task, and notice that the model is overfitting to the data. You decide to add in $\\ell_2$ regularization to penalize the weights. As you increase the $\\ell_2$ regularization coefficient, what will happen to the bias and variance of the model?\n\n(A) Bias increase ; Variance increase\n(B) Bias increase ; Variance decrease\n(C) Bias decrease ; Variance increase\n(D) Bias decrease ; Variance decrease",
            "correct_answer": 1,
            "model_answer": "Explanation:\n\nWhen we add $\\ell_2$ regularization to a linear regression model, we are adding a penalty term to the loss function that is proportional to the sum of the squared weights. This penalty term discourages the weights from taking on large values, which can lead to overfitting.\n\nAs we increase the regularization coefficient, the penalty term will increase, which will have the effect of shrinking the weights towards zero. This will reduce the variance of the model, since the weights will be closer to zero, which means the model will have less flexibility to fit the data. However, the bias of the model will also increase, since the weights will be closer to zero, which means the model will have a harder time predicting the mean of the data.\n\nTherefore, the answer is (C) Bias decrease ; Variance increase.",
            "correct": false
        },
        {
            "id": 97,
            "question": "Which one of the following is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them?\n\n(A) P(A | B) * P(B | C) * P(C | A)\n(B) P(C | A, B) * P(A) * P(B)\n(C) P(A, B | C) * P(C)\n(D) P(A | B, C) * P(B | A, C) * P(C | A, B)",
            "correct_answer": 2,
            "model_answer": "Explanation:\n\nWe are given Boolean random variables A, B, and C, and we are asked to find a probability expression that is equal to P(A, B, C) without any independence or conditional independence assumptions between any of them.\n\nThe first step is to break down the expression into smaller parts. We can start by considering the product rule:\n\nP(A, B, C) = P(A) * P(B) * P(C)\n\nThis is the most basic expression that satisfies the conditions of the problem. However, we need to find a more complex expression that does not assume any independence or conditional independence between the variables.\n\nOne way to do this is to use the chain rule:\n\nP(A, B, C) = P(A | B, C) * P(B, C)\n\nThis expression assumes that A is conditionally independent of B and C given their joint distribution. We can use the law of total probability to rewrite this expression as:\n\nP(A, B, C) = P(A | B, C) * P(B, C) = P(A | B, C) * P(B | C) * P(C)\n\nThis expression assumes that A is conditionally independent of B given C, and that B is conditionally independent of C given A.\n\nAnother way to approach this problem is to use the Bayes' theorem:\n\nP(A, B, C) = P(A | B, C) * P(B | C) * P(C)\n\nThis expression assumes that A is conditionally independent of B given C, and that B is conditionally independent of C given A.\n\nFinally, we can use the conditional probability formula:\n\nP(A, B, C) = P(A | B) * P(B | C) * P(C | A)\n\nThis expression assumes that A is conditionally independent of B given C, and that B is conditionally independent of C given A.\n\nTherefore, the answer is (C).",
            "correct": true
        },
        {
            "id": 8,
            "question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?\n\n(A) 2\n(B) 4\n(C) 8\n(D) 16",
            "correct_answer": 2,
            "model_answer": "To answer this question, we need to first understand what the Bayesian network represents. The network consists of three nodes, H, U, and W, and two directed edges, one from H to U and one from U to W. The arrows represent the direction of the causal relationship, with H causing U and U causing W.\n\nWe can represent this network using a directed graph, where each node represents a random variable and each directed edge represents a conditional probability distribution. For example, the edge from H to U represents the conditional probability distribution P(U|H), which gives the probability of U given H.\n\nTo determine the number of independent parameters needed for this network, we need to consider the number of parameters that are needed to specify the conditional probability distributions. In this case, there are three conditional probability distributions: P(U|H), P(W|U), and P(W|H).\n\nTo specify each conditional probability distribution, we need to specify the probability of the outcome given the condition. For example, to specify P(U|H), we need to specify the probability of U=1 given H=1, U=0 given H=0, and so on. This requires two parameters for each outcome, since there are two possible outcomes (U=0 or U=1) for each condition (H=0 or H=1).\n\nTherefore, we need a total of 8 parameters to specify the conditional probability distributions for this network. These parameters are independent of each other, since they do not depend on any other parameters. Therefore, the answer is (C) 8.",
            "correct": true
        },
        {
            "id": 32,
            "question": "Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can\u2019t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Statement 1: True, True\nReasoning: The radial basis kernel function can capture non-linear relationships in the data, which can lead to better classification performance. However, this is not guaranteed and depends on the specific dataset and choice of kernel function.\n\nStatement 2: True, False\nReasoning: The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM because a Perceptron can only classify linearly separable data, while a simple linear SVM can classify any data that can be represented by a hyperplane. Therefore, the VC dimension of a Perceptron is smaller than that of a simple linear SVM.",
            "correct": false
        },
        {
            "id": 15,
            "question": "Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "Statement 1: Layer Normalization is used in the original ResNet paper, not Batch Normalization.\n\nReasoning: Layer Normalization is a type of normalization technique that was introduced in the original ResNet paper. It is used to normalize the output of each layer in the network. Batch Normalization, on the other hand, is a technique that normalizes the input to each layer in the network. Therefore, Statement 1 is True.\n\nStatement 2: DCGANs use self-attention to stabilize training.\n\nReasoning: DCGANs (Deep Convolutional Generative Adversarial Networks) are a type of generative adversarial network (GAN) that uses self-attention to learn the features of the input data. Self-attention is a technique that allows the network to focus on specific features of the input data and ignore others. This helps to stabilize training and improve the performance of the network. Therefore, Statement 2 is True.\n\nTherefore, the answer is (C) True, False.",
            "correct": false
        },
        {
            "id": 63,
            "question": "What are support vectors?\n\n(A) The examples farthest from the decision boundary.\n(B) The only examples necessary to compute f(x) in an SVM.\n(C) The data centroid.\n(D) All the examples that have a non-zero weight \u03b1k in a SVM.",
            "correct_answer": 1,
            "model_answer": "Support vectors are the examples that have a non-zero weight \u03b1k in a SVM. This is because they are the examples that are farthest from the decision boundary and are necessary to compute f(x) in an SVM. Therefore, the answer is (D).",
            "correct": false
        },
        {
            "id": 57,
            "question": "Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 3,
            "model_answer": "Statement 1: L2 regularization of linear models tends to make models more sparse than L1 regularization.\n\nReasoning: L2 regularization adds a penalty term to the loss function that is proportional to the square of the weights. This means that smaller weights are penalized more heavily than larger weights. This tends to make the model more sparse, meaning that only a small number of weights are non-zero.\n\nStatement 2: Residual connections can be found in ResNets and Transformers.\n\nReasoning: Residual connections are a technique used in ResNets and Transformers that allows the model to learn better representations of the data by adding the identity function to the output of each layer. This means that the output of each layer is the sum of the input to that layer and the output of the previous layer. This allows the model to learn more complex representations of the data and improve its performance.\n\nTherefore, the answer is (C) True, False.",
            "correct": false
        }
    ]
}