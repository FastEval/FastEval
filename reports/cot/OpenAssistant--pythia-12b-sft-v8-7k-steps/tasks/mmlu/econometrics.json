{
    "score": 0.2,
    "model_outputs": [
        {
            "id": 0,
            "question": "Which one of the following is the most appropriate definition of a 99% confidence interval?\n\n(A) 99% of the time in repeated samples, the interval would contain the true value of the parameter\n(B) 99% of the time in repeated samples, the interval would contain the estimated value of the parameter\n(C) 99% of the time in repeated samples, the null hypothesis will be rejected\n(D) 99% of the time in repeated samples, the null hypothesis will not be rejected when it was false",
            "correct_answer": 0,
            "model_answer": "As a first step, we should check the meaning of each option:\n\nA. At least 99% of the times, the interval will contain the true value.\nB. At least 99% of the times, the interval will contain the estimated value.\nC. At least 99% of the times, the truth ... what? That is something that can vary with repeated samples. It might make sense if theInterval becomes smaller or larger.\nD. Similar to C, only the opposite.\n\nThe meaning of the different options seems to reduce to each other, so lets group them.\n\nA and B. At least 99% of the times, the interval will contain the correct value.\nThe width of the interval is the error introduced by the estimate. This error will have a standard deviation whose value is:\n hairs breadth to this legroom.\n\nThe width of the interval represents how confident we can be that the true value is inside the interval. We set this width to 99% of the error length, so theeron place we can write the width as an absolute number\n\"hairs breadth\" or a percentage:\n\"legroom\"\n\nThe width is divided into 100 parts: 99 parts correspond to the numerator in the width divided by the width. In this case the width is 0.99 * \"hairs breadth\" or 0.99 * \"legroom\".699 is very small, indicating a very precise estimate, which might be impracticable in some applications.1000 is also small, which also corresponds to a precise estimate.\n.\n\nD. Similar to A and B, at least.\nE. At least 99%, the truth is not relevant.Previously unsupported software is no good.",
            "correct": false
        },
        {
            "id": 1,
            "question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?\n\n(A) ADF is a single equation approach to unit root testing while PP is a systems approach\n(B) PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test\n(C) The PP test incorporates an automatic correction for autocorrelated residuals in the test regression\n(D) PP tests have good power in small samples whereas DF tests do not.",
            "correct_answer": 2,
            "model_answer": "Sure! Let me explain the reasoning step-by-step.\n\n(A) In the ADF approach, the Augmented Dickey-Fuller (ADF)statistic is used to test for a unit root in a regression model. The ADFstatistic is aSingle equation approach to unit root testing. This means that it is only one statistic and cannot test for both a unit root and a change in the variance structure at the same time.\n\n(B) In the Reverse Phillips-Perron (RPP) approach, there is a tests that tests for a unit root and a change in the variance structure at the same time. This test has the form:\n\nH0: 479-( integrableInitial valuet0)2 ERP0<=ceiling( Generally recommendedvalue/ cakelevel) <1/2 t0 where t0 is the panel data and ERP0 is the ).\n\nThe Reverse Phillips-Perron Test is the reverse of the Dickey-Fuller Test and therefore tests the opposite hypothesis. Therefore the answer must be (D) PP tests have good power in small samples whereas DF tests do not.\n\nTherefore the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing is that DF test only tests for a unit root while the PP test tests for both a unit root and a change in the variance structure.",
            "correct": false
        },
        {
            "id": 2,
            "question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?\n\n(A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude\n(B) It would be symmetrical about zero\n(C) It would rise less quickly for negative disturbances than for positive ones of the same magnitude\n(D) It would be zero for all positive disturbances",
            "correct_answer": 0,
            "model_answer": "Here are my steps:\n- Create the OLS orLinear Regression equation with the two endogenous variables on one side and the disturbance on the other, but not in closed form.\n- Toy OLS equation subject to samplingGoogle and read off the lag structure and elasticities by handCCorpus, like she/he did in this paper https://blogs.imf.org/stefan- optimize the degrees of freedom to get a more stable solution.\n(If my memory serves me right the papers by Christraud,that uses the Two E convergent Manning anddifferencevie with 31 lags)\n- Choose a random sample from the data, and invert the elasticity coefficients and volcano coeffs to get the news curve. Like this blog post looks something like - https://www. bankingAldMSME.org/blog/best- Practices-Choosing-OLS-D equation- Regression Model\n(However, I think this blog post is looking for serially correlated error terms, so I might have to blog about it myself... )\n- Plot the news curve you got from the accounting equation to see the shape of it:The shape of the news curve looks like this:\n->\n->\nThis shape represents the ex-post, ex-ante news impact curve for an OLS of the form\n\nS = b0 + b1(L1) + b2(L2) + u\n\n|\n\nThis ex-ante shapes allows to compare the degree of news effect of OLS results (if those exist) with FEs results.",
            "correct": false
        },
        {
            "id": 3,
            "question": "Which of the following statements is false concerning the linear probability model?\n\n(A) There is nothing in the model to ensure that the estimated probabilities lie between zero and one\n(B) Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one\n(C) The error terms will be heteroscedastic and not normally distributed\n(D) The model is much harder to estimate than a standard regression model with a continuous dependent variable",
            "correct_answer": 3,
            "model_answer": "I will do my best to explain my reasoning:\n\n(A) The linear probability model assumes that the conditional probabilities of observing an event (the success or failure of a model) given the value of a predictor variable (X) fall between zero and one. While there is nothing in the model that forces this to be the case, we do have some constraints: if the observed data is independent and identically distributed (IID), then the conditional probabilities must sum to one. Therefore, we must have p(X) >= 0 and p(X) <= 1 for all X. Given these constraints, we could make the assumption that p(X) = 0 at some point in the logistic regression even if it's rare in the population. I will still mark (A) as a false statement since there are other issues with logistic regression outside of the limitations of the linear probability model. Firstly, there is the issue with choosing a model. Secondly, what if we are overfitting?\n\n(B) Even if we observed the true probabilities, we could have too many observations with zero or one probability, leading to a wide range of probabilities. Therefore, it must be true that there would likely be such observations. I will mark (B) as true.\n\n(C) The error terms must follow a stringent distributional assumption, such as being normally distributed. The linear probability model is not designed to perform direct statistical inference on the error terms pertaining to parameter estimates. Therefore, (C) must be false.\n\n(D) The linear probability model is a simplified model for analyzing the data. Therefore, equivalent models would either be much more complex or more transparent than the linear probability model, so it must be (D) that is false.\n\nTherefore, the answer should be (D) \"the model is much harder to estimate than a standard regression model with a continuous dependent variable.\"",
            "correct": true
        },
        {
            "id": 4,
            "question": "Which of the following statements concerning the regression population and sample is FALSE?\n\n(A) The population is the total collection of all items of interest\n(B) The population can be infinite\n(C) In theory, the sample could be larger than the population\n(D) A random sample is one where each individual item from the population is equally likely to be drawn.",
            "correct_answer": 2,
            "model_answer": "I guess the sample can be larger than the population \n\nThere is no consensus about the number of people that can be surveyed in a representative way nowadays, it's probably inexact or even impossible to get a representative sample either for ethical or financial reasons",
            "correct": false
        },
        {
            "id": 5,
            "question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?\n\n(A) Asymptotically, the values of the two test statistics will be equal\n(B) The Q test has better small-sample properties than the Q*\n(C) The Q test is sometimes over-sized for small samples\n(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients.",
            "correct_answer": 1,
            "model_answer": "Let us first consider the help text of the question. We get:\n\n> Q: Are the values of the two test statistics equal?\n> \n> Qstar: When the autocorrelation is not zero, does the violation tend to increase with the sample size?\n\nNow two things can play a role:\n\n  * The asymptotic equality (A)\n  * The tendency of the statistics when the sample size is growing (D)",
            "correct": false
        },
        {
            "id": 6,
            "question": "A parsimonious model is one that\n\n(A) Includes too many variables\n(B) Includes as few variables as possible to explain the data\n(C) Is a well-specified model\n(D) Is a mis-specified model",
            "correct_answer": 0,
            "model_answer": "Step 1: Read the problem carefully to understand what the question is asking.\nStep 2: The question is asking for the characteristics of a parsimonious model.\nStep 3: A parsimonious model is one that includes as few variables as possible to explain the data while still being able to accurately predict future outcomes.\nStep 4: The correct answer must fit this description.\nStep 5: Based on the given criteria, the only answer that fits the description is (D) Mis-specified model.\nStep 6: Therefore, the answer to the question is (D).",
            "correct": false
        },
        {
            "id": 7,
            "question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?\n\n(A) They can be interpreted as continuously compounded changes in the prices\n(B) They can be added over time to give returns for longer time periods\n(C) They can be added across a portfolio of assets to give portfolio returns\n(D) They are usually fat-tailed",
            "correct_answer": 2,
            "model_answer": "Sure!\n\n(A) Continuous compounded returns can be interpreted as continuously compounded changes in pricing of an asset\n(B) Return can be compounded over time to add returns for later periods\n(C) Return (of a portfolio) can be added across assets to give portfolio returns\n\n(D) Continuous compounded returns are usually fat-tailed, meaning they have a heavy tail towards positive values, meaning that they can have a very large positive return with only a minute chance of having a very large negative return\n\n The correct answer is (D) Continuous compounded returns are usually fat-tailed, meaning they have a heavy tail towards positive values, meaning that they can have a very large positive return with only a minute chance of having a very large negative return. \nIn (A) there is no continuous interpretation of the log returns. \nIn (B) Return can't be compounded to add returns for later periods\nIn (C) there is no portfolio component.",
            "correct": true
        },
        {
            "id": 8,
            "question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns\n\n(A) (ii) and (iv) only\n(B) (i) and (iii) only\n(C) (i), (ii), and (iii) only\n(D) (i), (ii), (iii), and (iv)",
            "correct_answer": 1,
            "model_answer": "Steps:\n1. Understand that financial asset return time-series are serially correlated and exhibit a property known as \"volatility clustering,\" where past volatility values affect future volatility values. \n2. GARCH(1,1) models are used to capture serial correlation and the relationship between past and future volatility values. A GARCH(1,1) model can capture the following features:\n   a) Fat tails in the return distribution and \n(D) i), (ii), (iii), and (iv)",
            "correct": false
        },
        {
            "id": 9,
            "question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?\n\n(A) Less than -1\n(B) Between -1 and 0\n(C) Between 0 and 1\n(D) Bigger than 1",
            "correct_answer": 2,
            "model_answer": "I\u2019d expect the value of the GARCH-in-mean parameter estimate to be less than -1. I don\u2019t see any valid reason for it to be bigger than 1. For a time series of daily corporate bond percentage returns, it\u2019s reasonable to assume that the process is GARCH. It is also reasonable to assume that the process smoothly rolls over daily, so I don\u2019t see any explanation why the process would tend to explode, in which case the parameter estimate for GARCH-in-mean would probably be very big. Plenty of applications of GARCH (including this very page) ignore the assumption of explodeyness and still get interesting results, so I don\u2019t think the estimate would bound between -1 and 1. Finally, what does a GARCH-in-mean estimate even mean? If you\u2019re estimating an ARIMA, what do you care about the GARCH-in-mean parameter? Since the GARCH-M model smooths the data, it can\u2019t possibly \u201clearning\u201d anything not already contained in the data in the way that an ARIMA can, and the GARCH-in-mean shrinks toward zero as such a parameter would be an artifact of the smoothing. GARCH doesn\u2019t have an intercept term, so a GARCH-in-mean process is just aVAR with one degree of freedom. I\u2019d not expect an estimate for any such process to be bigger than 1. That\u2019s my reasoning. Let me know if I\u2019ve missed something since I\u2019m not an expert.",
            "correct": false
        }
    ]
}