[
    {"id": "602ed35c-05c8-4061-b0bb-8188d2419661", "model_type": "chatml", "model_name": "OpenAssistant/llama2-70b-oasst-sft-v10", "benchmarks": ["mt-bench", "human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "lm-evaluation-harness"], "model_args": {"default_system_message": "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.", "inference_backend": "vllm"}, "short_name": "OpenAssistant Llama-2 70B V10", "url": "https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10"},
    {"id": "e015dfa0-ec76-4ffc-9d65-9368cdbcf467", "model_type": "openchat-llama2-v1", "model_name": "Open-Orca/OpenOrca-Platypus2-13B", "benchmarks": ["mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "human-eval-plus", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "short_name": "OpenOrca-Platypus2-13B", "url": "https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B"},
    {"id": "93607cdf-2b52-426f-9c0d-4b1a1daf655f", "model_type": "alpaca-with-prefix", "model_name": "WizardLM/WizardMath-70B-V1.0", "benchmarks": ["mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "human-eval-plus"], "model_args": {"inference_backend": "vllm"}, "short_name": "WizardMath 70B V1.0", "url": "https://huggingface.co/WizardLM/WizardMath-70B-V1.0"},
    {"id": "6c582905-5298-4028-8287-787ba9464170", "model_type": "wizard-lm", "model_name": "WizardLM/WizardLM-70B-V1.0", "benchmarks": ["ds1000", "lm-evaluation-harness", "mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "human-eval-plus"], "model_args": {"inference_backend": "vllm"}, "short_name": "WizardLM 70B V1.0", "url": "https://huggingface.co/WizardLM/WizardLM-70B-V1.0"},
    {"id": "03eaca63-56c7-4e89-9b82-eeb6457c726a", "model_type": "openchat-llama2-v1", "model_name": "Open-Orca/OpenOrcaxOpenChat-Preview2-13B", "benchmarks": ["ds1000", "lm-evaluation-harness", "mt-bench", "human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu"], "model_args": {"inference_backend": "vllm"}, "short_name": "OpenOrca x OpenChat - Preview2 - 13B", "url": "https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B"},
    {"id": "6883299b-8de3-4980-ad39-45f945b754eb", "model_type": "alpaca-without-prefix", "model_name": "NousResearch/Nous-Hermes-Llama2-13b", "benchmarks": ["lm-evaluation-harness", "mt-bench", "human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu"], "model_args": {"inference_backend": "vllm"}, "short_name": "Nous-Hermes Llama-2 13B", "url": "https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b"},
    {"id": "04d3a878-ac70-4a40-a4d1-4d434de47e43", "model_type": "dolphin", "model_name": "ehartford/dolphin-llama-13b", "benchmarks": ["lm-evaluation-harness", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "human-eval-plus"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/ehartford/dolphin-llama-13b", "short_name": "Dolphin Llama 13B"},
    {"id": "f62b92a8-ba5c-42ac-9637-523e521cfa8b", "model_type": "llama2-chat", "model_name": "meta-llama/Llama-2-70b-chat-hf", "benchmarks": ["lm-evaluation-harness", "mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "human-eval-plus"], "model_args": {"inference_backend": "vllm", "tokenizer": "hf-internal-testing/llama-tokenizer"}, "url": "https://huggingface.co/meta-llama/Llama-2-70b-chat-hf", "short_name": "LLaMA-2 70B Chat"},
    {"id": "366d8f77-a86c-46d5-86ae-0b57223f10b4", "model_type": "llama2-chat", "model_name": "meta-llama/Llama-2-13b-chat-hf", "benchmarks": ["mt-bench", "lm-evaluation-harness", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "human-eval-plus"], "model_args": {"inference_backend": "vllm", "tokenizer": "hf-internal-testing/llama-tokenizer"}, "url": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf", "short_name": "LLaMA-2 13B Chat"},
    {"id": "2e225076-3c34-4e03-bc74-f4375f4d2d49", "model_type": "llama2-chat", "model_name": "meta-llama/Llama-2-7b-chat-hf", "benchmarks": ["lm-evaluation-harness", "mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "human-eval-plus"], "model_args": {"inference_backend": "vllm", "tokenizer": "hf-internal-testing/llama-tokenizer"}, "url": "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf", "short_name": "LLaMA-2 7B Chat"},
    {"id": "4cea59cd-33ac-4579-b832-cfd30e393108", "model_type": "chatml", "model_name": "mosaicml/mpt-7b-chat", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/mosaicml/mpt-7b-chat", "short_name": "MPT-7B-Chat"},
    {"id": "eb74c9e1-8836-4c3a-8f50-a25808d20eee", "model_type": "chatml", "model_name": "mosaicml/mpt-30b-chat", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/mosaicml/mpt-30b-chat", "short_name": "MPT-30B-Chat"},
    {"id": "31a8c8a1-05bd-45d9-8ef3-981e7704344c", "model_type": "open-assistant", "model_name": "OpenAssistant/oasst-sft-1-pythia-12b", "benchmarks": ["ds1000", "human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b", "short_name": "Open-Assistant Pythia-12B SFT-1"},
    {"id": "68263b05-9164-45d3-ae65-51f9fec5bd83", "model_type": "open-assistant", "model_name": "OpenAssistant/pythia-12b-sft-v8-7k-steps", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps", "short_name": "Open-Assistant Pythia-12B SFT-8"},
    {"id": "0bc02e0e-a875-4ffb-bfdb-6f815217a67f", "model_type": "alpaca-without-prefix", "model_name": "NousResearch/Nous-Hermes-13b", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm", "tokenizer": "hf-internal-testing/llama-tokenizer"}, "url": "https://huggingface.co/NousResearch/Nous-Hermes-13b", "short_name": "Nous-Hermes-13B"},
    {"id": "e732cae9-ad05-438b-9eb9-5788c20b95f9", "model_type": "alpaca-with-prefix", "model_name": "WizardLM/WizardCoder-15B-V1.0", "benchmarks": ["ds1000", "human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/WizardLM/WizardCoder-15B-V1.0", "short_name": "WizardCoder-15B"},
    {"id": "b4f51256-c00f-4342-892a-ba45d453180b", "model_type": "guanaco", "model_name": "timdettmers/guanaco-65b-merged", "benchmarks": ["human-eval-plus", "mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm", "tokenizer": "TheBloke/guanaco-65B-HF"}, "url": "https://huggingface.co/timdettmers/guanaco-65b-merged", "short_name": "Guanaco-65B"},
    {"id": "f8ffcb1c-8532-46f5-b945-ae829bf27204", "model_type": "stable-beluga", "model_name": "stabilityai/StableBeluga-13B", "benchmarks": ["lm-evaluation-harness", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "human-eval-plus"], "model_args": {"inference_backend": "hf_transformers", "default_system_message": "You are Stable Beluga 13B, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal."}, "short_name": "StableBeluga-13B", "url": "https://huggingface.co/stabilityai/StableBeluga-13B"},
    {"id": "8468efcd-9290-47ab-8111-fe07455317ce", "model_type": "open-assistant", "model_name": "OpenAssistant/falcon-40b-sft-top1-560", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/OpenAssistant/falcon-40b-sft-top1-560", "short_name": "Open-Assistant Falcon-40B SFT-Top1"},
    {"id": "4c715856-8605-48a9-9882-3d0e62e9d6bb", "model_type": "open-assistant", "model_name": "OpenAssistant/falcon-40b-sft-mix-1226", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226", "short_name": "Open-Assistant Falcon-40B SFT-Mix"},
    {"id": "0fe60bfa-64d2-45b6-a34a-d408789b5175", "model_type": "falcon-instruct", "model_name": "tiiuae/falcon-7b-instruct", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/tiiuae/falcon-7b-instruct", "short_name": "Falcon-7B-Instruct"},
    {"id": "b7b8331c-cfc2-47c0-8b93-f704842babc1", "model_type": "falcon-instruct", "model_name": "tiiuae/falcon-40b-instruct", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm"}, "url": "https://huggingface.co/tiiuae/falcon-40b-instruct", "short_name": "Falcon-40B-Instruct"},
    {"id": "beaa2c36-153c-4f63-9c5a-1395db65c958", "model_type": "stable-beluga", "model_name": "stabilityai/StableBeluga2", "benchmarks": ["mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "human-eval-plus"], "model_args": {"inference_backend": "vllm", "default_system_message": "You are Free Willy, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal."}, "url": "https://huggingface.co/stabilityai/StableBeluga2", "short_name": "Stable Beluga 2", "num_params": "70B"},
    {"id": "35b42dc8-34ad-4233-98b0-1c4e420dc46a", "model_type": "fastchat", "model_name": "lmsys/vicuna-7b-v1.3", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm", "tokenizer": "hf-internal-testing/llama-tokenizer"}, "url": "https://huggingface.co/lmsys/vicuna-7b-v1.3", "short_name": "Vicuna 7B V1.3"},
    {"id": "d08ed3dd-795a-4978-84fe-4286d2461a21", "model_type": "fastchat", "model_name": "lmsys/vicuna-33b-v1.3", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "lm-evaluation-harness"], "model_args": {"inference_backend": "vllm", "tokenizer": "hf-internal-testing/llama-tokenizer"}, "url": "https://huggingface.co/lmsys/vicuna-33b-v1.3", "short_name": "Vicuna 33B V1.3"},
    {"id": "98e2de77-2a90-4560-9e0a-e9e18f5d8356", "model_type": "openai", "model_name": "gpt-3.5-turbo-0613", "benchmarks": ["ds1000", "human-eval-plus", "mt-bench", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu"], "model_args": {}, "short_name": "GPT-3.5-Turbo-0613", "num_params": "proprietary"},
    {"id": "f5fc94f2-511f-4f57-bb3b-69876e829cb1", "model_type": "openai", "model_name": "gpt-3.5-turbo-0301", "benchmarks": ["human-eval-plus", "cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench"], "model_args": {}, "short_name": "GPT-3.5-Turbo-0301", "num_params": "proprietary"},
    {"id": "a44327c1-50a9-462b-ba96-34266470bf1a", "model_type": "openai", "model_name": "gpt-4-0613", "benchmarks": ["cot/gsm8k", "cot/math", "cot/bbh", "cot/mmlu", "mt-bench", "human-eval-plus"], "model_args": {}, "short_name": "GPT-4-0613", "num_params": "proprietary"}
]
