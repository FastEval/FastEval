{
    "Pandas": [
        "result = df.iloc[List]\nprint(result)",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\nresult = df.iloc[List].reset_index(drop=True)\nprint(result)\n",
        "result = df.loc[df['Qu1'].apply(lambda x: x in ['apple', 'egg']) == False, 'Qu1'] = 'other'\nresult['Qu2'] = df['Qu2']\nresult['Qu3'] = df['Qu3']\nprint(result)",
        "result = df.copy()\nfor column in ['Qu1', 'Qu2', 'Qu3']:\n    value_counts = pd.value_counts(df[column])\n    if value_counts >= 3:\n        result[column] = 'other'\n    else:\n        result[column] = df[column]\nprint(result)\ntest_data = DataFrame({'Qu1': ['other', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n                  'Qu2': ['other', 'banana', 'apple', 'apple', 'apple', 'other', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']})",
        "\nresult = example_df.copy()\nresult['Qu1'] = result['Qu1'].replace(r'apple', 'other', regex=True)\nresult['Qu1'] = result['Qu1'].replace(r'egg', 'other', regex=True)\nresult\ntest_data = DataFrame({'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']})",
        "result = df.copy()\nresult['Qu1'] = 'other' if df.Qu1.value_counts() >= 3 else df.Qu1\nresult['Qu2'] = 'other' if df.Qu2.value_counts() >= 2 else df.Qu2\nresult['Qu3'] = 'other' if df.Qu3.value_counts() >= 2 else df.Qu3\nprint(result)",
        "result = df.copy()\nresult['Qu1'] = 'other' if df.Qu1.isin(['apple', 'egg']).all() else 'other'\nresult['Qu2'] = 'other' if df.Qu2.isin(['banana', 'sausage']).all() else 'other'\nresult['Qu3'] = 'other' if df.Qu3.isin(['apple', 'potato']).all() else 'other'\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "df = df.drop_duplicates(subset='url', keep='first')\ndf['keep_if_dup'] = df['keep_if_dup'].astype(str).str.get_dummies()\ndf = df.set_index('url')\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\nresult = df[['id', 'url', 'keep_if_dup']]\nprint(result)",
        "result = {}\nfor name, v1, v2, v3 in zip(df['name'], df['v1'], df['v2'], df['v3']):\n    if name not in result:\n        result[name] = {}\n    result[name][v1] = {v2: v3}\nprint(result)",
        "df['datetime'] = df['datetime'].dt.tz_localize('UTC').dt.tz_convert(None).dt.strftime('%Y-%m-%d %H:%M:%S')\nresult = df\nprint(result)",
        "\n",
        "df['datetime'] = df['datetime'].dt.tz_localize('UTC').dt.tz_convert(None).dt.strftime('%Y-%m-%d %H:%M:%S')\nresult = df\nprint(result)",
        "df['datetime'] = df['datetime'].dt.tz_localize('UTC').dt.tz_convert(None).dt.strftime('%Y-%m-%d %H:%M:%S')\nresult = df\nprint(result)",
        "df['job'] = df['message'].str.extractall('job:\\s*(\\w+)')\ndf['money'] = df['message'].str.extractall('money:\\s*(\\d+)')\ndf['wife'] = df['message'].str.extractall('wife:\\s*(\\w+)')\ndf['group'] = df['message'].str.extractall('group:\\s*(\\w+)')\ndf['kids'] = df['message'].str.extractall('kids:\\s*(\\w+)')\nresult = df.loc[:, ['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids']]\nprint(result)",
        "result['score'] = result['score'] * 10\nresult = result[result['product'].isin(products)]\nresult['score'] = result['score'] / 10\nresult = result[result['product'].isin(products)]\nprint(result)",
        "result['score'] = result['score'] * (10 if result['product'] in products else 1)\nresult = result[['product', 'score']]\nprint(result)",
        "for product_index, product in enumerate(products):\n    df.loc[df['product'].isin(product), 'score'] *= 10\nresult = df\nprint(result)",
        "result['score'] = result['score'].mask(result['product'].isin(products), result['score'].min() if result['product'] == 1069104 else result['score'].max())\nresult = result[['product', 'score']]\nprint(result)",
        "result['category'] = df.A | df.B | df.C | df.D\nresult = result.astype({'category': 'category'})\nresult.drop(columns=['A', 'B', 'C', 'D'], axis=1, inplace=True)\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\nresult = df\nresult['category'] = pd.get_dummies(df, prefix='A', suffix='B')\nresult = result.drop(columns=['A', 'B'])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 1, 1],\n                   'C': [1, 1, 1, 0, 1, 0],\n                   'D': [0, 0, 0, 1, 1, 0]})\nresult = df\nresult['category'] = df.apply(lambda row: ', '.join(map(str, row.astype(bool))), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].dt.to_period(\"M\")\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].dt.to_period(\"M\")\nresult = df\nprint(result)\n",
        "\ndef get_month_name(date):\n    return date.strftime('%B')\ndef get_day_of_week(date):\n    return date.strftime('%A')\ndf['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x))\nfor date in List:\n    df.loc[df['Date'] == pd.to_datetime(date), 'Date'] = df['Date'].apply(lambda x: (get_month_name(x), get_day_of_week(x)), axis=1)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\nresult = df.shift(1, axis=0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\nresult = df.shift(1, axis=0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\nresult = df.shift(1, axis=0)\nprint(result)\n",
        "df = df.shift()\nresult = df\nprint(result)",
        "df.rename(columns={'HeaderA': 'HeaderAX'}, inplace=True)\nresult = df\nprint(result)",
        "\n# [Missing Code]\n",
        "df = df.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX', 'HeaderX': 'HeaderX'}, inplace=True)\nresult = df\nprint(result)",
        "df = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", \"val3\": \"mean\"})\nprint(result)\n",
        "df = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", \"val3\": \"sum\"})\nprint(result)\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"mean\"})\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.mean(axis=0, skipna=True, where=lambda x: x in row_list)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.sum(axis=0, skip=row_list, level=0, axis=0)[column_list]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.sum(axis=0, skip=row_list, level=0, axis=0)[column_list]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = df.value_counts()\nresult = result.astype('float64')\nresult.index.name = 'column'\nresult = result.sort_values(ascending=False)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = df.isnull().sum()\nresult.index = df.columns\nresult.name = 'null'\nresult.dtype = 'float64'\nprint(result)\n",
        "result = df.value_counts()\nresult = result.rename_axis(None, axis=1).reset_index()\nresult.columns = ['Column', 'Value Count']\nresult['Duplicates'] = result['Value Count'] == result['Value Count'].shift(1)\nresult['Duplicates'] = result['Duplicates'].astype(int)\nresult = result[result['Duplicates'] == 0]\nresult = result.set_index('Column')\nresult = result.astype(str)\nresult = result.replace('int64', 'dtype: int64')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: int64', 'dtype: int64')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')\nresult = result.replace('dtype: object', 'dtype: object')",
        "result = df.iloc[[0, 1]].unstack().reset_index()\nprint(result)",
        "result = df.iloc[[0, 1]].unstack().reset_index()\nprint(result)",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\nprint(result)\n",
        "df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1)\n",
        "df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n",
        "\n# [Missing Code]\n",
        "\n",
        "\n",
        "result[\"inv_A\"] = 1 / df[\"A\"]\nresult[\"inv_B\"] = 1 / df[\"B\"]\n",
        "result[\"exp_A\"] = np.exp(df[\"A\"])\nresult[\"exp_B\"] = np.exp(df[\"B\"])\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \": [e^1, e^2, e^3], \"exp_B \": [e^4, e^5, e^6]})\nprint(result)",
        "result[\"inv_A\"] = df[\"A\"].apply(lambda x: 1 / x)\nresult[\"inv_B\"] = df[\"B\"].apply(lambda x: 1 / x)\n",
        "result[\"sigmoid_A\"] = 1/(1+np.e**(-1)) * df[\"A\"]\nresult[\"sigmoid_B\"] = 1/(1+np.e**(-4)) * df[\"B\"]\n",
        "result = df.loc[df.idxmin(axis=1) + 1:, 'idx']\nresult = result.astype('datetime64[ns]')\nprint(result)",
        "\nimport pandas as pd\nimport numpy as np\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\nidx = pd.date_range('2017', periods=a.shape[0])\ndf = pd.DataFrame(a, index=idx, columns=list('abc'))\nresult = df.apply(lambda x: x.idxmin(), axis=1).reset_index(drop=True)\nresult = result.loc[result.index.get_level_values('level_0') == result.index.get_level_values('level_0').max()]\nresult = result.index.get_level_values('level_0')\nresult = result[result != result[0]]\nresult = result.to_numpy()\nresult = pd.Series(result, index=df.columns)\nresult = result.astype('datetime64[ns]')\nresult.name = 'date'\nresult\nprint(result)\n",
        "min_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date, freq='D'), 'user': ['a', 'a', 'b', 'b'], 'val': [0, 0, 0, 0]})\nresult['dt'] = result['dt'].astype('datetime64[D]')\nresult = result.set_index('dt')\nresult['val'] = df['val'].reindex(result.index)\nresult = result.sort_index()\nprint(result)",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\nresult = df.set_index('dt').unstack('user').fillna(0).stack().reset_index()\nprint(result)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date, freq='D'), 'user': ['a', 'a', 'b', 'b'], 'val': [233, 233, 233, 233]})\nresult['val'] = result['val'].astype(int)\nresult['val'] = result['val'].fillna(233)\nresult['val'] = result['val'].astype(int)\nresult = result.set_index('dt')\nresult = result.sort_index()\nresult = result.reset_index()\nprint(result)\n",
        "min_dt = df['dt'].min()\nmax_dt = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': ['a', 'a', 'b', 'b'], 'val': [33, 33, 2, 1]})\nresult['val'] = result['val'].fillna(method='ffill')\nresult['val'] = result['val'].fillna(method='bfill')\nresult['val'] = result['val'].ffill()\nresult['val'] = result['val'].bfill()\nresult['val'] = result['val'].astype(int)\nresult = result.set_index('dt')\nresult = result.sort_index()\nresult = result.reset_index()\nresult = result.rename(columns={'level_0': 'dt', 'user': 'user', 'val': 'val'})\nprint(result)\n",
        "min_dt = df['dt'].min()\nmax_dt = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': ['a', 'a', 'b', 'b'], 'val': [33, 33, 2, 1]})\nresult['dt'] = result['dt'].dt.to_pydatetime()\nresult['user'] = result['user'].astype('str')\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\nresult = result.set_index('dt')\nresult.sort_index(inplace=True)\nresult.reset_index(inplace=True)\nprint(result)\n",
        "\n",
        "\n# [Missing Code]\n",
        "\ndef f(df=example_df):\n    result = example_df.assign(name=example_df.name.astype('int'))\n    result['name'] = result['name'].replace(to_replace=result['name'], value=result['name'] + 1)\n    return result\n",
        "\n",
        "result = df.pivot_table(index='user', columns='date', values='value', aggfunc=lambda x: x, dropna=False)\nresult.columns = ['user', 'date', 'value', 'someBool']\nresult = result.reset_index()\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\nresult = df.pivot_table(index='user', columns='01/12/15', values='02/12/15', aggfunc='first')\nresult = result.join(df.pivot_table(index='user', columns='01/12/15', values='someBool', aggfunc='first'))\nresult.columns = ['user', '01/12/15', 'others', 'value']\nresult = result.reset_index()\nprint(result)\n",
        "result = df.pivot_table(index='user', columns='date', values='value', aggfunc=lambda x: x)\nresult.columns = ['user', 'date', 'value', 'someBool']\nresult = result.reset_index()\nprint(result)",
        "result = df[df.c > 0.5][columns]",
        "result = df[df.c > 0.45][locs].loc[:, columns]\nprint(result)",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in ['a', 'd']]\n    result = df[df.c > 0.5][locs].iloc[:, columns]\n    return result.to_numpy()\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in ['c']]\n    df_subset = df[df.c > 0.5][locs]\n    result = df_subset[columns].sum(axis=1)\n    return result\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    return result\n",
        "filter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(days=i)))\ndf = df[~df.index.isin(filter_dates)]\nresult = df[['ID', 'date', 'close']]\nprint(result)\n",
        "filter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(months=i)))\ndf = df[~df.index.isin(filter_dates)]\nresult = df[['ID', 'date', 'close']]\nprint(result)\n",
        "filter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(months=i)))\ndf = df[~df.index.isin(filter_dates)]\nresult = df[['ID', 'date', 'close']]\nprint(result)\n",
        "result = df.groupby(df.index // 3).mean()\nprint(result)\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = df.groupby(df.index // 3).mean()\nprint(result)\n",
        "result = df.groupby(df.index // 3).agg({'col1': 'sum'})\nprint(result)\nimport pandas as pd\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\nresult = df.groupby(df.index // 3).agg({'col1': 'sum'})\nprint(result)\n    col1",
        "",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = df.iloc[::3, 0].resample('3S').mean()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\nresult = df.groupby(df.index // 3).agg({'col1': 'sum', 'col1': 'mean'})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\nresult = df.groupby(df.index // 3 + (df.index % 3 != 0)).agg({'col1': 'sum', 'col1': 'mean'})\nprint(result)\n",
        "\nimport pandas as pd\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\ndf['A'].fillna(method='ffill', inplace=True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\ndf.A.fillna(method='ffill', inplace=True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\ndf['A'].fillna(method='ffill', inplace=True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\ndf ['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf ['time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\ndf ['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf [ 'time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\nresult = df\nprint(result)\n",
        "\ndef f(df=example_df):\n    df ['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\n    df ['time']= df.duration.str.extract(r'(?P<time>week|day|month|year)', expand=False)\n    return result\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\ndf ['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf [ 'time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\nresult = df\nprint(result)\n",
        "check = np.where([df[column] != df[column] for column in columns_check_list])\nresult = [False, False, False]\nprint(result)",
        "check = np.where([df[column] == df[column] for column in columns_check_list])\nresult = check.astype(int).any(axis=1)\nprint(result)\n",
        "df.index.levels[1] = pd.to_datetime(df.index.levels[1])\ndf = df.set_index('date', drop=True, append=True)\nresult = df\nprint(result)",
        "\nimport pandas as pd\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                 names=('name', 'datetime'))\ndf = pd.DataFrame({'fee': [100, 90, 80], 'credits':[7, 8, 9]}, index=index)\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\nresult = df\nprint(result)\n",
        "\ndef f(df):\n    df['date'] = pd.to_datetime(df['date'])\n    return df.set_index('date')['x'].unstack(level=0).swap_level(0, 1).to_numpy()\n",
        "df = df.set_index(['id', 'date'])[['x', 'y']]\nreturn df",
        "df = pd.melt(df, id_vars='Country', value_name='Var1', var_name='year')\nresult = df\nprint(result)",
        "df = pd.melt(df, id_vars='Country', value_name='Var1', var_name='year')\nresult = df.sort_values('year', ascending=False)\nresult = result.drop('year', axis=1)\nresult = result.rename(columns={'value': 'Var1', 'variable': 'year'})\nresult = result.set_index(['Country', 'year', 'Var1'])\nresult = result.unstack('Var1')\nresult = result.reset_index()\nresult = result.rename(columns={'level_0': 'Variable', 'year': 'var1', 'Var1': 'value'})\nresult = result.drop('value', axis=1)\nresult = result.sort_values(['Country', 'var1'])\nresult = result.reset_index(drop=True)\nprint(result)",
        "result = df[df.sum(axis=1) < 1]\nprint(result)",
        "filtered_df = df[df.sum(axis=1).abs() > 1]\nresult = filtered_df\nprint(result)",
        "result = df[df.abs().gt(1).any(axis=1) & (df.columns.str.startswith('Value'))]\nresult.columns = result.columns.str.replace('_', '')\nprint(result)",
        "df['A'] = df['A'].str.replace('&AMP;', '&')\ndf['C'] = df['C'].str.replace('&AMP;', '&')\nresult = df\nprint(result)",
        "df['A'] = df['A'].str.replace('&LT;', '<')\ndf['C'] = df['C'].str.replace('&LT;', '<')\nresult = df\nprint(result)",
        "\nexample_df = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\nresult = f(example_df)\nprint(result)\n",
        "df = df.replace('&AMP;', '&''<''>', regex=True)\ndf = df.replace('&LT;', '&''<''>', regex=True)\ndf = df.replace('&GT;', '&''<''>', regex=True)\nresult = df\nprint(result)",
        "df['A'] = df['A'].str.replace('&AMP;', '&')\ndf['C'] = df['C'].str.replace('&AMP;', '&')\nresult = df\nprint(result)",
        "\n",
        "\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\nresult = df.apply(lambda row: validate_single_space_name(row['name']), axis=1)\nresult = result.loc[result['name'].notnull(), 'name']\nresult = result.dropna(axis=1, how='all')\nresult = result.set_index('name')\nresult = result.unstack(level=0)\nresult.columns = ['first_name', 'middle_name', 'last_name']\nresult = result.reset_index()\nprint(result)\n",
        "result = df1.merge(df2, on='Timestamp', how='left')\nprint(result)",
        "df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\nresult = df1.merge(df2, on='Timestamp', how='left')\nprint(result)",
        "result['state'] = df.col1 if (df.col2 <= 50) and (df.col3 <= 50) else max(df.col1, df.col2, df.col3)\nprint(result)",
        "\ndef check_state(col1, col2, col3):\n    if col2 > 50 and col3 > 50:\n        return col1\n    else:\n        return col1 + col2 + col3\nresult['state'] = df.apply(lambda row: check_state(row['col1'], row['col2'], row['col3']), axis=1)\nprint(result)",
        "\ndef check_integer(row):\n    for value in row:\n        if not value.is_integer():\n            error_values.append(value)\nresult = df.apply(check_integer, axis=1)\nprint(result)\ndef check_integer(row):\n    for value in row:\n        if not value.is_integer():\n            error_values.append(value)\nresult = df.apply(check_integer, axis=1)\nprint(result)\n[Instruction]",
        "for index, row in df.iterrows():\n    if pd.isnumeric(row[\"Field1\"]) and not pd.isna(row[\"Field1\"]):\n        result.loc[index, \"Field1\"] = int(row[\"Field1\"])\nresult = result[result[\"Field1\"].astype(int).notnull()]\nprint(result)\nfor index, row in df.iterrows():\n    if pd.isnumeric(row[\"Field1\"]) and not pd.isna(row[\"Field1\"]):\n        result.loc[index, \"Field1\"] = int(row[\"Field1\"])\nresult = result[result[\"Field1\"].astype(int).notnull()]\nprint(result)\n[2, 1, 25]",
        "\ndef f(df=example_df):\n    result = []\n    for index, row in df.iterrows():\n        value = row[\"Field1\"]\n        if not pd.isnumeric(value):\n            result.append(value)\n    return result\nprint(f())\n",
        "result['val1'] /= result['val1'].sum()\nresult['val2'] /= result['val2'].sum()\nresult['val3'] /= result['val3'].sum()\nresult['val4'] /= result['val4'].sum()\nresult['val1'] *= 100\nresult['val2'] *= 100\nresult['val3'] *= 100\nresult['val4'] *= 100\nresult = result.set_index('cat')\nresult = result.unstack(fill_value=0)\nresult = result.round(2)\nresult = result.reset_index()\nresult\n",
        "result['percentage'] = result.apply(lambda row: (row['val1'] + row['val2'] + row['val3'] + row['val4']) * 1.0 / (row['val1'] + row['val2'] + row['val3'] + row['val4']), axis=1)\nresult = result.set_index('cat')\nresult['percentage'] = result['percentage'].astype(float) / result['percentage'].sum(axis=1) * 100\nresult = result.reset_index()\nresult\n",
        "\nimport pandas as pd\nimport io\ndata = io.StringIO(\"\"\"\nrs  alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP7      A/T      0    7      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP15     C/A      0   15      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n\"\"\")\ndf = pd.read_csv(data, delim_whitespace=True).set_index('rs')\ntest = ['TP3', 'TP7', 'TP18']\nresult = df.select(test)\nprint(result)\n",
        "result = df.loc[test]\nprint(result)",
        "df = df[~df.index.isin(test)]\nprint(df)",
        "df.loc[test]",
        "\nimport pandas as pd\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\ndef get_nearest_neighbour(car, df):\n    return df.loc[df['car'] == car, 'car'].iloc[0]\ndef calculate_euclidean_distance(car, df):\n    nearest_neighbour = get_nearest_neighbour(car, df)\n    return (df['x'][car] - df['x'][nearest_neighbour]) ** 2 + (df['y'][car] - df['y'][nearest_neighbour]) ** 2\ndf['nearest_neighbour'] = df.apply(lambda row: get_nearest_neighbour(row['car'], df), axis=1)\ndf['euclidean_distance'] = df.apply(lambda row: calculate_euclidean_distance(row['car'], df), axis=1)\ndf2 = df.groupby('time')['euclidean_distance'].mean()\ndf2\n",
        "\ndef get_farmost_neighbour(car, df):\n    farmost_neighbour = 0\n    euclidean_distance = 0\n    for i in range(len(df)):\n        if car != df.iloc[i]['car']:\n            x_diff = df.iloc[i]['x'] - df.iloc[car]['x']\n            y_diff = df.iloc[i]['y'] - df.iloc[car]['y']\n            distance = math.sqrt(x_diff ** 2 + y_diff ** 2)\n            if distance > euclidean_distance:\n                farmost_neighbour = i\n                euclidean_distance = distance\n    return farmost_neighbour, euclidean_distance\ndf2 = df.copy()\nfor i, car in enumerate(df['car']):\n    farmost_neighbour, euclidean_distance = get_farmost_neighbour(i, df)\n    df2.loc[i, 'farmost_neighbour'] = farmost_neighbour\n    df2.loc[i, 'euclidean_distance'] = euclidean_distance\ndf2\n",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\nresult = df\nprint(result)",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\nresult = df\nprint(result)",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\nresult = df\nprint(result)",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\nresult = df\nprint(result)",
        "random_state=0,\nn=int(len(df) * 0.2)\nsampled_rows = df.sample(n, random_state=0)\nsampled_rows.Quantity = 0\nresult = df.loc[sampled_rows.index]\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\nrandom_state=0,\nn=0.2,\nresult = df.sample(n, random_state=random_state)\nresult['ProductId'] = result['ProductId'].mask(result['ProductId'] == 0)\nresult = result.reset_index(drop=True)\nprint(result)\n",
        "result = df.sample(int(len(df) * 0.2))\nresult['Quantity'] = result['Quantity'] * 0\nresult = result.reset_index(drop=True)\nprint(result)",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate.assign(index_original=duplicate_bool)\nprint(result)\n",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate.assign(index_original=duplicate_bool)\nprint(result)\n",
        "\ndef f(df=example_df):\n    duplicate_bool = df.duplicated(subset=['col1', 'col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    index_original = duplicate.index.repeat(duplicate['col1'].value_counts().max())\n    duplicate['index_original'] = index_original\n    return duplicate\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate.assign(index_original=lambda x: x.index[0])\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate.assign(index_original=lambda x: x.index.max())\n",
        "result = df.groupby(['Sp', 'Mt'])['count'].max()\nresult = result.reset_index()\nresult = result[result['count'] == result['count']]\nprint(result)",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\nprint(result)\n",
        "result = df.groupby(['Sp', 'Mt'])['count'].min()\nresult = result.reset_index()\nresult = result[result['count'] == result['count']]\nprint(result)",
        "result = df.groupby(['Sp', 'Value'])['count'].max()\nresult = result.reset_index()\nprint(result)",
        "\nimport pandas as pd\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\ndf.query(\"Catergory==filter_list\")\nprint(result)\n",
        "\nimport pandas as pd\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\ndf.query(\"Catergory!=filter_list\")\nprint(result)\n",
        "value_vars = [('A', 'B', 'E'), ('A', 'B', 'F'), ('A', 'C', 'G'), ('A', 'C', 'H'), ('A', 'D', 'I'), ('A', 'D', 'J')]\nresult = pd.melt(df, value_vars=value_vars)\nprint(result)",
        "result = df.melt(id_vars=['variable_0', 'variable_1', 'variable_2'], value_vars=df.columns)\nprint(result)",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\nresult = df\nresult['cumsum'] = df['val'].cumsum()\nresult = result.sort_values(['id', 'cumsum']).reset_index(drop=True)\nprint(result)\n",
        "\n",
        "\n# [Missing Code]\n",
        "\n",
        "\nimport pandas as pd\nimport numpy as np\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if not x.isna().any() else np.nan)\nprint(result)\n",
        "\nr\nright   -3.0\n",
        "\n",
        "\ndef analyze_relationships(df):\n    relationships = []\n    for column in df.columns:\n        for other_column in df.columns:\n            if column != other_column:\n                if df[column].isin(df[other_column]).sum() == len(df[other_column]):\n                    relationships.append(f\"{column} {other_column} one-to-many\")\n                elif df[column].isin(df[other_column]).sum() == 1:\n                    relationships.append(f\"{column} {other_column} many-to-one\")\n                elif df[column].isin(df[other_column]).sum() > 1:\n                    relationships.append(f\"{column} {other_column} many-to-many\")\n    return relationships\nresult = analyze_relationships(df)\n",
        "\ndef relationship_type(df, columns):\n    relationships = []\n    for column1 in columns:\n        for column2 in columns:\n            if column1 != column2:\n                if column1.value_counts().unique().shape[0] == 2:\n                    relationships.append(f\"{column1} {column2} {relationship_type(column1.value_counts())}\")\n                else:\n                    relationships.append(f\"{column1} {column2} {relationship_type(column1.value_counts())}\")\n    return relationships\nresult = relationship_type(df, df.columns)\n",
        "def analyze_relationships(df):\n    relationships = {\n        'one-to-one': 0,\n        'one-to-many': 1,\n        'many-to-one': 2,\n        'many-to-many': 3,\n    }\n    for column in df.columns:\n        count_one_to_one = df[column].value_counts()[0]\n        count_one_to_many = df[column].value_counts()[1]\n        count_many_to_one = df[column].value_counts()[2]\n        count_many_to_many = df[column].value_counts()[3]\n        relationships[column] = (count_one_to_one + count_one_to_many + count_many_to_one + count_many_to_many) / 4\n    return pd.DataFrame.from_dict(dict(df.items()), orient='index', columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5'])\nresult = analyze_relationships(df)\nprint(result)",
        "\ndef analyze_relationships(df):\n    relationships = {\n        'one-2-one': 0,\n        'one-2-many': 0,\n        'many-2-one': 0,\n        'many-2-many': 0,\n    }\n    for column in df.columns:\n        for other_column in df.columns:\n            if df[column].isin(df[other_column]).sum() == len(df[other_column]):\n                relationships['one-2-many'] += 1\n            elif df[column].isin(df[other_column]).sum() == 1:\n                relationships['one-2-one'] += 1\n            elif df[other_column].isin(df[column]).sum() == len(df[column]):\n                relationships['many-2-one'] += 1\n            elif df[other_column].isin(df[column]).sum() >= 2:\n                relationships['many-2-many'] += 1\n    result = pd.DataFrame.from_dict(relationships, orient='index', columns=df.columns)\n    return result\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\nresult = analyze_relationships(df)\nprint(result)",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar'],\n                   'bank': [np.nan, 'abc', 'xyz']})\ndf = df[df['bank'].notnull()]\nresult = df[['firstname', 'lastname', 'email', 'bank']]\nprint(result)\n",
        "\nimport pandas as pd\ns = pd.Series(['2,144.78', '2,036.62', '1,916.60', '1,809.40', '1,711.97', '6,667.22', '5,373.59', '4,071.00', '3,050.20', '-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0'],\n              index=['2016-10-31', '2016-07-31', '2016-04-30', '2016-01-31', '2015-10-31', '2016-01-31', '2015-01-31', '2014-01-31', '2013-01-31', '2016-09-30', '2016-06-30', '2016-03-31', '2015-12-31', '2015-09-30', '2015-12-31', '2014-12-31', '2013-12-31', '2012-12-31'])\nresult = pd.to_numeric(s.astype(str).str.replace(',', ''), errors='coerce')\nprint(result)\n",
        "\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\nresult = df.groupby([\"Has Family\", \"No Family\"]).agg({\"SibSp\": \"mean\", \"Survived\": \"mean\", \"Parch\": \"mean\"})\nprint(result)\n",
        "\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'cokey':[11168155,11168155,11168155,11168156,11168156],\n                   'A':[18,0,56,96,0],\n                   'B':[56,18,96,152,96]})\nresult = df.groupby('cokey').sort_values(['A', 'B']).reset_index()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'cokey':[11168155,11168155,11168155,11168156,11168156],\n                   'A':[18,0,56,96,0],\n                   'B':[56,18,96,152,96]})\nresult = df.groupby('cokey').sort_values(['A', 'B']).reset_index()\nprint(result)\n",
        "result = df.set_index(['Caps', 'Lower', 'A', 'B'])\nresult = result.unstack(level=3)\nresult.columns = result.columns.map(''.join)\nresult = result.sort_index()\nprint(result)",
        "\nimport pandas as pd\nimport numpy as np\nl = [('A', '1', 'a'),  ('A', '1', 'b'), ('A', '2', 'a'), ('A', '2', 'b'), ('B', '1','a'),  ('B', '1','b')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\nresult = pd.DataFrame(df.values, columns=pd.MultiIndex.from_tuples(l, names=['Caps', 'Middle', 'Lower']))\nprint(result)\n",
        "\n",
        "\nimport numpy as np\nimport pandas as pd\nnp.random.seed(123)\nbirds = np.random.choice(['African Swallow', 'Dead Parrot', 'Exploding Penguin'], size=int(5e4))\nsomeTuple = np.unique(birds, return_counts=True)\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\nresult = df.groupby('a').b.apply(stdMeann)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\nresult = df.groupby('b').a.apply(stdMeann)\nprint(result)\n",
        "\ndef softmax(values):\n    max_value = max(values)\n    for value in values:\n        value /= max_value\n    return values\ndef min_max_normalization(values):\n    min_value = min(values)\n    max_value = max(values)\n    for value in values:\n        value -= min_value\n        value /= (max_value - min_value)\n    return values\nresult = df.groupby('a')['b'].apply(lambda x: (softmax(x), min_max_normalization(x))).reset_index()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,1,0,1],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\nresult = df[df.sum(axis=1) != 0]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[-1,-1,0,2],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\nresult = df[df.sum(axis=1) != 0]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n          index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\ns = s.sort_values(by=['index', 'value'], ascending=[True, True])\nprint(s)\n",
        "\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\nresult = df[df['A'].astype(str).str.isdigit()]\nprint(result)\n",
        "\n",
        "result = df.groupby(['Sp', 'Mt'])['count'].max()\nresult = result.reset_index()\nresult = result[result['count'] == result['count']]\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp':['MM2','MM2','MM4','MM4','MM4'],\n                   'Mt':['S4','S4','S2','S2','S2'],\n                   'Value':['bg','dgd','rd','cb','uyi'],\n                   'count':[10,1,2,8,8]})\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\nprint(result)\n",
        "result = df.groupby(['Sp', 'Mt'])['count'].min()\nresult = result.reset_index()\nresult = result[result['count'] == result['count']]\nprint(result)",
        "result = df.groupby(['Sp', 'Value'])['count'].max()\nresult = result.reset_index()\nprint(result)",
        "\nimport pandas as pd\nimport numpy as np\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\nresult = df\nresult['Date'] = df['Member'].map(dict)\nresult['Date'] = result['Date'].fillna(df['Date'])\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\nresult = df\nresult['Date'] = df['Member'].map(dict)\nresult['Date'] = result['Date'].fillna(df['Date'])\nprint(result)\n",
        "\n    result['Date'] = df['Member'].map(dict)\n    result['Date'].fillna(method='ffill', inplace=True)\n    ",
        "result['Date'] = df['Member'].map(dict)\nresult['Date'] = result['Date'].fillna(df['Date'])\nresult = result.dropna()\nprint(result)",
        "df1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count': 'count'})\nprint(df1)\n",
        "df1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count': ['Val', 'count']})\nprint(df1)\n",
        "\n# [Missing Code]\n",
        "\nresult1 = df.loc[df['B'] == 0, 'B'].sum()\nresult2 = df.loc[df['B'] != 0, 'B'].sum()\nresult1 = df.loc[df['C'] == 0, 'C'].sum()\nresult2 = df.loc[df['C'] != 0, 'C'].sum()\n",
        "\nresult1 = df.B.eq(0).sum()\nresult2 = df.B.eq(1).sum()\n",
        "",
        "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.mean)\nprint(result)",
        "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=lambda x: np.sum(x) if x == 'D' else np.mean(x) if x == 'E' else x)\nprint(result)",
        "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=lambda x: max(x) if x[0] == 'D' else min(x))",
        "result = df.explode('var2')\nprint(result)",
        "result = df.explode('var2')\nprint(result)",
        "result = df.explode('var2')\nprint(result)",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n",
        "\n",
        "\n",
        "result['2001'] = df['2001'].astype(float) / (df['2001'].astype(float).notnull().cumsum())\nresult['2002'] = df['2002'].astype(float) / (df['2002'].astype(float).notnull().cumsum())\nresult['2003'] = df['2003'].astype(float) / (df['2003'].astype(float).notnull().cumsum())\nresult['2004'] = df['2004'].astype(float) / (df['2004'].astype(float).notnull().cumsum())\nresult['2005'] = df['2005'].astype(float) / (df['2005'].astype(float).notnull().cumsum())\nresult['2006'] = df['2006'].astype(float) / (df['2006'].astype(float).notnull().cumsum())\nresult = result.fillna(0)\nprint(result)\n",
        "result['cumulative_average'] = result.iloc[::-1].agg(lambda x: x.mean() if x.sum() > 0 else np.nan, axis=1)\nresult['cumulative_average'] = result['cumulative_average'].fillna(method='ffill')\nresult['cumulative_average'] = result['cumulative_average'].astype(float)\nresult = result.set_index('Name')\nresult = result.T\nresult.columns = ['2001', '2002', '2003', '2004', '2005', '2006']\nresult = result.reset_index()\nresult\n",
        "cumulative_avg = df.groupby('Name')['2001', '2002', '2003', '2004', '2005', '2006'].transform('sum') / df.groupby('Name')['2001', '2002', '2003', '2004', '2005', '2006'].transform('count')\nresult = df.assign(cumulative_avg=cumulative_avg)\nreturn result",
        "\n# [Missing Code]\n",
        "\n",
        "\n# [Missing Code]\n",
        "\n",
        "df['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i] \nresult = df\nprint(result)",
        "df['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i] \nresult = df\nprint(result)",
        "df['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\nresult['Duration'] = result['Duration'].astype('timedelta64[s]')\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])\nresult.dropna(inplace=True)\nresult.reset_index(inplace=True, drop=True)\nresult = result.sort_values(by=['id', 'arrival_time'])",
        "\nimport pandas as pd\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\nresult = df.groupby(['key1']).apply(lambda x: x['key2'].eq('one').sum())\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\nresult = df.groupby(['key1']).apply(lambda x: x.loc[x['key2'] == 'two'].count())\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'gee', 'two', 'three', 'two']})\nresult = df.groupby(['key1']).apply(lambda x: x.key2.count() if x.key2.endswith(\"e\") else 0)\nprint(result)\n",
        "\n",
        "\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\ndf = df[(99 <= df['closing_price'] <= 101)]\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\ndf = df[~(99 <= df['closing_price'] <= 101)]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n",
        "\n# [Missing Code]\n",
        "result = df['SOURCE_NAME'].str.split('_', expand=True)\nresult = result.iloc[:, 0]\nresult = result.astype(str)\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]\nresult = result.str.replace('_', '')\nresult = result.str.split('_', expand=True)[0]",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\ndef fill_nan_values(df, first_half_value, second_half_value):\n    half_count = int(len(df) / 2)\n    df['Column_x'] = df['Column_x'].mask(df['Column_x'].isna(), first_half_value)\n    df['Column_x'] = df['Column_x'].mask(df['Column_x'].isna(), second_half_value)\n    return df\ndf = fill_nan_values(df, 0, 1)\nprint(df)\n",
        "\ndef fill_nan_values(df, fraction_0=0.3, fraction_0_5=0.3, fraction_1=0.4):\n    total_nan_values = df.isnull().sum()\n    nan_values_0 = int(total_nan_values * fraction_0)\n    nan_values_0_5 = int(total_nan_values * fraction_0_5)\n    nan_values_1 = int(total_nan_values * fraction_1)\n    df['Column_x'] = df['Column_x'].fillna(\n        {0: nan_values_0, 0.5: nan_values_0_5, 1: nan_values_1}\n    )\n    return df\ndf = pd.DataFrame({'Column_x': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]})\nresult = fill_nan_values(df)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,1,1,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\ndef fill_nan_with_50_percent_each(df, column_name):\n    total_values = df[column_name].count()\n    half_total_values = total_values // 2\n    df[column_name] = df[column_name].replace(np.nan, np.nan)\n    for i in range(half_total_values):\n        df.loc[df[column_name] == 0, column_name] = 0\n        df.loc[df[column_name] == 1, column_name] = 1\n    return df\ndf = fill_nan_with_50_percent_each(df, 'Column_x')\nprint(df)\n",
        "\n",
        "result = pd.concat([a, b, c], axis=1)\nresult = result.apply(lambda x: tuple(x), axis=1)\nprint(result)",
        "\n",
        "groups = groups.reset_index()\nresult = groups.pivot(index='username', columns='views', values='count')\nresult.fillna(0, inplace=True)\nresult.iloc[:, 1:5] = result.iloc[:, 1:5].astype(int)\nprint(result)",
        "\n",
        "\n[Output]\n{1: 4, 10: 1, 25: 1, 50: 1, 100: 1}\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].agg(lambda x: ', '.join(x))\nprint(result)\n",
        "result = df['text'].agg('-'.join)\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].agg(lambda x: ', '.join(x))\nprint(result)\n",
        "\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep='-')\nprint(result)\n",
        "result = pd.concat([df1, df2], axis=0)\nprint(result)",
        "result = pd.concat([df1, df2], axis=0)\nresult['date'] = result['date'].str.replace(' ', '')\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date']).reset_index(drop=True)\nprint(result)",
        "\n",
        "C = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = pd.merge(C, D, how='outer', on='A')\nresult.loc[result['B_x'].isna(), 'B'] = result['B_y']\nprint(result)\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = pd.merge(C, D, how='outer', on='A')\nresult.loc[result['B_x'].isna(), 'B'] = result['B_y']\nprint(result)",
        "C = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B'] = result['B'].fillna(method='ffill')\nprint(result)\n[Output]",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = C.merge(D, how='outer', on='A', suffixes=('', '_x'))\nresult['B_y'] = result['B_x']\nresult['dulplicated'] = (result['A'] != result['A_x']).astype(int)\nresult = result[['A', 'B', 'dulplicated']]\nprint(result)\n",
        "\n",
        "\n",
        "\n",
        "\nimport pandas as pd\nimport numpy as np\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\nresult = pd.concat([series], axis=1, keys=series.index)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\nresult = pd.concat([series], axis=1, keys=series.index)\nprint(result)\n",
        "result = [row for row in df.columns if s in row and not row.startswith(s + '-')]\nprint(result)",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\nresult = [row for row in df.iterrows() if s in row[1]]\nreturn pd.DataFrame(result, columns=df.columns)\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\nresult = [row for row in df.iterrows() if s in row[1]]\nprint(result)\n",
        "result = df.explode('codes')\nresult['code_0'] = result['codes'].str[0]\nresult['code_1'] = result['codes'].str[1]\nresult['code_2'] = result['codes'].str[2]\nresult['code_3'] = result['codes'].str[3]\nresult['code_4'] = result['codes'].str[4]\nresult['code_5'] = result['codes'].str[5]\nresult['code_6'] = result['codes'].str[6]\nresult['code_7'] = result['codes'].str[7]\nresult['code_8'] = result['codes'].str[8]\nresult['code_9'] = result['codes'].str[9]\nresult['code_10'] = result['codes'].str[10]\nresult['code_11'] = result['codes'].str[11]\nresult['code_12'] = result['codes'].str[12]\nresult['code_13'] = result['codes'].str[13]\nresult['code_14'] = result['codes'].str[14]\nresult['code_15'] = result['codes'].str[15]\nresult['code_16'] = result['codes'].str[16]\nresult['code_17'] = result['codes'].str[17]\nresult['code_18'] = result['codes'].str[18]\nresult['code_19'] = result['codes'].str[19]\nresult['code_20'] = result['codes'].str[20]\nresult['code_21'] = result['codes'].str[21]\nresult['code_22'] = result['codes'].str[22]\nresult['code_23'] = result['codes'].str[23]\nresult['code_24'] = result['codes'].str[24]\nresult['code_25'] = result['codes'].str[25]\nresult['code_26'] = result['codes'].str[26]\nresult['code_27'] = result['codes'].str[27]\nresult['code_28'] = result['codes'].str[28]\nresult['code_29'] = result['codes'].str[29]\nresult['code_30'] = result['codes'].str[30]\nresult['code_31'] = result['codes'].str[31]\nresult['code_32'] = result['codes'].str[32]\nresult['code_33'] = result['codes'].str[33]\nresult['code_34'] = result['codes'].str[34]\nresult['code_35'] = result['codes'].str[35]\nresult['code_36'] = result['codes'].str[36]\nresult['code_37'] = result['codes'].str[37]\nresult['code_38'] = result['codes'].str[38]\nresult['code_39'] = result['codes'].str[39]\nresult['code_40'] = result['codes'].str[40]\nresult['code_41'] = result['codes'].str[41]\nresult['code_42'] = result['codes'].str[42]\nresult['code_43'] = result['codes'].str[43]\nresult['code_44'] = result['codes'].str[44]\nresult['code_45'] = result['codes'].str[45]\nresult['code_46'] = result['codes'].str[46]\nresult['code_47'] = result['codes'].str[47]\nresult['code_48'] = result['codes'].str[48]\nresult['code_49'] = result['codes'].str[49]\nresult['code_50'] = result['codes'].str[50]\nresult['code_51'] = result['codes'].str[51]\nresult['code_52'] = result['codes'].str[52]\nresult['code_53'] = result['codes'].str[53]\nresult['code_54'] = result['codes'].str[54]\nresult['code_55'] = result['codes'].str[55]\nresult['code_56'] = result['codes'].str[56]",
        "\nimport pandas as pd\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\nresult = df.codes.apply(lambda x: pd.Series(x), axis=1)\ndf = df.join(result)\ndf.fillna(method='ffill', axis=1, inplace=True)\nprint(df)\n",
        "result = df.codes.apply(pd.Series)\nresult = result.stack()\nresult.name = 'codes'\nresult = result.reset_index(drop=True)\nresult.columns = ['code_1', 'code_2', 'code_3']\nresult = result.fillna(method='ffill')\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\nresult = df.col1.apply(lambda x: list(x))\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3],[4,5]]))\nresult = ''.join(str(x) for x in df.col1.values)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\nresult = ''.join(str(x) for x in df.col1)\nprint(result)\n",
        "result = df.groupby(result.Time.dt.floor('2 minutes')).mean()\nresult = result.reset_index()\nresult['Time'] = pd.to_datetime(result['Time'])\nprint(result)",
        "result = df.groupby(result.Time.dt.floor('3T')).mean()\nresult = result.reset_index()\nresult['Time'] = result['Time'].dt.round('3S')\nresult.dropna(inplace=True)\nresult.sort_values(by=['Time'], inplace=True)\nresult.reset_index(inplace=True)\nprint(result)\n",
        "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\nresult = df\nprint(result)",
        "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\nresult = df\nprint(result)",
        "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['RANK'] = df['RANK'].astype(str)\ndf['RANK'] = df['RANK'].str.zfill(3)\ndf['RANK'] = df['RANK'].astype(str) + '-' + df['TIME'].astype(str)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %H:%M:%S')\nresult = df\nprint(result)",
        "result = df[filt]\nprint(result)",
        "df[filt]",
        "\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n",
        "\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\nresult = df.iloc[0].equals(df.iloc[8])\nprint(result)\n",
        "\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\nresult = df.iloc[0, :].eq(df.iloc[8, :]).astype(bool)\nprint(result)\n",
        "\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\ndef compare_columns(row_0, row_8, df):\n    result = []\n    for col in range(df.shape[1]):\n        if df.iloc[0, col] != df.iloc[8, col]:\n            result.append((df.iloc[0, col], df.iloc[8, col]))\n    return result\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\nresult = compare_columns(0, 8, df)\nprint(result)\n",
        "ts = df['Value']\nts.index = df['Date']\nresult = ts\nprint(result)",
        "\n# [Missing Code]\n",
        "result = df.stack()\nresult = result.reset_index(level=0, drop=True)\nresult = result.rename(columns={0: 'A_0', 1: 'B_0', 2: 'C_0', 3: 'D_0', 4: 'E_0', 5: 'A_1', 6: 'B_1', 7: 'C_1', 8: 'D_1', 9: 'E_1', 10: 'A_2', 11: 'B_2', 12: 'C_2', 13: 'D_2', 14: 'E_2'})\nprint(result)",
        "df['dogs'] = df['dogs'].round(2)\nresult = df\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\nresult = df\nprint(result)\n",
        "df['Sum'] = df[list_of_my_columns].sum(axis=1)",
        "df['Avg'] = df[list_of_my_columns].mean(axis=1)",
        "df['Avg'] = df[list_of_my_columns].mean(axis=1)\nresult = df\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\ndf = df.sort_values(by=['treatment', 'dose', 'time'], ascending=[True, True, False])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\ndf.sort_index(axis=1, level=1, inplace=True)\nprint(df)\n",
        "df = df[~df.index.isin(['2020-02-17', '2020-02-18'])]\nprint(df)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\nresult = df.loc[~(df.index.date.isin(['2020-02-17', '2020-02-18']))]\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\nresult = df[np.where(corr > 0.3, True, False)]\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\nresult = df.filter(lambda x: x >= 0.3)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\ndf.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nprint(df)\n",
        "\ndef find_frequent_value(row):\n    frequent = 0\n    freq_count = 0\n    for bit in row:\n        if bit == 1:\n            frequent = 1\n            freq_count += 1\n    return frequent, freq_count\ndef create_frequent_freq_count_columns(data):\n    data['frequent'] = data.apply(lambda row: find_frequent_value(row), axis=1)\n    data['freq_count'] = data['frequent'].map(lambda x: x[1])\n    return data\ndata = pd.read_csv('myData.csv', sep=',')\nresult = create_frequent_freq_count_columns(data)\nprint(result)",
        "\ndef find_frequent_value(row):\n    max_freq_count = 0\n    frequent_value = 0\n    for value in row:\n        if value != frequent_value:\n            freq_count = freq_count + 1\n            if freq_count > max_freq_count:\n                max_freq_count = freq_count\n                frequent_value = value\n    return frequent_value, max_freq_count\ndef find_frequent_values(data):\n    data['frequent'] = data.apply(lambda row: find_frequent_value(row), axis=1)\n    data['freq_count'] = data['frequent'].apply(lambda x: x[1])\n    return data\ndata = pd.read_csv('myData.csv', sep=',')\nresult = find_frequent_values(data)\nprint(result)\n",
        "\ndef find_frequent_values(data):\n    frequent_values = []\n    freq_count = {}\n    for index, row in data.iterrows():\n        for column in row:\n            if column in freq_count:\n                freq_count[column] += 1\n            else:\n                freq_count[column] = 1\n        if len(frequent_values) < 3:\n            frequent_values.append(list(freq_count.keys()))\n    return pd.DataFrame({'frequent': frequent_values, 'freq_count': freq_count})\nresult = find_frequent_values(df)\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nprint(result)\n",
        "\n",
        "\nresult = df_c[['EntityNum', 'foo', 'a_col']]\n",
        "df_c = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = df_c[['EntityNum', 'foo', 'b_col']]\nprint(result)"
    ],
    "Numpy": [
        "result = a.shape\nprint(result)",
        "\nimport numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = x[~np.isnan(x)]\nprint(x)\n",
        "\nimport numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx[~np.isfinite(x)] = np.inf\nprint(x)\n",
        "\nx = x.astype(float).copy()\nx[0] = [float(num) for num in x[0] if num != np.nan]\nx[1] = [float(num) for num in x[1] if num != np.nan]\nresult = x.tolist()\nprint(result)\n",
        "\n",
        "b = np.zeros((len(a) + 1, a.max() + 1))\nb[np.arange(len(a)), a] = 1\nprint(b)\n",
        "\n",
        "\nimport numpy as np\na = np.array([1.5, -0.4, 1.3])\nb = np.zeros((len(a) + 1, 3))\nb[np.arange(len(a)), a] = 1\nprint(b)\n",
        "b = np.zeros((len(a) + 1, len(a[0]) + 1))\nfor i, row in enumerate(a):\n    for j, value in enumerate(row):\n        b[i, j + 1] = 1\nprint(b)",
        "\nimport numpy as np\na = np.array([1,2,3,4,5])\np = 25\nresult = np.percentile(a, p)\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nncol = 2\nB = A[::ncol, :ncol]\nprint(B)\n",
        "B = A[::nrow, :]\nprint(B)",
        "B = A.reshape((-1, ncol))\nprint(B)",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = A.reshape((-1, ncol))\nprint(B)\n",
        "result = np.roll(a, shift)\nprint(result)",
        "result = np.roll(a, shift, axis=0)\nprint(result)",
        "\nimport numpy as np\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = [-2, 3]\nresult = np.roll(a, shift=shift, axis=0)\nprint(result)\n",
        "\nimport numpy as np\nr = np.random.randint(3, size=(100, 2000)) - 1\nr_old, r_new = generate_same_random_array(r)\nprint(r_old, r_new)\ndef generate_same_random_array(r):\n    r_old = r\n    r_new = r\n    return r_old, r_new\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a, axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = a.argmin(axis=0)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a, axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a, axis=1)\nprint(result)\n",
        "\ndef f(a = example_a):\n    result = a.max(axis=0)\n    return result.argmax()\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = a.argmax(axis=1) - 1\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\na[z == False, :] = a[z == False, :]\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\na = a[~np.isnan(a).all(axis=1)]\nprint(a)\n",
        "\nimport numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \na = np.array(a)\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[10, 20, 30, 40, 50],\n       [ 6,  7,  8,  9, 10]])\npermutation = [0, 4, 1, 3, 2]\na = a[::-1, :]\na = a[np.array(permutation, dtype=np.int64), :]\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[[10, 20],\n        [30, 40]],\n       [[6,  7],\n        [8,  9]],\n\t[[10, 11],\n\t [12, 13]]])\npermutation = [1, 0, 2]\nresult = np.moveaxis(a, permutation, 0)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nresult = np.unravel_index(np.argmin(a), a.shape)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nresult = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n",
        "\n",
        "result = np.sin(degree)\nprint(result)",
        "result = np.cos(degree)\nresult = np.degrees(result)\nprint(result)",
        "result = 0\nif number < 90:\n    result = 1\nelif number < 180:\n    result = 0\nelse:\n    result = 1\nprint(result)",
        "\nimport numpy as np\nvalue = 1.0\nresult = np.arcsin(value) * 180 / np.pi\nif result < 0:\n    result += 180\nprint(result)\n",
        "result = np.pad(A, (length // A.size + (length % A.size > 0)), 'constant', constant_values=0)\nprint(result)",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.pad(A, (length - len(A)), '0')\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(4).reshape(2, 2)\npower = 5\nresult = a**power\nprint(result)\n",
        "\ndef f(a = example_a, power = 5):\n    result = a**power\n    return result\n",
        "from fractions import gcd\nresult = (numerator // denominator, denominator // gcd(numerator, denominator))\nprint(result)",
        "from fractions import gcd\ndef f(numerator = 98, denominator = 42):\n    gcd = gcd(numerator, denominator)\n    return (numerator // gcd, denominator // gcd) if gcd != 1 else (numerator, denominator)\n",
        "\nresult = (7, 3)\n",
        "\nimport numpy as np\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = (a + b + c) / 3\nprint(result)\n",
        "result = np.maximum(a, b)\nresult = np.maximum(result, c)\n",
        "result = a[np.ix_(a.shape[-1] - 1, 0), np.arange(a.shape[0])]\nprint(result)",
        "result = a[np.arange(a.shape[0] - 1, -1, -1), np.arange(a.shape[1] - 1, -1, -1)]\nprint(result)",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\nresult = a[np.ix_(diagonal[::-1], diagonal)]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\nresult = a[np.ix_(diagonal[0], diagonal[1])]\nprint(result)\n",
        "result = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\nprint(result)",
        "result = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\nprint(result)",
        "result = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\nreturn result",
        "result = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\nprint(result)",
        "result = np.fromstring(mystr, dtype=int, sep='')\n",
        "result = a[:, col] * multiply_number\nresult = np.cumsum(result)\n",
        "result = a[row] * multiply_number\ncumulative_sum = np.cumsum(result)\nprint(cumulative_sum)\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\nresult = a[row] * multiply_number\ncumulative_sum = np.cumsum(result)\nprint(cumulative_sum)",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\ndivide_number = 5.2\nresult = a[row] / divide_number\nresult = np.sum(result)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\nresult = np.linalg.matrix_rank(a)\nprint(result)\n",
        "\nrow_size = len(a)\nresult = row_size\n",
        "\nimport numpy as np\nimport scipy.stats\na = np.random.randn(40)\nb = 4*np.random.randn(50)\np_value = scipy.stats.ttest_2tailed(a, b, equal_var=False)\nprint(p_value)\n",
        "p_value = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')[2]",
        "\nimport numpy as np\nimport scipy.stats\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\np_value = scipy.stats.ttest_ind(amean, anobs, bmean, bvar, bnobs, equal_var=False)\nprint(p_value)\n",
        "\nimport numpy as np\nA=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\noutput = np.setxor1d(A, B, A.dtype)\nprint(output)\n",
        "\n# [Missing Code]\n",
        "\nsort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "sort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)",
        "result = np.argsort(b, axis=0, kind='mergesort')\nprint(result)",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\na[:, 1] = 0\nprint(a)\n",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\na = a[:2, :]\nprint(a)\n",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\na[:, 1] = a[:, 1] + 1\na[:, 3] = a[:, 3] + 1\nprint(a)\n",
        "\nimport numpy as np\na = np.arange(12).reshape(3, 4)\ndel_col = np.array([1, 2, 4, 5])\nresult = a[:, ~np.isin(np.arange(a.shape[1]), del_col)]\nprint(result)\n",
        "a.insert(pos, element)",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\npos = 1\nelement = [3,5]\na[pos] = element\nprint(a)\n",
        "\nimport numpy as np\nexample_a = np.asarray([1,2,3,4])\ndef f(a = example_a, pos=2, element = 66):\n    a[pos] = element\n    return a\n",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\npos = [1, 2]\nelement = np.array([[3, 5], [6, 6]])\na[pos[0]] = a[pos[0]].insert(pos[1], element[0])\na[pos[1]] = a[pos[1]].insert(pos[2], element[1])\nprint(a)\n",
        "\nimport numpy as np\npairs = [(2, 3), (3, 4), (4, 5)]\narray_of_arrays = np.array([np.arange(a*b).reshape(a,b) for (a, b) in pairs])\nresult = np.array(array_of_arrays, copy=True)\nprint(result)\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\nresult = np.all(a == a[None, :])\nprint(result)\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\nresult = np.all(a == a[:, None])\nprint(result)\n",
        "\ndef f(a = example_a):\n    return np.all(a == a[0])\n",
        "print(result)",
        "\nimport numpy as np\nexample_x = np.linspace(0, 1, 20)\nexample_y = np.linspace(0, 1, 30)\ndef f(x = example_x, y = example_y):\n    result = (cos(x)**4 + sin(y)**2) * (x[1] - x[0]) / 2\n    return result\n",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\nresult = ecdf(grades)\nprint(result)\n",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\neval = np.array([88, 87, 62])\nresult = ecdf(grades)(eval)\nprint(result)\n",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\nthreshold = 0.5\nlow, high = grades.searchsorted(ecdf(grades), 'right', threshold) - 1\nprint(low, high)\n",
        "randomLabel = np.random.randint(1, size=numbers)\none_ratio = 0.9\nsize = 1000\nnums = np.random.choice(2, size=size, p=[one_ratio, 1 - one_ratio])\nprint(nums)",
        "\nimport torch\nimport numpy as np\na = torch.ones(5)\na_np = a.detach().numpy()\nprint(a_np)\n",
        "\nimport torch\nimport numpy as np\na = np.ones(5)\na_pt = torch.from_numpy(a)\nprint(a_pt)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = tf.ones([2,3,4])\na_np = a.numpy()\nprint(a_np)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = np.argsort(a)[::-1]\nprint(result)\n",
        "result = np.argsort(a)[:-1]\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\nresult = a.argsort()[-N:]\nprint(result)\n",
        "\nimport numpy as np\nA = np.arange(16).reshape(4, 4)\nn = 5\nprint(A ** n)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\nresult = np.array([[a[i, j] for j in range(2)] for i in range(2)])\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\nresult = np.array([[a[i, j] for j in range(2)] for i in range(2)])\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\nresult = np.array([[a[i, j] for j in range(2)] for i in range(2)])\nprint(result)\n",
        "result = np.array([[a[0][0], a[0][1]],\n                  [a[1][0], a[1][1]],\n                  [a[2][0], a[2][1]],\n                  [a[3][0], a[3][1]]])\nprint(result)",
        "\nimport numpy as np\na = np.array([[[ 0,  1,  2],\n        [ 6,  7,  8]],    \n       [[ 3,  4,  5],\n        [ 9, 10, 11]], \n       [[12, 13, 14],\n        [18, 19, 20]],    \n       [[15, 16, 17],\n        [21, 22, 23]]])\nh = 4\nw = 6\nresult = np.reshape(a, (h, w))\nprint(result)\n",
        "result = np.array([[a[0][0], a[0][1]],\n                  [a[1][0], a[1][1]],\n                  [a[2][0], a[2][1]],\n                  [a[3][0], a[3][1]]])\nprint(result)",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 5\nresult = a[:, low:high+1]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 0\nhigh = 2\nresult = a[low:high+1, :]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 10\nresult = a[:, low:high+1]\nprint(result)\n",
        "\nimport numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(string.replace(']', '], ').replace('[', '['))\nprint(a)\n",
        "\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\nresult = np.random.uniform(np.log(min), np.log(max), size=n)\nprint(result)\n",
        "\nimport numpy as np\nmin = 0\nmax = 1\nn = 10000\nresult = np.random.uniform(np.log(min), np.log(max), size=n)\nprint(result)\n",
        "\ndef f(min=1, max=np.e, n=10000):\n    result = np.random.uniform(np.log(min), np.log(max), n)\n    return result\n",
        "\nimport numpy as np\nimport pandas as pd\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nB = A.cumprod(a) * (1 - b) + b * A.shift(-1)\nprint(B)\n",
        "B = A.cumprod(axis=0, weights=[a, b, c])\nprint(B)",
        "result = np.empty(0)\nprint(result)",
        "result = np.empty((3, 0))\nprint(result)",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = np.ravel_multi_index(index, dims)\nprint(result)\n",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = np.ravel_multi_index(index, dims)\nprint(result)\n",
        "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df)",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.cumsum(a, axis=0)\nresult[accmap] = result[accmap] + a[accmap]\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = a[index]\nresult = np.max(result)\nprint(result)\n",
        "\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = a[index]\nresult = result.min()\nprint(result)\n",
        "\nimport numpy as np\nx = [[2, 2, 2],\n     [2, 2, 2],\n     [2, 2, 2]]\ny = [[3, 3, 3],\n     [3, 3, 3],\n     [3, 3, 1]]\nz = np.array(x) + np.array(y)\nprint(z)\n",
        "\nresult = np.random.choice(lista_elegir, samples, probabilit)\n",
        "result = np.pad(a, ((low_index, high_index), (low_index, high_index)), 'constant', constant_values=(0, 0))\nprint(result)",
        "\n[0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2]\n",
        "\nimport numpy as np\nx = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\nresult = [num for num in x if num.real != 0]\nprint(result)\n",
        "bin_data = np.split(data, np.where(np.diff(data) > 0, 1, 0) * bin_size + 1)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\nprint(bin_data_mean)",
        "bin_data = np.split(data, np.where(np.diff(data) > 0, 1, 0) * bin_size + 1)\nbin_data_max = np.max(bin_data, axis=0)\nprint(bin_data_max)",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\nbin_data = np.bincount(data.ravel(), minlength=bin_size)\nbin_data_mean = bin_data.reshape(len(data), bin_size) / bin_data_mean.sum(axis=1)\nprint(bin_data_mean)\n",
        "\n",
        "bin_data = np.split(data, np.cumsum(np.ones(len(data)), axis=1) - 1)\nbin_data_mean = [np.mean(row, axis=0) for row in bin_data]\nprint(bin_data_mean)",
        "bin_data = np.zeros((data.shape[0], bin_size))\nfor i in range(data.shape[0]):\n    for j in range(bin_size):\n        bin_data[i, j] = data[i, i + j]\nbin_data_mean = np.mean(bin_data, axis=1)\nprint(bin_data_mean)",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\nresult = smoothclamp(x)\nprint(result)\n",
        "\nresult = smoothclamp(x, N=N)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\nresult = np.correlate(a, b, mode='same')\nprint(result)\n",
        "result = df.values.reshape(4, 15, 5)\nprint(result)",
        "result = df.values.reshape(15, 4, 5)\nprint(result)",
        "result = np.unpackbits(np.uint8(a), m)\n",
        "result = np.zeros((len(a), m))\nfor i, num in enumerate(a):\n    result[i, :np.count_bits(num)] = np.unpackbits(np.uint8(num))\nprint(result)\n",
        "result = np.zeros((1, m))\nfor num in a:\n    result = np.bitwise_xor(result, np.unpackbits(np.uint8(num)))\nprint(result)\n",
        "result = (np.mean(a) - 3 * np.std(a), np.mean(a) + 3 * np.std(a))",
        "sigma = np.std(a)\nresult = (np.mean(a) - 2 * sigma, np.mean(a) + 2 * sigma)",
        "\ndef f(a = example_a):\n    mu, sigma, sigma2 = 0, 0, 0\n    n = len(a)\n    a = a - a.mean()\n    sigma2 = np.sum(a ** 2) / (n - 1)\n    sigma = np.sqrt(sigma2)\n    mu = a.mean()\n    result = (mu - 3 * sigma, mu + 3 * sigma)\n    return result",
        "\nimport numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\nresult = (a - np.mean(a)) / np.std(a) * 2\nresult[result < 0] = True\nresult[result >= 0] = False\nprint(result)\n",
        "\nimport numpy as np\nDataArray = np.arange(-5.5, 10.5)\npercentile = 50\nprob = np.percentile(DataArray, percentile)\nprint(prob)\n",
        "a[zero_rows][zero_cols] = 0\na[a == [zero_rows, zero_cols]].fill(0)\nprint(a)",
        "\nimport numpy as np\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = [1, 3]\nzero_cols = [1, 2]\na[zero_rows, zero_cols] = 0\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\na[1] = 0\na[0] = 0\nprint(a)\n",
        "mask = np.zeros_like(a, dtype=bool)\nmax_values = np.amax(a, axis=1)\nmask[np.arange(len(max_values)), np.argmax(a, axis=1)] = True\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmin_values = a.min(axis=1)\nmask = min_values.astype(bool)\nprint(mask)\n",
        "\nimport numpy as np\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\nresult = np.corrcoef(post, distance)[:1]\nprint(result)\n",
        "result = np.empty((N, M, M))\nfor i in range(N):\n    result[i, :, :] = xi[:, i].dot(xi[:, i].T)\n",
        "\n",
        "is_contained = a.any(number == 0)\nprint(is_contained)",
        "C = A[~np.isin(A, B)]\nprint(C)",
        "C = A[A == B].astype(int)\nprint(C)",
        "C = A[np.where(np.logical_and(A == B[0], A == B[1]), True, False)]\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = np.argsort(rankdata(a))[::-1]\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = np.flipud(rankdata(a).astype(int))\nprint(result)\n",
        "\ndef f(a = example_a):\n    result = np.flipud(rankdata(a).astype(int))\n    return result\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = A[np.ix_(np.arange(5), second, np.arange(3))]\n",
        "arr = numpy.zeros((20,)*3 + (10,))\nprint(arr)",
        "\nfrom numpy import linalg as LA\nimport numpy as np\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\nl1 = X.sum(axis=1)\nresult = X / l1.reshape(5, 1)\nprint(result)\n",
        "\nfrom numpy import linalg as LA\nimport numpy as np\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\nresult = np.array([LA.norm(v, ord=2) for v in X])\nprint(result)\n",
        "\nresult = np.array([LA.norm(v, ord=np.inf) for v in X])\nprint(result)\n",
        "\n",
        "\nimport numpy as np\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\nprint(result)\n",
        "result = np.sqrt(np.sum(np.square(a[i, :] - a[j, :]), axis=1))\n",
        "result = np.sqrt(np.sum(np.square(a[i, :] - a[j, :]), axis=1))\n",
        "AVG = np.mean(NA, axis=0)",
        "AVG = np.mean(NA, axis=0)",
        "\nimport numpy as np\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\nresult = a[~np.isclose(a, 0) & ~np.isclose(a - 1, 0) & ~np.isclose(a - 2, 0)]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\nresult = a.copy()\nresult[1:] = result[1:][:, None] != result[1:][:, None]\nresult = result.astype(int)\nresult[result == 0] = -1\nresult = result.astype(int)\nresult = result.ravel()\nresult = result.tolist()\nresult = [x for x in result if x != -1]\nprint(result)\n",
        "df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\ndf = df.set_index(['lat', 'lon'])\ndf = df.sort_index()\nprint(df)",
        "\nimport numpy as np\nimport pandas as pd\nexample_lat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\nexample_lon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\nexample_val=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\ndef f(lat = example_lat, lon = example_lon, val = example_val):\n    df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\n    df = df.sort_index(axis=1)\n    return df\n",
        "\ndf = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\ndf['maximum'] = df.apply(lambda row: max(row['lat'], row['lon']), axis=1)\nprint(df)",
        "result = np.lib.stride_tricks.as_strided(a, (1, 1), (size[0] - 1, size[1] - 1), (size[0] - 1, size[1] - 1))\n",
        "result = np.lib.stride_tricks.as_strided(a, (1, 1), (size[0] - 1, size[1] - 1), (size[0] - 1, size[1] - 1))\n",
        "\nimport numpy as np\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\nresult = np.mean(a)\nprint(result)\n",
        "result = np.mean(a)\nreturn result",
        "Z = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\nresult = Z[:, :, -1]\nprint(result)",
        "\nimport numpy as np\na = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\nresult = a[-1:, :]\nprint(result)\n",
        "result = any(c == cnt for cnt in CNTS)\nprint(result)",
        "\ndef is_member(c, CNTS):\n    return any(np.all(c == a) for a in CNTS)\nresult = is_member(c, CNTS)\nprint(result)",
        "result = intp.interp2d(x_new, y_new, a, kind='linear')\nprint(result)",
        "\n# [Missing Code]\n",
        "i = np.diag(i)\nprint(i)",
        "\nimport numpy as np\na = np.array([[1,0,2,3],[0,5,3,4],[2,3,2,10],[3,4, 10, 7]])\na[np.triu_indices_from(a, k=1)] = 0\nprint(a)\n",
        "result = pd.date_range(start, end, freq=np.timedelta64(1, 's'))\n",
        "result = np.where(x == a)[0]\nif result == -1:\n    result = np.where(y == b)[0]\nprint(result)",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nresult = [3, 5]\nprint(result)\n",
        "result = np.polyfit(x, y, deg)\n",
        "from scipy.optimize import minimize\ndef func(params, x, y):\n    a, b, c = params\n    return sum((y[i] - (a + b * x[i] + c * x[i] ** 2) ** 2) for i in range(len(x)))\ndef grad_func(params, x, y):\n    a, b, c = params\n    return np.array([2 * (y[i] - (a + b * x[i] + c * x[i] ** 2)) for i in range(len(x))])\ndef hess_func(params, x, y):\n    a, b, c = params\n    return np.array([2 * (2 * (y[i] - (a + b * x[i] + c * x[i] ** 2))) for i in range(len(x))])\ndef jacobian(params, x, y):\n    a, b, c = params\n    return np.array([[2 * (y[i] - (a + b * x[i] + c * x[i] ** 2)) for i in range(len(x))] for i in range(len(x))])\ndef hessian(params, x, y):\n    a, b, c = params\n    return np.array([[2 * (2 * (y[i] - (a + b * x[i] + c * x[i] ** 2))) for i in range(len(x))] for i in range(len(x))])\ndef main():\n    result = minimize(func, [1, 1, 1], args=(x, y), method='L-BFGS-B', options={'maxiter': 1000})\n    print(result)\nif __name__ == '__main__':\n    main()",
        "\nimport numpy as np\nimport pandas as pd\na = np.arange(4)\ndf = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\ntemp_arr = [0, 1, 2, 3]\nfor i, row in df.iterrows():\n    row[0] -= temp_arr[i]\nprint(df)\n",
        "\nimport numpy as np\nA = np.random.rand(5, 6, 3)\nB = np.random.rand(3, 3)\nresult = np.einsum('ijk,jl->ilk', A, B)\nprint(result)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\na = np.array([[-1, 2], [-0.5, 6]])\nresult = MinMaxScaler().fit_transform(a)\nprint(result)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\narr = np.array([[1.0,2.0,3.0],[0.1, 5.1, 100.1],[0.01, 20.1, 1000.1]])\nresult = MinMaxScaler()\nresult.fit(arr)\nresult.transform(arr)\nprint(arr)\n",
        "\nresult = MinMaxScaler().fit_transform(a)\n",
        "\n# [Missing Code]\n",
        "\narr_temp = arr.copy()\nmask = arr_temp < n1\nmask2 = arr_temp >= n2\narr[mask] = 0\narr[mask2] = 30\narry[~mask2] = arry[~mask2] + 5\nprint(arr)",
        "\nimport numpy as np\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\nresult = np.nonzero(s1 != s2)[0].shape[0]\nprint(result)\n",
        "\nimport numpy as np\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\ns1 = np.append(s1, np.nan)\ns2 = np.append(s2, np.nan)\nresult = np.isnan(s1) * np.isnan(s2)\nresult[result] = False\nresult[~result] = True\nprint(result)\n",
        "result = np.array_equal(a, a)\nprint(result)",
        "\nimport numpy as np\na = [np.array([np.nan, 2, 3]), np.array([1, np.nan, 3]), np.array([1, 2, np.nan])]\nresult = all(np.isnan(array).all() for array in a)\nprint(result)\n",
        "\n",
        "\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\nresult = np.pad(a, ((0, 0), (0, 11)), 'constant', constant_values=(element,))\nprint(result)\n",
        "\ndef f(arr = example_arr, shape=(93,13)):\n    return np.pad(arr, ((0, 93-arr.shape[0]), (0, 13-arr.shape[1])), 'constant')\n",
        "\n",
        "\nimport numpy as np\na = np.arange(12)\na.reshape(a.shape[0] // 3, 3)\nprint(a)\n",
        "result = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        result[i, j] = a[i, j, b[i, j]]\nprint(result)",
        "\nimport numpy as np\na = np.array( # dims: 3x3x2\n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\nb = np.array( # dims: 3x3\n    [[1, 1, 1],\n    [1, 1, 1],\n    [1, 1, 1]]\n)\nresult = a[b]\nprint(result)\n",
        "result = np.take(a, b, axis=2)\nprint(result)\n",
        "result = np.sum(a[b[:, 0], b[:, 1], b[:, 2]])\nprint(result)\n",
        "result = np.sum(a[b[:, 0], b[:, 1], b[:, 2]])\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\ndata = {'a': [1, 2, 3, 4, 5], 'b': [2, -6, 0, -4, 100]}\ndf = pd.DataFrame(data)\nresult = df[df['a'].between(1, 4, inclusive=True)]['b']\nprint(result)\n",
        "result = im.reshape(3, 5)\nresult[result != 0] = 1\nresult = result.reshape(3, 5)\nreturn result\n",
        "\nimport numpy as np\nA = np.array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]])\nresult = A[~np.isnan(A)]\nprint(result)\n",
        "result = im.astype(bool)\nresult = result.astype(int)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)\nresult = np.where(result, 0, result)",
        "\nimport numpy as np\nim = np.array([[0,0,0,0,0,0],\n               [0,0,5,1,2,0],\n               [0,1,8,0,1,0],\n               [0,0,0,7,1,0],\n               [0,0,0,0,0,0]])\nresult = im.reshape(3, 5)\nresult[result != 0] = 1\nprint(result)\n"
    ],
    "Matplotlib": [
        "\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(figsize=(10, 6))\nx = 10 * np.random.randn(10)\ny = x\nclusterplot(x, y, cmap=\"RdBu\", s=100, linewidths=1, cbar_kws={\"shrink\": 0.5})\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"x-y\")\nplt.tight_layout()\nplt.show()\n",
        "\nax = plt.gca()\nax.yaxis.set_minor_formatter(ticker.NullFormatter())\nax.yaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "\nax = plt.gca()\nax.minor_tick_num = 5\n",
        "\nax = plt.gca()\nax.xaxis.set_minor_formatter(plt.NullFormatter())\n",
        "",
        "",
        "\nfig, ax = plt.subplots()\nax.plot(x, y, marker='o', markersize=3, markerfacecolor='none', markeredgecolor='black', markeredgewidth=1)\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y, marker='o', markersize=10, linestyle='None', markerfacecolor='none', markeredgecolor='black', markeredgewidth=2)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n",
        "ax.set_ylim(0, 40)",
        "\n",
        "",
        "",
        "",
        "sns.set(style=\"ticks\")\nplt.plot(x, y, label=\"2x random data\")\nplt.legend()\nplt.show()",
        "",
        "plt.plot(x, y, marker='o', markersize=7)",
        "plt.legend(fontsize=20)",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\n# Set legend title and font size\nplt.legend(title=\"xyz\", fontsize=20)\n",
        "\nl.set_markersize(30)\nl.set_markerfacecolor('w')\nl.set_alpha(0.2)\n",
        "l.set_markers(markersize=30, markeredgecolor=\"black\")",
        "l.set_color(\"red\")\nplt.gca().set_color(\"red\")\n",
        "plt.xlabel(plt.xlabel(), rotation=45, ha='right')",
        "plt.xlabel(plt.xlabel(), rotation=45, ha='right')",
        "plt.xticks(np.arange(0, 2 * np.pi, step=np.pi / 2))",
        "plt.legend(loc=\"best\", frameon=False, handletextpad=0.1, prop={\"size\": 12})",
        "",
        "\nplt.imshow(H, cmap=plt.cm.bw)\nplt.show()\n",
        "plt.xlabel('X')\nplt.xlim(0, 2 * np.pi)",
        "g.axes[0].set_title(g.axes[0].get_title(), fontdict={'rotation': 90})",
        "myTitle = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\nplt.title(myTitle, fontsize=12)\nplt.show()",
        "plt.gca().invert_yaxis()",
        "plt.xlim(-1.5, 1.5)",
        "y_ticks = [-1, 1]\nplt.ylim(y_ticks[0], y_ticks[1])\nplt.yticks(y_ticks)",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n# Plot x, then y, then z, but so that x covers y and y covers z\nax.plot(x, color='r', alpha=0.8, label='x')\nax.plot(y, color='g', alpha=0.8, label='y')\nax.plot(z, color='b', alpha=0.8, label='z')\n# Adjust the xlim and ylim to cover the entire area\nax.set_xlim(min(x) - 0.1, max(x) + 0.1)\nax.set_ylim(min(y) - 0.1, max(y) + 0.1)\n# Add a legend\nax.legend(loc='best', frameon=False)\n",
        "\nfig, ax = plt.subplots()\nax.scatter(x, y, c='blue', edgecolors='black', s=50)\nplt.show()\n",
        "",
        "plt.yticks(np.arange(min(df[\"coverage\"]), max(df[\"coverage\"]) + 1, 1000000))",
        "\n",
        "",
        "",
        "plt.gca().axes[0].set_xlabel('')",
        "plt.gca().set_xticklabels([])",
        "plt.xticks(np.arange(3, 6, 1))\nplt.grid(which='major', axis='x', linestyle='--', alpha=0.2)",
        "\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(3))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(4))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(5))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(6))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(7))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(8))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(9))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(10))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(11))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(12))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(13))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(14))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(15))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(16))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(17))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(18))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(19))\nplt.gca().yaxis.set_major_formatter(lambda x, pos: str(x))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(20))",
        "\n# Add yticks and horizontal grid at y positions 3 and 4\nplt.yticks([3, 4], ['3', '4'])\nplt.yticks(np.arange(3, 6, 1), ['3', '4', '5'])\n# Add xticks and vertical grid at x positions 1 and 2\nplt.xticks([1, 2], ['1', '2'])\nplt.xticks(np.arange(1, 4, 1), ['1', '2', '3'])\n",
        "\nx_grid = np.arange(min(x), max(x) + 1, 1)\ny_grid = np.arange(min(y), max(y) + 1, 1)\nplt.grid(True)\nplt.plot(x_grid, y_grid, 'k-', linewidth=0.5)\n",
        "plt.legend(loc=\"lower right\")",
        "\n# Adjust subplot padding to have enough space to display axis labels\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n# Plot the same data again\nfor ax in axes:\n    ax.clear()\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\n",
        "\n",
        "ax.xaxis.major_locator_mode = \"xy\"",
        "",
        "plt.plot(x, y)",
        "\nfig, ax = plt.subplots()\nax.set_yticks(y)\nax.set_yticklabels(y)\nax.set_ylim(bottom=0)\nax.set_yticklabels(y, rotation=90)\nplt.plot(x, y)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and label y axis \"Y\"\n# Show y axis ticks on the left and y axis label on the right\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_ylabel(\"Y\")\nax.set_yticks(np.arange(10))\nax.set_yticklabels(np.arange(10))\nax.tick_params(axis='y', direction='in')\nplt.show()\n",
        "",
        "",
        "\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", ax=ax)\nax.set_title(\"Joint Regression Plot of Total Bill and Tip\")\nax.set_xlabel(\"Total Bill\")\nax.set_ylabel(\"Tip\")\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(10, 5))\ns1 = df[\"s1\"]\ns2 = df[\"s2\"]\nax.bar(df[\"celltype\"], s1, align=\"center\", alpha=0.7, color=\"r\")\nax.bar(df[\"celltype\"], s2, bottom=s1, align=\"center\", alpha=0.7, color=\"g\")\nax.set_xticks(df[\"celltype\"])\nax.set_xticklabels(df[\"celltype\"], rotation=90, horizontalalignment='center')\nax.set_ylim([0, max(s1) + max(s2)])\nax.set_title(\"Bar Plot of s1 and s2 by celltype\")\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(10, 5))\ns1 = df[\"s1\"]\ns2 = df[\"s2\"]\nax.bar(df[\"celltype\"], s1, width=0.8, align='center', alpha=0.7)\nax.bar(df[\"celltype\"], s2, width=0.8, bottom=s1, align='center', alpha=0.7)\nax.set_xticklabels(df[\"celltype\"], rotation=45)\nplt.xlabel(\"Cell Type\")\nplt.ylabel(\"Values\")\nplt.title(\"Bar Plot\")\nplt.legend([\"s1\", \"s2\"], loc=\"best\")\nplt.tight_layout()\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and label the x axis as \"X\"\n# Make both the x axis ticks and the axis label red\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xlabel('X', fontdict={'color': 'red'})\nax.set_xticks(x, fontdict={'color': 'red'})\nplt.show()\n",
        "",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xticklabels(x, fontsize=10, horizontalalignment='right', verticalalignment='top')\nax.tick_params(axis='x', labelsize=10)\nax.tick_params(axis='y', labelsize=10)\nplt.show()\n",
        "",
        "",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\n# Add legend for all three curves in both subplots\nax.legend(loc=0, handles=[Swdown, Rn, temp], labels=[\"Swdown\", \"Rn\", \"temp\"])\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make two side-by-side subplots and and in each subplot, plot y over x\n# Title each subplot as \"Y\"\nfig, ax = plt.subplots(ncols=2, nrows=1, sharex=True, sharey=True)\nax[0].plot(x, y)\nax[1].plot(x, y)\n# Add titles to each subplot\nax[0].set_title(\"Y\")\nax[1].set_title(\"Y\")\nplt.show()\n",
        "sns.scatter(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=\"species\",\n    size=30,\n    s=10,\n    palette=\"RdYlBu_r\",\n    alpha=0.8,\n    linewidths=0.5,\n    edgecolor=\"white\",\n)",
        "",
        "",
        "",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\n# Make a histogram of x and show outline of each bar in the histogram\n# Make the outline of each bar has a line width of 1.2\nfig, ax = plt.subplots()\n# Create a histogram of x\nn, bins, patches = plt.hist(x, 10, normed=True, facecolor='white', alpha=0.8, linewidths=1.2)\n# Add outline to each bar\nfor patch in patches:\n    xy = patch.get_xydata()\n    plt.plot(xy[0], xy[1], 'k', linewidth=1.2)\nplt.show()\n",
        "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, squeeze=False)\nfig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95)\nax[0].set_aspect('equal')\nax[1].set_aspect('equal')\nplt.plot(x, y, 'k-', c=None, label='')\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left', borderaxes=ax)\nplt.show()",
        "",
        "",
        "",
        "\n# Create two subplots with the same dimensions as the x and y matrices\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(10, 5))\n# Set the same colormap for both subplots\ncmap = plt.get_cmap('RdBu_r')\nax[0].imshow(x, cmap=cmap, interpolation='nearest')\nax[1].imshow(y, cmap=cmap, interpolation='nearest')\n# Set the same colorbar for both subplots\ncb = plt.colorbar(shown=True, ax=ax)\n",
        "from matplotlib.pylab import plt\nfig, ax = plt.subplots()\nfor i in range(x.shape[1]):\n    ax.plot(x[:, i], label=f'{i}')\nplt.legend()\nplt.show()",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\nfig, ax = plt.subplots(ncols=2, nrows=1, sharex=True, sharey=True)\nax[0].set_title(\"Y and Z\", fontsize=14)\nax[0].set_xlabel(\"X\")\nax[0].set_ylabel(\"Y\")\nax[0].plot(x, y)\nax[1].set_title(\"Y and Z\", fontsize=14)\nax[1].set_xlabel(\"A\")\nax[1].set_ylabel(\"Z\")\nax[1].plot(a, z)\nplt.tight_layout()\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.scatter(points[:, 0], points[:, 1], marker='o', s=100)\nax.set_yscale('log')\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n",
        "",
        "ax.plot(x, y)\nax.set_xticks(x)\nax.set_xticklabels(np.arange(10))\nax.set_yticks(y)\nax.set_yticklabels(np.arange(10))\nplt.show()",
        "",
        "\nfig, ax = plt.subplots()\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlim(0, 1000)\nax.set_ylim(0, 1000)\nax.set_xticks([1, 10, 100, 1000])\nax.set_xticklabels(['1', '10', '100', '1000'])\nax.set_yticks([1, 10, 100, 1000])\nax.set_yticklabels(['1', '10', '100', '1000'])\nplt.plot(x, y, 'k-', label='data')\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n",
        "",
        "\n# Calculate the total sum of the data\ntotal_sum = sum(data)\n# Create a histogram with bins of equal width\nbins = np.arange(min(data), max(data) + 1, 1)\nhist, bin_edges = np.histogram(data, bins=bins)\n# Calculate the cumulative sum of the histogram\ncum_hist = np.cumsum(hist)\n# Renormalize the histogram to sum up to 1\nrenormalized_hist = cum_hist / total_sum\n# Format the y tick labels into percentage\ny_tick_labels = [str(int(x * 100)) + '%' for x in range(len(renormalized_hist))]\n# Plot the histogram\nplt.bar(bin_edges, renormalized_hist, align='center', alpha=0.7, label='Renormalized')\nplt.xlabel('Value')\nplt.ylabel('Percentage')\nplt.xticks(bin_edges, y_tick_labels)\nplt.legend()\nplt.show()\n",
        "",
        "from matplotlib.pylab import figlegend\nfig, ax1 = plt.subplots(1, 2, sharex=True, sharey=True)\nax1[0].plot(x, y)\nax1[1].plot(z, a)\nfiglegend(ax1, ['y', 'a'], loc='best', frameon=False)\nplt.show()",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\nfig, ax = plt.subplots(1, 1)\nplt.xlim(1, 10)\nplt.xticks(range(1, 10))\nax.plot(y, x)\n# change the second x axis tick label to \"second\" but keep other labels in numerical\nax.set_xticklabels(range(1, 10), minor=False)\nax.set_xticklabels(range(1, 10), minor=True, fontdict={'fontsize': 8})\nax.set_xticklabels(range(1, 10), minor=True, fontdict={'fontsize': 8, 'fontname': 'Arial', 'weight': 'bold', 'verticalalignment': 'bottom'})\nax.set_xticklabels(range(1, 10), minor=True, fontdict={'fontsize': 8, 'fontname': 'Arial', 'weight': 'bold', 'verticalalignment': 'bottom'}, ha='center')\nax.set_xticklabels(range(1, 10), minor=True, fontdict={'fontsize': 8, 'fontname': 'Arial', 'weight': 'bold', 'verticalalignment': 'bottom'}, ha='center', rotation=90)\nax.set_xticklabels(range(1, 10), minor=True, fontdict={'fontsize': 8, 'fontname': 'Arial', 'weight': 'bold', 'verticalalignment': 'bottom'}, ha='center', rotation=90, label='second')\nplt.show()\n",
        "",
        "\nxticks = plt.gca().get_xticks()\nxticks.extend([2.1, 3, 7.6])\nplt.gca().set_xticks(xticks)\nplt.gca().relim()\nplt.gca().autoscale_view()\n",
        "\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=60, horizontalalignment='left')\n",
        "\nplt.gca().yaxis.set_tick_params(direction='in', angle=-60)\nplt.gca().xaxis.set_tick_params(verticalalignment='top')\n",
        "plt.xticks(fontsize=8, fontweight='bold', size=8, color='w', alpha=0.5)",
        "\nplt.xlim(0, 9)  # Remove the margin before the first xtick\nplt.ylim(-0.1, 9.1)  # Use greater than zero margin for the yaxis\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Create a new figure with customized margins\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(x, y)\n# Set the x margin to 0.1 and the y margin to 0.05\nax.set_xmargin(0.1)\nax.set_ymargin(0.05)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\nfor i, ax in enumerate(ax.flat):\n    ax.plot(x, y, label=f\"y={y}\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.title(\"Figure\")\nplt.tight_layout()\nplt.show()\n",
        "",
        "plt.scatter(x, y, marker='.', s=100, hatch='//')",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Make a scatter plot with x and y and remove the edge of the marker\n# Use vertical line hatch for the marker\nfig, ax = plt.subplots()\nax.scatter(x, y, marker='o', s=50, c='r', hatch='//')\nplt.show()\n",
        "plt.scatter(x, y, marker='*')",
        "plt.scatter(x, y, s=100, c='k', hatch='*//')",
        "",
        "\nx = np.linspace(0.1, 2 * np.pi, 41)\ny = np.exp(np.sin(x))\n# make a stem plot of y over x and set the orientation to be horizontal\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nax.stem(x, y, base_width=0.7, orientation='horizontal')\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n",
        "",
        "\n# Create sample data\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# Create a bar chart with the given data\nfig, ax = plt.subplots()\nax.bar(data, data, align='center', alpha=0.7)\n# Add a vertical line at x=3 and label it \"cutoff\"\nplt.axvline(x=3, color='r', linestyle='--', label='cutoff')\n# Add a legend\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxes=False, prop={'size': 14})\n",
        "",
        "\nfig, ax = plt.subplots()\n# Create a FancyArc object for each element in the list\nfancy_arcs = [FancyArc(start=0, end=0.4, radius=0.8, color=color) for color in plt.cm.get_cmap(\"Reds\")(l)]\n# Add the FancyArcs to the plot\nax.add_artist(fancy_arcs)\n# Set the wedge width to be 0.4\nax.wedgeprops_defaults(wedgewidth=0.4)\n# Add the labels\nax.set_xticks(())\nax.set_yticks(())\nax.set_xticklabels(l)\nax.set_yticklabels(l)\n",
        "",
        "",
        "\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\nax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, labeldistance=1.2, pctdistance=0.75)\nax.pie.label_formatter = FuncFormatter(lambda x, pos, pie: f\"{int(x * 100)}%\")\nplt.show()",
        "\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\nax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, labeldistance=1.2, pctdistance=0.75)\nax.pie.label_formatter = FuncFormatter(lambda x, pos, pie: f\"{int(x * 100)}%\")\nplt.show()",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart but use transparent marker with non-transparent edge\nfig, ax = plt.subplots()\nax.plot(x, y, marker='o', markersize=10, markerfacecolor='none', markeredgecolor='black', linewidth=1)\nplt.show()\n",
        "sns.distplot(df[\"bill_length_mm\"], color=\"blue\")\nplt.axvline(55, color=\"green\")",
        "\n# Specify the values of blue bars (height)\nblue_bar = (23, 25, 17)\n# Specify the values of orange bars (height)\norange_bar = (19, 18, 14)\n# Plot the blue bar and the orange bar side-by-side in the same bar plot.\n# Make  sure the bars don't overlap with each other.\nfig, ax = plt.subplots(figsize=(10, 5))\n# Create a bar plot with the given bars\nax.bar(np.arange(len(blue_bar)), blue_bar, width=0.8, align='edge', color='blue')\nax.bar(np.arange(len(orange_bar)) + 0.8, orange_bar, width=0.8, align='edge', color='orange')\n# Set the x-axis labels and tick labels\nax.set_xticks(np.arange(len(blue_bar)) + 0.8)\nax.set_xticklabels(range(len(blue_bar)))\nax.set_xlim(0, len(blue_bar) + 1)\n# Set the y-axis labels and tick labels\nax.set_yticks(np.arange(max(blue_bar) + 1))\nax.set_yticklabels(range(max(blue_bar) + 1))\nax.set_ylim(0, max(blue_bar) + 1)\n# Add a legend\nax.legend(('Blue', 'Orange'), loc='upper left', bbox_to_anchor=(1, 1), fancybox=True, shadow=True)\n",
        "\nx = np.arange(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\na = np.arange(10)\n# Make two subplots\n# Plot y over x in the first subplot and plot z over a in the second subplot\n# Label each line chart and put them into a single legend on the first subplot\nfig, ax = plt.subplots(ncols=2, nrows=1, sharex=True, sharey=True, figsize=(10, 5))\n# Plot y over x in the first subplot\nax[0].plot(x, y, label='y')\nax[0].set_ylabel('y')\nax[0].set_xlabel('x')\n# Plot z over a in the second subplot\nax[1].plot(a, z, label='z')\nax[1].set_ylabel('z')\nax[1].set_xlabel('a')\n# Add a single legend to the first subplot\nax[0].legend(loc='best', frameon=False)\n",
        "\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n# Plot y over x with a scatter plot\nax.scatter(x, y, c=y, cmap=\"Spectral\")\n# Adjust the aspect ratio to make the x-axis labels more readable\nax.set_aspect(\"equal\", adjustable=\"box\")\n# Add a legend\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.)\n# Adjust the x-axis and y-axis labels\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n# Adjust the title\nplt.title(\"Scatter plot with Spectral colormap\")\n# Show the plot\nplt.show()\n",
        "plt.plot(x, y, 'o')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(b=True, which='both', axis='both')\nplt.tick_params(axis='both', which='both', direction='in', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False, labelright=False)",
        "\nfig, ax = plt.subplots(1, 1, sharex=False, sharey=False)\nsns.factorplot(data=df, x=\"bill_length_mm\", y=\"sex\", hue=\"species\", aspect=1, ax=ax, palette=\"RdYlBu_r\", legend_out=True)\nplt.tight_layout()\n",
        "plt.circle((0.5, 0.5), radius=0.2)",
        "plt.plot(x, y)\nplt.title(\"$\\phi$\", fontdict={'weight': 'bold'})",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x with a legend of \"Line\"\n# Adjust the length of the legend handle to be 0.3\nfig, ax = plt.subplots()\nax.plot(x, y, label=\"Line\")\nax.legend(handles=[plt.Rectangle((0, 0), 0.3, 0.3, fill=False, edgecolor='black', label='Line')], loc='upper left')\nplt.show()\n",
        "plt.legend(loc=\"best\", prop={\"size\": 12})",
        "",
        "from matplotlib.colors import ListedColormap\ncmap = ListedColormap([\"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \"gray\", \"brown\", \"pink\", \"cyan\"])\nfig, ax = plt.subplots()\nim = ax.imshow(data, cmap=cmap)\nplt.colorbar(im, ax=ax)\nplt.show()",
        "plt.plot(x, y)\nplt.title(\"Figure 1\", weight='bold')",
        "sns.pairplot(df, hue=\"id\", x_vars=\"x\", y_vars=\"y\", diag_kind=\"kde\", aspect=1, diag_kwds={\"c\": \"w\", \"bw\": True})",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and invert the x axis\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.invert_xaxis()\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nplt.show()\n",
        "",
        "",
        "",
        "\n# Create a histogram with the given specifications\nbins = np.arange(0, 11, 2)\nhist, bins = np.histogram(x, bins=bins)\n# Plot the histogram\nplt.bar(bins[:-1], hist, align='edge', alpha=0.7, label='Histogram')\nplt.xlabel('Frequency')\nplt.ylabel('Probability')\nplt.title('Histogram of x')\nplt.legend()\nplt.show()\n",
        "",
        "\nx0, y0 = np.meshgrid(np.zeros(x.shape[0]), np.zeros(x.shape[1]))\nplt.contourf(x0, y0, z, alpha=0.5, colors='white')\n",
        "ax.errorbar(box_position, box_height, yerr=box_errors, color=c)",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\nfig, ax = plt.subplots(nrows=4, ncols=4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        ax[i, j].set_xlabel(f\"x_{i+1}\")\n        ax[i, j].set_ylabel(f\"y_{j+1}\")\n        ax[i, j].plot(x, y)\nplt.tight_layout()\nplt.show()\n",
        "plt.matshow(d)\nfig = plt.gcf()\nfig.set_size_inches(8, 8)\nplt.show()",
        "plt.table(df, bbox=[0, 0, 1, 1])",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart. Show x axis tick labels on both top and bottom of the figure.\nplt.plot(x, y)\nplt.gca().xaxis.set_tick_params(both=True)\nplt.show()\n",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart. Show x axis tick labels but hide the x axis ticks\nplt.plot(x, y)\nplt.xlabel('x axis')\nplt.ylabel('y axis')\nplt.xlim(0, 10)\nplt.xticks(())\nplt.show()\n",
        "\ndf = sns.load_dataset(\"exercise\")\n",
        "\ndf = sns.load_dataset(\"exercise\")\n# Make catplots of scatter plots by using \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\n# Change the xlabels to \"Exercise Time\" and \"Exercise Time\"\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nsns.catplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=ax, kind=\"scatter\", aspect=1.5)\nax.set_xlabel(\"Exercise Time\")\nax.set_ylabel(\"Pulse\")\nax.set_title(\"Exercise Time vs. Pulse by Kind and Diet\")\nplt.show()\n",
        "",
        "",
        "plt.plot(x, y)",
        "",
        "\nfig, ax = plt.subplots()\nax.plot(t, a, label='a', linewidth=2)\nax.plot(t, b, label='b', linewidth=2)\nax.plot(t, c, label='c', linewidth=2)\nax.legend(loc='best')\nax.set_title('a, b, c')\nax.set_xlabel('t')\nax.set_ylabel('Amplitude')\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, ax=ax, legend=False)\nplt.tight_layout()\nplt.show()\n",
        "",
        "\nfrom mpl_toolkits.mplot3d import Axes3D\nx = np.random.random(10)\ny = np.random.random(10)\nz = np.random.random(10)\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c='r', marker='o')\nax.view_init(100, 50)\nplt.show()\n",
        "",
        ""
    ],
    "Tensorflow": [
        "\nimport tensorflow as tf\nx = tf.Variable(0)\nx.assign(1)\nresult = x\n",
        "\nimport tensorflow as tf\nx = tf.Variable(0)\nx.assign(114514)\nresult = x.numpy()[0]\nprint(result)\n",
        "result = tf.one_hot(labels, 10)\nprint(result)",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\nresult = tf.constant(labels, dtype=tf.int32)\nprint(result)\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\nresult = tf.constant(labels, dtype=tf.int32)\nprint(result)\n",
        "result = tf.one_hot(labels, 10)\nreturn result",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\nresult = tf.constant(labels, dtype=tf.int32)\nprint(result)\n",
        "\ndef my_map_func(i):\n  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n",
        "\ndef f(input=example_input):\n    result = []\n    for i in input:\n        result.extend([i, i + 1, i + 2])\n    return result\n",
        "result = tf.ones([8, 1])\nresult = result.repeat(8, axis=0)\nresult = result.view(8, 1)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.ones(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])\nresult = result.repeat(8, axis=1)\nresult = result.view(8, 8)\nresult = tf.scatter_nd(\n    indices=tf.range(8),\n    updates=tf.zeros(8)\n)\nresult = result.gather(axis=1)\nresult = tf.cast(result, tf.int8)\nresult = tf.reshape(result, [8, 1])",
        "\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\nresult = tf.ones([8, 1])\nresult = result.gather(axis=0, indices=lengths)\nresult = result.astype(tf.int32)\nresult = tf.where(result < 1, 0, 1)\nresult = tf.reshape(result, [8, 1])\nprint(result)\n",
        "result = tf.ones(shape=(8,), dtype=tf.int8)\nresult[:len(lengths)] = tf.cast(tf.range(len(lengths)), tf.int8)\nresult[len(lengths):] = tf.constant(1, dtype=tf.int8)\nprint(result)",
        "result = tf.ones([8, 1])\nresult = result.expand_dims(0)\nresult = result * tf.cast(tf.range(8), tf.float32)\nresult = result < lengths\nresult = result.astype(tf.int32)\nresult = result.reshape([8, 1])\nresult = tf.cast(result, tf.bool)\nresult = tf.tile(result, [1, 8])\nreturn result\n",
        "result = tf.ones(shape=(8,), dtype=tf.int8)\nresult[:len(lengths)] = tf.cast(tf.range(len(lengths)), tf.int8)\nresult[len(lengths):] = tf.constant(0, dtype=tf.int8)\nprint(result)",
        "\nresult = tf.concat([a, b], axis=0)\n",
        "\ndef f(a=example_a,b=example_b):\n    result = tf.stack([a, b])\n    return result\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 1, 512))\nresult = a.reshape(a.shape[0], a.shape[1], a.shape[2], -1)\nprint(result)\n",
        "result = tf.expand_dims(a, axis=-1)\nprint(result)",
        "result = tf.reshape(a, [1, 50, 100, 1, 512])\nprint(result)",
        "tf.reduce_sum(A, axis=1)",
        "tf.reduce_prod(A, axis=1)",
        "A = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\nresult = tf.reciprocal(A)\nprint(result)",
        "\nimport tensorflow as tf\na = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nb = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\nresult = tf.reduce_sum(tf.square(a - b), axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\na = tf.constant([\n  [1,1,1],\n  [0,1,1]\n])\nb = tf.constant([\n  [0,0,1],\n  [1,1,1]\n])\nresult = tf.reduce_max(tf.abs(a - b), axis=1)\nprint(result)\n",
        "result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\nreturn result",
        "\nimport tensorflow as tf\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\nresult = x[tf.cast(y, tf.int32), z]\nprint(result)\n",
        "m = x[row, col]\nresult = [1, 6]\nprint(result)",
        "\ndef f(x=example_x,y=example_y,z=example_z):\n    result = x[tf.cast(y, tf.int32), tf.cast(z, tf.int32)]\n    return result\n",
        "result = tf.einsum('...ij,...jk->...ik', A, B)\nprint(result)",
        "result = tf.einsum('...ij,...jk->...ik', A, B)\nprint(result)",
        "result = [bytes.decode(b) for b in x]",
        "\ndef f(x=example_x):\n    result = []\n    for byte_string in x:\n        result.append(byte_string.decode('utf-8'))\n    return result\n",
        "y = x.apply(lambda x: tf.math.divide_no_nan(x, tf.math.reduce_sum(tf.math.cast(tf.math.not_equal(x, 0), tf.float32), axis=-1)))\nprint(y)",
        "y = tf.math.reduce_mean(x, axis=-1, keepdims=True)\ny = tf.math.reduce_sum(y, axis=0) / tf.shape(x)[0]\nresult = tf.math.square(y - tf.reduce_mean(y, axis=0))\nresult = result / tf.reduce_sum(result)\nprint(result)",
        "\ndef f(x=example_x):\n    batch_size, num_features = x.shape[:2]\n    result = tf.zeros((batch_size, num_features - 1), dtype=tf.float32)\n    for batch_index in range(batch_size):\n        for feature_index in range(num_features - 1):\n            non_zero_count = tf.reduce_sum(tf.cast(x[batch_index, feature_index] != 0, tf.int32))\n            result[batch_index, feature_index] = x[batch_index, feature_index] / non_zero_count\n    return result",
        "import tensorflow as tf\ntry:\n    Session = tf.Session\nexcept AttributeError:\n    Session = tf.compat.v1.Session\ntf.random.set_seed(10)\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\nwith Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A, B)))\nprint(result)\n",
        "\nimport tensorflow as tf\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\nresult = tf.argmax(a, axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\nresult = tf.argmax(a, axis=1)\nprint(result)\n",
        "\ndef f(a=example_a):\n    result = tf.argmax(a, axis=1)\n    return result\n",
        "\nimport tensorflow as tf\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\nresult = tf.argmin(a, axis=1)\nprint(result)\n",
        "model.save('export/1')",
        "result = tf.random.uniform([], 1, 4, dtype=tf.int32)\n",
        "result = tf.random.uniform([], 2, 5, dtype=tf.int32)\n",
        "result = tf.random.uniform([1, 10], minval=1, maxval=4, dtype=tf.int32, seed=seed_x)\n",
        "result = tf.VERSION\n"
    ],
    "Scipy": [
        "result = scipy.optimize.curve_fit(lambda x, A, B: A * np.log(x) + B, x, y, p0=[1, 1])\n",
        "\nimport numpy as np\nimport scipy\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\nresult = scipy.optimize.curve_fit(lambda x, A, B: A + B * np.log(x), x, y, p0=[1, 1])\nprint(result)\n",
        "from scipy.optimize import curve_fit\nresult = curve_fit(lambda p, x, y: p[0] * np.exp(p[1] * x) + p[2], p0, y, x)\nprint(result)",
        "\n(0.021080234718821145, 0.76584491300591395)\n",
        "from scipy.stats import kstest\nresult = kstest(x, y, 'two_samp')[1]\np_value = kstest(x, y, 'two_samp')[0]\nif p_value > alpha:\n    result = True\nelse:\n    result = False\nprint(result)",
        "from scipy.optimize import minimize\nfrom math import *\ndef f(a, b, c):\n    return ((a + b - c) - 2) ** 2 + ((3 * a - b - c) ** 2 + sin(b) + cos(b) + 4)\nresult = optimize.minimize(f, initial_guess, method='Powell', options={'maxiter': 1000})\nprint(result)\nfrom scipy.optimize import minimize\nfrom math import *\ndef f(a, b, c):\n    return ((a + b - c) - 2) ** 2 + ((3 * a - b - c) ** 2 + sin(b) + cos(b) + 4)\nresult = optimize.minimize(f, initial_guess, method='Powell', options={'maxiter': 1000})\nprint(result)",
        "\nimport numpy as np\nimport scipy.stats\nz_scores = np.array([-3, -2, 0, 2, 2.5])\np_values = scipy.stats.norm.cdf(-z_scores)\nprint(p_values)\n",
        "\nimport scipy.stats\nimport numpy as np\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\np_values = scipy.stats.norm.cdf(-z_scores, loc=mu, scale=sigma)\nprint(p_values)\n",
        "\nimport numpy as np\nimport scipy.stats\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\nz_scores = scipy.stats.norm.ppf(p_values)\nprint(z_scores)\n",
        "\nimport numpy as np\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\nx = 25\nresult = stats.lognorm.cdf(x, loc=mu, scale=stddev)\nprint(result)\n",
        "\n",
        "\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\nresult = sa * sb\nprint(result)\n",
        "\ndef f(sA = example_sA, sB = example_sB):\n    result = sA * sB\n    return result\n",
        "result = scipy.interpolate.griddata(points, V, request, method='nearest')\nprint(result)",
        "result = scipy.interpolate.griddata(points[:, 0], points[:, 1], V, (request[0, 0], request[0, 1]), method='nearest')\nresult.append(scipy.interpolate.griddata(points[:, 0], points[:, 1], V, (request[1, 0], request[1, 1]), method='nearest'))\nprint(result)",
        "\nfrom scipy import misc\nfrom scipy.ndimage import rotate\nimport numpy as np\ndata_orig = misc.face()\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nangle = np.random.randint(1, 360)\ndata_rot = rotate(data_orig, angle)\nxrot, yrot = data_rot[x0 - 1, y0 - 1]\nprint(data_rot, (xrot, yrot))\n",
        "result = M.diagonal()\nprint(result)",
        "result = stats.kstest(times, \"uniform\")\nprint(result)",
        "def f(times, rate, T):\n    ks_statistic, p_value = stats.kstest(times, \"uniform\", args=(rate, T))\n    return ks_statistic, p_value",
        "from scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n\treturn times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\nresult = stats.kstest(times, \"uniform\", nboot=1000)\nprint(result[0])\n",
        "Feature = c1.tolil()\nFeature = sparse.hstack((Feature, c2.tolil()))\nFeature = Feature.tocsr()\nprint(Feature)",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.hstack([c1, c2])\nprint(Feature)\n",
        "c1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.hstack([c1, c2])\nprint(Feature)",
        "\ndef minimize_distance(points1, points2):\n    n = points1.shape[0]\n    dist = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            dist[i, j] = np.linalg.norm(points1[i] - points2[j])\n    result = scipy.optimize.linear_sum_assignment(dist)\n    return result\npoints1 = np.array([(x, y) for x in np.linspace(-1, 1, 7) for y in np.linspace(-1, 1, 7)])\npoints2 = 2 * np.random.rand(points1.shape[0], 2) - 1\nresult = minimize_distance(points1, points2)\nprint(result)",
        "result = scipy.spatial.KDTree(points1).nearest_neighbors(points2, 2)[:, 0]\nprint(result)",
        "b.setdiag(0)",
        "result = ndimage.measurements.label(img > threshold)\nprint(result.sum())",
        "result = ndimage.measurements.label(img > threshold)\nprint(result.sum())",
        "\ndef f(img = example_img):\n    threshold = 0.75\n    result = ndimage.measurements.label(img, structure=np.ones((3, 3)), votes=1, return_counts=True)\n    regions = np.unique(result, return_counts=True)[1]\n    count = sum(regions[result > 1])\n    return count",
        "\ndef find_regions(img, threshold):\n    regions = []\n    for x in range(img.shape[0]):\n        for y in range(img.shape[1]):\n            value = img[x, y]\n            if value > threshold:\n                regions.append((x, y))\n                for dx in range(-1, 2):\n                    for dy in range(-1, 2):\n                        if (x + dx, y + dy) in regions:\n                            break\n                else:\n                    continue\n    return regions\nresult = []\nfor region in find_regions(img, threshold):\n    x, y = region\n    center_x = x + int(img.shape[0] / 2)\n    center_y = y + int(img.shape[1] / 2)\n    dx = center_x - x\n    dy = center_y - y\n    distance = np.sqrt(dx ** 2 + dy ** 2)\n    result.append(distance)\nreturn result",
        "M.make_symmetric()\nprint(M)",
        "\n",
        "result = scipy.ndimage.binary_fill_holes(square)\nprint(result)",
        "\ndef remove_isolated_cells(image):\n    mask = np.zeros_like(image)\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            if image[i, j] != 0:\n                count = 0\n                for x in range(-1, 2):\n                    for y in range(-1, 2):\n                        if image[i + x, j + y] == 0:\n                            count += 1\n                            if count == 4:\n                                mask[i, j] = 0\n                                break\n    return mask * image\nsquare = remove_isolated_cells(square)\nprint(square)",
        "mean = np.mean(col.data)\nstandard_deviation = np.std(col.data)\nprint(mean)\nprint(standard_deviation)",
        "Max = np.amax(col)\nMin = np.amin(col)\nprint(Max)\nprint(Min)",
        "Median = np.median(col.data)\nMode = np.mode(col.data)\nprint(Median)\nprint(Mode)",
        "def fourier_series(x, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15):\n    return a1 * np.cos(1 * np.pi / tau * x) + \\\n           a2 * np.cos(2 * np.pi / tau * x) + \\\n           a3 * np.cos(3 * np.pi / tau * x) + \\\n           a4 * np.cos(4 * np.pi / tau * x) + \\\n           a5 * np.cos(5 * np.pi / tau * x) + \\\n           a6 * np.cos(6 * np.pi / tau * x) + \\\n           a7 * np.cos(7 * np.pi / tau * x) + \\\n           a8 * np.cos(8 * np.pi / tau * x) + \\\n           a9 * np.cos(9 * np.pi / tau * x) + \\\n           a10 * np.cos(10 * np.pi / tau * x) + \\\n           a11 * np.cos(11 * np.pi / tau * x) + \\\n           a12 * np.cos(12 * np.pi / tau * x) + \\\n           a13 * np.cos(13 * np.pi / tau * x) + \\\n           a14 * np.cos(14 * np.pi / tau * x) + \\\n           a15 * np.cos(15 * np.pi / tau * x)\n",
        "from scipy.spatial.distance import cdist\ndef calculate_pairwise_distances(raster_array):\n    n_rows, n_cols = raster_array.shape\n    distances = np.zeros((n_rows, n_cols))\n    for i in range(n_rows):\n        for j in range(n_cols):\n            distances[i, j] = cdist(raster_array[i, :], raster_array[j, :], 'euclidean')\n    return distances\nresult = calculate_pairwise_distances(example_array)",
        "from scipy.spatial.distance import cdist\ndef calculate_pairwise_distances(raster_array):\n    distances = np.empty((raster_array.shape[0], raster_array.shape[0]))\n    for i in range(raster_array.shape[0]):\n        for j in range(raster_array.shape[0]):\n            if i != j:\n                distances[i, j] = cdist(raster_array[i], raster_array[j], 'cityblock')\n    return distances\nresult = calculate_pairwise_distances(example_array)\n",
        "def f(example_array = example_arr):\n    distances = scipy.spatial.distance.cdist(example_array, example_array, 'euclidean')\n    result = []\n    for i in range(len(distances)):\n        for j in range(len(distances[0])):\n            result.append((i, j, distances[0, j]))\n    return result",
        "result = interpolate.splev(x_val, tck, der = 0)\nreturn result",
        "\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x1, x2, x3, x4)\n",
        "\nresult = ss.anderson_ksamp(x1, x2)\n",
        "\n# [Missing Code]\n",
        "result = sa.nnz == 0\nprint(result)",
        "result = sa.sum() == 0\nprint(result)",
        "result = block_diag(*a)",
        "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\nprint(p_value)",
        "from scipy import stats\nexample_pre_course_scores = np.random.randn(10)\nexample_during_course_scores = np.random.randn(10)\ndef f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n    ranksums_result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = ranksums_result.pvalue\n    return p_value\n",
        "kurtosis_result = np.mean(np.mean(a ** 4)) - 3\n",
        "\n",
        "result = interpolate.interp2d(s, t, z)(s1, t1) - interpolate.interp2d(s, t, z)(s2, t2)",
        "result = scipy.interpolate.interp2d(x, y, z)(s, t)\nreturn result",
        "result = np.array([vor.region_point_to_region[point] for point in extraPoints])\nprint(result)",
        "\nresult = np.array([vor.region_point_to_region(extraPoint) for extraPoint in extraPoints])\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.sparse as sparse\nnp.random.seed(10)\nmax_vector_size = 1000\nvectors = [np.random.randint(100,size=900),np.random.randint(100,size=max_vector_size),np.random.randint(100,size=950)]\nresult = sparse.csr_matrix(vectors)\nprint(result)\n",
        "origin = 1\nb = scipy.ndimage.median_filter(a, 3, origin=origin)\nprint(b)",
        "result = M[row, column]",
        "result = [M[2, 3], M[1, 0]]\nprint(result)",
        "new_array[:, :, :] = scipy.interpolate.interp2d(x, array, x_new, x_new, kind='linear')\n",
        "def NormalDistro(u, o2, x):\n    dev = abs((x - u) / o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer / 2\n    return(P)\nprob = NormalDistro(u, o2, x)\nprint(prob)",
        "\ndef f(x = 2.5, u = 1, o2 = 3):\n    return f(x, u, o2)\n",
        "\nimport numpy as np\nimport scipy.fft as sf\nN = 8\nresult = sf.dctn(N, normed=True)\nprint(result)\n",
        "diags(matrix, [-1, 0, 1], (5, 5)).toarray()",
        "M = scipy.stats.binom.pmf(i, j, N, p)\nresult = np.array(M)\nprint(result)",
        "result = df.apply(lambda row: stats.zscore(row), axis=1)\ndf['sample1'], df['sample2'], df['sample3'] = result['sample1'], result['sample2'], result['sample3']\nprint(df)",
        "result = pd.DataFrame(stats.zscore(df.sample1, ddof=1), index=df.index, columns=['sample1'])\nresult = pd.concat([result, pd.DataFrame(stats.zscore(df.sample2, ddof=1), index=df.index, columns=['sample2']), pd.DataFrame(stats.zscore(df.sample3, ddof=1), index=df.index, columns=['sample3'])], axis=1)\nprint(result)",
        "result = df.assign(data=lambda x: x.sample1 + x.sample2 + x.sample3, zscore=lambda x: stats.zscore(x.sample1, x.sample2, x.sample3))\ndf = df.join(result)\nprint(df)",
        "result = df.copy()\nfor col in ['sample1', 'sample2', 'sample3']:\n    data = df[col].astype(float)\n    zscore = stats.zscore(data)\n    result[col + '_zscore'] = zscore\n    result[col] = data.round(3)\nresult.reset_index(inplace=True)\nprint(result)",
        "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)",
        "mid = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [0, 1], [0, 0]])\nresult = distance.cdist(y, x, 'euclidean', mid)\nprint(result)",
        "\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [0, 1], [0, 0]])\nresult = distance.cdist(scipy.dstack((y, x)), mid)\nprint(result)\n",
        "\ndef f(shape = (6, 6)):\n    mid = np.array([[0, 0], [shape[0] / 2, 0], [0, shape[1] / 2], [shape[0] / 2, shape[1] / 2], [0, 0]])\n    result = distance.cdist(np.dstack((np.arange(shape[0]), np.arange(shape[1]))), mid)\n    return result\n",
        "\nimport numpy as np\nimport scipy.ndimage\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\nresult = scipy.ndimage.zoom(x, 2, order=1).reshape(shape)\nprint(result)\n",
        "result = scipy.optimize.minimize(residual, fit_params, args=(a, y), method='L-BFGS-B', options={'maxiter': 1000})\nprint(result)",
        "\ndef main():\n    # simple one: a(M,N) = a(3,5)\n    a = np.array([ [ 0, 0, 1, 1, 1 ],\n                   [ 1, 0, 1, 0, 1 ],\n                   [ 0, 1, 0, 1, 0 ] ])\n    # true values of x\n    x_true = np.array([10, 13, 5, 8, 40])\n    # data without noise\n    y = func(x_true,a)\n    #************************************\n    # Apriori x0\n    x0 = np.array([2, 3, 1, 4, 20])\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = scipy.optimize.minimize(residual, fit_params, args=(a, y), method='L-BFGS-B', bounds=x_lower_bounds)\n    print(out)\nif __name__ == '__main__':\n    main()",
        "def dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)",
        "def dN1_dt_simple(t, N1):\n    return -100 * N1\nsol = solve_ivp(fun=dN1_dt_simple, t_span=[0, 10], y0=[N0, np.sin(t) if 0 < t < 2 * np.pi else 2 * np.pi])\nresult = sol.y\nprint(result)",
        "def dN1_dt_simple(t, N1):\n    return -100 * N1 - cos(t)\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)",
        "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "result = sa.hstack()\nresult = result.append(sb, axis=1)\nprint(result)",
        "sa.sum(sb)",
        "\nimport scipy.integrate\nc = 5\nlow = 0\nhigh = 1\ndef integral(c):\n    eqn = 2 * x * c\n    result, error = scipy.integrate.quad(lambda x: eqn, low, high)\n    return result\nI = []\nfor n in range(len(c)):\n    result = integral(c[n])\n    I.append(result)\nprint(I)\n",
        "\nimport scipy.integrate\ndef f(c=5, low=0, high=1):\n    return integrate.quad(lambda x: 2 * x * c, low, high)\n",
        "\nimport numpy as np\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'dok', random_state = 42)\nx = 99\nV[V != 0] += x\nprint(V)\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\nV[V != 0] += x\nprint(V)\n",
        "V[V != 0] += x\nV[V != 0] += y\nprint(V)",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport scipy\nimport numpy as np\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\na = a.astype(np.bool)\nprint(a)\n",
        "\nimport scipy\nimport numpy as np\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\na = a.astype(np.bool)\nprint(a)\n",
        "result = []\nfor i, cluster in enumerate(data):\n    centroid = centroids[i]\n    min_dist = float('inf')\n    closest_index = -1\n    for j, sample in enumerate(cluster):\n        dist = (sample - centroid).sum()\n        if dist < min_dist:\n            min_dist = dist\n            closest_index = j\n    result.append(closest_index)\nprint(result)",
        "result = []\nfor i, cluster in enumerate(centroids):\n    closest_point = scipy.spatial.KDTree(data).query(cluster, k=1)[0]\n    result.append(closest_point)\nprint(result)",
        "result = scipy.spatial.KDTree(centroids).query(data, k=k, return_distances=False, return_indices=True)[:, 1]\nprint(result)",
        "result = fsolve(eqn, xdata, args=(a, bdata))\n",
        "result = fsolve(eqn, xdata, args=(adata,))\n",
        "result = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d), range_start=range_start, range_end=range_end)\nprint(result)",
        "result = kstest_result[1]\nprint(result)",
        "from datetime import datetime\ndef rolling_integral(df, window_size):\n    result = []\n    for i in range(1, len(df) - window_size + 1):\n        start = df.index[i - 1]\n        end = df.index[i]\n        time_span = end - start\n        integral = integrate.trapz(df['A'][start:end], time=time_span)\n        result.append(integral)\n    return pd.Series(result, index=df.index)\ndf = pd.read_csv(io.StringIO(string), sep = '\\s+')\nintegral_df = rolling_integral(df, 5)\nprint(integral_df)",
        "from scipy.interpolate import griddata\nresult = griddata((x, y), (2.7, 2.3), method='linear')\nprint(result)",
        "\n",
        "result = sciopt.fminbound(e, pmin, pmax, args=(x, y))\nprint(result)",
        "\nresult = signal.argrelmax(arr, n=n)\n",
        "result = []\nfor i in range(len(arr)):\n    for j in range(len(arr[0])):\n        if arr[i][j] <= arr[i - n:i + n + 1][j].min() and arr[i][j] >= arr[i - n:i + n + 1][j].max():\n            result.append([i, j])\nprint(result)",
        "df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]"
    ],
    "Sklearn": [
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\nprint(data1)\n",
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)\nprint(data1)",
        "\nimport numpy as np\nfrom sklearn.datasets import load_boston\nimport pandas as pd\ndata = load_boston()\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\nprint(data1)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndef solve(data):\n    result = pd.DataFrame(data.data, columns=data.feature_names_)\n    return result\ndata1 = solve(data)\nprint(data1)\n",
        "\ndf_out = df.apply(lambda row: np.where(row, 1, 0), axis=1)\ndf_out.columns = ['Apple', 'Orange', 'Banana', 'Grape']\ndf_out = df_out.astype(int)\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\none_hot_encoder = OneHotEncoder(handle_unknown='ignore')\ndf_out = one_hot_encoder.fit_transform(df)\ndf_out = pd.DataFrame(one_hot_encoder.transform(df), columns=one_hot_encoder.get_feature_names_out())\nprint(df_out)\n",
        "\ndf_out = df.iloc[:, :-1].join(df[['Apple', 'Banana', 'Grape', 'Orange', 'Suica']].apply(pd.Series).astype(int).eq(0).astype(int).add(1))\ndf_out.columns = ['Apple', 'Banana', 'Grape', 'Orange', 'Suica']\ndf = df.join(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\ndf_out = pd.get_dummies(df, columns=['Col3'])\nprint(df_out)\n",
        "df_out = df.iloc[:, :-1].join(df[df.columns[-1]].apply(pd.Series).astype('int').eq(0).astype(int).astype('int').astype(str).replace({0: '0', 1: '1'}).tolist())\ndf_out.columns = ['Col1', 'Col2', 'Apple', 'Orange', 'Banana', 'Grape']\nprint(df_out)\n",
        "proba = 1 / (1 + np.exp(-x))\n",
        "proba = model.predict_proba(x_predict)\nprint(proba)",
        "transformed_data = pd.concat([df_origin, transform_output], axis=1)\ndf = pd.concat([df_origin, transformed_data], axis=1)\nprint(df)\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ntransformed_data = pd.concat([df_origin, transform_output], axis=1)\ndf = pd.concat([df_origin, transformed_data], axis=1)\nprint(df)\n",
        "transformed_data = csr_matrix(transform_output)\ndf_merged = pd.concat([df_origin, transformed_data], axis=1)\nprint(df_merged)\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ntransformed_data = csr_matrix(transform_output)\ndf_merged = pd.concat([df_origin, transformed_data], axis=1)\nprint(df_merged)",
        "\ndef solve(df, transform_output):\n    # Convert the sparse matrix to a dense matrix\n    dense_matrix = transform_output.toarray()\n    # Reshape the dense matrix to match the shape of the original dataframe\n    dense_matrix = dense_matrix.reshape(df.shape)\n    # Set the dense matrix as the value of the original column\n    df.iloc[:, 0] = dense_matrix\n    # Return the modified dataframe\n    return df\ndf = solve(df_origin, transform_output)\nprint(df)",
        "\n# Remove the 'poly' step\nestimators.pop(1)\nclf = Pipeline(estimators)\nprint(len(clf.steps))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())]\nclf = Pipeline(estimators)\ndel clf.steps[1]\nprint(len(clf.steps))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dIm', PCA()), ('pOly', PolynomialFeatures()), ('svdm', SVC())]\nclf = Pipeline(estimators)\nsteps = clf.named_steps\nsteps.pop(1)\nclf = Pipeline(estimators[:1] + estimators[2:])\nprint(clf.named_steps)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nsteps = clf.named_steps\nsteps.insert(steps.index('svdm') - 1, 't1919810')\nclf = Pipeline(estimators, steps=steps)\n",
        "\n    # [Missing Code]\n    ",
        "\nmodel = xgb.XGBRegressor()\nGridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY, fit_params={\"early_stopping_rounds\": 42, \"eval_metric\": \"mae\", \"eval_set\": [[testX, testY]]})",
        "proba = logreg.predict_proba(X)[:, 1]\nprint(proba)",
        "proba = logreg.predict_proba(X)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ninversed = scaler.inverse_transform(scaled)\nprint(inversed)\n",
        "def solve(data, scaler, scaled):\n    t_scaled = scaler.inverse_transform(scaled)\n    return t_scaled\ninversed = solve(data, scaler, scaled)\nprint(inversed)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\nprint(model_name)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\nprint(model_name)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import LinearSVC\nmodel = LinearSVC()\nmodel_name = model.__class__.__name__\nprint(model_name)\n",
        "tf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\nprint(tf_idf_out)",
        "tf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)",
        "select_out = pipe.steps[0]['select'].fit_transform(data, target)",
        "\ngrid_search = GridSearchCV(bc, param_grid, cv=3, n_jobs=-1)\ngrid_search.fit(X_train, y_train)",
        "\nX = np.array(X)\ny = np.array(y)\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\n",
        "\nX = np.array(X)\ny = np.array(y)\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\n",
        "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess)\nX = tfidf_vectorizer.fit_transform(corpus)\nprint(tfidf_vectorizer.get_feature_names())\n",
        "\ndef prePro(text):\n    return text.lower()\nvectorizer = TfidfVectorizer(preprocessor=prePro)\n",
        "from sklearn import preprocessing\ndef scale_dataframe(data):\n    data = data.copy()\n    scaler = preprocessing.StandardScaler()\n    scaler.fit(data)\n    df_out = scaler.transform(data)\n    return df_out\ndata = load_data()\ndf_out = scale_dataframe(data)\nprint(df_out)\n",
        "df_out = preprocessing.scale(data)\n",
        "coef = grid.best_estimator_.coef_[0]\nprint(coef)",
        "coef = grid.best_estimator_.coef_[:, 0]\nprint(coef)",
        "\ncolumn_names = list(X.columns)\n",
        "\ncolumn_names = model.get_feature_names()\n",
        "\ncolumn_names = list(X.columns)\n",
        "\ncolumn_names = list(X.columns)\n",
        "closest_50_samples = km.predict(X)[p]\n",
        "closest_50_samples = km.predict(X)[0][:50]\n",
        "closest_100_samples = []\nfor i in range(len(X)):\n    if p == 2:\n        closest_100_samples.append(X[i])\n        if len(closest_100_samples) == 100:\n            break\n",
        "\ndef get_samples(p, X, km):\n    centers = km.cluster_centers_\n    idx = np.argwhere(km.labels_ == p)\n    samples = X[idx]\n    return samples[:50]\nclosest_50_samples = get_samples(p, X, km)\nprint(closest_50_samples)",
        "\nX_train = get_dummies(X_train)\n",
        "\nX_train = get_dummies(X_train)\ny_train = get_dummies(y_train)\n",
        "from sklearn.svm import SVC\nsvc = SVC(kernel='rbf', gamma='auto')\nsvc.fit(X, y)\npredict = svc.predict(X)\n",
        "regressor = sklearn.svm.SVR(kernel='rbf', gamma='auto', coef0=0.0, tol=0.001, degree=3, shrinking=True, cache_size=200, verbose=False)\nregressor.fit(X, y)\npredict = regressor.predict(X)\nprint(predict)",
        "from sklearn.svm import SVC\nregressor = SVC(kernel='poly', degree=2)\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "regressor = sklearn.svm.SVR(kernel='poly', degree=2)\nregressor.fit(X, y)\npredict = regressor.predict(X)\nprint(predict)",
        "cosine_similarities_of_queries = tfidf.transform(queries).dot(tfidf.transform(documents)) / (np.linalg.norm(tfidf.transform(queries)) * np.linalg.norm(tfidf.transform(documents)))\n",
        "cosine_similarities_of_queries = tfidf.transform(queries).dot(tfidf.transform(documents)) / (np.linalg.norm(tfidf.transform(queries)) * np.linalg.norm(tfidf.transform(documents)))\n",
        "cosine_similarities_of_queries = tfidf.transform(queries)\ncosine_similarities = cosine_similarities_of_queries.dot(cosine_similarities_of_queries)\ncosine_similarities = cosine_similarities / (np.linalg.norm(cosine_similarities_of_queries) ** 2 + 1e-10)\ncosine_similarities = np.asarray(cosine_similarities)\nreturn cosine_similarities",
        "\nnew_features = one_hot_encoder.fit_transform(features).toarray()\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nf = load_data()\nnew_f = pd.DataFrame(f, columns=['t1', 't2', 't3', 't4', 't5', 't6', 't7'])\nnew_f = new_f.astype({'t1': 'int', 't2': 'int', 't3': 'int', 't4': 'int', 't5': 'int', 't6': 'int', 't7': 'int'})\nnew_f = new_f.values\nnew_f = np.array(new_f)\nprint(new_f)\n",
        "\nnew_features = one_hot_encoder.fit_transform(features).toarray()\n",
        "new_features = pd.DataFrame(features, columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\nnew_features = new_features.astype('float')\nnew_features.fillna(0, inplace=True)\nnew_features = new_features.astype(np.float64)\nnew_features = np.asarray(new_features)\nnew_features = np.reshape(new_features, (len(features), -1))\nreturn new_features",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\n# Convert the list of features to a 2D-array\nnew_features = np.array(features)\n# Convert the 2D-array to a Pandas DataFrame\nnew_features = pd.DataFrame(new_features, columns=f)\nprint(new_features)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\ndata_matrix = load_data()\ncluster_labels = hcl.linkage(data_matrix, method='complete').labels_\nprint(cluster_labels)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\ndata_matrix = load_data()\ncluster_labels = hcl.linkage(data_matrix, method='complete').labels_\nprint(cluster_labels)\n",
        "from scipy.cluster import hierarchy as hcl\ncluster_labels = hcl.linkage(simM, method='complete').labels_\nprint(cluster_labels)\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\nsimM = load_data()\ncluster_labels = hcl.linkage(simM, method='complete').labels_\nprint(cluster_labels)",
        "\n# Define the preprocessing steps\nscaler = StandardScaler()\ncenterer = FunctionTransformer(lambda x: x - np.mean(x, axis=0))\nonehotencoder = OneHotEncoder(handle_unknown='ignore')\nlabelencoder = LabelEncoder()\n# Create a ColumnTransformer object\nct = ColumnTransformer([\n    ('scaler', scaler, ['x1', 'x2']),\n    ('centerer', centerer, ['x1', 'x2']),\n    ('onehotencoder', onehotencoder, ['target']),\n    ('labelencoder', labelencoder, ['target'])\n])\n# Fit the ColumnTransformer to the data\nct.fit(data)\n# Transform the data using the fitted ColumnTransformer\ntransformed_data = ct.transform(data)\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer, RobustScaler, MaxAbsScaler, Normalizer, Binarizer, LabelEncoder, OneHotEncoder, KBinsDiscretizer, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEncoder, LabelBinarizer, LabelEnc",
        "from sklearn.preprocessing import BoxCox\nbox_cox_data = data.copy()\nbox_cox_data = BoxCox(data, feature_range=(0, 1))\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\nfrom sklearn.preprocessing import BoxCox\nbox_cox_data = BoxCox().fit_transform(data)\nprint(box_cox_data)\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, YeoJohnson\nyeo_johnson_data = data\nyeo_johnson_scaler = YeoJohnson()\nyeo_johnson_scaler.fit(yeo_johnson_data)\nyeo_johnson_data = yeo_johnson_scaler.transform(yeo_johnson_data)\n",
        "from sklearn.preprocessing import MinMaxScaler\nyeo_johnson_data = data.apply(lambda x: (x - x.mean()) / x.std())\n",
        "vectorizer = CountVectorizer(max_df=0.85, min_df=1, max_features=None, stop_words='english', analyzer='word', binary=False)\ntransformed_text = vectorizer.fit_transform(text)\nprint(transformed_text)",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\n# Split the dataset into training and testing sets\nx_train, y_train, x_test, y_test = train_test_split(dataset, dataset[dataset.columns[-1]], test_size=0.2, random_state=42)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nfrom sklearn.model_selection import train_test_split\nx, y = data.drop('target', axis=1), data['target']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\n# Split the dataset into training and testing sets\nx_train, y_train, x_test, y_test = train_test_split(dataset, dataset[dataset.columns[-1]], test_size=0.33, random_state=42)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\ndef solve(data):\n    x_train, y_train, x_test, y_test = train_test_split(data, test_size=0.2, random_state=42)\n    le = LabelEncoder()\n    y_train = le.fit_transform(y_train)\n    y_test = le.transform(y_test)\n    return x_train, y_train, x_test, y_test\nx_train, y_train, x_test, y_test = solve(dataset)",
        "from sklearn.cluster import KMeans\ndf = load_data()\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\nprint(labels)",
        "\nfrom sklearn.cluster import KMeans\ndf = load_data()\nX = df['mse'].values\nf2 = list(range(0, len(X)))\nX = np.array(list(zip(X, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\nprint(labels)\n",
        "selected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]",
        "selected_feature_names = np.asarray(vectorizer.get_feature_names())[LinearSVC(C=1.0, penalty='l1', max_iter=1000, tol=0.0001, class_weight=None, dual=False, fit_intercept=True, n_jobs=None, random_state=None, verbose=False).get_support()]",
        "selected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]",
        "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())",
        "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript', 'TypeScript', 'SQL', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n[Output]\n['Jscript', 'TypeScript', 'SQL', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX']\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', 'Java', 'TeamCity', 'TypeScript', 'UI Design', 'Web', 'Integration', 'Database design', 'UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n[Output]\n['Jscript', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', 'Java', 'TeamCity', 'TypeScript', 'UI Design', 'Web', 'Integration', 'Database design', 'UX']",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:, 0], npMatrix[:, 1]\n    slope = LinearRegression().fit(X, Y)\n    m = slope.coef_[0]\n    slopes.append(m)\nprint(slopes)\n",
        "\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:, 0], npMatrix[:, 1]\n    slope = LinearRegression().fit(X, Y)\n    m = slope.coef_[0]\n    series = np.concatenate((series, m), axis=0)\nslopes = series.tolist()\nprint(slopes)",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "df['Sex'] = LabelEncoder.fit_transform(df['Sex'])\n",
        "\ndef load_data():\n    return np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), np.array([1, 2, 3]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), np.array([1, 2, 3])\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\nprint(training_set_score)\nprint(test_set_score)",
        "transformed = MinMaxScaler().fit_transform(np_array)\n",
        "transformed = MinMaxScaler().fit_transform(np_array)\n",
        "new_a = (a - a.min()) / (a.max() - a.min())\nreturn new_a",
        "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict(b)\nprint(predict)",
        "new_X = np.array(X)\nclf.fit(new_X, ['2', '3'])",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['asdf', '1'], ['asdf', '0']]\nclf = DecisionTreeClassifier()\nnew_X = np.array(X, dtype=str)\nclf.fit(new_X, ['2', '3'])\n",
        "new_X = np.array(X)\nclf.fit(new_X, ['4', '5'])",
        "predict = logReg.predict(X)\nprint(predict)",
        "X = dataframe.iloc[:, 0:5]\ny = dataframe.iloc[:, 5]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\npredict = logReg.predict(X)\nprint(predict)",
        "\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\nprint(train_dataframe)\nprint(test_dataframe)",
        "\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\nprint(train_dataframe)\nprint(test_dataframe)",
        "\n    train_size = 0.2\n    train_dataframe, test_dataframe = cross_validation.train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort([\"date\"])\n    test_dataframe = test_dataframe.sort([\"date\"])\n    return train_dataframe, test_dataframe\ntrain_dataframe, test_dataframe = solve(features_dataframe)\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\n# [Missing Code]\n",
        "\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].transform(scaler.fit_transform)\n",
        "words = \"Hello @friend, this is a good day. #good.\"\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names())",
        "feature_names = count.get_feature_names_out()\nprint(feature_names)",
        "full_results = GridSearch_fitted.cv_results_\nfull_results = pd.DataFrame(full_results)\nfull_results.columns = ['parameter', 'score']\nreturn full_results",
        "full_results = GridSearch_fitted.cv_results_\nfull_results = full_results.sort_values(by=['mean_fit_time'], ascending=False)\nfull_results = full_results.reset_index()\nfull_results = full_results.rename(columns={'mean_fit_time': 'Accuracy'})\nprint(full_results)",
        "\nfitted_model = load_data()\n# Save the model in the file named \"sklearn_model\"\nwith open(\"sklearn_model\", \"wb\") as f:\n    pickle.dump(fitted_model, f)\n",
        "vectors = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = np.dot(vectors, vectors.T) / (vectors.T @ vectors)\ncosine_similarity_matrix = np.diag(1 - np.diag(cosine_similarity_matrix))\nprint(cosine_similarity_matrix)"
    ],
    "Pytorch": [
        "optim = torch.optim.SGD(..., lr=0.01)\ndef adjust_learning_rate(optimizer, new_lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\ndef main():\n    optim = torch.optim.SGD(..., lr=0.01)\n    adjust_learning_rate(optim, 0.001)\nif __name__ == \"__main__\":\n    main()\n",
        "optim = torch.optim.SGD(..., lr=0.01)\ndef update_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\ndef check_lr(optimizer, loss_increase):\n    if loss_increase:\n        update_lr(optimizer, 0.001)\ndef train(epoch, model, train_loader, optimizer, criterion):\n    model.train()\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\ndef main():\n    model = torch.nn.Linear(1, 1)\n    train_loader = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(torch.randn(10, 1), torch.randint(0, 10, size=(10,))),\n        batch_size=1, shuffle=True)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n    criterion = torch.nn.CrossEntropyLoss()\n    for epoch in range(10):\n        train(epoch, model, train_loader, optimizer, criterion)\n        check_lr(optimizer, False)\n        if epoch % 2 == 0:\n            check_lr(optimizer, True)\nif __name__ == '__main__':\n    main()",
        "optim = torch.optim.SGD(..., lr=0.005)\ndef adjust_learning_rate(optimizer, new_lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\ndef main():\n    optim = torch.optim.SGD(..., lr=0.005)\n    adjust_learning_rate(optim, 0.0005)\nif __name__ == \"__main__\":\n    main()\n",
        "optim = torch.optim.SGD(..., lr=0.005)\nif loss_increase:\n    optim.lr = 0.0005\n",
        "embedded_input = torch.Tensor(word2vec.wv.syn0.data)\ninput_Tensor = torch.tensor(input_Tensor)\nembedded_input = embedded_input.unsqueeze(0)\ninput_Tensor = input_Tensor.to(dtype=embedded_input.dtype)\ninput_Tensor = input_Tensor.unsqueeze(0)\ninput_Tensor = torch.cat((input_Tensor, embedded_input), dim=1)\nprint(input_Tensor)",
        "embedded_input = torch.Tensor(word2vec.wv.syn0.data)\nreturn embedded_input",
        "x = torch.rand(4,4)\npx = pd.DataFrame(x.detach().numpy())\nprint(px)",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x.detach().cpu().numpy())\nprint(px)\n",
        "x = torch.rand(6,6)\npx = pd.DataFrame(x.detach().numpy())\nprint(px)",
        "C = B[:, A_log] # Throws error",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_logical, B = load_data()\nC = B[:, A_logical]\nprint(C)\n",
        "C = B[:, A_log] # Throws error",
        "C = B[:, A_log] # Throws error",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    C = B[:, A_log]\n    return C\nC = solve(A_log, B)\nprint(C)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\nC = B[A_log]\nprint(C)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nidx, B = load_data()\nC = B.index_select(0, idx)\nprint(C)\n",
        "x_array = np.array([[0.5, 1.0, 2.0], [4.0, 6.0, 8.0]], dtype=np.float16)\nx_tensor = torch.from_numpy(x_array)\nprint(x_tensor)",
        "\n# [Missing Code]\n",
        "from torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport torch\ndef Convert(a):\n    x_tensor = torch.from_numpy(a).float()\n    return x_tensor\nx_tensor = Convert(x_array)\nprint(x_tensor)",
        "mask = torch.zeros(len(lens), lens[0]).to(torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = torch.LongTensor([1] * length)\nprint(mask)",
        "mask = torch.zeros(len(lens), 9)\nfor i, length in enumerate(lens):\n    mask[i, :length] = torch.ones(length)\nprint(mask)",
        "mask = torch.zeros(len(lens), lens[0]).to(torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = torch.arange(length, dtype=torch.long)\nprint(mask)",
        "\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nTensor_2D = load_data()\nTensor_3D = torch.diag(Tensor_2D)\nprint(Tensor_3D)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nTensor_2D = load_data()\ndef Convert(t):\n    drag_ele = torch.tensor(np.diag(Tensor_2D))\n    result = torch.zeros(Tensor_2D.shape[0], 1, 1)\n    result[:, 0, 0] = drag_ele\n    return result\nTensor_3D = Convert(Tensor_2D)\nprint(Tensor_3D)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nab = torch.cat((a, b), dim=0)\nprint(ab)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nab = torch.cat((a, b), dim=0)\nprint(ab)\n",
        "\ndef solve(a, b):\n    ab = torch.cat((a, b), dim=0)\n    return ab\nab = solve(a, b)\nprint(ab)\n",
        "a[ : , lengths : , : ] = 0\nprint(a)",
        "a[ : , lengths : , : ] = 2333\nprint(a)",
        "a[ : , : lengths , : ] = 0\nprint(a)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.rand((10, 1000, 23))\nlengths = torch.randint(1000, (10,))\na[ : , : lengths , : ] = 2333\nprint(a)\n",
        "list_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nprint(tensor_of_tensors)",
        "new_tensors = torch.stack(list, dim=0)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\ndef Convert(lt):\n    tt = torch.tensor(lt)\n    return tt\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n",
        "tensor_of_tensors = torch.stack(list_of_tensors, dim=0)",
        "\nimport numpy as np\nimport torch\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = torch.tensor([t[i] for i in idx])\nprint(result)\n",
        "\nimport numpy as np\nimport torch\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = t[idx]\nprint(result)\n",
        "\nimport numpy as np\nimport torch\nt = torch.tensor([[-0.2,  0.3],\n    [-0.5,  0.1],\n    [-0.4,  0.2]])\nidx = np.array([1, 0, 1])\nresult = torch.tensor([t[idx[i]] for i in range(len(idx))])\nprint(result)\n",
        "result = x.gather(1, ids)\nprint(result)",
        "result = x.gather(1, ids)\nprint(result)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nids, x = load_data()\nresult = x[ids == 1]\nprint(result)\n",
        "def get_class_probability(softmax_output):\n    max_probability = softmax_output.max(dim=1)[0]\n    class_indices = (softmax_output == max_probability).nonzero().squeeze(1)\n    return class_indices.astype(int)\nsoftmax_output = [[0.2, 0.1, 0.7], [0.6, 0.2, 0.2], [0.1, 0.8, 0.1]]\nresult = get_class_probability(softmax_output)\nprint(result)\n[0, 1, 2]",
        "\ndef get_class_probability(softmax_output):\n    max_probability = softmax_output.max(dim=1)[0]\n    class_indices = (softmax_output == max_probability).nonzero().squeeze(1)\n    return class_indices.astype(int)\nsoftmax_output = torch.tensor([[0.7, 0.2, 0.1], [0.2, 0.6, 0.2], [0.1, 0.1, 0.8]])\nresult = get_class_probability(softmax_output)\nprint(result)\n",
        "def get_lowest_probability_class(softmax_output):\n    probabilities = softmax_output.cpu().numpy()\n    indices = np.argmax(probabilities, axis=1)\n    classes = np.array([2, 1, 0])\n    result = np.zeros(len(softmax_output), dtype=int)\n    for i in range(len(result)):\n        result[i] = classes[indices[i]]\n    return torch.tensor(result)\nsoftmax_output = torch.tensor([[0.2, 0.1, 0.7], [0.6, 0.3, 0.1], [0.15, 0.8, 0.05]])\nlowest_probability_class = get_lowest_probability_class(softmax_output)\nprint(lowest_probability_class)\n[Output]\n[1, 2, 2]",
        "\ndef solve(softmax_output):\n    y_pred = softmax_output.max(dim=1)[1].type_as(softmax_output)\n    return y_pred\ny = solve(softmax_output)\nprint(y)\n",
        "\ndef solve(softmax_output):\n    y = torch.zeros(softmax_output.shape[0], dtype=torch.long)\n    for i in range(softmax_output.shape[0]):\n        y[i, softmax_output[i, 0].item()] = 1\n    return y\ny = solve(softmax_output)\nprint(y)",
        "\nloss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False)\nif size_average:\n    loss /= mask.data.sum()\nreturn loss",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_equal = np.sum(A == B)\nprint(cnt_equal)\n",
        "cnt_equal = np.sum(A == B)\nprint(cnt_equal)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\nnot_equal = (A != B).sum()\ncnt_not_equal = not_equal.item()\nprint(cnt_not_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ndef Count(A, B):\n    cnt_equal = np.equal(A, B).sum()\n    return cnt_equal\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_equal = np.equal(A.flatten()[-x:], B.flatten()[-x:]).sum()\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_not_equal = np.sum(A[:, -x:].ne(B[:, -x:]))\nprint(cnt_not_equal)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nmask, clean_input_spectrogram, output= load_data()\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\nprint(output)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nmask, clean_input_spectrogram, output= load_data()\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\nprint(output)\n",
        "\nmin_abs = torch.min(torch.abs(x), torch.abs(y))\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nsigned_min = min_abs * (sign_x if min_abs == torch.abs(x) else sign_y)\nprint(signed_min)",
        "max_abs = torch.max(torch.abs(x), torch.abs(y))\nmax_sign = torch.sign(max_abs)\nsigned_max = torch.where(torch.eq(x, max_abs), max_sign, torch.where(torch.eq(y, max_abs), max_sign, torch.zeros_like(max_sign)))\nprint(signed_max)\nimport numpy as np\nimport pandas as pd\nimport torch\nx, y = load_data()\nmax_abs = torch.max(torch.abs(x), torch.abs(y))\nmax_sign = torch.sign(max_abs)\nsigned_max = torch.where(torch.eq(x, max_abs), max_sign, torch.where(torch.eq(y, max_abs), max_sign, torch.zeros_like(max_sign)))\nprint(signed_max)",
        "\ndef solve(x, y):\n    min_abs = torch.min(torch.abs(x), torch.abs(y))\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    selected_elements = torch.where(torch.abs(x) < min_abs, sign_x, torch.where(torch.abs(y) < min_abs, sign_y, torch.zeros_like(x)))\n    return selected_elements\nsigned_min = solve(x, y)\nprint(signed_min)",
        "confidence_score = torch.nn.Softmax(dim=1)(output)[:, 0]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nresult = torch.cat([a[:, :-1].float(), b[:, 1:].float(), torch.mean(a[:, -1:])], dim=1)\nprint(result)\n",
        "\n",
        "result = np.zeros((8, 8))\nresult[np.arange(8), np.arange(8)] = t.numpy()\nprint(result)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = load_data()\nresult = torch.cat([t, new], dim=0)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = load_data()\nresult = torch.stack([t, new])\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nhid_dim = 32\ndata = torch.randn(10, 2, 3, hid_dim)\ndata = data.view(10, 2 * 3, hid_dim)\nW = torch.randn(hid_dim)\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n"
    ]
}