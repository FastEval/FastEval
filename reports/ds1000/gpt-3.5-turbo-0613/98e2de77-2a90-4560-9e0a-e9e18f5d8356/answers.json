{
    "Pandas": [
        "result = df.iloc[List]",
        "```python\ndf = df.iloc[List]\nresult = len(df[df['Type'] != df['Type'].shift()])\n```",
        "```python\nresult = df.apply(lambda x: x.mask(pd.value_counts(x) < 2, 'other'))\n```",
        "```python\nresult = df.apply(lambda x: x.mask(pd.value_counts(x) < 3, 'other'))\n```",
        "```python\nresult = df.apply(lambda x: x.mask(pd.value_counts(x) < 2, 'other'))\n```",
        "```python\n# Replace values in Qu1 column\ndf['Qu1'] = df['Qu1'].where(df['Qu1'].map(df['Qu1'].value_counts()) >= 3, 'other')\n\n# Replace values in Qu2 and Qu3 columns\ndf['Qu2'] = df['Qu2'].where(df['Qu2'].map(df['Qu2'].value_counts()) >= 2, 'other')\ndf['Qu3'] = df['Qu3'].where(df['Qu3'].map(df['Qu3'].value_counts()) >= 2, 'other')\n\nresult = df\n```",
        "```python\n# Replace values in Qu1 column\nvalue_counts_qu1 = df['Qu1'].value_counts()\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x not in value_counts_qu1[value_counts_qu1 >= 3].index and x != 'apple' else x)\n\n# Replace values in Qu3 column\nvalue_counts_qu3 = df['Qu3'].value_counts()\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x not in value_counts_qu3[value_counts_qu3 >= 2].index else x)\n\nresult = df\n```",
        "result = df.drop_duplicates(subset='url', keep='first')\nresult = result[result['keep_if_dup'] == 'Yes']",
        "result = df.drop_duplicates(subset='url', keep='first')",
        "result = df.drop_duplicates(subset='url', keep='last')",
        "```python\nresult = df.groupby('name').apply(lambda x: x.set_index('name').to_dict(orient='index')).to_dict()\n```",
        "df['datetime'] = df['datetime'].dt.tz_localize(None)",
        "```python\nresult = df['datetime'].dt.tz_localize(None)\n```",
        "df['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')",
        "df['datetime'] = df['datetime'].dt.tz_localize(None)",
        "```python\n# [Missing]\nresult = pd.concat([df, df['message'].str.extractall(r'(\\w+): ([\\w\\s]+)')[1].unstack().add_prefix('')], axis=1)\n```",
        "df.loc[df['product'].isin(products), 'score'] *= 10",
        "```python\ndf.loc[~df['product'].isin(products), 'score'] = df.loc[~df['product'].isin(products), 'score'] * 10\n```",
        "df.loc[df['product'].isin(products[0] + products[1]), 'score'] *= 10",
        "```python\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'].apply(lambda x: (x - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min()))\n```",
        "df['category'] = df.idxmax(axis=1)",
        "df['category'] = df.idxmax(axis=1)",
        "```python\nresult = df.apply(lambda x: df.columns[x.astype(bool)].tolist(), axis=1)\n```",
        "df['Date'] = df['Date'].dt.strftime('%b-%Y')",
        "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')",
        "result = df[df['Date'].between(List[0], List[1])].copy()\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y %A')",
        "df = df.shift(1, axis=0).fillna(df.iloc[-1])",
        "df = df.shift(-1, axis=0)",
        "df = df.shift(1, axis=0).fillna(df.iloc[-1]).rename(index={df.index[0]: df.index[-1]})",
        "df = df.shift(1).iloc[[-1]].append(df.shift(-1))",
        "df.rename(columns=lambda x: x + 'X', inplace=True)",
        "df.columns = 'X' + df.columns",
        "```python\ndf.columns = ['X' + col if not col.endswith('X') else col for col in df.columns]\n```",
        "result = df.groupby('group').agg({\"group_color\": \"first\"})\nresult = result.join(df.filter(like='val').mean())",
        "result = df.groupby('group').agg({\"group_color\": \"first\"})\nresult = result.join(df.filter(like='val').sum())",
        "```python\nresult = df.groupby('group').agg({\"group_color\": \"first\"})\nvalue_columns = [col for col in df.columns if col.startswith('val')]\nfor col in value_columns:\n    if col.endswith('2'):\n        result[col] = df.groupby('group')[col].mean()\n    else:\n        result[col] = df.groupby('group')[col].sum()\n```",
        "result = df.loc[row_list, column_list].mean()",
        "result = df.loc[row_list, column_list].sum()",
        "result = df.loc[row_list, column_list].sum()",
        "result = df.apply(pd.Series.value_counts)",
        "result = df.isnull().sum()",
        "result = df.apply(pd.Series.value_counts)\nresult = result.transpose()\n\n# [Missing]\n\nprint(result)",
        "result = df.iloc[1:].reset_index(drop=True)\nresult.columns = df.iloc[0]\nresult",
        "result = df.iloc[1:].reset_index(drop=True)\nresult.columns = df.iloc[0]\nresult",
        "```python\nresult = df.ffill(axis=1).bfill(axis=1)\n```",
        "```python\nresult = df.ffill(axis=1).bfill(axis=1)\n```",
        "```python\nresult = df.fillna(method='ffill')\n```",
        "```python\ndf.loc[df['value'] < thresh, 'lab'] = 'X'\ndf = df.groupby('lab').sum()\n```",
        "```python\ndf.loc[df['value'] >= thresh] = df.loc[df['value'] >= thresh].mean()\nresult = df\n```",
        "```python\nresult = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\naverage = df[(df['value'] < section_left) | (df['value'] > section_right)]['value'].mean()\nresult = result.append(pd.DataFrame({'value': [average]}, index=['X']))\n```",
        "result = df.copy()\nfor col in df.columns:\n    result[f\"inv_{col}\"] = 1 / df[col]",
        "result = df.copy()\nresult = result.join(pd.DataFrame(np.exp(result[col]), columns=['exp_'+col]) for col in result.columns)",
        "result = df.copy()\nresult = result.assign(**{f\"inv_{col}\": 1/result[col] for col in result.columns if col != '0'})",
        "```python\nresult = df.copy()\nresult = result.join(result.apply(lambda x: 1/(1+np.exp(-x)), axis=0).add_prefix('sigmoid_'))\n```",
        "result = df[df.columns[df.idxmin().values[0]:].max().idxmax()].idxmax()",
        "result = df.idxmax().mask(df.idxmax().cumsum() > df.idxmin().cumsum()).ffill()",
        "```python\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\ndate_range = pd.date_range(start=min_date, end=max_date)\n\nresult = df.set_index(['user', 'dt']).reindex(pd.MultiIndex.from_product([df['user'].unique(), date_range], names=['user', 'dt'])).fillna(0).reset_index()\n```",
        "```python\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\nall_dates = pd.date_range(start=min_date, end=max_date, freq='D')\nusers = df['user'].unique()\n\nresult = pd.DataFrame({'dt': all_dates, 'user': users})\nresult = result.merge(df, on=['dt', 'user'], how='left').fillna(0)\n```",
        "```python\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\ndate_range = pd.date_range(start=min_date, end=max_date)\n\nresult = df.set_index(['user', 'dt']).reindex(pd.MultiIndex.from_product([df['user'].unique(), date_range], names=['user', 'dt'])).reset_index()\nresult['val'] = result['val'].fillna(233)\n\n```",
        "df = df.set_index('dt').groupby('user').resample('D').ffill().reset_index()",
        "df = df.set_index('dt').groupby('user').resample('D').ffill().reset_index().sort_values('dt')\nresult = df.copy()\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')",
        "df['name'] = pd.factorize(df['name'])[0]",
        "df['a'] = df.groupby('name').ngroup() + 1",
        "df['name'] = pd.factorize(df['name'])[0]",
        "df['ID'] = df.groupby(['name', 'a']).ngroup() + 1",
        "```python\nresult = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n```",
        "```python\nresult = pd.melt(df, id_vars=['user'], var_name='others', value_name='value')\n```",
        "```python\nresult = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n```",
        "result = df[df['c'] > 0.5][columns]",
        "result = df.loc[df['c'] > 0.45, columns]",
        "```python\nimport pandas as pd\ndef f(df, columns=['b', 'e']):\n    result = df[df['c'] > 0.5][columns].values\n    return result\n```",
        "result = df[df['c'] > 0.5][columns]\nresult['sum'] = result.sum(axis=1)",
        "result = df[df['c'] > 0.5][columns]",
        "df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['diff'] = df['date'].diff().dt.days\ndf = df[df['diff'] > X]\nresult = df[['ID', 'date', 'close']]",
        "```python\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['diff'] = df['date'].diff().dt.days\ndf = df[df['diff'] >= X*7]\nresult = df.drop(columns='diff')\n```",
        "```python\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['date_diff'] = df['date'].diff()\ndf = df[df['date_diff'] > pd.Timedelta(weeks=X)]\ndf = df.drop('date_diff', axis=1)\nresult = df\n```",
        "```python\nresult = df.groupby(df.index // 3).mean()\n```",
        "```python\nresult = df.groupby(df.index // 3).mean()\n```",
        "```python\nresult = df.groupby(df.index // 4).sum()\n```",
        "```python\nresult = df[::-1].rolling(3).mean()[::-1].iloc[2::3]\n```",
        "```python\nresult = df.groupby(df.index // 3).sum().append(df.groupby(df.index // 2).mean()).reset_index(drop=True)\n```",
        "```python\nresult = pd.concat([df.groupby(df.index // 3).sum(), df.groupby(df.index // 2).mean()]).sort_index().reset_index(drop=True)\n```",
        "df['A'] = df['A'].replace(0, method='ffill')",
        "df['A'] = df['A'].replace(0, method='ffill')",
        "df['A'] = df['A'].replace(0, pd.NA).fillna(method='ffill').fillna(method='bfill')",
        "df['number'] = df['duration'].str.extract('(\\d+)')\ndf['time'] = df['duration'].str.extract('(\\D+)')\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "df['numer'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ndf['time_day'] = df.time.replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "```python\ndf['number'] = df['duration'].str.extract('(\\d+)')\ndf['time'] = df['duration'].str.extract('(\\D+)')\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n```",
        "df['number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ndf['time_day'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\ndf['time_day'] *= df['number']",
        "result = np.where(np.any(df1[columns_check_list] != df2[columns_check_list], axis=1))",
        "result = np.where(np.all(df1[columns_check_list] == df2[columns_check_list], axis=1))",
        "df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)",
        "df.index.levels[1] = pd.to_datetime(df.index.levels[1])",
        "```python\nimport pandas as pd\nimport numpy as np\n\ndef f(df):\n    df.reset_index(inplace=True)\n    df['date'] = pd.to_datetime(df['date'])\n    return df.values\n```",
        "```python\ndf.index = pd.to_datetime(df.index.get_level_values(0))\ndf = df.swaplevel().sort_index()\n```",
        "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')",
        "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(by='year', ascending=False)\nresult = df.pivot(index=['Variable', 'Country', 'year'], columns='Variable', values='value').reset_index()",
        "result = df[(df.filter(like='Value').abs() < 1).all(axis=1)]",
        "result = df[(df.filter(like='Value').abs() > 1).any(axis=1)]",
        "```python\ncolumns = [col for col in df.columns if col.startswith('Value')]\ndf = df[df[columns].abs() > 1].dropna()\ndf.columns = [col.replace('Value_', '') for col in df.columns]\nresult = df\n```",
        "df = df.replace('&AMP;', '&', regex=True)",
        "df = df.replace('&LT', '<', regex=True)",
        "result = df.replace('&AMP;', '&', regex=True)",
        "df = df.replace(['&AMP;', '&LT;', '&GT;'], ['&', '<', '>'], regex=True)",
        "df = df.replace('&AMP;', '&', regex=True)",
        "```python\ndf['first_name'] = df['name'].apply(lambda x: x.split(' ')[0] if len(x.split(' ')) == 2 else x)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[1] if len(x.split(' ')) == 2 else None)\n```",
        "df[['1_name', '2_name']] = df['name'].str.split(' ', 1, expand=True)",
        "```python\ndf['first name'] = df['name'].str.split(' ').str[0]\ndf['middle_name'] = df['name'].apply(lambda x: ' '.join(x.split(' ')[1:-1]) if len(x.split(' ')) > 2 else None)\ndf['last_name'] = df['name'].str.split(' ').str[-1]\ndf.drop('name', axis=1, inplace=True)\n```",
        "result = pd.merge(df2, df1, on='Timestamp', how='left')",
        "result = pd.merge_asof(df1, df2, on='Timestamp')",
        "df['state'] = df[['col1', 'col2', 'col3']].max(axis=1)",
        "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)",
        "```python\nerror_values = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        error_values.append(row['Field1'])\nresult = error_values\n```",
        "```python\nresult = df[df['Field1'].apply(lambda x: isinstance(x, int))]\nresult = result['Field1'].tolist()\n```",
        "```python\nimport pandas as pd\n\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n\ndef f(df=example_df):\n    result = []\n    for index, row in df.iterrows():\n        if not isinstance(row['Field1'], int):\n            result.append(row['Field1'])\n    return result\n```",
        "```python\nresult = df.set_index('cat').div(df.sum(axis=1), axis=0)\n```",
        "```python\ntotal = df.sum(axis=1)\nresult = df.iloc[:, 1:].div(total, axis=0)\n```",
        "result = df.loc[test]",
        "result = df.loc[test]",
        "df = df.drop(test)",
        "result = df.loc[test].drop_duplicates()",
        "```python\nimport numpy as np\n\n# Calculate pairwise distances between cars\ndistances = np.sqrt((df['x'].values[:, np.newaxis] - df['x'].values) ** 2 + (df['y'].values[:, np.newaxis] - df['y'].values) ** 2)\n\n# Set the diagonal elements to infinity to exclude self-distances\nnp.fill_diagonal(distances, np.inf)\n\n# Find the nearest neighbor for each car\nnearest_neighbor = np.argmin(distances, axis=1)\n\n# Calculate the Euclidean distance to the nearest neighbor\neuclidean_distance = np.min(distances, axis=1)\n\n# Add the nearest neighbor and Euclidean distance to the dataframe\ndf['nearest_neighbour'] = nearest_neighbor\ndf['euclidean_distance'] = euclidean_distance\n\nresult = df\n```",
        "```python\n# Calculate the pairwise distances between cars\ndistances = df.groupby('time').apply(lambda x: pd.DataFrame(squareform(pdist(x[['x', 'y']]))))\n\n# Get the farmost neighbour for each car\ndf2 = pd.DataFrame()\nfor t in df['time'].unique():\n    df_t = distances.loc[t]\n    farmost_neighbour = df_t.idxmax(axis=1)\n    euclidean_distance = df_t.max(axis=1)\n    df_t['car'] = df['car']\n    df_t['farmost_neighbour'] = farmost_neighbour\n    df_t['euclidean_distance'] = euclidean_distance\n    df2 = pd.concat([df2, df_t])\n\ndf2.reset_index(inplace=True)\ndf2 = df2[['time', 'car', 'farmost_neighbour', 'euclidean_distance']]\n\nresult = df2\n```",
        "```python\ndf[\"keywords_all\"] = df.apply(lambda row: \",\".join(row.dropna()), axis=1)\n```",
        "```python\ndf[\"keywords_all\"] = df.apply(lambda row: \"-\".join(row.dropna()), axis=1)\n```",
        "```python\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n```",
        "```python\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n```",
        "df_sample = df.sample(frac=0.2, random_state=0)\ndf.loc[df_sample.index, 'Quantity'] = 0",
        "df_sample = df.sample(frac=0.2, random_state=0)\ndf.loc[df_sample.index, 'ProductId'] = 0",
        "df_sampled = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\ndf_sampled['Quantity'] = 0\ndf.update(df_sampled)",
        "df['index_original'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\nresult = df.loc[df.duplicated(subset=['col1','col2'], keep='first')]",
        "df['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int) * df.index\nresult = df.loc[df['index_original'] != 0]",
        "df['index_original'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)",
        "```python\ndf['index_original'] = duplicate.index[0]\n```",
        "```python\nduplicate['index_original'] = duplicate.index\n```",
        "result = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].max()])",
        "```python\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n```",
        "```python\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])\n```",
        "result = df.groupby(['Sp', 'Value']).apply(lambda x: x[x['count'] == x['count'].max()])",
        "result = df.query(\"Category in @filter_list\")",
        "result = df.query(\"Category != @filter_list\")",
        "result = pd.melt(df, value_vars=df.columns)",
        "result = pd.melt(df, value_vars=[tuple(col) for col in df.columns])",
        "df['cumsum'] = df.groupby('id')['val'].cumsum()",
        "df['cumsum'] = df.groupby('id')['val'].cumsum()",
        "df['cumsum'] = df.groupby('id')['val'].cumsum()",
        "df['cummax'] = df.groupby('id')['val'].cummax()",
        "df['cumsum'] = df.groupby('id')['val'].cumsum().clip(lower=0)",
        "result = df.groupby('l')['v'].sum(skipna=False)",
        "result = df.groupby('r')['v'].sum(skipna=False)",
        "```python\nresult = df.groupby('l')['v'].sum(skipna=False)\n```",
        "```python\nresult = []\ncolumns = df.columns\n\nfor i in range(len(columns)):\n    for j in range(len(columns)):\n        if i != j:\n            if df[columns[i]].nunique() == df[columns[j]].nunique():\n                result.append(f'{columns[i]} {columns[j]} one-to-one')\n            elif df[columns[i]].nunique() > df[columns[j]].nunique():\n                result.append(f'{columns[i]} {columns[j]} one-to-many')\n            else:\n                result.append(f'{columns[i]} {columns[j]} many-to-one')\n        else:\n            result.append(f'{columns[i]} {columns[j]} many-to-many')\n```",
        "```python\nresult = []\ncolumns = df.columns.tolist()\n\nfor i in range(len(columns)):\n    for j in range(len(columns)):\n        if i != j:\n            if df[columns[i]].nunique() == df[columns[j]].nunique():\n                result.append(f\"{columns[i]} {columns[j]} many-2-many\")\n            elif df[columns[i]].nunique() > df[columns[j]].nunique():\n                result.append(f\"{columns[i]} {columns[j]} one-2-many\")\n            else:\n                result.append(f\"{columns[i]} {columns[j]} many-2-one\")\n        else:\n            result.append(f\"{columns[i]} {columns[j]} one-2-one\")\n```",
        "```python\nresult = df.corr().applymap(lambda x: 'one-to-one' if x == 1 else 'one-to-many' if x == 0 else 'many-to-one' if x == -1 else 'many-to-many')\n```",
        "```python\nresult = df.corr().applymap(lambda x: 'one-2-one' if x == 1 else 'many-2-one' if x > 0.5 else 'one-2-many' if x < -0.5 else 'many-2-many')\n```",
        "df = df.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep=False)",
        "result = pd.to_numeric(s.astype(str).str.replace(',',''), errors='coerce')",
        "result = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0))['Survived'].mean()",
        "result = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0))['SibSp'].mean()",
        "result = df.groupby([(df['SibSp'] == 1) & (df['Parch'] == 1), \n                     (df['SibSp'] == 0) & (df['Parch'] == 0), \n                     (df['SibSp'] == 0) & (df['Parch'] == 1), \n                     (df['SibSp'] == 1) & (df['Parch'] == 0)])['Survived'].mean()",
        "df = df.groupby('cokey').apply(lambda x: x.sort_values('A'))",
        "df = df.groupby('cokey').apply(lambda x: x.sort_values('A')).reset_index(drop=True)",
        "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])",
        "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])",
        "df.columns = pd.MultiIndex.from_tuples(df.columns)",
        "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})",
        "result = df.groupby('a')['b'].agg(['mean', 'std'])",
        "```python\nresult = df.groupby('b')['a'].agg(['mean', 'std'])\n```",
        "```python\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: np.exp(x) / np.sum(np.exp(x)))\ndf['min-max'] = df.groupby('a')['b'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n```",
        "```python\nresult = df.loc[:, (df != 0).any(axis=0)].loc[(df != 0).any(axis=1)]\n```",
        "```python\nresult = df.loc[(df.sum(axis=1) != 0), (df.sum(axis=0) != 0)]\n```",
        "result = df.loc[(df.max(axis=1) <= 2), (df.max(axis=0) <= 2)]",
        "df[df > 2] = 0",
        "result = s.sort_values(ascending=True).sort_index()",
        "df = pd.DataFrame({'index': s.index, '1': s.values})\ndf = df.sort_values(by=['1', 'index'], ascending=[True, True])",
        "result = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]",
        "result = df[df['A'].apply(lambda x: isinstance(x, str))]",
        "```python\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n```",
        "```python\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n```",
        "result = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])",
        "result = df.groupby(['Sp','Value']).apply(lambda x: x[x['count'] == x['count'].max()])",
        "df['Date'] = df['Member'].map(dict).fillna(df['Date'])",
        "df['Date'] = df['Member'].map(dict).fillna('17/8/1926')",
        "```python\ndf['Date'] = df['Member'].map(dict).fillna(df['Date'])\n```",
        "df['Date'] = df['Member'].map(dict).fillna('17/8/1926').apply(lambda x: pd.to_datetime(x, format='%d/%m/%Y').strftime('%d-%b-%Y'))",
        "df['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Val'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Val'].transform('count')",
        "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).transform('size')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year')).transform('size')\ndf['Count_Val'] = df.groupby(['Date', 'Val']).transform('size')",
        "df['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year')).transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.weekday.rename('weekday')]).transform('count')\ndf['Count_Val'] = df.groupby(['Val']).transform('count')",
        "```python\nresult1 = df.groupby('Date').apply(lambda x: (x == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x != 0).sum())\n```",
        "```python\nresult1 = df.groupby('Date').apply(lambda x: (x % 2 == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x % 2 != 0).sum())\n```",
        "result = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})",
        "result = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})",
        "result = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})",
        "result = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})",
        "import dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=1)\ndf = df.assign(var2=df.var2.str.split(',')).explode('var2')\n\nresult = df.compute()",
        "import dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=1)\n\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2').compute()",
        "import dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=1)\n\nresult = df.assign(var2=df.var2.str.split('-')).explode('var2')\n\nresult = result.compute()",
        "df['new'] = df['str'].apply(lambda x: sum(not c.isalpha() for c in x))",
        "df['new'] = df['str'].apply(lambda x: sum(1 for c in x if not c.isalpha()))",
        "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)",
        "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)",
        "df[['fips', 'medi', 'row']] = df['row'].str.split(' ', 2, expand=True)",
        "```python\nresult = df.set_index('Name').apply(lambda row: row[row != 0].mean(), axis=1).reset_index()\n```",
        "```python\ndf = df.set_index('Name')\nresult = df.apply(lambda row: row[row != 0].cumsum()[::-1].mean(), axis=1).reset_index()\nresult.columns = df.columns\n```",
        "```python\nresult = df.copy()\nresult.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda row: row[row != 0].cumsum() / (row != 0).cumsum(), axis=1)\n```",
        "```python\nresult = df.iloc[:, 1:].apply(lambda row: row[row != 0].mean(), axis=1)\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda row: row.replace(0, row[row != 0].mean()), axis=1)\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda row: row[::-1].cumsum()[::-1] / (row != 0)[::-1].cumsum()[::-1], axis=1)\ndf.iloc[:, 1:] = df.iloc[:, 1:].round(6)\n```",
        "df['Label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)",
        "```python\ndf['label'] = (df['Close'].diff() > 0).astype(int) - (df['Close'].diff() < 0).astype(int)\n```",
        "df['label'] = (df['Close'].diff() > 0).astype(int) - (df['Close'].diff() < 0).astype(int)\ndf['label'].iloc[0] = 1\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')",
        "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\ndf['Duration'] = df['Duration'].fillna(pd.Timedelta('NaT'))",
        "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()",
        "df['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['Duration'] = (pd.to_datetime(df['departure_time'].shift(-1), format='%d-%b-%Y %H:%M:%S') - pd.to_datetime(df['arrival_time'], format='%d-%b-%Y %H:%M:%S')).dt.total_seconds()",
        "result = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')",
        "result = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')",
        "result = df[df['key2'].str.endswith(\"e\")].groupby('key1').size().reset_index(name='count')",
        "min_result = df.index.min()\nmax_result = df.index.max()",
        "mode_result = df.index.mode()[0]\nmedian_result = df.index.median()",
        "df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]",
        "df = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]",
        "df1 = df.loc[df.groupby(\"item\")[\"diff\"].idxmin()]",
        "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]",
        "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]",
        "result = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]",
        "df['Column_x'].fillna(pd.Series([0, 1]), inplace=True)",
        "```python\ndf['Column_x'] = df['Column_x'].fillna(0, limit=int(len(df)*0.3))\ndf['Column_x'] = df['Column_x'].fillna(0.5, limit=int(len(df)*0.3))\ndf['Column_x'] = df['Column_x'].fillna(1)\n```",
        "df['Column_x'].fillna(0, inplace=True)\ndf['Column_x'].fillna(1, inplace=True)",
        "```python\nresult = pd.concat([a, b], axis=1).apply(tuple, axis=1).to_frame()\n```",
        "```python\nresult = pd.concat([a, b, c], axis=1).apply(tuple, axis=1).to_frame()\nresult.columns = ['one', 'two']\n```",
        "```python\nresult = pd.concat([a, b], axis=1).applymap(tuple)\n```",
        "result = df.groupby(['username', pd.cut(df['views'], bins)]).size().unstack().fillna(0)",
        "result = df.groupby(['username', pd.cut(df['views'], bins)]).size().unstack().fillna(0)",
        "result = df.groupby(['username', pd.cut(df['views'], bins)]).size().unstack().fillna(0)",
        "result = df['text'].str.cat(sep=', ')",
        "result = df['text'].str.cat(sep='-')",
        "result = df['text'].str.cat(sep=', ')[::-1]",
        "result = df['text'].str.cat(sep=', ')",
        "result = df['text'].str.cat(sep='-')",
        "result = pd.concat([df1, df2], axis=0, sort=False)",
        "result = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)",
        "result = pd.concat([df1, df2], axis=0).sort_values(['id', 'date']).fillna(method='ffill')",
        "result = pd.merge(C, D, how='right', on='A')",
        "result = pd.merge(C, D, how='outer', on='A').fillna({'B_x': C['B']})",
        "result = pd.merge(C, D, how='left', on='A').fillna(C).rename(columns={'B_x': 'B', 'B_y': 'duplicated'})",
        "result = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())",
        "result = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())",
        "result = df.groupby('user').apply(lambda x: x[['amount', 'time']].values.tolist()).tolist()",
        "```python\ndf = pd.DataFrame(series.tolist(), index=series.index)\n```",
        "```python\ndf = pd.DataFrame(series.tolist(), index=series.index)\n```",
        "result = [col for col in df.columns if s in col]",
        "result = df.loc[:, df.columns.str.contains(s) & ~df.columns.str.match(s)]",
        "```python\nresult = [col for col in df.columns if s in col and col != s]\ndf = df.rename(columns={col: f'{s}{i+1}' for i, col in enumerate(result)})\n```",
        "result = pd.DataFrame(df['codes'].tolist()).add_prefix('code_')",
        "result = pd.DataFrame(df['codes'].tolist()).add_prefix('code_')",
        "```python\nresult = pd.DataFrame(df['codes'].tolist()).add_prefix('code_')\n```",
        "result = [item for sublist in df['col1'] for item in sublist]",
        "result = ','.join([','.join(map(str, lst[::-1])) for lst in df['col1']])",
        "result = ','.join([str(item) for sublist in df['col1'] for item in sublist])",
        "df['Time'] = df['Time'].dt.floor('2T')",
        "df['Time'] = df['Time'].dt.floor('3Min')\nresult = df.groupby('Time').sum().reset_index()",
        "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)",
        "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)",
        "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')",
        "result = df[filt[df.index.get_level_values('a')].values]",
        "result = df[filt[df.index.get_level_values('a')].values]",
        "result = df.loc[[0, 8]].isnull().any()",
        "result = df.loc[[0, 8]].T.dropna(axis=1).columns.tolist()",
        "result = df.loc[[0, 8]].isnull().any()",
        "result = [(df.iloc[0, i], df.iloc[8, i]) for i in range(df.shape[1]) if df.iloc[0, i] != df.iloc[8, i]]",
        "ts = pd.Series(df['Value'], index=df['Date'])",
        "result = df.values.flatten()",
        "result = df.values.flatten()",
        "df['dogs'] = df['dogs'].round(2)",
        "df['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)",
        "df['Sum'] = df[list_of_my_columns].sum(axis=1)",
        "df['Avg'] = df[list_of_my_columns].mean(axis=1)",
        "df['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)",
        "df_sorted = df.sort_index(level='time')",
        "df.sort_values(by=['VIM', 'time'])",
        "sp = sp[~sp.index.date.isin([pd.to_datetime('2020-02-17').date(), pd.to_datetime('2020-02-18').date()])]",
        "sp = sp[~sp.index.date.isin([pd.to_datetime('2020-02-17').date(), pd.to_datetime('2020-02-18').date()])]",
        "result = corr[corr > 0.3].stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']",
        "result = corr[corr > 0.3].stack().reset_index()[[0, 'level_1', 'level_2']]",
        "df.columns = list(df.columns[:-1]) + ['Test']",
        "df.columns = [df.columns[0]] + df.columns[1:]",
        "```python\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n```",
        "```python\ndf['frequent'] = df.mode(axis=1).iloc[:, 0]\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n```",
        "```python\ndf['frequent'] = df.apply(lambda row: row[row == row.mode()[0]].index.tolist(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n```",
        "result = df.groupby([\"id1\",\"id2\"]).mean().reset_index()",
        "result = df.replace('NULL', 0).groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()",
        "result = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum')",
        "result = df_c.drop(columns=['a_col'])"
    ],
    "Numpy": [
        "result = a.shape",
        "x = x[~np.isnan(x)]",
        "x = np.nan_to_num(x, nan=np.inf)",
        "result = [row[~np.isnan(row)].tolist() for row in x]",
        "b = np.eye(np.max(a)+1)[a]",
        "b = np.eye(np.max(a)+1)[a]",
        "b = np.eye(a.max() - a.min() + 1)[a - a.min()]",
        "b = np.eye(len(a))[np.argsort(a)]",
        "b = np.eye(np.max(a)+1)[a.flatten()].reshape(a.shape + (np.max(a)+1,))",
        "result = np.percentile(a, p)",
        "B = np.reshape(A, (-1, ncol))",
        "B = np.reshape(A, (nrow, -1))",
        "B = np.reshape(A, (-1, ncol))",
        "B = np.reshape(A, (-1, ncol))",
        "result = np.roll(a, shift)",
        "result = np.roll(a, shift, axis=1)",
        "result = np.roll(a, shift, axis=1)",
        "np.random.seed(0)\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nnp.random.seed(0)\nr_new = np.random.randint(3, size=(100, 2000)) - 1",
        "result = np.argmax(a)",
        "result = np.unravel_index(np.argmin(a), a.shape)",
        "result = np.unravel_index(np.argmax(a), a.shape, order='F')",
        "result = np.unravel_index(np.argmax(a), a.shape)",
        "```python\nimport numpy as np\nexample_a = np.array([[10,50,30],[60,20,40]])\ndef f(a = example_a):\n    result = np.argmax(a)\n    return result\n```",
        "result = np.unravel_index(np.argsort(a, axis=None)[-2], a.shape)",
        "```python\na = np.delete(a, np.where(np.isnan(a))[1], axis=1)\n```",
        "a = a[~np.isnan(a).any(axis=1)]",
        "result = np.array(a)",
        "a = a[:, permutation]",
        "result = a[permutation]",
        "result = np.unravel_index(np.argmin(a), a.shape)",
        "result = np.unravel_index(np.argmax(a), a.shape)",
        "result = np.unravel_index(np.argmin(a), a.shape)",
        "result = np.sin(np.radians(degree))",
        "result = np.cos(np.radians(degree))",
        "result = 0 if np.sin(np.deg2rad(number)) > np.sin(number) else 1",
        "result = np.arcsin(value) * (180 / np.pi)",
        "result = np.pad(A, (0, length - len(A)), 'constant')",
        "result = np.pad(A, (0, length - len(A)), mode='constant')",
        "print(a**power)",
        "result = np.power(a, power)",
        "result = np.divide.reduce([numerator, denominator])",
        "result = np.divide(numerator, denominator).as_integer_ratio()",
        "result = np.divide(numerator, denominator).as_integer_ratio()",
        "result = np.mean([a, b, c], axis=0)",
        "result = np.maximum(a, np.maximum(b, c))",
        "```python\nresult = np.diag(np.fliplr(a))\n```",
        "result = np.diag(np.fliplr(a))",
        "```python\nresult = np.diag(np.fliplr(a))\n```",
        "```python\nresult = np.diag(np.fliplr(a))\n```",
        "```python\nresult = X.flatten().tolist()\n```",
        "result = X.flatten('C')",
        "```python\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = X.flatten().tolist()\n    return result\n```",
        "result = X.flatten(order='F')",
        "result = np.array([int(digit) for digit in mystr])",
        "result = np.cumsum(a[:, col] * multiply_number)",
        "result = np.cumsum(a[row] * multiply_number)",
        "result = np.prod(a[row] / divide_number)",
        "result = np.linalg.qr(a)[0]",
        "result = a.shape[1]",
        "p_value = scipy.stats.ttest_ind(a, b, equal_var=False).pvalue",
        "p_value = scipy.stats.ttest_ind(a[~np.isnan(a)], b[~np.isnan(b)], equal_var=False).pvalue",
        "p_value = scipy.stats.ttest_ind_from_stats(amean, np.sqrt(avar), anobs, bmean, np.sqrt(bvar), bnobs, equal_var=False).pvalue",
        "output = np.array([x for x in A if not any(np.array_equal(x, y) for y in B)])",
        "output = np.concatenate((np.setdiff1d(A, B, axis=0), np.setdiff1d(B, A, axis=0)))",
        "c = b[np.argsort(a, axis=0)]",
        "c = b[np.argsort(a, axis=0)]",
        "c = b[np.argsort(a, axis=0)]",
        "result = b[np.argsort(a.sum(axis=(1,2)))]",
        "```python\na = np.delete(a, 2, axis=1)\n```",
        "```python\na = np.delete(a, 2, axis=0)\n```",
        "```python\na = np.delete(a, [0, 2], axis=1)\n```",
        "result = np.delete(a, del_col, axis=1)",
        "a = np.insert(a, pos, element)",
        "a = np.insert(a, pos, element, axis=0)",
        "a = np.insert(a, pos, element)",
        "```python\na = np.insert(a, pos, element, axis=0)\n```",
        "result = np.array([np.copy(arr) for arr in array_of_arrays])",
        "result = np.all(a == a[0], axis=0)",
        "result = np.all(a == a[:, 0].reshape(-1, 1), axis=0)",
        "result = np.all(np.all(a == a[0], axis=1))",
        "import scipy.integrate as spi\n\n# Define the function to integrate\ndef f(x, y):\n    return np.cos(x)**4 + np.sin(y)**2\n\n# Create a meshgrid of x and y values\nX, Y = np.meshgrid(x, y)\n\n# Evaluate the function at each point of the meshgrid\nZ = f(X, Y)\n\n# Use Simpson's rule to integrate the function over the meshgrid\nresult = spi.simps(spi.simps(Z, y), x)",
        "result = np.sum((np.cos(x[:, None])**4 + np.sin(y)**2) * (x[1]-x[0]) * (y[1]-y[0]))",
        "result = np.sort(grades)",
        "result = np.searchsorted(np.sort(grades), eval, side='right') / len(grades)",
        "low = np.min(grades)\nhigh = np.max(grades)",
        "nums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])",
        "a_np = a.numpy()",
        "a_pt = torch.from_numpy(a)",
        "a_np = a.numpy()",
        "a_tf = tf.convert_to_tensor(a)",
        "result = np.argsort(a)[::-1]",
        "result = np.argsort(a)",
        "result = np.argsort(a)[-N:][::-1]",
        "result = np.linalg.matrix_power(A, n)",
        "```python\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n```",
        "```python\nresult = np.lib.stride_tricks.sliding_window_view(a, (2, 2))\n```",
        "```python\nresult = a.reshape((a.shape[0]//2, 2, a.shape[1]//2, 2)).swapaxes(1, 2).reshape(-1, 2, 2)\n```",
        "```python\nresult = a.reshape(-1, patch_size, patch_size)\n```",
        "result = a.transpose(0, 2, 1).reshape(h, w)",
        "result = np.array([a[i:i+patch_size, j:j+patch_size] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])",
        "result = a[:, low:high]",
        "result = a[low:high+1]",
        "result = a[:, low:high]",
        "a = np.array(eval(string))",
        "result = np.exp(np.random.uniform(np.log(min), np.log(max), n))",
        "result = np.exp(np.random.uniform(np.log(min), np.log(max), n))",
        "result = np.exp(np.random.uniform(low=np.log(min), high=np.log(max), size=n))",
        "```python\nB = pd.Series(np.zeros(len(A)))\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n```",
        "B = pd.Series(index=A.index)\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]",
        "result = np.empty((0,))",
        "result = np.empty((3,0))",
        "result = np.ravel_multi_index(index, dims, order='F')",
        "result = np.ravel_multi_index(index, dims, order='C')",
        "values = np.zeros((2,3), dtype=[('a', 'int'), ('b', 'float')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)",
        "result = np.bincount(accmap, weights=a)\n",
        "result = np.maximum.reduceat(a, np.unique(index))",
        "result = np.bincount(accmap, weights=a)",
        "result = np.minimum.reduceat(a, index)",
        "z = np.add(x, y)",
        "result = np.random.choice(lista_elegir, samples, p=probabilit)",
        "result = a[max(0, low_index):min(a.shape[0], high_index), max(0, low_index):min(a.shape[1], high_index)]",
        "result = x[x >= 0]",
        "result = x[np.iscomplex(x)]",
        "bin_data = [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\nbin_data_mean = np.mean(bin_data, axis=1)",
        "bin_data = [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\nbin_data_max = np.max(bin_data, axis=1)",
        "bin_data = np.array([data[:, i:i+bin_size] for i in range(0, data.shape[1], bin_size)])\nbin_data_mean = np.mean(bin_data, axis=2)",
        "```python\nbin_data = np.array([data[i:i+bin_size] for i in range(len(data)-bin_size, -1, -bin_size)])\nbin_data_mean = np.mean(bin_data, axis=1)\n```",
        "```python\nbin_data = np.flip(np.array([data[:, i:i+bin_size] for i in range(data.shape[1]-bin_size+1, -1, -bin_size)]), axis=1)\nbin_data_mean = np.mean(bin_data, axis=2)\n```",
        "```python\nbin_data = np.array([row[-bin_size:] for row in data])\nbin_data_mean = np.mean(bin_data, axis=1)\n```",
        "result = np.clip(3 * x ** 2 - 2 * x ** 3, x_min, x_max)",
        "def smoothclamp(x, N=5):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0, 1)\n    result = t ** N * (3 - 2 * t)\n    result = result * (x_max - x_min) + x_min\n    return result",
        "result = np.correlate(a, np.roll(b, 1), mode='valid')",
        "result = df.values.reshape(4, 15, 5)",
        "result = df.values.reshape((15, 4, 5))",
        "result = np.unpackbits(a[:, np.newaxis], axis=1)[:, -m:]",
        "result = np.unpackbits(np.uint8(a.reshape(-1,1)), axis=1)[:, -m:]",
        "result = np.unpackbits(np.uint8(a.reshape(-1,1)), axis=1)[:,-m:]\nresult = np.logical_xor.reduce(result, axis=0)",
        "result = (np.mean(a) - 3 * np.std(a), np.mean(a) + 3 * np.std(a))",
        "```python\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 2*std, mean + 2*std)\n```",
        "```python\nimport numpy as np\nexample_a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\ndef f(a = example_a):\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3 * std, mean + 3 * std)\n    return result\n```",
        "```python\nimport numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\nmean = np.mean(a)\nstd = np.std(a)\n\nthreshold = mean + (2 * std)\nresult = np.abs(a - mean) > threshold\n```\n",
        "prob = np.percentile(DataArray, percentile)",
        "a[:, zero_cols] = 0\na[zero_rows, :] = 0",
        "a[zero_rows, :] = 0\na[:, zero_cols] = 0",
        "a[1,:] = 0\na[:,0] = 0",
        "mask = (a == np.amax(a, axis=1)[:, None])",
        "mask = (a == np.min(a, axis=1)[:, np.newaxis])",
        "result = np.corrcoef(post, distance)[0, 1]",
        "result = np.einsum('ij,ik->jik', X, X)",
        "X = np.sqrt(np.sum(Y, axis=2))",
        "is_contained = np.isin(number, a)",
        "C = np.setdiff1d(A, B)",
        "C = np.intersect1d(A, B)",
        "C = np.intersect1d(A, B)",
        "result = len(a) - rankdata(a).astype(int) + 1",
        "result = len(a) - rankdata(a).astype(int) - 1",
        "result = len(a) - rankdata(a).astype(int) + 1",
        "```python\ndists = np.stack((x_dists, y_dists), axis=2)\n```",
        "dists = np.stack((x_dists, y_dists), axis=2)",
        "result = a[:, second, :][:, :, third]",
        "arr = np.zeros((20, 10, 10, 2))",
        "result = X / np.sum(np.abs(X), axis=1, keepdims=True)",
        "result = X / np.linalg.norm(X, axis=1, keepdims=True)",
        "result = np.divide(X, np.max(np.abs(X), axis=1, keepdims=True))",
        "result = np.select(df['a'].astype(str).str.contains(target), choices, default=df['a'])",
        "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n\nresult = squareform(pdist(a))\n\nprint(result)\n```",
        "result = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        result[i, j] = np.linalg.norm(a[i] - a[j])\n",
        "```python\nresult = np.triu(np.linalg.norm(a[:, np.newaxis] - a, axis=2))\n```",
        "AVG = np.mean(NA.astype(float), axis=0)",
        "AVG = np.mean(NA.astype(float), axis=0)",
        "NA = NA.astype(float)",
        "result = np.delete(np.unique(a), np.where(np.diff(np.unique(a)) == 0))",
        "```python\nresult = np.unique(np.trim_zeros(a, 'b'), axis=0)\n```",
        "```python\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n```",
        "```python\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n```",
        "```python\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df[['lat', 'lon', 'val']].max(axis=1)\n```",
        "```python\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        result.append(a[i:i+size[0], j:j+size[1]])\n```",
        "```python\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        result.append(a[i:i+size[0], j:j+size[1]])\n```",
        "result = np.mean(a)",
        "result = np.mean(a)",
        "result = Z[..., -1:]",
        "result = a[-1:, ...]",
        "result = any(np.array_equal(c, arr) for arr in CNTS)",
        "result = any(np.array_equal(c, arr) for arr in CNTS)",
        "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)",
        "df['Q_cum'] = df.groupby('D')['Q'].cumsum()",
        "i = np.diag(i)",
        "```python\na[np.triu_indices(a.shape[0], k=1)] = 0\n```",
        "result = pd.date_range(start=start, end=end, periods=n)",
        "result = np.where((x == a) & (y == b))[0][0] if np.sum((x == a) & (y == b)) > 0 else -1",
        "result = np.where((x == a) & (y == b))[0]",
        "```python\nresult = np.polyfit(x, y, 2)\n```",
        "```python\nimport numpy as np\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\ndegree = 3\n\n# [Missing]\ncoefficients = np.polyfit(x, y, degree)\n\nprint(coefficients)\n```",
        "```python\ndf.apply(lambda x: x - temp_arr)\n```",
        "result = np.einsum('ijk,jl->ilk', A, B)",
        "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)",
        "result = MinMaxScaler().fit_transform(arr)",
        "```python\nresult = np.zeros_like(a)\nscaler = MinMaxScaler()\nfor i in range(a.shape[0]):\n    result[i] = scaler.fit_transform(a[i])\n```",
        "mask3 = mask ^ mask2",
        "```python\nmask = arr < n1[:, np.newaxis]\nmask2 = arr >= n2[:, np.newaxis]\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narry[~mask2] = 30\n```",
        "result = np.count_nonzero(np.isclose(s1, s2))",
        "result = np.count_nonzero(np.isnan(s1) != np.isnan(s2))",
        "result = all(np.array_equal(a[i], a[i+1]) for i in range(len(a)-1))",
        "result = all(np.isnan(x).all() for x in a)",
        "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant', constant_values=0)",
        "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')",
        "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), constant_values=element)",
        "result = np.pad(arr, ((0, shape[0]-arr.shape[0]), (0, shape[1]-arr.shape[1])), mode='constant', constant_values=0)",
        "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')",
        "a.reshape(a.shape[0]//3, 3)",
        "result = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b]",
        "result = a[:,:,1] * b",
        "result = a[np.arange(b.shape[0])[:, None], np.arange(b.shape[1]), b]",
        "result = np.sum(a[np.arange(a.shape[0])[:, np.newaxis, np.newaxis], np.arange(a.shape[1])[np.newaxis, :, np.newaxis], b])",
        "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b])",
        "result = df.loc[(df['a'] > 1) & (df['a'] <= 4), 'b'].tolist()",
        "result = im[1:-1, 1:-1]",
        "```python\nresult = A[np.ix_(A.any(1), A.any(0))]\n```",
        "```python\nresult = im[(im != 0).any(axis=1)][:, (im != 0).any(axis=0)]\n```",
        "result = im[1:-1, 1:-1]"
    ],
    "Tensorflow": [
        "```python\nx.assign(1)\n```",
        "```python\nimport tensorflow as tf\n\nx = tf.Variable(0)\n\nx.assign(114514)\n\nresult = x\n```",
        "```python\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n```",
        "```python\nimport tensorflow as tf\n\nlabels = [0, 6, 5, 4, 2]\n\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n\nprint(result)\n```",
        "```python\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n```",
        "```python\nimport tensorflow as tf\n\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    return result\n```",
        "```python\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n```",
        "```python\ndef my_map_func(i):\n  return [i, i+1, i+2]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n```",
        "result = []\nfor i in input:\n    result.extend(my_map_func(i))\nreturn result",
        "```python\nimport tensorflow as tf\n\n\nlengths = [4, 3, 5, 2]\n\nresult = tf.sequence_mask(lengths, maxlen=8, dtype=tf.int32)\n\nprint(result)\n```",
        "```python\nimport tensorflow as tf\n\n\nlengths = [4, 3, 5, 2]\n\nresult = tf.sequence_mask(lengths, maxlen=8, dtype=tf.int32)\n\nprint(result)\n```",
        "```python\nimport tensorflow as tf\n\n\nlengths = [4, 3, 5, 2]\n\nresult = tf.sequence_mask(lengths, maxlen=8, dtype=tf.float32)\n\nprint(result)\n```",
        "```python\nimport tensorflow as tf\n\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    max_length = max(lengths)\n    mask = tf.sequence_mask(lengths, maxlen=max_length)\n    padded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_length]])\n    return padded_mask\n```",
        "```python\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length)\npadded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_length]])\nresult = tf.cast(padded_mask, dtype=tf.float32)\n```",
        "result = tf.transpose(tf.reshape(tf.stack(tf.meshgrid(a, b)), (2, -1)))",
        "result = tf.transpose(tf.reshape(tf.stack(tf.meshgrid(a, b)), (2, -1)))",
        "result = tf.squeeze(a)",
        "result = tf.expand_dims(a, axis=2)",
        "result = tf.reshape(a, (1, 50, 100, 1, 512))",
        "result = tf.reduce_sum(A, axis=1)",
        "result = tf.reduce_prod(A, axis=1)",
        "result = tf.math.reciprocal(A)",
        "result = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)",
        "result = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)",
        "result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)",
        "m = tf.gather_nd(x, tf.stack((y, z), axis=1))",
        "m = tf.gather_nd(x, tf.stack((row, col), axis=1))",
        "result = tf.gather_nd(x, tf.stack([y, z], axis=1))",
        "result = tf.einsum('bik,bjk->bij', A, B)",
        "result = tf.matmul(A, tf.transpose(B, perm=[0, 2, 1]))",
        "result = [s.decode('utf-8') for s in x]",
        "```python\nresult = tf.strings.unicode_decode(x, 'UTF-8').numpy().tolist()\n```",
        "result = tf.reduce_sum(x, axis=-2) / tf.reduce_sum(tf.cast(x != 0, tf.float32), axis=-2)",
        "result = tf.reduce_sum(x, axis=-2) / tf.math.count_nonzero(x, axis=-2, keepdims=True)",
        "```python\nresult = tf.reduce_sum(x, axis=-2) / tf.reduce_sum(tf.cast(x != 0, tf.float32), axis=-2)\n```",
        "```python\nimport tensorflow as tf\n\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n\nprint(result)\n```",
        "result = tf.argmax(a, axis=1)",
        "result = tf.argmax(a, axis=1)",
        "result = tf.argmax(a, axis=1)",
        "result = tf.argmin(a, axis=0)",
        "model.save(\"export/1\")",
        "result = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32, seed=seed_x)",
        "```python\nimport tensorflow as tf\n\nseed_x = 10\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform([114], minval=2, maxval=6, dtype=tf.int32)\n\nprint(result)\n```",
        "```python\nimport tensorflow as tf\n\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform([10], minval=1, maxval=5, dtype=tf.int32)\n    return result\n```",
        "result = tf.__version__"
    ],
    "Scipy": [
        "result = np.polyfit(np.log(x), y, 1)",
        "result = np.polyfit(np.log(x), y, 1)",
        "```python\ndef func(x, A, B, C):\n    return A * np.exp(B * x) + C\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0=p0)\n```",
        "statistic, p_value = stats.ks_2samp(x, y)",
        "result = stats.ks_2samp(x, y)",
        "result = optimize.minimize(f, initial_guess)\nresult = result.x",
        "p_values = scipy.stats.norm.cdf(z_scores)",
        "p_values = scipy.stats.norm.cdf(z_scores, loc=mu, scale=sigma)",
        "z_scores = scipy.stats.norm.ppf(p_values)",
        "result = stats.lognorm.cdf(x, s=stddev, scale=np.exp(mu))",
        "expected_value = np.exp(mu + (stddev**2)/2)\nmedian = np.exp(mu)",
        "result = sa.multiply(sb)",
        "result = sA.multiply(sB)",
        "interp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)",
        "interp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)",
        "xrot, yrot = rotate([x0, y0], -angle, reshape=False, origin=(0, 0))",
        "result = M.diagonal()",
        "result = stats.kstest(times, 'uniform')",
        "result = stats.kstest(times, 'uniform')",
        "result = stats.kstest(times, 'uniform', alternative='two-sided', mode='asymp')",
        "Feature = sparse.hstack([c1, c2])",
        "Feature = sparse.hstack([c1, c2])",
        "Feature = sparse.vstack([c1, c2])",
        "```python\ndistance_matrix = scipy.spatial.distance.cdist(points1, points2)\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(distance_matrix)\nresult = col_ind.tolist()\n```",
        "```python\n# Calculate the pairwise Manhattan distances between points1 and points2\ndistances = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\n\n# Use the Hungarian algorithm to find the optimal assignment\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(distances)\n\n# Get the indices of the points in points2 that correspond to the points in points1\nresult = col_ind.tolist()\n```",
        "b.setdiag(0)",
        "labels, num_regions = ndimage.label(img > threshold)\nresult = num_regions",
        "result = ndimage.label(img < threshold)",
        "result = ndimage.label(img > threshold)",
        "```python\nlabeled, num_features = ndimage.label(img > threshold)\ncom = ndimage.center_of_mass(img, labeled, range(1, num_features+1))\nresult = [np.sqrt((x-0)**2 + (y-0)**2) for x, y in com]\n```",
        "M = M + M.transpose() - sparse.diags(M.diagonal())",
        "sA = sA + sA.T - sA.diagonal()",
        "```python\nlabeled_array, num_features = scipy.ndimage.label(square)\nsizes = scipy.ndimage.sum(square, labeled_array, range(num_features + 1))\nmask_size = sizes < 2\nremove_pixel = mask_size[labeled_array]\nsquare[remove_pixel] = 0\n```",
        "```python\nlabeled_array, num_features = scipy.ndimage.label(square)\nsizes = scipy.ndimage.sum(square, labeled_array, range(num_features + 1))\nmask_sizes = sizes == 1\nsquare[mask_sizes[labeled_array]] = 0\n```",
        "mean = col.mean()\nstandard_deviation = col.std()",
        "Max = col.max()\nMin = col.min()",
        "Median = np.median(col.data)\nMode = np.argmax(np.bincount(col.data))",
        "popt, pcov = curve_fit(lambda x, *a: fourier8(x, *a[:degree]), z, Ua, p0=[1]*degree)",
        "result = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(example_array, metric='euclidean'))",
        "```python\n# Calculate pairwise Manhattan distances\nresult = scipy.spatial.distance.cdist(np.argwhere(example_array != 0), np.argwhere(example_array != 0), metric='cityblock')\n```",
        "```python\ndef f(example_array = example_arr):\n    # Get the unique IDs in the array\n    unique_ids = np.unique(example_array)\n\n    # Initialize an empty result array\n    result = []\n\n    # Iterate over each unique ID\n    for id1 in unique_ids:\n        # Get the indices where the current ID is present\n        indices1 = np.where(example_array == id1)\n\n        # Iterate over the remaining unique IDs\n        for id2 in unique_ids[unique_ids > id1]:\n            # Get the indices where the other ID is present\n            indices2 = np.where(example_array == id2)\n\n            # Calculate the pairwise Euclidean distance between the two sets of indices\n            distance = scipy.spatial.distance.cdist(indices1, indices2, metric='euclidean').min()\n\n            # Append the result to the result array\n            result.append([id1, id2, distance])\n\n    return result\n```",
        "result = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k=2, s=4)\n    y_ext = interpolate.splev(x_val, tck, der=0)\n    result[i] = y_ext",
        "```python\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\n```",
        "result = ss.anderson_ksamp([x1, x2])",
        "df[['AB', 'AC', 'BC']] = df[['B', 'C']].rolling(3).apply(lambda x: stats.kendalltau(x[0], x[1])[0])",
        "result = sa.nnz == 0",
        "result = sa.nnz == 0",
        "result = block_diag(*a)",
        "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue",
        "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue",
        "kurtosis_result = np.sum((a - np.mean(a))**4) / (len(a) * np.var(a)**2)",
        "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)",
        "result = scipy.interpolate.interp2d(x, y, z)(s, t)",
        "result = scipy.interpolate.interp2d(x, y, z)(s, t)",
        "result = vor.point_region\nresult = [result[vor.find_simplex(point)] for point in extraPoints]",
        "result = vor.point_region\nresult = [result[vor.closest_point(p)] for p in extraPoints]",
        "result = sparse.csr_matrix(vectors, dtype=np.float64)",
        "b = scipy.ndimage.median_filter(a, 3, origin=1)",
        "result = M[row, column]",
        "result = M[row, column].tolist()",
        "new_array = scipy.interpolate.interp1d(x, array, axis=0)(x_new)",
        "prob = NormalDistro(u, o2, x)",
        "prob = NormalDistro(u, o2, x)",
        "result = sf.dct(np.eye(N))",
        "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()",
        "```python\nresult = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        result[i,j] = scipy.stats.binom.pmf(j, i, p)\n```",
        "result = df.apply(stats.zscore, axis=1)",
        "result = df.apply(stats.zscore)",
        "result = pd.DataFrame(index=df.index, columns=pd.MultiIndex.from_product([['data', 'zscore'], df.columns]))\n\nresult.loc[:, ('data', slice(None))] = df\nresult.loc[:, ('zscore', slice(None))] = stats.zscore(df, axis=1)\n\nresult",
        "```python\nzscore_df = df.apply(stats.zscore).round(3)\nresult = pd.concat([df, zscore_df], keys=['data', 'zscore'])\n```",
        "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)",
        "mid = np.array([[shape[0]/2, shape[1]/2]])",
        "```python\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\n\nmid = np.array([shape[0]//2, shape[1]//2]).reshape(1, 1, 2)\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), mid, metric='cityblock')\n\nprint(result)\n```",
        "```python\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef f(shape = (6, 6)):\n    y, x = np.indices(shape)\n    mid = np.array([(shape[0]-1)/2, (shape[1]-1)/2])\n    result = distance.cdist(np.dstack((y, x)), np.array([mid]))\n    return result\n```",
        "result = scipy.ndimage.zoom(x, (shape[0]/x.shape[0], shape[1]/x.shape[1]), order=1)",
        "out = scipy.optimize.minimize(residual, x0, args=(a, y))",
        "out = scipy.optimize.minimize(func, x0, args=(a,), bounds=[(lb, None) for lb in x_lower_bounds])",
        "sol = scipy.integrate.solve_ivp(fun=lambda t, N1: [-100 * N1[0] + np.sin(t)], t_span=time_span, y0=[N0,])",
        "sol = solve_ivp(fun=lambda t, N1: -100 * N1 + t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi, t_span=[0, 100e-3], y0=[N0,])",
        "sol = scipy.integrate.solve_ivp(fun=lambda t, N: [-100 * N[0] - np.cos(t)], t_span=time_span, y0=[N0,])",
        "for t in range(4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})",
        "result = sparse.vstack((sa, sb))",
        "result = sparse.hstack((sa, sb))",
        "result, error = scipy.integrate.quad(lambda x: 2*c*x, low, high)",
        "result, error = scipy.integrate.quad(lambda x: 2*c*x, low, high)",
        "A = V + x",
        "A = V + x",
        "A = V + x\nB = A + y",
        "```python\nsa = sa.multiply(1 / np.sqrt(sa.power(2).sum(axis=0)))\n```",
        "```python\n# [Missing]\nsa = sa.multiply(1 / Len)\n```",
        "a[a > 0] = 1",
        "```python\na[a > 0] = 1\n```",
        "result = []\nfor centroid in centroids:\n    distances = scipy.spatial.distance.cdist(data, [centroid])\n    closest_index = np.argmin(distances)\n    result.append(closest_index)",
        "result = []\nfor centroid in centroids:\n    distances = scipy.spatial.distance.cdist(data, [centroid])\n    closest_point = data[np.argmin(distances)]\n    result.append(closest_point)",
        "result = []\nfor centroid in centroids:\n    distances = scipy.spatial.distance.cdist(data, [centroid])\n    k_closest_indices = np.argsort(distances.flatten())[:k]\n    result.append(k_closest_indices[k-1])\nresult = np.array(result)",
        "result = [fsolve(lambda a: eqn(x, a, b), x0=0.5) for x, b in zip(xdata, bdata)]",
        "result = []\nfor x, a in zip(xdata, adata):\n    sol = fsolve(eqn, x0=0.5, args=(a,))[0]\n    result.append([sol, a])\nresult",
        "result = sp.stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d))",
        "result = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d))",
        "integral_df = df.groupby(pd.Grouper(freq='5S')).apply(lambda x: integrate.trapz(x['A'], x.index))",
        "result = scipy.interpolate.griddata(x, y, eval)",
        "```python\n# [Missing]\nweights, _ = np.histogram(a['A1'], bins=np.arange(max(a['A1'])+2))\nweights = weights / len(a['A1'])\n```\n\nor\n\n```python\n# [Missing]\nweights = np.bincount(a['A1']) / len(a['A1'])\n```",
        "result = sciopt.fminbound(e, pmin, pmax, args=(x,y))",
        "result = signal.argrelextrema(arr, np.less_equal, order=n)",
        "result = np.argwhere((arr <= signal.convolve2d(arr, np.ones((1, 2*n+1)), mode='same'))).tolist()",
        "df = df.select_dtypes(include=[np.number])\ndf = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]"
    ],
    "Sklearn": [
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "```python\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\ndf_out = pd.concat([df, df_out], axis=1)\n```",
        "```python\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n```",
        "```python\ndf_out = pd.get_dummies(df['Col4'].apply(pd.Series).stack()).sum(level=0)\ndf_out = pd.concat([df, df_out], axis=1)\n```",
        "```python\ndf_out = pd.get_dummies(df.iloc[:, -1].apply(pd.Series).stack()).sum(level=0)\n```\n",
        "```python\ndf_out = pd.get_dummies(df.iloc[:, -1].apply(pd.Series).stack()).sum(level=0)\n```",
        "```python\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Fit model:\nsvmmodel = suppmach.LinearSVC()\ncalibrated_model = CalibratedClassifierCV(svmmodel, cv=5, method='sigmoid')\ncalibrated_model.fit(X, y)\n\n# Get probability estimates:\nproba = calibrated_model.predict_proba(x_test)\n```",
        "calibrated_model = sklearn.calibration.CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)",
        "df = pd.concat([df_origin, pd.DataFrame(transform_output.toarray())], axis=1)",
        "df = pd.concat([df_origin, pd.DataFrame(transform_output.toarray())], axis=1)",
        "result = pd.concat([df, pd.DataFrame(transform_output.toarray())], axis=1)",
        "del clf.named_steps['poly']",
        "del clf.named_steps['dim_svm']",
        "del clf.named_steps['reduce_dIm']",
        "del clf.steps['poly']",
        "clf.steps.pop(1)",
        "del clf.named_steps['pOly']",
        "gridsearch.fit(trainX, trainY, **fit_params)",
        "```python\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [(testX, testY)]}\n\ngridsearch.fit(trainX, trainY, **fit_params)\n```",
        "```python\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test))\n```",
        "```python\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test)[:, 1])\n```",
        "```python\ninversed = scaler.inverse_transform(scaled)\n```",
        "```python\ninversed = scaler.inverse_transform(scaled)\n```",
        "model_name = model.__class__.__name__",
        "model_name = model.__class__.__name__",
        "model_name = model.__class__.__name__",
        "tf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)",
        "tf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)",
        "select_out = pipe.named_steps['select'].transform(data)",
        "```python\nclf = GridSearchCV(bc, param_grid)\nclf.fit(X_train, y_train)\n```",
        "regressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X.reshape(-1, 1), y)",
        "regressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X.reshape(-1, 1), y)",
        "preprocessor = preprocess",
        "```python\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\nprint(tfidf.preprocessor)\n```",
        "df_out = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)",
        "df_out = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)",
        "coef = grid.best_estimator_['model'].coef_",
        "coef = grid.best_estimator_['model'].coef_",
        "column_names = X.columns[model.get_support()]",
        "model = SelectFromModel(clf, prefit=True)\nselected_features = model.get_support()\ncolumn_names = X.columns[selected_features]",
        "column_names = X.columns[model.get_support()]",
        "column_names = X.columns.tolist()",
        "km.fit(X)\nclosest_50_samples = km.transform(X)[:, p].argsort()[:50]",
        "km.fit(X)\nclosest_50_samples = km.transform(X)[:, p].argsort()[:50]\nclosest_50_samples = X[closest_50_samples]",
        "km.fit(X)\nclosest_100_samples = X[np.argsort(np.linalg.norm(X - km.cluster_centers_[p], axis=1))[:100]]",
        "samples = km.transform(X)[:, p].argsort()[:50]",
        "X_train = pd.get_dummies(X_train)",
        "X_train = pd.get_dummies(X_train)",
        "```python\nfrom sklearn.svm import SVR\n\nmodel = SVR(kernel='rbf')\nmodel.fit(X, y)\npredict = model.predict(X)\n```",
        "```python\nfrom sklearn.svm import SVR\n\nregressor = SVR(kernel='rbf')\nregressor.fit(X, y)\npredict = regressor.predict(X)\n```",
        "```python\nfrom sklearn.svm import SVR\n\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X, y)\npredict = model.predict(X)\n```",
        "```python\nfrom sklearn.svm import SVR\n\nregressor = SVR(kernel='poly', degree=2)\nregressor.fit(X, y)\npredict = regressor.predict(X)\n```",
        "cosine_similarities_of_queries = tfidf.transform(queries).dot(tfidf.transform(documents).T).toarray()",
        "cosine_similarities_of_queries = tfidf.transform(queries).dot(tfidf.transform(documents).T).toarray()",
        "cosine_similarities_of_queries = tfidf.transform(queries)",
        "```python\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n```",
        "```python\nnew_f = pd.get_dummies(pd.DataFrame(f).stack()).sum(level=0)\n```",
        "```python\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n```",
        "```python\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n```",
        "```python\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n```",
        "```python\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average').fit_predict(data_matrix)\n```",
        "```python\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average').fit_predict(data_matrix)\n```",
        "```python\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average').fit_predict(simM)\n```",
        "```python\n# [Missing]\ncluster_labels = scipy.cluster.hierarchy.cut_tree(scipy.cluster.hierarchy.linkage(data_matrix), n_clusters=2).flatten()\n```",
        "```python\n# [Missing]\nlinkage_matrix = scipy.cluster.hierarchy.linkage(data_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n```",
        "```python\n# [Missing]\nlinkage_matrix = scipy.cluster.hierarchy.linkage(simM, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n```",
        "```python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n```",
        "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)",
        "```python\nfrom sklearn.preprocessing import PowerTransformer\n\n# Perform Box-Cox transformation\nbox_cox_data = PowerTransformer(method='box-cox').fit_transform(data)\n```",
        "```python\nfrom sklearn.preprocessing import PowerTransformer\n\ntransformer = PowerTransformer(method='box-cox')\nbox_cox_data = transformer.fit_transform(data)\n```",
        "```python\nfrom sklearn.preprocessing import PowerTransformer\n\nyeo_johnson = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = yeo_johnson.fit_transform(data)\n```",
        "yeo_johnson = sklearn.preprocessing.PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = yeo_johnson.fit_transform(data)",
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b|[\\!\\?\\\"\\']')\ntransformed_text = vectorizer.fit_transform([text]).toarray()",
        "```python\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset into features (x) and target class (y)\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n```",
        "```python\nfrom sklearn.model_selection import train_test_split\n\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n```",
        "```python\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset into features (x) and target class (y)\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n```",
        "```python\nimport numpy as np\nimport pandas as pd\n\ndef solve(data):\n    # Splitting dataset into features (x) and target class (y)\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n\n    # Splitting dataset into training set and testing set\n    from sklearn.model_selection import train_test_split\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    return x_train, y_train, x_test, y_test\n\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n```",
        "f1 = df['mse'].values.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(f1)\nlabels = kmeans.predict(f1)",
        "```python\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n\nprint(labels)\n```",
        "```python\nfeatureSelector = sklearn.svm.LinearSVC(penalty='l1', dual=False)\nfeatureSelector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.coef_[0] != 0]\n```",
        "```python\nfeatureSelector = LinearSVC(penalty='l1', dual=False)\nfeatureSelector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.coef_[0] != 0]\n```",
        "featureSelector = sklearn.feature_selection.SelectFromModel(sklearn.svm.LinearSVC(penalty='l1'))\nfeatureSelector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]",
        "```python\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n```",
        "```python\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n```",
        "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()",
        "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()",
        "```python\nslopes = np.array([]) # blank array to store slopes\n\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:, 0], npMatrix[:, 1]\n    slope = LinearRegression().fit(X, Y)\n    m = slope.coef_[0]\n    slopes = np.append(slopes, m)\n```",
        "```python\nslopes = []\nfor col in df1.columns:\n    if col != 'Time':\n        df2 = df1[~np.isnan(df1[col])]\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        slopes.append(m)\nslopes = np.array(slopes)\n```",
        "transformed_df = df.copy()\ntransformed_df['Sex'] = LabelEncoder().fit_transform(transformed_df['Sex'])",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])",
        "transformed_df = df.copy()\nle = LabelEncoder()\ntransformed_df['Sex'] = le.fit_transform(transformed_df['Sex'])\n",
        "ElasticNet = sklearn.linear_model.ElasticNet()",
        "transformed = MinMaxScaler().fit_transform(np_array.reshape(-1, 1)).reshape(np_array.shape)",
        "transformed = MinMaxScaler().fit_transform(np_array.flatten().reshape(-1, 1)).reshape(np_array.shape)",
        "```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\ndef Transform(a):\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a.reshape(-1, 1)).reshape(a.shape)\n    return new_a\n\ntransformed = Transform(np_array)\nprint(transformed)\n```",
        "predict = clf.predict(b)",
        "new_X = np.array(X)",
        "new_X = np.array(X)",
        "new_X = pd.DataFrame(X).apply(lambda x: pd.factorize(x)[0])",
        "X = dataframe.iloc[:, :-1].astype(float)",
        "X = dataframe.iloc[:, 1:-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)",
        "train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=0.2, shuffle=False)",
        "train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=0.8, shuffle=False)",
        "train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=0.2, shuffle=False)",
        "df[['X2_scale', 'X3_scale']] = df.groupby('Month')[['X2', 'X3']].transform(lambda x: scaler.fit_transform(x))",
        "myData[['new_A2', 'new_A3']] = myData.groupby('Month')[['A2', 'A3']].transform(lambda x: scaler.fit_transform(x))",
        "count.fit_transform([words])\nfeature_names = count.get_feature_names()",
        "feature_names = vocabulary.get_feature_names_out()",
        "```python\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n```",
        "```python\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_).sort_values(by='mean_fit_time')\n```",
        "```python\nimport joblib\n\n# Save the model in the file named \"sklearn_model\"\njoblib.dump(fitted_model, \"sklearn_model\")\n```",
        "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix)"
    ],
    "Pytorch": [
        "optim.param_groups[0]['lr'] = 0.001",
        "optim.param_groups[0]['lr'] = 0.001",
        "```python\noptim.param_groups[0]['lr'] = 0.0005\n```",
        "optim.param_groups[0]['lr'] = 0.0005",
        "embedded_input = torch.tensor(word2vec.wv[input_Tensor])",
        "embedded_input = torch.tensor([word2vec.wv[word] for word in input_Tensor])",
        "px = pd.DataFrame(x.numpy())",
        "px = px.applymap(lambda x: x.item())",
        "px = pd.DataFrame(x.numpy())",
        "C = B[:, A_log.nonzero().squeeze()]",
        "C = B[:, A_logical]",
        "C = B[:, A_log]",
        "C = B[:, A_log]",
        "C = B[:, A_log.nonzero().squeeze()]",
        "C = B[:, A_log]",
        "C = B.index_select(1, idx)",
        "```python\nx_tensor = torch.from_numpy(np.array([torch.Tensor(x) for x in x_array]))\n```",
        "x_tensor = torch.tensor(x_array)",
        "```python\ndef Convert(a):\n    t = torch.from_numpy(np.array([torch.from_numpy(np.array(i)) for i in a]))\n    return t\n```",
        "```python\nmask = torch.zeros(len(lens), max(lens))\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n```",
        "```python\nmask = torch.zeros(len(lens), max(lens)).long()\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n```",
        "```python\nmask = torch.zeros(len(lens), max(lens)).long()\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n```",
        "```python\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\n```",
        "```python\nTensor_3D = torch.diag_embed(Tensor_2D)\n```",
        "result = torch.diag_embed(t)",
        "ab = torch.cat((a, b), dim=0)",
        "ab = torch.cat((a, b), dim=0)",
        "ab = torch.cat((a, b), 0)",
        "a[:, lengths.max():, :] = 0",
        "a[:, lengths:, :] = 2333",
        "a[:, lengths.int():, :] = 0",
        "a[:, :lengths, :] = 2333",
        "tensor_of_tensors = torch.stack(list_of_tensors)",
        "new_tensors = torch.stack(list)",
        "```python\ntt = torch.stack(lt)\n```",
        "tensor_of_tensors = torch.stack(list_of_tensors)",
        "result = t[idx]",
        "result = t[np.arange(t.shape[0]), idx]",
        "result = t[np.arange(t.shape[0]), idx]",
        "result = torch.gather(x, 1, ids.unsqueeze(2).expand(-1, -1, x.size(2))).squeeze(1)",
        "result = torch.gather(x, 1, ids.unsqueeze(2).expand(-1, -1, x.size(2)).long().squeeze())",
        "result = torch.gather(x, 1, torch.argmax(ids, dim=1).unsqueeze(1))[:,:,0]",
        "y = torch.argmax(softmax_output, dim=1).unsqueeze(1)",
        "```python\ny = torch.argmax(softmax_output, dim=1).unsqueeze(1)\n```",
        "```python\ny = torch.argmin(softmax_output, dim=1).unsqueeze(1)\n```",
        "y = torch.argmax(softmax_output, dim=1).unsqueeze(1)",
        "y = torch.argmax(softmax_output, dim=1)",
        "```python\nloss = cross_entropy2d(images, labels)\n```",
        "cnt_equal = np.sum(A == B)",
        "cnt_equal = np.sum(A == B)",
        "cnt_not_equal = np.count_nonzero(A != B)",
        "cnt_equal = np.sum(A == B)",
        "cnt_equal = np.sum(A[-x:] == B[-x:])",
        "cnt_not_equal = np.sum(A[-x:] != B[-x:])",
        "tensors_31 = torch.split(a, chunk_dim, dim=3)",
        "tensors_31 = torch.chunk(a, chunk_dim, dim=2)",
        "output[mask == 1] = clean_input_spectrogram[mask == 1]",
        "output = output * mask + clean_input_spectrogram * (1 - mask)",
        "```python\nsigned_min = torch.where(torch.abs(x) <= torch.abs(y), x, y)\n```",
        "```python\nsigned_max = torch.mul(torch.sign(x), torch.max(torch.abs(x), torch.abs(y)))\n```",
        "```python\nsigned_min = torch.where(torch.abs(x) < torch.abs(y), x, y)\n```",
        "confidence_score = torch.nn.functional.softmax(output, dim=1)[:, 1]",
        "result = torch.cat((a[:, :-1], (a[:, -1] + b[:, 0]) / 2, b[:, 1:]), dim=1)",
        "result = torch.cat((a[:, :-1], (a[:, -1] + b[:, 0]) / 2, b[:, 1:]), dim=1)",
        "r = torch.cat([torch.zeros(1, 1, 4), t, torch.zeros(1, 1, 4)], dim=1)",
        "result = torch.zeros((t.shape[0]+2, t.shape[1]+2))\nresult[1:-1, 1:-1] = t\nresult",
        "result = torch.cat([torch.tensor([[-1, -1, -1, -1]]), t, torch.tensor([[-1, -1, -1, -1]])], dim=0)",
        "result = torch.matmul(data, W.unsqueeze(1)).squeeze()"
    ]
}