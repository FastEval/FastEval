{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\ndf = df.iloc[List]\nresult = sum(df['Type'] != df['Type'].shift())\n",
        "\nfor column in df.columns:\n    value_counts = df[column].value_counts()\n    values_to_change = value_counts[value_counts >= 2].index.tolist()\n    df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n",
        "\nfor column in df.columns:\n    counts = df[column].value_counts()\n    values_to_change = counts[counts >= 3].index.tolist()\n    df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n",
        "\n    for column in df.columns:\n        value_counts = df[column].value_counts()\n        values_to_keep = value_counts[value_counts >= 2].index\n        df[column] = df[column].apply(lambda x: x if x in values_to_keep else 'other')\n    ",
        "\nvalue_counts_qu1 = pd.value_counts(df.Qu1)\nvalue_counts_qu2 = pd.value_counts(df.Qu2)\nvalue_counts_qu3 = pd.value_counts(df.Qu3)\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if value_counts_qu1[x] < 3 else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if value_counts_qu2[x] < 2 else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if value_counts_qu3[x] < 2 else x)\n",
        "\nvalue_counts_qu1 = pd.value_counts(df.Qu1)\nvalue_counts_qu2 = pd.value_counts(df.Qu2)\nvalue_counts_qu3 = pd.value_counts(df.Qu3)\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x not in value_counts_qu1[value_counts_qu1 >= 3].index and x != 'apple' else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if x not in value_counts_qu2[value_counts_qu2 >= 2].index else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x not in value_counts_qu3[value_counts_qu3 >= 2].index else x)\nresult = df\n",
        "\ndf['keep'] = df.duplicated(subset='url') & (df['keep_if_dup'] == 'No')\nresult = df[~df['keep']].drop(columns='keep')\n",
        "\ndf = df.drop_duplicates(subset='url', keep='first')  # Remove duplicates based on 'url' column\ndf = df[df['drop_if_dup'] == 'No']  # Keep rows where 'drop_if_dup' is 'No'\n",
        "\ndf = df.drop_duplicates(subset='url', keep='last')\ndf = df[df['keep_if_dup'] == 'Yes']\n",
        "\nresult = {}\nfor index, row in df.iterrows():\n    current_dict = result\n    for col in df.columns[:-1]:\n        value = row[col]\n        if value not in current_dict:\n            current_dict[value] = {}\n        current_dict = current_dict[value]\n    current_dict[df.columns[-1]] = row[df.columns[-1]]\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    ",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\nimport pandas as pd\nimport re\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\n# Extract key-value pairs from the message column\npattern = r'\\[(.*?)\\]'\ndf['message'] = df['message'].str.extract(pattern, expand=False)\n# Split the key-value pairs into separate columns\ndf = df.join(df['message'].str.split(', ', expand=True).apply(lambda x: x.str.split(': ', expand=True)).stack().unstack())\n# Rename the columns\ndf.columns = ['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids']\nresult = df[['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids']]\nprint(result)\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\ndf.loc[~df['product'].isin(products), 'score'] = df.loc[~df['product'].isin(products), 'score'] * 10\n",
        "\nfor p in products:\n    df.loc[df['product'].isin(p), 'score'] *= 10\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1069104, 1069105]\n# Begin of Missing Code\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'].apply(lambda x: (x - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min()))\n# End of Missing Code\nresult = df\nprint(result)\n",
        "\ndf['category'] = df.idxmax(axis=1)\n",
        "\ncategory = df.idxmax(axis=1)\ndf['category'] = category\n",
        "\ndf['category'] = df.apply(lambda row: list(df.columns[row == 1]), axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\nmask = (df['Date'] >= '2019-01-17') & (df['Date'] <= '2019-02-20')\ndf_filtered = df.loc[mask]\ndf_filtered['Date'] = df_filtered['Date'].dt.strftime('%d-%b-%Y %A')\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf.iloc[0]['#1'] = df.iloc[-1]['#1']\n",
        "\ndf = df.shift(1)\ndf.iloc[-1] = df.iloc[0]\n",
        "\ndf_shifted = df.shift(1)\ndf_shifted.iloc[0] = df.iloc[-1]\ndf_shifted.iloc[-1] = df.iloc[0]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n# Shift the first row of the first column down 1 row\ndf.iloc[0, 0] = df.iloc[-1, 0]\n# Shift the last row of the first column to the first row\ndf.iloc[-1, 0] = df.iloc[0, 0]\nresult = df\nprint(result)\n",
        "\ndf.columns = [col + 'X' for col in df.columns]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = [col + 'X' if not col.endswith('X') else 'X' + col for col in df.columns]\n",
        "\nvalue_columns = [col for col in df.columns if col.startswith('val')]\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"mean\" for col in value_columns}})\n",
        "\nvalue_columns = [col for col in df.columns if col.startswith('val')]\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"sum\" for col in value_columns}})\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val42':[1,1,4,5,1] })\n# Get the list of value columns\nvalue_columns = [col for col in df.columns if col.startswith('val')]\n# Get the list of value columns ending with '2'\nmean_columns = [col for col in value_columns if col.endswith('2')]\n# Get the list of value columns not ending with '2'\nsum_columns = [col for col in value_columns if not col.endswith('2')]\n# Group by 'group' column and calculate the mean for mean_columns and the sum for sum_columns\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"mean\" for col in mean_columns}, **{col: \"sum\" for col in sum_columns}})\nprint(result)\n",
        "\nresult = df.loc[row_list, column_list].mean()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.loc[row_list, column_list].sum()\nresult = result[result != result.max()]\n",
        "\nresult = df.apply(pd.Series.value_counts)\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor column in df.columns:\n    result += \"---- \" + column + \" ---\\n\"\n    result += str(df[column].value_counts()) + \"\\n\"\n",
        "\nresult = df.iloc[0:1].append(df.iloc[1:2]).reset_index(drop=True)\n",
        "\nresult = df.iloc[0:1].append(df.iloc[1]).reset_index(drop=True)\n",
        "\nresult = df.fillna(method='ffill', axis=1).fillna(np.nan)\n",
        "\nresult = df.fillna(method='ffill', axis=1)\n",
        "\nresult = df.fillna(method='ffill')\n",
        "\ndf.loc[df['value'] < thresh] = df.loc[df['value'] < thresh].sum()\n",
        "\ndf.loc['X'] = df.loc[df['value'] >= thresh].mean()\ndf = df.loc[df['value'] < thresh]\n",
        "\ndf.loc[(df['value'] < section_left) | (df['value'] > section_right), 'lab'] = 'X'\ndf = df.groupby('lab').mean()\n",
        "\nresult = pd.concat([df, 1/df], axis=1)\nresult.columns = list(df.columns) + [\"inv_\" + col for col in df.columns]\n",
        "\nresult = pd.concat([df, np.exp(df)], axis=1)\nresult.columns = df.columns.tolist() + ['exp_' + col for col in df.columns]\n",
        "\nresult = df.copy()\nresult = result.assign(inv_A=1/df['A'], inv_B=1/df['B'])\n",
        "import numpy as np\nresult = df.copy()\nfor col in df.columns:\n    result[f\"sigmoid_{col}\"] = 1 / (1 + np.exp(-df[col]))\nprint(result)",
        "\nmin_idx = df.idxmin()\nresult = df.apply(lambda x: x[:min_idx[x.name]].idxmax())\n",
        "\nmin_idx = df.idxmin()\nresult = df.idxmax().where(df.columns.get_indexer(df.idxmax()) >= df.columns.get_indexer(min_idx))\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(start=min_date, end=max_date)\nusers = df['user'].unique()\nresult = pd.DataFrame({'dt': date_range, 'user': users})\nresult = result.merge(df, on=['dt', 'user'], how='left').fillna(0)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nall_dates = pd.date_range(start=min_date, end=max_date)\nexpanded_df = pd.DataFrame({'dt': all_dates})\nresult = pd.merge(expanded_df, df, on='dt', how='left').fillna(0)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nall_dates = pd.date_range(start=min_date, end=max_date, freq='D')\nexpanded_df = pd.DataFrame({'dt': all_dates, 'user': df['user'].unique(), 'val': 233})\nresult = pd.merge(expanded_df, df, on=['dt', 'user'], how='left').fillna(233)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nusers = df['user'].unique()\nexpanded_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date), 'user': users})\nresult = pd.merge(expanded_df, df, on=['dt', 'user'], how='left')\nresult['val'] = result.groupby('user')['val'].ffill()\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(start=min_date, end=max_date, freq='D')\ndf_expanded = pd.DataFrame({'dt': date_range})\ndf_expanded['user'] = df['user'].unique()\ndf_expanded['val'] = df.groupby('user')['val'].transform('max')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n# Create a dictionary to map each unique name to a unique ID\nname_id_map = {name: i+1 for i, name in enumerate(df['name'].unique())}\n# Replace the 'name' column with the corresponding ID using the map function\ndf['name'] = df['name'].map(name_id_map)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n# Begin of Missing Code\ndf['a'] = df.groupby('name').ngroup() + 1\n# End of Missing Code\nprint(df)\n",
        "\n    df['name'] = pd.factorize(df['name'])[0] + 1\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n# Create a new column 'ID' by combining 'name' and 'a'\ndf['ID'] = df['name'] + df['a'].astype(str)\n# Replace 'name' and 'a' with the unique IDs\ndf['name'] = df['ID']\ndf['a'] = df['ID']\n# Drop the 'ID' column\ndf.drop('ID', axis=1, inplace=True)\nprint(df)\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\nresult = pd.melt(df, id_vars=['user'], var_name='others', value_name='value')\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\nresult = df.loc[df['c'] > 0.5, columns]\n",
        "\nresult = df.loc[df['c'] > 0.45, columns].values\n",
        "\n    result = df[df['c'] > 0.5][columns].values\n    ",
        "\n    result = df[df['c'] > 0.5][columns]\n    result['sum'] = result.sum(axis=1)\n    ",
        "\n    result = df[df['c'] > 0.5][columns]\n    ",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 145.99, 146.73, 171.10]})\nX = 365\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n# Sort dataframe by date\ndf = df.sort_values('date')\n# Initialize a list to store the indices of rows to be removed\nrows_to_remove = []\n# Iterate over each row in the dataframe\nfor index, row in df.iterrows():\n    # Get the current date\n    current_date = row['date']\n    \n    # Check if there are any rows within X days of the current date\n    overlapping_rows = df[(df['date'] > current_date) & (df['date'] <= current_date + timedelta(days=X))]\n    \n    # If there are overlapping rows, add their indices to the list of rows to be removed\n    if len(overlapping_rows) > 1:\n        rows_to_remove.extend(overlapping_rows.index[1:])\n        \n# Remove the overlapping rows from the dataframe\ndf = df.drop(rows_to_remove)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n# Sort dataframe by date\ndf = df.sort_values('date')\n# Initialize an empty list to store the filtered rows\nfiltered_rows = []\n# Iterate over each row in the dataframe\nfor index, row in df.iterrows():\n    # Check if the current row is the first row or if it is X weeks away from the previous filtered row\n    if len(filtered_rows) == 0 or (row['date'] - filtered_rows[-1]['date']) >= timedelta(weeks=X):\n        # Add the current row to the filtered rows list\n        filtered_rows.append(row)\n# Create a new dataframe with the filtered rows\nresult = pd.DataFrame(filtered_rows)\nprint(result)\n",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 145.99, 146.73, 171.10]})\nX = 52\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n# Sort dataframe by date\ndf = df.sort_values('date')\n# Initialize a list to store the filtered rows\nfiltered_rows = []\n# Iterate over each row in the dataframe\nfor index, row in df.iterrows():\n    # Check if the current row is the first row or if it is not within X weeks of the previous row\n    if index == 0 or (row['date'] - df.loc[index-1, 'date']).days > X*7:\n        # Add the current row to the filtered rows list\n        filtered_rows.append(row)\n# Create a new dataframe with the filtered rows\nresult = pd.DataFrame(filtered_rows)\n# Convert date column back to string format\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\nprint(result)\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 3).sum()\n",
        "\nresult = df.groupby(df.index // 4).sum()\n",
        "\nresult = df[::-1].rolling(3).mean()[::-1].iloc[2::3]\n",
        "\nresult = []\ni = 0\nwhile i < len(df):\n    if i % 5 < 3:\n        result.append(df.loc[i:i+2, 'col1'].sum())\n        i += 3\n    else:\n        result.append(df.loc[i:i+1, 'col1'].mean())\n        i += 2\n",
        "\nresult = pd.DataFrame()\nfor i in range(len(df), 0, -5):\n    if i >= 3:\n        result = pd.concat([result, pd.DataFrame({'col1': [df['col1'][i-3:i].sum()]})])\n    else:\n        result = pd.concat([result, pd.DataFrame({'col1': [df['col1'][i-2:i].mean()]})])\nresult = result.reset_index(drop=True)\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, pd.NA).ffill().bfill().fillna(0)\n",
        "\ndf['number'] = df['duration'].str.extract('(\\d+)')\ndf['time'] = df['duration'].str.extract('(\\D+)')\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\ndf['time'] = df['duration'].str.extract(r'(\\D+)')\ndf['number'] = df['duration'].str.extract(r'(\\d+)')\ndf['time_day'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\n    df['number'] = df['duration'].str.extract('(\\d+)')\n    df['time'] = df['duration'].str.extract('(\\D+)')\n    df['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n    ",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ndf['time_day'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1}) * df['number']\n",
        "\nresult = np.where(np.any([df1[column] != df2[column] for column in columns_check_list], axis=0), True, False)\n",
        "\nresult = np.where(np.all(df1[columns_check_list] == df2[columns_check_list], axis=1))\n",
        "\ndf.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1, inplace=True)\n",
        "\ndf.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1, inplace=True)\n",
        "\n    df.reset_index(inplace=True)\n    df['date'] = pd.to_datetime(df['date'])\n    output = df.values\n    ",
        "\n    df.index = pd.to_datetime(df.index.get_level_values(0))\n    df = df.swaplevel().sort_index()\n    ",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\ndf = df.pivot(index=['Variable', 'Country', 'year'], columns='Variable', values='value').reset_index()\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(by=['year'], ascending=False)\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nresult = df[abs(df[columns]) < 1].dropna()\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nresult = df[abs(df[columns]) > 1].dropna()\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value_')]\ndf = df[df[columns].abs() > 1].dropna()\ndf.columns = [col.replace('Value_', '') for col in df.columns]\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT', '<', regex=True)\n",
        "\n    df = df.replace('&AMP;', '&', regex=True)\n    ",
        "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if validate_single_space_name(x) else x)\ndf['first_name'] = df['first_name'].apply(lambda x: x.split(' ')[0] if x else x)\ndf['last_name'] = df['last_name'].apply(lambda x: x.split(' ')[-1] if x else x)\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].str.split(' ').str[1]\ndf.loc[df['2_name'].isnull(), '1_name'] = df['name']\ndf.loc[df['2_name'].isnull(), '2_name'] = ''\ndf = df[['1_name', '2_name']]\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['first name'] = df['name'].apply(lambda x: x.split(' ')[0] if validate_single_space_name(x) else x)\ndf['middle_name'] = df['name'].apply(lambda x: x.split(' ')[1] if validate_single_space_name(x) else None)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[-1] if validate_single_space_name(x) else None)\ndf.drop('name', axis=1, inplace=True)\n",
        "\nresult = pd.merge(df2, df1, on='Timestamp', how='left')\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n# Begin of Missing Code\nerror_values = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        error_values.append(row['Field1'])\n# End of Missing Code\nresult = error_values\nprint(result)\n",
        "\nresult = []\nfor index, row in df.iterrows():\n    if isinstance(row['Field1'], int):\n        result.append(row['Field1'])\n",
        "\n    result = []\n    for index, row in df.iterrows():\n        if not isinstance(row['Field1'], int):\n            result.append(row['Field1'])\n    ",
        "\ntotal = df.sum(axis=1)\ndf.iloc[:, 1:] = df.iloc[:, 1:].div(total, axis=0)\n",
        "\ntotal = df.sum(axis=0)\ndf_percentage = df.copy()\ndf_percentage.iloc[:, 1:] = df_percentage.iloc[:, 1:].div(total[1:])\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\n",
        "\n    result = df.loc[test].drop_duplicates()\n    ",
        "\nimport pandas as pd\nfrom scipy.spatial import distance\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Calculate pairwise distances between cars\ndistances = df.groupby('time').apply(lambda x: distance.cdist(x[['x', 'y']], x[['x', 'y']]))\ndistances = pd.DataFrame(distances.explode().tolist(), index=distances.index)\n# Find nearest neighbor for each car\nnearest_neighbor = distances.groupby('time').apply(lambda x: x.idxmin(axis=1))\nnearest_neighbor.columns = ['nearest_neighbour']\n# Calculate Euclidean distance to nearest neighbor\neuclidean_distance = distances.min(axis=1)\neuclidean_distance.columns = ['euclidean_distance']\n# Combine results into a single dataframe\nresult = pd.concat([df['car'], nearest_neighbor, euclidean_distance], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Calculate the pairwise distances between cars\ndistances = []\nfor i in range(len(df)):\n    xi = df.loc[i, 'x']\n    yi = df.loc[i, 'y']\n    dist = np.sqrt((df['x'] - xi)**2 + (df['y'] - yi)**2)\n    distances.append(dist)\n# Find the farmost neighbour for each car\nfarmost_neighbours = []\nfor i in range(len(df)):\n    dist = distances[i]\n    farmost_neighbour = dist.idxmax()\n    farmost_neighbours.append(farmost_neighbour)\n# Calculate the euclidean distance between each car and its farmost neighbour\neuclidean_distances = []\nfor i in range(len(df)):\n    xi = df.loc[i, 'x']\n    yi = df.loc[i, 'y']\n    farmost_neighbour = farmost_neighbours[i]\n    xf = df.loc[farmost_neighbour, 'x']\n    yf = df.loc[farmost_neighbour, 'y']\n    euclidean_distance = np.sqrt((xf - xi)**2 + (yf - yi)**2)\n    euclidean_distances.append(euclidean_distance)\n# Add the farmost neighbour and euclidean distance to the dataframe\ndf['farmost_neighbour'] = farmost_neighbours\ndf['euclidean_distance'] = euclidean_distances\nresult = df\nprint(result)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \",\".join(row.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \"-\".join(row.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \"-\".join(row.dropna()), axis=1)\n",
        "\nrandom_rows = df.sample(frac=0.2, random_state=0)\ndf.loc[random_rows.index, 'Quantity'] = 0\n",
        "\nrandom_rows = df.sample(frac=0.2, random_state=0)\nrandom_rows['ProductId'] = 0\ndf.update(random_rows)\n",
        "\ndf['Quantity'] = df.groupby('UserId')['Quantity'].apply(lambda x: x.sample(frac=0.2, random_state=0).replace(x, 0))\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\ndf['index_original'] = duplicate_bool.groupby(duplicate_bool).cumcount()\n",
        "\nduplicate['index_original'] = duplicate.index\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.index[0]\n    result = duplicate\n    ",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index[0]\nprint(duplicate)\n",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\n# Find the index of the last duplicate\nlast_duplicate_index = duplicate.index[-1]\n# Add a new column to the duplicate dataframe\nduplicate['index_original'] = last_duplicate_index\nprint(duplicate)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp':['MM2','MM2','MM4','MM4','MM4'],\n                   'Mt':['S4','S4','S2','S2','S2'],\n                   'Value':['bg','dgd','rd','cb','uyi'],\n                   'count':[10,1,2,8,8]})\nresult = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\nresult = df[df.groupby(['Sp', 'Mt'])['count'].transform(min) == df['count']]\nprint(result)\n",
        "\nresult = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n",
        "\nresult = df.query(\"Category in @filter_list\")\n",
        "\nresult = df.query(\"Category != @filter_list\")\n",
        "\nvalue_vars = [tuple(col) for col in df.columns]\nresult = pd.melt(df, value_vars=value_vars)\n",
        "\ndf = pd.melt(df, value_vars=[tuple(col) for col in df.columns])\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf.loc[df['cumsum'] < 0, 'cumsum'] = 0\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('r')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('l')['v'].sum().reset_index()\nresult.loc[result['v'].isnull(), 'v'] = np.nan\n",
        "\nresult = []\ncolumns = df.columns.tolist()\nfor i in range(len(columns)):\n    for j in range(len(columns)):\n        if i != j:\n            if df[columns[i]].nunique() == df[columns[j]].nunique():\n                result.append(f\"{columns[i]} {columns[j]} one-to-one\")\n            elif df[columns[i]].nunique() > df[columns[j]].nunique():\n                result.append(f\"{columns[i]} {columns[j]} one-to-many\")\n            else:\n                result.append(f\"{columns[i]} {columns[j]} many-to-one\")\n        else:\n            result.append(f\"{columns[i]} {columns[j]} many-to-many\")\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].nunique() == df[col2].nunique():\n                result.append(f'{col1} {col2} many-2-many')\n            elif df[col1].nunique() > df[col2].nunique():\n                result.append(f'{col1} {col2} one-2-many')\n            else:\n                result.append(f'{col1} {col2} many-2-one')\n        else:\n            result.append(f'{col1} {col2} one-2-one')\n",
        "\nresult = df.corr().applymap(lambda x: 'one-to-one' if x == 1 else 'one-to-many' if x == -1 else 'many-to-one' if x == 0 else 'many-to-many')\n",
        "\nresult = df.corr().applymap(lambda x: 'one-2-one' if abs(x) == 1 else 'one-2-many' if abs(x) > 0.5 else 'many-2-one' if abs(x) < 0.5 else 'many-2-many')\n",
        "\ndf = df.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last')\n",
        "\nresult = pd.to_numeric(s.astype(str).str.replace(',',''), errors='coerce')\n",
        "\nresult = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0))['Survived'].mean()\n",
        "\nresult = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0)).mean()['SibSp']\n",
        "\nresult = df.groupby([(df['SibSp'] == 1) & (df['Parch'] == 1), \n                     (df['SibSp'] == 0) & (df['Parch'] == 0), \n                     (df['SibSp'] == 0) & (df['Parch'] == 1), \n                     (df['SibSp'] == 1) & (df['Parch'] == 0)])['Survived'].mean()\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "import numpy as np\nresult = df.groupby('a')['b'].agg(['mean', 'std'])\n",
        "import numpy as np\nresult = df.groupby('b')['a'].agg(['mean', 'std'])\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n# Begin of Missing Code\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: softmax(x))\ndf['min-max'] = df.groupby('a')['b'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n# End of Missing Code\nresult = df\nprint(result)\n",
        "\nresult = df.loc[:, (df != 0).any(axis=0)].loc[(df != 0).any(axis=1)]\n",
        "\nresult = df.loc[(df.sum(axis=1) != 0), (df.sum(axis=0) != 0)]\n",
        "\nresult = df.loc[(df.max(axis=1) <= 2), (df.max(axis=0) <= 2)]\n",
        "\ndf[df > 2] = 0\nresult = df\n",
        "\nresult = s.sort_values(ascending=True)\n",
        "\ndf = pd.DataFrame({'index': s.index, '1': s.values})\ndf = df.sort_values(by=['1', 'index'])\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp':['MM2','MM2','MM4','MM4','MM4'],\n                   'Mt':['S4','S4','S2','S2','S2'],\n                   'Value':['bg','dgd','rd','cb','uyi'],\n                   'count':[10,1,2,8,8]})\nresult = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\nresult = df[df.groupby(['Sp', 'Mt'])['count'].transform(min) == df['count']]\nprint(result)\n",
        "\nresult = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n",
        "\ndf['Date'] = df['Member'].map(dict)\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\n",
        "\n    df['Date'] = df['Member'].map(dict)\n    ",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Date'].transform('count')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Date'].transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val'])['Date'].transform('count')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Date'].transform('count')\ndf['Count_w'] = df.groupby(df['Date'].dt.weekday.rename('weekday'))['Date'].transform('count')\ndf['Count_Val'] = df.groupby('Val')['Date'].transform('count')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\nresult1 = df[df == 0].groupby('Date').count()\nresult2 = df[df != 0].groupby('Date').count()\nprint(result1)\nprint(result2)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\nresult1 = df.groupby('Date').apply(lambda x: (x % 2 == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x % 2 != 0).sum())\nprint(result1)\nprint(result2)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\nimport pandas as pd\nimport dask.dataframe as dd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\ndf = dd.from_pandas(df, npartitions=1)\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2').reset_index(drop=True)\nresult = result.compute()\nprint(result)\n",
        "\nimport pandas as pd\nimport dask.dataframe as dd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\ndf = dd.from_pandas(df, npartitions=1)\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2').reset_index(drop=True)\nresult = result.compute()\nprint(result)\n",
        "\nimport pandas as pd\nimport dask.dataframe as dd\ndf = pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1,2,3], columns=['var1', 'var2'])\ndf = dd.from_pandas(df, npartitions=1)\nresult = df.assign(var2=df['var2'].str.split('-')).explode('var2').reset_index(drop=True)\nresult = result.compute()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\nprint(result)\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'medi', 'row']] = df['row'].str.split(' ', 2, expand=True)\n",
        "\ndf = df.set_index('Name')  # Set 'Name' column as the index\ndf = df.replace(0, pd.NA)  # Replace 0 with NaN\ndf = df.cumsum(axis=1)  # Calculate cumulative sum along each row\ndf = df.div(df.count(axis=1), axis=0)  # Divide by the count of non-null values along each row\ndf = df.fillna(0)  # Replace NaN with 0\ndf = df.reset_index()  # Reset the index to default\n",
        "\ndf = df.set_index('Name')  # Set 'Name' column as the index\ndf = df.apply(lambda x: x[::-1].cumsum()[::-1].mask(x == 0).fillna(0))  # Calculate cumulative average while ignoring zeros\ndf = df.reset_index()  # Reset the index to default\n",
        "\n    result = df.copy()\n    result.iloc[:, 1:] = df.iloc[:, 1:].replace(0, pd.NA).expanding(axis=1).mean().fillna(0)\n    ",
        "\ndf = df.set_index('Name')  # Set 'Name' column as the index\ndf = df.apply(lambda x: x[::-1].cumsum()[::-1].div((x != 0)[::-1].cumsum()[::-1]))  # Calculate cumulative average\ndf = df.reset_index()  # Reset the index to default\n",
        "\ndf['Label'] = 0\ndf.loc[df['Close'] - df['Close'].shift(1) > 0, 'Label'] = 1\ndf.loc[0, 'Label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf.loc[0, 'label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\nimport pandas as pd\nimport numpy as np\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n# Convert arrival_time and departure_time to datetime64[ns]\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\n# Calculate the time difference between rows\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\n# Replace NaT values with NaN\ndf['Duration'] = df['Duration'].replace(pd.NaT, np.nan)\nresult = df\nprint(result)\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%Y-%m-%d %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], format='%Y-%m-%d %H:%M:%S')\ndf['Duration'] = (df['departure_time'].shift(-1) - df['arrival_time']).dt.total_seconds()\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\nresult = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\n",
        "\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n",
        "\nresult = df[df['key2'].str.endswith(\"e\")].groupby('key1').size().reset_index(name='count')\n",
        "\nmin_result = df.index.min()\nmax_result = df.index.max()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n# Begin of Missing Code\nmode_result = df.index.mode()[0]\nmedian_result = df.index.median()\n# End of Missing Code\nprint(mode_result, median_result)\n",
        "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n",
        "\ndf1 = df.loc[df.groupby(\"item\")[\"diff\"].idxmin()]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n",
        "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]\n    ",
        "\nnum_nan = df['Column_x'].isnull().sum()\nnum_fill_0 = int(num_nan / 2)\nnum_fill_1 = num_nan - num_fill_0\ndf['Column_x'].fillna(0, limit=num_fill_0, inplace=True)\ndf['Column_x'].fillna(1, limit=num_fill_1, inplace=True)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n# Calculate the number of NaN values in the column\nnum_nan = df['Column_x'].isnull().sum()\n# Calculate the number of NaN values to be filled with each value\nnum_zeros = int(num_nan * 0.3)\nnum_half = int(num_nan * 0.3)\nnum_ones = num_nan - num_zeros - num_half\n# Fill the NaN values with the desired values\ndf['Column_x'].fillna(0, limit=num_zeros, inplace=True)\ndf['Column_x'].fillna(0.5, limit=num_half, inplace=True)\ndf['Column_x'].fillna(1, inplace=True)\nresult = df\nprint(result)\n",
        "\nnum_zeros = int(df['Column_x'].isnull().sum() / 2)\nnum_ones = int(df['Column_x'].isnull().sum() / 2)\ndf['Column_x'].fillna(0, limit=num_zeros, inplace=True)\ndf['Column_x'].fillna(1, limit=num_ones, inplace=True)\n",
        "\nresult = pd.DataFrame(np.dstack((a.values, b.values)), columns=a.columns)\n",
        "\ndataframes = [a, b, c]\nresult = pd.concat([df.stack().apply(tuple).unstack() for df in dataframes], axis=1)\n",
        "\nmax_len = max(len(a), len(b))\nresult = pd.DataFrame(index=range(max_len), columns=a.columns)\nfor col in a.columns:\n    result[col] = list(zip(a[col].values[:max_len], b[col].values[:max_len]))\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = pd.concat([df1, df2], axis=0, ignore_index=True)\nresult = result.fillna({'city': 'NaN', 'district': 'NaN'})\nprint(result)\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)\nresult = result.fillna('NaN')\nprint(result)\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult = result.fillna(method='ffill')\nprint(result)\n",
        "\nresult = pd.merge(C, D, how='left', on='A').fillna(C)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_x'].fillna(result['B_y'], inplace=True)\nresult.drop(columns=['B_y'], inplace=True)\nresult.rename(columns={'B_x': 'B'}, inplace=True)\n",
        "\nresult = pd.merge(C, D, how='left', on='A')\nresult['dulplicated'] = result['B_y'].notnull()\nresult['B'] = result['B_y'].fillna(result['B_x'])\nresult = result[['A', 'B', 'dulplicated']]\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['amount', 'time']].values.tolist()[::-1])\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index)\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index, columns=[0, 1, 2, 3])\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = df.columns[df.columns.str.contains(s) & ~df.columns.str.match(s)].tolist()\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = pd.DataFrame(df['codes'].to_list()).add_prefix('code_')\n",
        "\nresult = pd.DataFrame(df['codes'].to_list()).add_prefix('code_')\n",
        "\nresult = pd.DataFrame(df['codes'].to_list()).add_prefix('code_')\n",
        "\nresult = [item for sublist in df['col1'] for item in sublist]\n",
        "\nresult = ','.join(','.join(map(str, lst[::-1])) for lst in df['col1'])\n",
        "\nresult = ','.join([str(item) for sublist in df['col1'] for item in sublist])\n",
        "\ndf['Time'] = df['Time'].dt.floor('2min')\nresult = df.groupby('Time').mean().reset_index()\n",
        "\ndf = df.set_index('Time')\ndf = df.resample('3T').sum()\ndf = df.reset_index()\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert 'TIME' column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)  # Add 'RANK' column by ranking 'TIME' within each 'ID' group\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert 'TIME' column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)  # Add 'RANK' column by ranking 'TIME' within each 'ID' group\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n",
        "\nresult = df[filt[df.index.get_level_values('a')]]\n",
        "\nresult = df[filt[df.index.get_level_values('a')]]\n",
        "\nresult = df.loc[[0, 8]].isnull().any()\nresult = result[result].index.tolist()\n",
        "\nresult = df.loc[[0, 8]].T.apply(lambda x: x.nunique() == 1 and x.notna().all(), axis=1)\nresult = result[result].index.tolist()\n",
        "\nresult = df.loc[[0, 8]].isnull().apply(lambda x: x[0] != x[1], axis=0).index.tolist()\n",
        "\nrow_0 = df.iloc[0]\nrow_8 = df.iloc[8]\nresult = [(row_0[col], row_8[col]) for col in df.columns if row_0[col] != row_8[col]]\n",
        "\nts = df.set_index('Date')['Value']\n",
        "\nresult = df.stack().reset_index(drop=True).to_frame().T\nresult.columns = [f\"{col}_{i+1}\" for i, col in enumerate(result.columns)]\n",
        "\nresult = pd.DataFrame(df.values.flatten(), index=df.columns + '_' + df.index.astype(str)).T\n",
        "\ndf['dogs'] = df['dogs'].round(2)\n",
        "\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\nresult = df.sort_index(level='time')\n",
        "\nresult = df.sort_values(by=['VIM', 'time'], ascending=[True, True])\n",
        "\ndf = df[~((df.index.date == pd.to_datetime('2020-02-17').date()) | (df.index.date == pd.to_datetime('2020-02-18').date()))]\n",
        "\ndf = df[~df.index.date.isin([pd.to_datetime('2020-02-17').date(), pd.to_datetime('2020-02-18').date()])]\ndf['Day of Week'] = df.index.strftime('%d-%b-%Y %A')\nresult = df['Day of Week']\n",
        "\nresult = corr[corr > 0.3].stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult = result[['Col1', 'Col2', 'Pearson Correlation Coefficient']]\n",
        "\nresult = corr[corr > 0.3].stack().reset_index().iloc[:, [0, 2]]\nresult.columns = [0, 3]\nresult = result.set_index([0, 3]).squeeze()\n",
        "\ndf.columns = list(df.columns[:-1]) + ['Test']\n",
        "\ndf.columns = ['Test'] + list(df.columns[1:])\n",
        "\ndf['frequent'] = df.mode(axis=1).iloc[:, 0]\ndf['freq_count'] = df.sum(axis=1)\n",
        "\ndf['frequent'] = df.mode(axis=1).iloc[:, 0]\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n",
        "\nfrequent_values = []\nfreq_counts = []\nfor index, row in df.iterrows():\n    value_counts = row.value_counts()\n    max_count = value_counts.max()\n    frequent_values.append(list(value_counts[value_counts == max_count].index))\n    freq_counts.append(max_count)\ndf['frequent'] = frequent_values\ndf['freq_count'] = freq_counts\n",
        "\nresult = df.replace('NULL', pd.NA).groupby([\"id1\",\"id2\"]).mean().reset_index()\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\ndf['bar'] = df['bar'].replace('NULL', 0).astype(float)\nresult = df.groupby([\"id1\",\"id2\"]).mean()\nprint(result)\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum')\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum', 'b_col']], on='EntityNum')\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nresult = [list(filter(lambda a: not np.isnan(a), sublist)) for sublist in x]\n",
        "\nb = np.eye(np.max(a)+1)[a]\n",
        "\nb = np.eye(np.max(a)+1)[a]\n",
        "\nb = np.zeros((a.size, a.max() - a.min() + 1))\nb[np.arange(a.size), a - a.min()] = 1\n",
        "\nb = np.zeros((len(a), len(a)))\nfor i, val in enumerate(a):\n    index = np.argsort(a).tolist().index(i)\n    b[i][index] = 1\n",
        "\nb = np.eye(np.max(a)+1)[a.flatten()]\nb = b.reshape(a.shape + (b.shape[1],))\n",
        "\nresult = np.percentile(a, p)\n",
        "\nB = np.reshape(A, (-1, ncol))\n",
        "\nB = np.reshape(A, (nrow, -1))\n",
        "\nB = np.reshape(A, (-1, ncol))\n",
        "\nB = np.reshape(A, (-1, ncol))\n",
        "\nresult = np.roll(a, shift)\nresult[:shift] = np.nan\nresult[shift:] = a[:len(a)-shift]\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "import numpy as np\n# Set the random seed to a fixed value\nnp.random.seed(0)\n# Generate the random array\nr = np.random.randint(3, size=(100, 2000)) - 1\n# Generate r_old and r_new using the same random seed\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\nprint(r_old, r_new)",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape, order='F')\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    result = np.unravel_index(np.argmax(a), a.shape)\n    ",
        "\nmax_value = np.max(a)  # find the maximum value in the array\na[a == max_value] = -np.inf  # set the maximum value to negative infinity\nsecond_max_value = np.max(a)  # find the second maximum value in the array\nresult = np.unravel_index(np.argmax(a), a.shape)  # get the unraveled index of the second maximum value\n",
        "\na = a[:, ~np.isnan(a).any(axis=0)]\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nradian = np.radians(degree)\nresult = np.sin(radian)\n",
        "\nradian = np.radians(degree)\nresult = np.cos(radian)\n",
        "\nresult = 0 if np.sin(np.deg2rad(number)) > np.sin(number) else 1\n",
        "\nresult = np.arcsin(value) * 180 / np.pi\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant')\n",
        "\nresult = np.pad(A, (0, length - len(A)), mode='constant')\n",
        "\na = np.power(a, power)\n",
        "\n    result = np.power(a, power)\n    ",
        "\nresult = Fraction(numerator, denominator)\n",
        "\n    from fractions import Fraction\n    result = Fraction(numerator, denominator)\n    ",
        "\nresult = Fraction(numerator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, np.maximum(b, c))\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\nresult = np.diag(np.fliplr(a))\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\nresult = X.flatten().tolist()\n",
        "\nresult = X.flatten(order='C')\n",
        "\n    result = X.flatten().tolist()\n    ",
        "\nresult = X.flatten(order='F')\n",
        "\nresult = np.array([int(x) for x in mystr])\n",
        "\nresult = np.cumsum(a[:, col] * multiply_number)\n",
        "\nresult = np.cumsum(a[row] * multiply_number)\n",
        "\nresult = np.prod(a[row] / divide_number)\n",
        "\nresult = np.linalg.qr(a)[0]\n",
        "\nresult = a.shape[1]\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nimport numpy as np\nimport scipy.stats\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n# Remove NaN values from a and b\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n# Perform two-tailed t-test\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\nprint(p_value)\n",
        "\n# Calculate the standard error of the difference between the means\nse_diff = np.sqrt((avar/anobs) + (bvar/bnobs))\n# Calculate the t-statistic\nt_stat = (amean - bmean) / se_diff\n# Calculate the degrees of freedom\ndf = anobs + bnobs - 2\n# Calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_stat), df))\n",
        "\noutput = np.setdiff1d(A, B, axis=0)\n",
        "\noutput = np.concatenate((np.setdiff1d(A, B), np.setdiff1d(B, A)))\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices, np.arange(b.shape[1])[:, np.newaxis], np.arange(b.shape[2])]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)[::-1]\nc = b[sort_indices]\n",
        "\nsum_a = np.sum(a, axis=(1, 2))\nsorted_indices = np.argsort(sum_a)\nresult = b[sorted_indices]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0, 2], axis=1)\n",
        "\nresult = np.delete(a, del_col, axis=1)\n",
        "\na = np.insert(a, pos, element)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\n    a = np.insert(a, pos, element)\n    ",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\nresult = np.copy(array_of_arrays)\n",
        "\nresult = np.all(a == a[0], axis=0)\n",
        "\nresult = np.all(a[:, 0] == a[:, 1:])\n",
        "\n    result = np.all(a[0] == a[1:], axis=0)\n    ",
        "\nX, Y = np.meshgrid(x, y)\nZ = np.cos(X)**4 + np.sin(Y)**2\nresult = simps(simps(Z, y), x)\n",
        "\n    X, Y = np.meshgrid(x, y)\n    Z = np.cos(X)**4 + np.sin(Y)**2\n    result = simps(simps(Z, y), x)\n    ",
        "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\nresult = ecdf(grades)(grades)\n",
        "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\nresult = ecdf(grades)(eval)\n",
        "\necdf = np.cumsum(grades / np.sum(grades))\nlow = np.min(grades[ecdf < threshold])\nhigh = np.max(grades[ecdf < threshold])\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\na_np = a.numpy()\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[-N:][::-1]\n",
        "\nresult = np.linalg.matrix_power(A, n)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\nresult = np.array(result)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = []\nfor i in range(0, a.shape[0] - patch_size + 1, patch_size):\n    for j in range(0, a.shape[1] - patch_size + 1, patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        result.append(patch)\nresult = np.array(result)\n",
        "\nresult = np.reshape(a, (h, w))\n",
        "\nresult = []\nfor i in range(0, a.shape[0] - patch_size + 1, patch_size):\n    for j in range(0, a.shape[1] - patch_size + 1, patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        result.append(patch)\n",
        "\nresult = a[:, low:high+1]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:, low:high]\n",
        "\na = np.array(eval(string))\n",
        "\nresult = np.exp(np.random.uniform(low=np.log(min), high=np.log(max), size=n))\n",
        "\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\n    result = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n    ",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nresult = np.ravel_multi_index(index, dims, order='F') + 1\n",
        "\nresult = np.ravel_multi_index(index, dims, order='C')\n",
        "\nvalues = np.zeros((2,3), dtype=[('a', 'int'), ('b', 'float')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\nresult = np.zeros(np.max(index)+1)\nfor i in range(len(a)):\n    if a[i] > result[index[i]]:\n        result[index[i]] = a[i]\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\nresult = np.zeros(np.max(index)+1)\nfor i in range(len(a)):\n    if index[i] >= 0:\n        result[index[i]] = min(result[index[i]], a[i])\n",
        "\nz = np.add(x, y)\n",
        "\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\nresult = a[max(low_index, 0):min(high_index, a.shape[0]), max(low_index, 0):min(high_index, a.shape[1])]\nresult = np.pad(result, ((0, max(high_index - a.shape[0], 0)), (0, max(high_index - a.shape[1], 0))), mode='constant')\n",
        "\nresult = x[x >= 0]\n",
        "\nresult = x[np.iscomplex(x)]\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_max = np.max(bin_data, axis=1)\n",
        "\nbin_data = np.array([data[:, i:i+bin_size] for i in range(0, data.shape[1], bin_size)])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nn = len(data)\nnum_bins = n // bin_size\nbin_data = np.split(data[-n:], num_bins)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nn_rows, n_cols = data.shape\nn_bins = n_cols // bin_size\n# Create an empty array to store the binned data\nbin_data = np.empty((n_rows, n_bins), dtype=object)\n# Iterate over each row of the data\nfor i in range(n_rows):\n    # Iterate over each bin\n    for j in range(n_bins):\n        # Get the indices for the current bin\n        start = n_cols - (j+1)*bin_size\n        end = n_cols - j*bin_size\n        \n        # Extract the elements for the current bin\n        bin_elements = data[i, start:end]\n        \n        # Store the bin elements in the bin_data array\n        bin_data[i, j] = bin_elements\n# Calculate the mean of each bin\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nn_rows, n_cols = data.shape\nn_bins = n_cols // bin_size\n# Create an array to store the binned data\nbin_data = np.zeros((n_rows, n_bins), dtype=object)\n# Iterate over each row\nfor i in range(n_rows):\n    # Iterate over each bin\n    for j in range(n_bins):\n        # Get the start and end indices of the bin\n        start = n_cols - (j+1)*bin_size\n        end = n_cols - j*bin_size\n        \n        # Slice the row to get the bin\n        bin_data[i, j] = tuple(data[i, start:end])\n# Calculate the mean of each bin\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nimport numpy as np\ndef smoothclamp(x):\n    return np.clip(3 * x**2 - 2 * x**3, x_min, x_max)\nx = 0.25\nx_min = 0\nx_max = 1\nresult = smoothclamp(x)\nprint(result)\n",
        "\nimport numpy as np\ndef smoothclamp(x, N=5):\n    return np.clip((x - x_min) / (x_max - x_min), 0, 1) ** N\nx = 0.25\nx_min = 0\nx_max = 1\nN = 5\nresult = smoothclamp(x, N=N)\nprint(result)\n",
        "\nresult = np.correlate(a, np.roll(b[::-1], 1), mode='valid')\n",
        "\nresult = df.values.reshape(4, 15, 5)\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\nresult = np.unpackbits(a[:, np.newaxis], axis=1)[:, -m:]\n",
        "\nresult = np.unpackbits(np.uint8(a.reshape(-1, 1)), axis=1)[:, -m:]\n",
        "\nresult = np.unpackbits(a[:, np.newaxis], axis=1)[:, -m:]\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 3 * std, mean + 3 * std)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 2 * std, mean + 2 * std)\n",
        "\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3*std, mean + 3*std)\n    ",
        "\nmean = np.mean(a)\nstd = np.std(a)\nthreshold = mean + 2 * std\nresult = np.abs(a - mean) > threshold\n",
        "\nmasked_data = np.ma.masked_where(DataArray < 0, DataArray)\nprob = np.percentile(masked_data, percentile)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[1,:] = 0\na[:,0] = 0\n",
        "\nmask = (a == np.amax(a, axis=1)[:, np.newaxis])\n",
        "\nmin_values = np.min(a, axis=1)\nmask = np.equal(a, min_values[:, np.newaxis])\n",
        "\nresult, _ = pearsonr(post, distance)\n",
        "\nresult = np.einsum('ij,ik->jik', X, X)\n",
        "\nX = np.zeros((Y.shape[1], Y.shape[2]))\nfor i in range(Y.shape[0]):\n    X += Y[i]\n",
        "\nis_contained = np.isin(number, a)\n",
        "\nC = np.setdiff1d(A, B)\n",
        "\nC = np.intersect1d(A, B)\n",
        "\nC = np.intersect1d(A, B)\n",
        "\nresult = len(a) - rankdata(a).astype(int) + 1\n",
        "\nresult = len(a) - rankdata(a).astype(int) - 1\n",
        "\n    result = len(a) - rankdata(a).astype(int) + 1\n    ",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\nresult = a[:, second, :][:, :, third]\n",
        "\narr = np.zeros((20,10,10,2))\n",
        "\nX_normalized = X / np.linalg.norm(X, ord=1, axis=1, keepdims=True)\n",
        "\nresult = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
        "\nresult = np.max(np.abs(X), axis=1)\n",
        "\nresult = np.select([df['a'].astype(str).str.contains(target)], choices, default=np.nan)\n",
        "\nimport numpy as np\nfrom scipy.spatial.distance import pdist\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n# Calculate pairwise distances\nresult = pdist(a)\nprint(result)\n",
        "\ndistances = squareform(pdist(a))\nresult = distances[0]\n",
        "\ndistances = pdist(a)\nresult = squareform(distances)\n",
        "\nNA = NA.astype(float)\nAVG = np.mean(NA, axis=0)\n",
        "\nNA = np.array([float(x) if x != 'inf' else np.inf for x in NA])\nAVG = np.mean(NA, axis=0)\n",
        "\nNA = np.array([float(x) if x != 'np.inf' else np.inf for x in NA])\nAVG = np.mean(NA, axis=0)\n",
        "\nresult = np.delete(a, np.where(np.diff(a) == 0))\nresult = result[result != 0]\n",
        "\nresult = np.unique(a[np.concatenate(([True], np.diff(a.flatten()) != 0))])\n",
        "\ndata = np.column_stack((lat.flatten(), lon.flatten(), val.flatten()))\ndf = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\n",
        "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ",
        "\ndata = np.column_stack((lat.flatten(), lon.flatten(), val.flatten()))\ndf = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\ndf['maximum'] = df.max(axis=1)\n",
        "\nresult = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        window = a[max(0, i-size[0]//2):min(a.shape[0], i+size[0]//2+1), max(0, j-size[1]//2):min(a.shape[1], j+size[1]//2+1)]\n        result.append(window)\n",
        "\nresult = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        window = a[max(0, i-size[0]//2):min(a.shape[0], i+size[0]//2+1), max(0, j-size[1]//2):min(a.shape[1], j+size[1]//2+1)]\n        result.append(window)\n",
        "\nresult = np.nanmean(a)\n",
        "\n    result = np.mean(a.real)\n    ",
        "\nresult = Z[..., -1:]\n",
        "\nresult = a[-1:]\n",
        "\nresult = any(np.array_equal(c, cnt) for cnt in CNTS)\n",
        "\nresult = any(np.array_equal(c, arr) for arr in CNTS)\n",
        "\nf = intp.interp2d(np.arange(0, 4), np.arange(0, 4), a, kind='linear')\nresult = f(x_new, y_new)\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n",
        "\ni = np.diag(i)\n",
        "\nnp.fill_diagonal(a, 0)\n",
        "\nstart_dt = datetime.strptime(start, \"%d-%b-%Y %H:%M:%S.%f\")\nend_dt = datetime.strptime(end, \"%d-%b-%Y %H:%M:%S.%f\")\ndelta = (end_dt - start_dt) / (n - 1)\nresult = pd.date_range(start=start_dt, end=end_dt, periods=n)\n",
        "\nresult = np.where((x == a) & (y == b))[0][0] if np.sum((x == a) & (y == b)) > 0 else -1\n",
        "\nresult = np.where((x == a) & (y == b))[0]\n",
        "\ncoefficients = np.polyfit(x, y, 2)\nresult = list(coefficients)\n",
        "\nresult = np.polyfit(x, y, degree)\n",
        "\ndf = df.apply(lambda x: x - a)\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.flatten().reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n",
        "\nscaler = MinMaxScaler()\nresult = np.zeros_like(a)\nfor i in range(a.shape[0]):\n    result[i] = scaler.fit_transform(a[i])\n",
        "\narr_temp = arr.copy()\nmask = arr_temp < -10\nmask2 = arr_temp >= 15\nmask3 = mask | mask2\narr[mask] = 0\narr[mask3] = arr_temp[mask3] + 5\narr[~mask2] = 30\n",
        "\nfor i in range(arr.shape[0]):\n    mask = arr[i] < n1[i]\n    mask2 = arr[i] >= n2[i]\n    mask3 = mask ^ mask2\n    arr[i][mask] = 0\n    arr[i][mask3] = arr[i][mask3] + 5\n    arr[i][~mask2] = 30\n",
        "\nresult = np.count_nonzero(~np.isclose(s1, s2))\n",
        "\nresult = np.sum(np.isnan(s1) != np.isnan(s2))\n",
        "\nresult = all(np.array_equal(a[0], x) for x in a[1:])\n",
        "\nresult = all(np.isnan(arr).all() for arr in a)\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), constant_values=element)\n",
        "\n    result = np.zeros(shape)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n    ",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\na = a.reshape(int(a.shape[0]/3), 3)\n",
        "\nresult = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]].squeeze()\n",
        "\nresult = a[:,:,1] * b\n",
        "\nresult = a[np.arange(b.shape[0])[:, None], np.arange(b.shape[1]), b]\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b])\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b])\n",
        "\nresult = df.loc[(df['a'] > 1) & (df['a'] <= 4), 'b'].tolist()\nresult = [np.nan if x not in result else x for x in df['b']]\n",
        "\nresult = im[(im != 0).any(axis=1)][:, (im != 0).any(axis=0)]\n",
        "\nresult = A[np.ix_((A != 0).any(1), (A != 0).any(0))]\n",
        "\nresult = im[1:-1, 1:-1]\n",
        "\nresult = im[1:-1, 1:-1]\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n",
        "\nplt.minorticks_on()\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\nplt.minorticks_on()\n",
        "\nplt.minorticks_on()\nplt.tick_params(axis='x', which='both', bottom=True, top=True)\n",
        "\nline_styles = ['-', '--', '-.', ':']\nfor style in line_styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style)\nplt.show()\n",
        "\nline_styles = ['-', '--', '-.', ':']\nfor style in line_styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style)\nplt.show()\n",
        "\nplt.plot(x, y, marker='D', linestyle='-', linewidth=1)\n",
        "\nplt.plot(x, y, marker='D', linewidth=2)\n",
        "\nax.set_ylim(0, 40)\n",
        "\nplt.axvspan(2, 4, facecolor='red', alpha=0.5)\n",
        "\nx = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n",
        "\nx = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n",
        "\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.show()\n",
        "\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(data=df, x='x', y='y')\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', markersize=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20)\n",
        "\nl.set_markerfacecolor(l.get_markerfacecolor() + (0.2,))\n",
        "\nl.set_markeredgecolor('black')\n",
        "\nl.set_color('red')\nl.set_markerfacecolor('red')\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(np.arange(0, 2*np.pi+1, 2*np.pi/10))\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.gca().xaxis.set_label_coords(1, -0.1)\n",
        "\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
        "\nplt.title(\"\\n\".join(myTitle[i:i+20] for i in range(0, len(myTitle), 20)))\n",
        "\nplt.gca().invert_yaxis()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nplt.plot(x, color='blue')\nplt.plot(y, color='orange')\nplt.plot(z, color='green')\nplt.fill_between(range(len(x)), x, color='blue', alpha=0.3)\nplt.fill_between(range(len(y)), y, color='orange', alpha=0.3)\nplt.fill_between(range(len(z)), z, color='green', alpha=0.3)\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolors='black', facecolors='blue')\nplt.show()\n",
        "\nplt.xticks(np.arange(10))\nplt.yticks(np.arange(0, 2.1, 0.5))\n",
        "\nplt.ticklabel_format(style='plain', axis='y')\n",
        "\nax.axhline(1, linestyle='dashed', color='red')\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nax1.plot(x, y1)\nax1.set_ylabel('y1')\nax2.plot(x, y2)\nax2.set_ylabel('y2')\nplt.xlabel('x')\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nax1.plot(x, y1)\nax1.set_ylabel('y1')\nax2.plot(x, y2)\nax2.set_xlabel('x')\nax2.set_ylabel('y2')\nsns.despine(ax=ax1)\nsns.despine(ax=ax2)\nplt.show()\n",
        "\nplt.xlabel(\"\")\n",
        "\nplt.xticks([])\n",
        "\nplt.xticks([3, 4])\nplt.grid(axis='x')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y')\nplt.xticks([1, 2])\nplt.grid(axis='x')\n",
        "\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n# show grids\nplt.grid(True)\nplt.show()\n",
        "\nplt.legend(loc=\"lower right\")\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), tight_layout=True)\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\n",
        "\nx = np.arange(10)\ny = np.arange(10, 20)\nz = np.arange(10)\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\nplt.show()\n",
        "\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xticks(x)\nplt.margins(x=0)\nplt.subplots_adjust(bottom=0.2)\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.yaxis.tick_right()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.tick_params(axis='y', labelleft=True, labelright=False)\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='green')\nplt.show()\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='green')\nplt.show()\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\nplt.show()\n",
        "\nplt.bar(df[\"celltype\"], df[\"s1\"])\nplt.xticks(rotation=0)\nplt.xlabel(\"celltype\")\nplt.show()\n",
        "\nplt.bar(df[\"celltype\"], df[\"s1\"])\nplt.xticks(rotation=45)\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1\")\nplt.title(\"Bar Plot of s1\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.xticks(color=\"red\")\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.axhline(0, color='red')\n",
        "\nplt.plot(x, y)\nplt.xticks(fontsize=10, rotation='vertical')\n",
        "\nx_values = [0.22058956, 0.33088437, 2.20589566]\nfor x in x_values:\n    plt.axvline(x=x, color='r', linestyle='--')\n",
        "\nfig, ax = plt.subplots()\nim = ax.imshow(rand_mat)\n# Set the tick labels on top\nax.xaxis.tick_top()\n# Create colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\n# Set tick labels and invert y-axis labels\nax.set_xticks(numpy.arange(len(xlabels)))\nax.set_yticks(numpy.arange(len(ylabels)))\nax.set_xticklabels(xlabels)\nax.set_yticklabels(ylabels[::-1])\n# Rotate the tick labels and set their alignment\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n# Loop over data dimensions and create text annotations\nfor i in range(len(ylabels)):\n    for j in range(len(xlabels)):\n        text = ax.text(j, i, round(rand_mat[i, j], 2),\n                       ha=\"center\", va=\"center\", color=\"w\")\nplt.show()\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n# copy the code of the above plot and edit it to have legend for all three curves in the two subplots\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax2.legend(loc=1)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title(\"Y\")\naxs[1].plot(x, y)\naxs[1].set_title(\"Y\")\nplt.show()\n",
        "\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\nplt.show()\n",
        "\nplt.scatter(a, b)\nfor i, txt in enumerate(c):\n    plt.annotate(txt, (a[i], b[i]))\nplt.xlabel('a')\nplt.ylabel('b')\nplt.title('Scatter plot of a over b')\nplt.show()\n",
        "\nplt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.title(\"Legend\", fontweight=\"bold\")\nplt.show()\n",
        "\nplt.hist(x, edgecolor='black', linewidth=1.2)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nax1.plot(x, y)\nax1.set_title('Subplot 1')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax2.plot(x, y)\nax2.set_title('Subplot 2')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nplt.tight_layout()\nplt.show()\n",
        "\nplt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n",
        "\nplt.hist([x, y], bins=10, label=['x', 'y'])\nplt.legend()\nplt.show()\n",
        "\nx = [a, c]\ny = [b, d]\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2)\nim1 = ax1.imshow(x, cmap='viridis')\nax1.set_title('X')\nim2 = ax2.imshow(y, cmap='plasma')\nax2.set_title('Y')\nfig.colorbar(im1, ax=[ax1, ax2])\nplt.show()\n",
        "\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\nax1.plot(x, y)\nax1.set_title('Y')\nax2.plot(a, z)\nax2.set_title('Z')\nfig.suptitle('Y and Z')\nplt.show()\n",
        "\nx = [point[0] for point in points]\ny = [point[1] for point in points]\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Plot of y over x\", fontsize=20)\nplt.xlabel(\"x\", fontsize=18)\nplt.ylabel(\"y\", fontsize=16)\nplt.show()\n",
        "\nax.plot(x, y)\nax.set_xticks(x)\nax.set_xticklabels(x+1)\n",
        "\nfor line, color in zip(lines, c):\n    plt.plot([line[0][0], line[1][0]], [line[0][1], line[1][1]], color=color)\nplt.show()\n",
        "\nplt.loglog(x, y)\nplt.xticks([1, 10, 100], [1, 10, 100])\nplt.yticks([1, 10, 100], [1, 10, 100])\nplt.show()\n",
        "\nplt.plot(df.index, df['A'], marker='o', label='A')\nplt.plot(df.index, df['B'], marker='o', label='B')\nplt.plot(df.index, df['C'], marker='o', label='C')\nplt.plot(df.index, df['D'], marker='o', label='D')\nplt.legend()\nplt.show()\n",
        "\nplt.hist(data, bins=10, density=True)\nplt.gca().yaxis.set_major_formatter(plt.PercentFormatter(1))\nplt.yticks(np.arange(0, 1.1, 0.1))\n",
        "\nplt.plot(x, y, marker='o', alpha=0.5)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y, label='y')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax2.plot(z, a, label='a')\nax2.set_xlabel('z')\nax2.set_ylabel('a')\nfig.legend(loc='center')\nplt.show()\n",
        "\nfig, axes = plt.subplots(2, 1, figsize=(8, 10))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axes[0])\naxes[0].set_title(\"Regression plot of bill_depth_mm over bill_length_mm\")\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axes[1])\naxes[1].set_title(\"Regression plot of flipper_length_mm over bill_length_mm\")\nplt.tight_layout()\nplt.show()\n",
        "\nlabels = [str(i) if i != 2 else \"second\" for i in range(1, 10)]\nplt.xticks(range(1, 10), labels)\n",
        "\nplt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n",
        "\nextra_ticks = [2.1, 3, 7.6]\nexisting_ticks = plt.xticks()[0]\nnew_ticks = np.concatenate((existing_ticks, extra_ticks))\nplt.xticks(new_ticks)\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.xticks(rotation=-60, va='top')\n",
        "\nplt.xticks(alpha=0.5)\n",
        "\nplt.margins(x=0, y=0.1)\n",
        "\nplt.margins(x=0.02, y=0.0)\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\nfig.suptitle(\"Figure\")\naxs[0].plot(x, y)\naxs[0].set_title(\"Subplot 1\")\naxs[1].plot(x, y)\naxs[1].set_title(\"Subplot 2\")\nplt.show()\n",
        "\ndf.plot(kind='line')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n",
        "\nplt.scatter(x, y, marker='|', hatch='/', edgecolor='black', linewidth=1)\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolors='none', marker='o', hatch='|')\n",
        "\nplt.scatter(x, y, marker='*', hatch='*')\nplt.show()\n",
        "\nplt.scatter(x, y, s=100, marker='*', hatch='|')\n",
        "\n",
        "\nplt.stem(x, y, orientation='horizontal')\nplt.show()\n",
        "\nplt.bar(d.keys(), d.values(), color=[c[key] for key in d.keys()])\nplt.show()\n",
        "\nplt.axvline(x=3, color='r', linestyle='-', label='cutoff')\nplt.legend()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, polar=True)\nax.bar(labels, height)\n",
        "\nplt.pie(data, labels=l, wedgeprops=dict(width=0.4))\nplt.gca().set_aspect(\"equal\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(True, linestyle='--', color='blue')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.minorticks_on()\nax.grid(which='minor', linestyle='dashed', color='gray')\nax.grid(which='major', visible=False)\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title(\"Activities\")\nplt.legend(labels, loc=\"best\")\nplt.axis('equal')\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.title(\"Activities\")\nplt.legend(labels, loc=\"best\")\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', linestyle='-', color='blue', markerfacecolor='none', markeredgecolor='blue', alpha=0.5)\n",
        "\nplt.axvline(x=55, color=\"green\")\n",
        "\nbar_width = 0.35\nindex = np.arange(len(blue_bar))\nplt.bar(index, blue_bar, bar_width, label='Blue', color='blue')\nplt.bar(index + bar_width, orange_bar, bar_width, label='Orange', color='orange')\nplt.xlabel('Group')\nplt.ylabel('Height')\nplt.title('Comparison of Blue and Orange Bars')\nplt.xticks(index + bar_width/2, ('A', 'B', 'C'))\nplt.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\nax1.plot(x, y, label='y over x')\nax2.plot(a, z, label='z over a')\nax1.legend(loc='upper right')\n",
        "\nplt.scatter(x, y, c=y, cmap='Spectral')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(0, 10, 1))\nplt.show()\n",
        "\ng = sns.factorplot(x=\"sex\", y=\"bill_length_mm\", col=\"species\", data=df, kind=\"bar\", sharey=False)\nplt.show()\n",
        "\ncircle = plt.Circle((0.5, 0.5), 0.2, color='blue')\nfig, ax = plt.subplots()\nax.add_artist(circle)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\mathbf{\\phi}$', fontsize=16)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(markerscale=0.1)\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\nplt.show()\n",
        "\nplt.legend(ncol=2)\n",
        "\nplt.legend()\nplt.plot(x[3], y[3], marker=\"o\", markersize=10, color=\"red\")\nplt.plot(x[7], y[7], marker=\"o\", markersize=10, color=\"blue\")\n",
        "\nplt.imshow(data, cmap='hot')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r\"$\\bf{Figure}$ 1\")\n",
        "\nsns.pairplot(df, x_vars='x', y_vars='y', hue='id')\nplt.legend().remove()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n",
        "\nplt.scatter(x, y)\nplt.axis('auto')\n",
        "\nplt.scatter(x, y, color='red', edgecolor='black')\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax in axs.flatten():\n    ax.plot(x, y)\n",
        "\nplt.hist(x, bins=5, range=(0, 10), rwidth=2)\nplt.show()\n",
        "\nplt.plot(x, y, 'b-', label='Data')\nplt.fill_between(x, y-error, y+error, color='gray', alpha=0.5, label='Error')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n",
        "\nplt.axhline(0, color='white')\nplt.axvline(0, color='white')\n",
        "\nax.errorbar(box_position, box_height, yerr=box_errors, fmt=\"none\", ecolor=c)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title(\"Y\")\nax2.plot(a, z)\nax2.set_title(\"Z\")\nax2.title.set_position([0.5, 1.05])\nplt.show()\n",
        "\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xticks(np.arange(0, 10, 2))\n        axs[i, j].set_yticks(np.arange(0, 10, 2))\nplt.show()\n",
        "\nplt.figure(figsize=(8, 8))\nplt.matshow(d)\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(8, 6))\nax.axis('off')\ntable = ax.table(cellText=df.values, colLabels=df.columns, loc='center')\ntable.set_bbox([0, 0, 1, 1])\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.xaxis.set_ticks_position('both')\nax.yaxis.set_ticks_position('both')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.xaxis.set_ticks_position('both')\nax.yaxis.set_ticks_position('left')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(x)\nplt.tick_params(axis='x', which='both', bottom=False, top=False)\nplt.show()\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"swarm\")\ng.set_titles(\"Group: {col_name}\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"strip\")\ng.set_axis_labels(\"Exercise Time\", \"Pulse\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"swarm\")\ng.set_axis_labels(\"\", \"\")\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\n",
        "\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, legend=False)\nplt.show()\n",
        "\ng = sns.FacetGrid(df, row=\"b\", height=3, aspect=3)\ng.map(sns.pointplot, \"a\", \"c\")\ng.set(xticks=np.arange(0, 31, 2))\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\n",
        "\ngs = gridspec.GridSpec(nrow, ncol, wspace=0.0, hspace=0.0)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = plt.subplot(gs[i, j])\n        ax.imshow(x, cmap='gray')\n        ax.axis('off')\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\ndef my_map_func(i):\n  return [i, i+1, i+2]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\nelement = ds.make_one_shot_iterator().get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n",
        "\n    ds = tf.data.Dataset.from_tensor_slices(input)\n    ds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n    result = []\n    iterator = ds.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    with tf.compat.v1.Session() as sess:\n        while True:\n            try:\n                result.append(sess.run(next_element))\n            except tf.errors.OutOfRangeError:\n                break\n    ",
        "\nmax_length = tf.reduce_max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.int32)\npadding_length = tf.shape(mask)[1] - max_length\npadding = tf.zeros((tf.shape(mask)[0], padding_length), dtype=tf.int32)\nresult = tf.concat([mask, padding], axis=1)\n",
        "\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length)\npadded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_length]])\nresult = tf.cast(padded_mask, dtype=tf.int32)\n",
        "\nmax_length = tf.reduce_max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\nresult = tf.pad(mask, paddings=[[0, 0], [8 - max_length, 0]])\n",
        "\n    max_length = max(lengths)\n    mask = tf.sequence_mask(lengths, max_length)\n    padded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_length]])\n    result = tf.cast(padded_mask, dtype=tf.int32)\n    ",
        "\nmax_length = tf.reduce_max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\npadded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_length]])\nresult = tf.cast(padded_mask, dtype=tf.int32)\n",
        "\nresult = tf.transpose(tf.reshape(tf.tile(a, [tf.size(b)]), [tf.size(b), tf.size(a)])) + tf.reshape(tf.tile(b, [tf.size(a)]), [tf.size(a), tf.size(b)])\n",
        "\n    result = tf.stack(tf.meshgrid(a, b), axis=-1)\n    result = tf.reshape(result, [-1, 2])\n    ",
        "\nresult = tf.squeeze(a, axis=2)\n",
        "\nresult = tf.expand_dims(a, axis=2)\n",
        "\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n",
        "\n    result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n    ",
        "\nindices = tf.stack([y, z], axis=1)\nresult = tf.gather_nd(x, indices)\n",
        "\nm = tf.gather_nd(x, tf.stack((row, col), axis=1))\n",
        "\n    m = tf.gather_nd(x, tf.stack((y, z), axis=1))\n    ",
        "\nresult = tf.einsum('bik,bjk->bij', A, B)\n",
        "\nresult = tf.matmul(A, tf.transpose(B, perm=[0, 2, 1]))\n",
        "\nresult = [s.decode('utf-8') for s in x]\n",
        "\n    result = tf.strings.decode(x, 'utf-8')\n    ",
        "\nmask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\nsum_values = tf.reduce_sum(x, axis=-2)\ncount_nonzero = tf.reduce_sum(mask, axis=-2)\nresult = tf.divide(sum_values, count_nonzero)\n",
        "\nmask = tf.cast(tf.not_equal(x, 0), tf.float32)\nsum_values = tf.reduce_sum(x, axis=-2)\ncount_values = tf.reduce_sum(mask, axis=-2)\nresult = sum_values / count_values\n",
        "\n    mask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\n    sum_values = tf.reduce_sum(x, axis=-2)\n    count_nonzero = tf.reduce_sum(mask, axis=-2)\n    result = tf.divide(sum_values, count_nonzero)\n    ",
        "import tensorflow as tf\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\nprint(result)",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    result = tf.argmax(a, axis=1)\n    ",
        "\nresult = tf.argmin(a, axis=0)\n",
        "\ntf.saved_model.save(model, \"export/1\")\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=6, dtype=tf.int32)\n",
        "\nimport tensorflow as tf\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n    return result\n",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0=p0)\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\nresult = stats.ks_2samp(x, y)\nresult = result.pvalue > alpha\n",
        "\ndef f(x):\n    a, b, c = x\n    return ((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4\nresult = optimize.minimize(f, initial_guess)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores, loc=mu, scale=sigma)\n",
        "\nz_scores = scipy.stats.norm.ppf(p_values)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nresult = sa.multiply(sb)\n",
        "\n    result = sA.multiply(sB)\n    ",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ndef get_rotated_coordinates(x, y, angle):\n    # Calculate the center of the image\n    center_x = data_orig.shape[1] / 2\n    center_y = data_orig.shape[0] / 2\n    # Convert the angle to radians\n    angle_rad = np.radians(angle)\n    # Calculate the new coordinates\n    x_rot = np.cos(angle_rad) * (x - center_x) - np.sin(angle_rad) * (y - center_y) + center_x\n    y_rot = np.sin(angle_rad) * (x - center_x) + np.cos(angle_rad) * (y - center_y) + center_y\n    return x_rot, y_rot\nxrot, yrot = get_rotated_coordinates(x0, y0, angle)\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, 'uniform')\n",
        "\n    result = stats.kstest(times, 'uniform')\n    ",
        "\nresult = stats.kstest(times, 'uniform')\nresult = result.pvalue >= 0.05\n",
        "\nFeature = vstack([c1, c2])\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2)  # calculate the distance matrix\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)  # solve the assignment problem\nresult = col_ind.tolist()  # convert the column indices to a list\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)\nresult = col_ind.tolist()\n",
        "\nb = b.tolil()\nb.setdiag(0)\nb = b.tocsr()\n",
        "\nlabeled_array, num_features = ndimage.label(img > threshold)\nresult = num_features\n",
        "\nlabeled_array, num_features = ndimage.label(img < threshold)\nresult = num_features\n",
        "\n    labeled_array, num_features = ndimage.label(img > threshold)\n    result = num_features\n    ",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n# Find regions of cells which value exceeds the threshold\nlabeled_array, num_features = ndimage.label(img > threshold)\n# Calculate the center of mass for each region\ncom = ndimage.center_of_mass(img, labeled_array, range(1, num_features+1))\n# Calculate the distance between the center of mass and the top left corner\ndistances = [np.sqrt(x**2 + y**2) for x, y in com]\nresult = distances\nprint(result)\n",
        "\nM = M + M.T\n",
        "\n    sA = sA + sA.T - np.diag(sA.diagonal())\n    ",
        "\n# Use scipy.ndimage.label to label connected components in the array\nlabeled_array, num_features = scipy.ndimage.label(square)\n# Iterate through each labeled component\nfor label in range(1, num_features + 1):\n    # Find the coordinates of the labeled component\n    component_coords = np.argwhere(labeled_array == label)\n    \n    # Check if the component has a width of 1\n    if component_coords.shape[0] == 1:\n        # Set the value of the component to 0\n        square[component_coords[0][0], component_coords[0][1]] = 0\n",
        "\nimport numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = np.random.randint(1, 255, size=(12, 12))\nnp.random.seed(12)\nx, y = (32 * np.random.random((2, 20))).astype(int)\nsquare[x, y] = np.random.randint(1, 255, size=(20,))\n# Begin of Missing Code\n# Find connected components in the square array\nlabels, num_labels = scipy.ndimage.label(square)\n# Iterate over each label and check if it is completely surrounded by zeros\nfor label in range(1, num_labels + 1):\n    # Get the coordinates of the label\n    label_coords = np.argwhere(labels == label)\n    \n    # Check if all neighboring cells are zeros\n    is_surrounded = all(square[coord[0], coord[1]] == 0 for coord in label_coords)\n    \n    # If the label is completely surrounded by zeros, set all its cells to zero\n    if is_surrounded:\n        square[label_coords[:, 0], label_coords[:, 1]] = 0\n# End of Missing Code\nprint(square)\n",
        "\nmean = col.mean()\nstandard_deviation = col.std()\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nMedian = np.median(col.data)\nMode = np.argmax(np.bincount(col.data))\n",
        "\ndef fourier(x, *a):\n    result = 0\n    for i in range(1, degree+1):\n        result += a[i-1] * np.cos(i * np.pi / tau * x)\n    return result\npopt, pcov = curve_fit(fourier, z, Ua)\n",
        "\nresult = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(example_array, metric='euclidean'))\n",
        "\nresult = scipy.spatial.distance.cdist(np.argwhere(example_array > 0), np.argwhere(example_array > 0), metric='cityblock')\n",
        "\n    # Calculate the coordinates of each patch\n    patches = np.unique(example_array)\n    patch_coords = []\n    for patch in patches:\n        patch_coords.append(np.argwhere(example_array == patch))\n    \n    # Calculate pairwise Euclidean distances between patches\n    distances = []\n    for i in range(len(patch_coords)):\n        for j in range(i+1, len(patch_coords)):\n            dist = scipy.spatial.distance.cdist(patch_coords[i], patch_coords[j], metric='euclidean')\n            distances.append([patches[i], patches[j], np.min(dist)])\n    \n    result = np.array(distances)\n    ",
        "\nresult = []\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k=2, s=4)\n    y_ext = interpolate.splev(x_val, tck, der=0)\n    result.append(y_ext)\nresult = np.array(result)\n",
        "\nresult = ss.anderson_ksamp([x1, x2, x3, x4])\nstatistic = result.statistic\ncritical_values = result.critical_values\nsignificance_level = result.significance_level\n",
        "\nimport numpy as np\nimport scipy.stats as ss\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\n# Concatenate the two arrays\ndata = np.concatenate([x1, x2])\n# Perform the Anderson-Darling test\nresult = ss.anderson_ksamp(data)\n# Interpret the result\ncritical_value = result.significance_level[2]  # 5% significance level\nif result.statistic > critical_value:\n    result = False\nelse:\n    result = True\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndef tau1(x):\n    y = np.array(A['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\ndf['AB'] = df['B'].rolling(3).apply(tau1)\ndf['AC'] = df['C'].rolling(3).apply(tau1)\ndf['BC'] = df['C'].rolling(3).apply(lambda x: tau1(df['B'].rolling(3).apply(tau1)))\nprint(df)\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = block_diag(*a)\n",
        "\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    ",
        "\nn = len(a)\nmean = np.mean(a)\nvariance = np.var(a, ddof=0)\nkurtosis_result = np.sum((a - mean)**4) / (n * variance**2)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\n",
        "\ninterp_func = scipy.interpolate.interp2d(x, y, z, kind='cubic')\nresult = interp_func(s, t)\n",
        "\n    interp_func = scipy.interpolate.interp2d(x[:,0], y[0,:], z, kind='cubic')\n    result = interp_func(s, t).flatten()\n    ",
        "\nresult = np.zeros(len(extraPoints), dtype=int)\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    result[i] = region_index\n",
        "\nresult = np.zeros(len(vor.regions)-1, dtype=int)\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    result[region_index] += 1\n",
        "\n# Find the maximum size of the vectors\nmax_size = max(len(vector) for vector in vectors)\n# Create a list of vectors with padded zeros\npadded_vectors = [np.pad(vector, (0, max_size - len(vector)), mode='constant') for vector in vectors]\n# Convert the list of vectors to a sparse matrix\nresult = sparse.csr_matrix(padded_vectors)\n",
        "\nb = scipy.ndimage.median_filter(a, 3, origin=1)\n",
        "\nresult = M[row, column]\n",
        "\nresult = M[row, column].tolist()\n",
        "\nnew_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[:, i, j])\n        new_array[:, i, j] = f(x_new)\n",
        "\ndev = abs((x-u)/o2)\nP_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\nP_outer = 1 - P_inner\nP = P_inner + P_outer/2\nprob = P\n",
        "\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    prob = P\n    ",
        "\ndef dctmtx(N):\n    result = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            result[i, j] = np.cos((2 * i + 1) * j * np.pi / (2 * N)) * np.sqrt(2 / N) if i != 0 else np.cos((2 * i + 1) * j * np.pi / (2 * N)) / np.sqrt(N)\n    return result\n",
        "\nresult = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\n",
        "\nresult = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        result[i,j] = scipy.stats.binom.pmf(j, i, p)\n",
        "\nresult = df.apply(stats.zscore, axis=1)\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\nzscore_df = df.apply(stats.zscore, axis=1, result_type='broadcast')\nresult = pd.concat([df, zscore_df.rename(index=lambda x: x + ' zscore')], axis=0)\n",
        "\nzscore_df = pd.DataFrame(stats.zscore(df), index=df.index, columns=df.columns)\nresult = pd.concat([df, zscore_df], keys=['data', 'zscore'])\nresult = result.round(3)\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\nresult = get_distance_2(np.arange(shape[0]), np.arange(shape[1]))\n",
        "\nmid = np.array([shape[0]//2, shape[1]//2])\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), np.array([mid]))\n",
        "\n    rows, cols = shape\n    y, x = np.indices(shape)\n    mid = np.array([(rows-1)/2, (cols-1)/2])\n    result = distance.cdist(np.dstack((y, x)), mid)\n    ",
        "\nzoom_factor = min(shape[0] / x.shape[0], shape[1] / x.shape[1])\nresult = scipy.ndimage.zoom(x, zoom_factor, order=1)\n",
        "\ndef objective(x):\n    return np.dot(a, x**2)\nout = scipy.optimize.minimize(objective, x0)\n",
        "\ndef objective(x):\n    return np.dot(a, x**2)\nout = scipy.optimize.minimize(objective, x0, bounds=[(lb, None) for lb in x_lower_bounds], method='L-BFGS-B')\n",
        "\nimport scipy.integrate\nimport numpy as np\ndef dN1_dt(t, N1):\n    return -100 * N1 + np.sin(t)\nN0 = 10\ntime_span = [-0.1, 0.1]\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\nimport scipy.integrate\nimport numpy as np\ndef dN1_dt(t, N1):\n    return -100 * N1 - np.cos(t)\nN0 = 10\ntime_span = [-0.1, 0.1]\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nfor t in range(4):\n    def const(x, t=t):\n        return x[t]\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sparse.vstack([sa, sb])\n",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\nimport scipy.integrate\nfrom numpy import array\nfn = 'cooltemp.dat'\nc = loadtxt(fn, unpack=True, usecols=[1])\nI = []\nfor n in range(len(c)):\n    # equation\n    eqn = lambda x: 2 * c[n] * x\n    # integrate \n    result, error = scipy.integrate.quad(eqn, 0, 1)\n    I.append(result)\nI = array(I)\nprint(I)\n",
        "\n    def eqn(x):\n        return 2*c*x\n    \n    result, error = scipy.integrate.quad(eqn, low, high)\n    ",
        "\nV = V + sparse.dok_matrix((V.shape[0], V.shape[1]), dtype=np.float64)\nV[V != 0] += x\n",
        "\nV.data += x\n",
        "\nV.data += x\nV.data += y\n",
        "\n#csc sparse matrix\nsa = sa.tocsc()\n#iterate through columns\nfor Col in range(sa.shape[1]):\n   Column = sa[:,Col].data\n   List = [x**2 for x in Column]\n   #get the column length\n   Len = math.sqrt(sum(List))\n   #normalize the column\n   sa[:,Col] = (1/Len) * Column\n",
        "\n# csr sparse matrix\nsa = sa.tocsr()\n# iterate through columns\nfor Col in range(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    # get the column length\n    Len = math.sqrt(sum(List))\n    # normalize the column\n    sa[:,Col] = (1/Len) * Column\n",
        "\na[a > 0] = 1\n",
        "\na[a > 0] = 1\n",
        "\nresult = []\nfor i in range(len(centroids)):\n    cluster = np.where(np.argmin(scipy.spatial.distance.cdist(data, [centroids[i]])) == 0)[0]\n    closest_element = cluster[np.argmin(scipy.spatial.distance.cdist(data[cluster], [centroids[i]]))]\n    result.append(closest_element)\n",
        "\nresult = []\nfor i in range(len(centroids)):\n    cluster_points = data[np.where(labels == i)]\n    centroid = centroids[i]\n    closest_point = cluster_points[np.argmin(scipy.spatial.distance.cdist(cluster_points, [centroid]))]\n    result.append(closest_point)\n",
        "\nresult = []\nfor centroid in centroids:\n    distances = scipy.spatial.distance.cdist(data, [centroid])\n    closest_indices = np.argsort(distances.flatten())[:k]\n    result.append(closest_indices[k-1])\n",
        "\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(a, x, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\nresult = []\nfor x, b in zip(xdata, bdata):\n    root = fsolve(eqn, x0=0.5, args=(x, b))\n    result.append(root)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(b, x, a):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\nresult = []\nfor x, a in zip(xdata, adata):\n    root = fsolve(eqn, x0=0.5, args=(x, a))\n    result.append(root)\nprint(result)\n",
        "\nresult = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d))\n",
        "\nresult = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d))\nresult = result.pvalue >= 0.05\n",
        "\nintegral_df = df.groupby(pd.Grouper(freq='5S')).apply(lambda x: integrate.trapz(x['A'], x['Time'].astype(np.datetime64)))\n",
        "\nx = np.array(x)\ny = np.array(y)\neval = np.array(eval)\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\n",
        "\nimport scipy.optimize as sciopt\nimport numpy as np\nimport pandas as pd\na = pd.DataFrame({'A1': [0, 1, 2, 3, 2, 1, 6, 0, 1, 1, 7, 10]})\n# Define the negative log-likelihood function\ndef neg_log_likelihood(params):\n    probabilities = params / np.sum(params)  # Normalize the parameters to probabilities\n    log_probs = np.log(probabilities)\n    frequencies = np.bincount(a['A1'])\n    return -np.sum(frequencies * log_probs)\n# Initial guess for the parameters\ninitial_params = np.ones(12)\n# Maximize the negative log-likelihood function\nresult = sciopt.minimize(neg_log_likelihood, initial_params, method='Nelder-Mead')\n# Extract the best parameters\nbest_params = result.x\n# Normalize the parameters to probabilities\nweights = best_params / np.sum(best_params)\nprint(weights)\n",
        "\nresult = sciopt.minimize(e, x0=[0, 0], args=(x, y), bounds=[(pmin[0], pmax[0]), (pmin[1], pmax[1])])\n",
        "\nresult = signal.argrelextrema(arr, np.less_equal, order=n)[0]\n",
        "\nresult = []\nfor i in range(len(arr)):\n    for j in range(n, len(arr[i])-n):\n        if arr[i][j] <= np.max(arr[i][j-n:j+n+1]) and arr[i][j] <= np.max(arr[i][j-n:j+n+1]):\n            result.append([i, j])\n",
        "\nnumeric_columns = df.select_dtypes(include=[np.number]).columns\ndf = df[(np.abs(stats.zscore(df[numeric_columns])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n    result = pd.DataFrame(data.data, columns=data.feature_names)\n    ",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.DataFrame(mlb.fit_transform(df['Col3']), columns=mlb.classes_)\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.DataFrame(mlb.fit_transform(df['Col3']), columns=mlb.classes_)\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\ndf = load_data()\n# Extract the last column as a separate Series\ncol = df.iloc[:, -1]\n# Use MultiLabelBinarizer to one-hot encode the column\nmlb = MultiLabelBinarizer()\nencoded_col = pd.DataFrame(mlb.fit_transform(col), columns=mlb.classes_)\n# Concatenate the original dataframe with the encoded column\ndf_out = pd.concat([df.iloc[:, :-1], encoded_col], axis=1)\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\ndf = load_data()\n# Get the last column of the dataframe\nlast_column = df.iloc[:, -1]\n# Create an instance of MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\n# One-hot encode the last column\none_hot_encoded = mlb.fit_transform(last_column)\n# Create a new dataframe with the one-hot encoded columns\ndf_out = pd.concat([df.iloc[:, :-1], pd.DataFrame(one_hot_encoded, columns=mlb.classes_)], axis=1)\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\ndf = load_data()\n# Get the last column of the dataframe\nlast_column = df.iloc[:, -1]\n# Create an instance of MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\n# One-hot encode the last column\none_hot_encoded = mlb.fit_transform(last_column)\n# Create a new dataframe with the one-hot encoded columns\ndf_out = pd.concat([df.iloc[:, :-1], pd.DataFrame(one_hot_encoded, columns=mlb.classes_)], axis=1)\nprint(df_out)\n",
        "\ncalibrated_svc = CalibratedClassifierCV(svmmodel, cv=5)\ncalibrated_svc.fit(X, y)\nproba = calibrated_svc.predict_proba(x_test)\n",
        "\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n",
        "\ntransform_output = pd.DataFrame(transform_output.toarray())\ndf = pd.concat([df_origin, transform_output], axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ndef solve(df, transform_output):\n    # Convert the csr_matrix to a pandas DataFrame\n    transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n    \n    # Concatenate the original DataFrame and the transformed DataFrame\n    result = pd.concat([df, transform_output], axis=1)\n    \n    return result\ndf = solve(df_origin, transform_output)\nprint(df)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Delete the step at index 1\nclf.steps.pop(1)\n# Insert a new step at index 1\nclf.steps.insert(1, ('new_step', SomeTransformer()))\nprint(len(clf.steps))\n",
        "\nsteps = clf.steps\n# Delete the step at index 1\nsteps.pop(1)\n# Insert a new step at index 1\nsteps.insert(1, ('new_step', SVC()))\n",
        "\nclf.named_steps.pop('pOly')\n",
        "\n# Insert a step\nclf.steps.insert(1, ('new_step', SomeTransformer()))\n# Delete a step\nclf.steps.pop(2)\n",
        "\n# Inserting a step\nclf.steps.insert(1, ('new_step', SVC()))\n# Deleting a step\ndel clf.steps[2]\n",
        "\nsteps = clf.named_steps\nsteps.insert(2, ('t1919810', PCA()))\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\nmodel = xgb.XGBRegressor()\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\ngridsearch.fit(trainX, trainY, **fit_params)\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [(testX, testY)]}\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test))\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test)[:, 1])\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nclf = GridSearchCV(bc, param_grid)\nclf.fit(X_train, y_train)\n",
        "\n# Reshape y to have shape (n_samples, )\ny = y.reshape(-1)\n",
        "\n# Reshape y to match the number of samples in X\ny = y.reshape(-1, 1)\n",
        "\ndef preprocess(s):\n    return s.upper()\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\ndef prePro(text):\n    return text.lower()\n",
        "\nscaler = preprocessing.StandardScaler()\ndf_out = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
        "\nscaler = preprocessing.StandardScaler()\ndf_out = pd.DataFrame(scaler.fit_transform(data), index=data.index, columns=data.columns)\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\nselected_features = model.get_support()\ncolumn_names = X.columns[selected_features]\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = df.columns[model.get_support()]\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n# Get the selected feature indices\nselected_feature_indices = model.get_support(indices=True)\n# Get the column names of the selected features\ncolumn_names = X.columns[selected_feature_indices]\nprint(column_names)\n",
        "import pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = df.columns[model.get_support()].tolist()  # Get the selected column names\nprint(column_names)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\n# Get the cluster centers\ncenters = km.cluster_centers_\n# Get the distances between each sample and the p^th center\ndistances = np.linalg.norm(X - centers[p], axis=1)\n# Get the indices of the 50 samples closest to the p^th center\nclosest_indices = np.argsort(distances)[:50]\n# Get the actual samples\nclosest_50_samples = X[closest_indices]\nprint(closest_50_samples)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\n# Get the cluster centers\ncenters = km.cluster_centers_\n# Get the distances between each sample and the p^th center\ndistances = np.linalg.norm(X - centers[p], axis=1)\n# Sort the distances and get the indices of the 50 closest samples\nclosest_indices = np.argsort(distances)[:50]\n# Get the 50 closest samples\nclosest_50_samples = X[closest_indices]\nprint(closest_50_samples)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\n# Get the cluster centers\ncenters = km.cluster_centers_\n# Get the distances between each sample and the p^th center\ndistances = np.linalg.norm(X - centers[p], axis=1)\n# Sort the distances and get the indices of the 100 closest samples\nclosest_indices = np.argsort(distances)[:100]\n# Get the actual samples corresponding to the closest indices\nclosest_100_samples = X[closest_indices]\nprint(closest_100_samples)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\ndef get_samples(p, X, km):\n    # Fit the KMeans algorithm on the data X\n    km.fit(X)\n    \n    # Get the cluster centers\n    cluster_centers = km.cluster_centers_\n    \n    # Find the Euclidean distance between each sample in X and the cluster center \"p\"\n    distances = np.linalg.norm(X - cluster_centers[p], axis=1)\n    \n    # Sort the samples based on their distances in ascending order\n    sorted_indices = np.argsort(distances)\n    \n    # Take the first 50 samples from the sorted list as the closest samples\n    closest_samples = X[sorted_indices[:50]]\n    \n    return closest_samples\nclosest_50_samples = get_samples(p, X, km)\nprint(closest_50_samples)\n",
        "\nX_train_encoded = pd.get_dummies(X_train)\n",
        "\nX_train_encoded = pd.get_dummies(X_train[0])\nX_train = pd.concat([X_train.drop(columns=[0]), X_train_encoded], axis=1)\n",
        "\nmodel = svm.SVR(kernel='rbf')\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\nregressor = SVR(kernel='rbf')\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nsvm = SVR()\nsvm.fit(X_poly, y)\npredict = svm.predict(X_poly)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T).toarray()\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    cosine_similarities_of_queries = cosine_similarity(query_matrix, tfidf_matrix)\n    return cosine_similarities_of_queries\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n",
        "\n# Create an empty DataFrame to store the one-hot encoded features\nnew_features = pd.DataFrame()\n# Iterate over each sample in the features list\nfor i, sample in enumerate(features):\n    # Create a dictionary to store the one-hot encoded values for the current sample\n    sample_dict = {}\n    \n    # Iterate over each feature in the current sample\n    for feature in sample:\n        # Set the value of the current feature to 1 in the sample dictionary\n        sample_dict[feature] = 1\n    \n    # Append the sample dictionary as a new row to the new_features DataFrame\n    new_features = new_features.append(sample_dict, ignore_index=True)\n# Fill missing values with 0\nnew_features = new_features.fillna(0)\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nnew_f = pd.DataFrame(mlb.fit_transform(f), columns=mlb.classes_)\n",
        "\n# Create an empty DataFrame to store the one-hot encoded features\nnew_features = pd.DataFrame()\n# Iterate over each sample in the features list\nfor sample in features:\n    # Create a dictionary to store the one-hot encoded values for each feature\n    one_hot_dict = {}\n    \n    # Iterate over each feature in the sample\n    for feature in sample:\n        # Set the value of the feature to 1 in the dictionary\n        one_hot_dict[feature] = 1\n    \n    # Append the one-hot encoded values for the sample to the new_features DataFrame\n    new_features = new_features.append(one_hot_dict, ignore_index=True, sort=False)\n# Fill missing values with 0\nnew_features = new_features.fillna(0)\n",
        "\n    # Create an empty DataFrame to store the one-hot encoded features\n    new_features = pd.DataFrame()\n    # Iterate over each sample in the features list\n    for i, sample in enumerate(features):\n        # Create a dictionary to store the one-hot encoded values for the current sample\n        sample_dict = {}\n        # Iterate over each feature in the current sample\n        for feature in sample:\n            # Set the value of the current feature to 1 in the sample_dict\n            sample_dict[feature] = 1\n        # Append the sample_dict as a new row to the new_features DataFrame\n        new_features = new_features.append(sample_dict, ignore_index=True)\n    # Fill any missing values with 0\n    new_features = new_features.fillna(0)\n    ",
        "\n# Create a MultiLabelBinarizer object\nmlb = sklearn.preprocessing.MultiLabelBinarizer()\n# Fit and transform the features using the MultiLabelBinarizer\nnew_features = mlb.fit_transform(features)\n# Create a DataFrame from the transformed features\nnew_features = pd.DataFrame(new_features, columns=mlb.classes_)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(simM)\n",
        "\ndistance_matrix = np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\ndistance_matrix = 1 - np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\ndistM = 1 - np.array(simM)\nlinkage_matrix = hierarchy.linkage(distM, method='complete')\ncluster_labels = hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\ntransformer = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = transformer.fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b|[\\!\\?\\\"\\']')\ntransformed_text = vectorizer.fit_transform([text])\n",
        "\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
        "\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    ",
        "\nf1 = df['mse'].values\nX = f1.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\nprint(labels)\n",
        "\n# Create an instance of LinearSVC with penalty='l1'\nfeatureSelector = LinearSVC(penalty='l1', dual=False)\n# Fit the featureSelector on the training data\nfeatureSelector.fit(X, y)\n# Get the indices of the selected features\nselected_feature_indices = featureSelector.coef_.nonzero()[1]\n# Get the names of the selected features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nlsvc = LinearSVC(penalty='l1', dual=False)\nlsvc.fit(X, y)\nselected_feature_indices = np.where(lsvc.coef_[0] != 0)[0]\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\n    featureSelector = LinearSVC(penalty='l1', dual=False)\n    featureSelector.fit(X, y)\n    selected_feature_indices = featureSelector.coef_.nonzero()[1]\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = np.array([]) # blank array to store slopes\nfor col in df1.columns:\n    if col != 'Time': # exclude the 'Time' column\n        df2 = df1[~np.isnan(df1[col])] # remove NaN values for each column\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        slopes = np.append(slopes, m)\nprint(slopes)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    if col != 'Time':\n        df2 = df1[~np.isnan(df1[col])]\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        slopes.append(m)\nprint(slopes)\n",
        "\nlabel_encoder = LabelEncoder()\ndf['Sex'] = label_encoder.fit_transform(df['Sex'])\ntransformed_df = df['Sex']\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ntransformed_df = df['Sex']\n",
        "\n    label_encoder = LabelEncoder()\n    transformed_df = df.copy()\n    transformed_df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    ",
        "\nElasticNet = linear_model.ElasticNet() # create an ElasticNet instance\nElasticNet.fit(X_train, y_train) # fit data\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.flatten().reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.flatten().reshape(-1, 1))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nnp_array = load_data()\ndef Transform(a):\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a.reshape(-1, 1)).reshape(a.shape)\n    return new_a\ntransformed = Transform(np_array)\nprint(transformed)\n",
        "# [Missing Code]\npredict = clf.predict(b)\n",
        "\n# Convert the string data in X to numerical values\nnew_X = np.array(X)\nfor i in range(len(new_X)):\n    for j in range(len(new_X[i])):\n        new_X[i][j] = float(new_X[i][j])\n",
        "\nnew_X = np.array(X)\nnew_X[:, 1] = new_X[:, 1].astype(float)\n",
        "\n# Convert the string data in X to numerical values\nnew_X = np.array([[ord(x[0]), int(x[1])] for x in X])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\n# Git rid of the name of the animal\n# And change the hunter/scavenger to 0/1\ndataframe = dataframe.drop([\"Name\"], axis=1)\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\npredict = logReg.predict(X)\nprint(predict)\n",
        "\nX = dataframe.iloc[:, 1:-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n",
        "\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\ndef solve(features_dataframe):\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort_values(by=\"date\")\n    test_dataframe = test_dataframe.sort_values(by=\"date\")\n    return train_dataframe, test_dataframe\ntrain_dataframe, test_dataframe = solve(features_dataframe)\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\ncols = ['X2', 'X3']\ndf[cols + '_scale'] = df.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncols = myData.columns[2:4]\nmyData[['new_A2', 'new_A3']] = myData.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n",
        "\nresults = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = results[['params', 'mean_test_score', 'std_test_score']]\n",
        "\nresults = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = results.sort_values(by='mean_fit_time')\n",
        "\njoblib.dump(fitted_model, \"sklearn_model\")\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndf = load_data()\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\nprint(cosine_similarity_matrix)\n"
    ],
    "Pytorch": [
        "\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n# Load your input data\ndef load_data():\n    # Your implementation to load the data\n    pass\ninput_Tensor = load_data()\n# Train the word2vec model\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n# Convert word2vec embedding weights to a NumPy array\nembedding_weights = word2vec.wv.vectors\n# Create a PyTorch embedding layer with the same dimensions as the word2vec embedding weights\nembedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_weights))\n# Embed your input data using the embedding layer\nembedded_input = embedding_layer(torch.LongTensor(input_Tensor))\nprint(embedded_input)\n",
        "\n    # Convert word2vec embedding weights to a NumPy array\n    embedding_weights = np.array(word2vec.wv.vectors)\n    # Create a PyTorch embedding layer with the same dimensions as the word2vec embedding weights\n    embedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_weights))\n    # Embed the input data using the embedding layer\n    embedded_input = embedding_layer(input_Tensor)\n    ",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_logical.nonzero().squeeze()]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\n    C = B[:, A_log.nonzero().squeeze()]\n    ",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B.index_select(1, idx)\n",
        "\nx_tensor = torch.from_numpy(np.stack(x_array))\n",
        "\nx_tensor = torch.tensor(x_array)\n",
        "\nimport pandas as pd\nimport torch\nimport numpy as np\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\ndef Convert(a):\n    t = torch.tensor(a.tolist())\n    return t\nx_tensor = Convert(x_array)\nprint(x_tensor)\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\nmask = get_mask(lens)\nprint(mask)\n",
        "\ndiag_ele = torch.diagonal(Tensor_2D)\nTensor_3D = torch.diag_embed(diag_ele)\n",
        "\n    diag_ele = torch.diag(t)\n    result = torch.unsqueeze(diag_ele, dim=2)\n    ",
        "\na = torch.unsqueeze(a, 0)\nab = torch.cat((a, b), 0)\n",
        "\nif a.shape[0] > b.shape[0]:\n    padding = torch.zeros(a.shape[0] - b.shape[0], b.shape[1])\n    b = torch.cat((b, padding), dim=0)\nelif b.shape[0] > a.shape[0]:\n    padding = torch.zeros(b.shape[0] - a.shape[0], a.shape[1])\n    a = torch.cat((a, padding), dim=0)\n",
        "\n    ab = torch.cat((a, b.unsqueeze(0)), dim=0)\n    ",
        "\nfor i in range(a.size(0)):\n    a[i, lengths[i]:, :] = 0\n",
        "\nfor i in range(a.size(0)):\n    a[i, lengths[i]:, :] = 2333\n",
        "\nfor i in range(a.size(0)):\n    a[i, :int(lengths[i]), :] = 0\n",
        "\nfor i in range(a.size(0)):\n    a[i, :int(lengths[i]), :] = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n    ",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = t[np.arange(t.shape[0]), idx]\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, torch.unsqueeze(ids, 2)).squeeze(1)\n",
        "\n_, y = torch.max(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.max(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.max(softmax_output, dim=1, keepdim=True)\n",
        "\n    _, y = torch.max(softmax_output, dim=1)\n    y = y.view(-1, 1)\n    ",
        "\n    _, y = torch.max(softmax_output, dim=1)\n    ",
        "\nloss = cross_entropy2d(images, labels)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = np.count_nonzero(A != B)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ndef Count(A, B):\n    cnt_equal = np.sum(A == B)\n    return cnt_equal\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n",
        "\nx = A.shape[0] // 2\ncnt_equal = np.sum(A[-x:] == B[-x:])\n",
        "\nx = A.shape[0] // 2\ncnt_not_equal = np.sum(A[-x:] != B[-x:])\n",
        "\na_split = torch.split(a, chunk_dim, dim=3)\ntensors_31 = [a_split[i] for i in range(len(a_split))]\n",
        "\na_split = torch.chunk(a, a.shape[2] - chunk_dim + 1, dim=2)\ntensors_31 = [a_split[i] for i in range(a.shape[2] - chunk_dim + 1)]\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_values = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = min_values * sign_x * sign_y\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.mul(sign_x, max)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nx, y = load_data()\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_values = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = min_values * sign_x * sign_y\n    return signed_min\nsigned_min = solve(x, y)\nprint(signed_min)\n",
        "\noutput = MyNet(input)\nconfidence_score = torch.nn.functional.softmax(output, dim=1)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\nresult = torch.cat((a[:, :-1], (a[:, -1] + b[:, 0]) / 2, b[:, 1:]), dim=1)\nprint(result)\n",
        "\n    result = torch.cat((a[:, :-1], (a[:, -1] + b[:, 0]) / 2, b[:, 1:]), dim=1)\n    ",
        "\nzeros = torch.zeros(1, 4, 2)\nresult = torch.cat([zeros, t, zeros], dim=1)\n",
        "\nnew = torch.zeros((1, 4))\nresult = torch.cat([new, t, new], dim=0)\n",
        "\nnew = new.repeat(t.shape[0], 1)\nr = torch.cat([new.unsqueeze(0), t, new.unsqueeze(0)], dim=0)\n",
        "\nresult = torch.matmul(data, W.unsqueeze(1)).squeeze()\n"
    ]
}