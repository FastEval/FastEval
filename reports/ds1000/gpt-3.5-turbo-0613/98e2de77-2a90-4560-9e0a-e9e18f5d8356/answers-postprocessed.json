{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\ndf = df.iloc[List]\nresult = len(df[df['Type'] != df.reset_index()['index']])\n",
        "\nfor column in df.columns:\n    value_counts = df[column].value_counts()\n    values_to_change = value_counts[value_counts >= 2].index.tolist()\n    df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n",
        "\nfor column in df.columns:\n    counts = df[column].value_counts()\n    values_to_change = counts[counts >= 3].index.tolist()\n    df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n",
        "\n    for column in df.columns:\n        value_counts = df[column].value_counts()\n        values_to_change = value_counts[value_counts >= 2].index.tolist()\n        df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n    ",
        "\nvalue_counts_qu1 = pd.value_counts(df.Qu1)\nvalue_counts_qu2 = pd.value_counts(df.Qu2)\nvalue_counts_qu3 = pd.value_counts(df.Qu3)\n\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if value_counts_qu1[x] < 3 else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if value_counts_qu2[x] < 2 else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if value_counts_qu3[x] < 2 else x)\n\nresult = df\n",
        "\nvalue_counts_qu1 = pd.value_counts(df.Qu1)\nvalue_counts_qu2 = pd.value_counts(df.Qu2)\nvalue_counts_qu3 = pd.value_counts(df.Qu3)\n\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x not in value_counts_qu1[value_counts_qu1 >= 3].index and x != 'apple' else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if x not in value_counts_qu2[value_counts_qu2 >= 2].index else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x not in value_counts_qu3[value_counts_qu3 >= 2].index else x)\n\nresult = df\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result.append(df[df['keep_if_dup'] == 'Yes'])\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first') | (df['drop_if_dup'] == 'No')\n",
        "\nresult = df.drop_duplicates(subset='url', keep='last')\nresult = result[result['keep_if_dup'] == 'Yes']\n",
        "\n\n\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\n\nresult = {}\nfor index, row in df.iterrows():\n    current_dict = result\n    for col in df.columns[:-1]:\n        value = row[col]\n        if value not in current_dict:\n            current_dict[value] = {}\n        current_dict = current_dict[value]\n    current_dict[df.columns[-1]] = row[df.columns[-1]]\n\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    ",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\ndef extract_key_value_pairs(message):\n    pairs = re.findall(r'(\\w+):\\s*([^,\\]]+)', message)\n    return dict(pairs)\n\ndf['message'] = df['message'].apply(extract_key_value_pairs)\nresult = pd.concat([df.drop('message', axis=1), df['message'].apply(pd.Series)], axis=1)\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\ndf.loc[~df['product'].isin(products), 'score'] = df.loc[~df['product'].isin(products), 'score'] * 10\n",
        "\ndf.loc[df['product'].isin(products[0] + products[1]), 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'].apply(lambda x: (x - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min()))\n",
        "\ndf['category'] = df.idxmax(axis=1)\n",
        "\ndf['category'] = df.apply(lambda row: row.idxmax(), axis=1)\n",
        "\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\nmask = (df['Date'] >= pd.to_datetime(List[0])) & (df['Date'] <= pd.to_datetime(List[1]))\nresult = df.loc[mask, 'Date'].dt.strftime('%d-%b-%Y %A')\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf.iloc[0]['#1'] = df.iloc[-1]['#1']\n",
        "\ndf = df.shift(-1)\ndf.iloc[-1] = df.iloc[0]\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf['#1'].iloc[-1] = df['#1'].iloc[0]\ndf['#2'] = df['#2'].shift(-1)\ndf['#2'].iloc[0] = df['#2'].iloc[-1]\n",
        "\ndf['#1'] = df['#1'].shift(-1)\ndf.iloc[-1, 0] = df.iloc[0, 0]\n",
        "\ndf.columns = [col + 'X' for col in df.columns]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = [col + 'X' if not col.endswith('X') else 'X' + col for col in df.columns]\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\"})\nresult = result.join(df.filter(like='val').mean())\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\"})\nresult = result.join(df.filter(like='val').sum())\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\"})\nfor column in df.columns:\n    if column.startswith('val') and column.endswith('2'):\n        result[column] = df.groupby('group')[column].mean()\n    else:\n        result[column] = df.groupby('group')[column].sum()\n",
        "\nresult = df.loc[row_list, column_list].mean()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.loc[row_list, column_list].sum()\nresult = result[result < result.max()]\n",
        "\nresult = df.apply(pd.Series.value_counts)\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor column in df.columns:\n    result += \"---- \" + column + \" ---\\n\"\n    result += str(df[column].value_counts()) + \"\\n\"\n",
        "\nresult = df.iloc[0:1].append(df.iloc[1:2], ignore_index=True)\n",
        "\nresult = df.iloc[0:1].append(df.iloc[1:2], ignore_index=True)\n",
        "\nresult = df.ffill(axis=1).bfill(axis=1)\n",
        "\nresult = df.ffill(axis=1).bfill(axis=1)\n",
        "\nresult = df.fillna(method='ffill')\n",
        "\ndf.loc[df['value'] < thresh] = df.loc[df['value'] < thresh].sum()\n",
        "\ndf.loc[df['value'] >= thresh] = df.loc[df['value'] >= thresh].mean()\nresult = df\n",
        "\ndf.loc[(df['value'] < section_left) | (df['value'] > section_right), 'lab'] = 'X'\ndf = df.groupby('lab').mean()\n",
        "\nresult = df.copy()\nresult = pd.concat([result, 1/result], axis=1, keys=['', 'inv_'])\nresult.columns = result.columns.map(''.join)\n",
        "\nresult = pd.concat([df, np.exp(df)], axis=1)\nresult.columns = df.columns.tolist() + ['exp_' + col for col in df.columns]\n",
        "\nresult = df.copy()\nresult = result.join(1 / result, rsuffix='_inv')\n",
        "\nresult = df.copy()\nresult = result.join(pd.DataFrame(np.exp(-result.values), columns=result.columns, index=result.index).apply(lambda x: 1 / (1 + x), axis=0).add_prefix('sigmoid_'))\n",
        "\nmin_idx = df.idxmin()\nresult = df.apply(lambda x: x[:min_idx[x.name]].idxmax())\n",
        "\nmin_idx = df.idxmin()\nresult = df.idxmax().where(df.columns.get_indexer(df.idxmax()) > df.columns.get_indexer(min_idx)).dropna()\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\ndf_expanded = pd.DataFrame({'dt': date_range})\ndf_expanded['user'] = df['user'].unique()[0]\ndf_expanded['val'] = 0\nresult = pd.concat([df, df_expanded]).sort_values('dt').reset_index(drop=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nall_dates = pd.date_range(start=min_date, end=max_date)\nresult = df.set_index(['user', 'dt']).reindex(pd.MultiIndex.from_product([df['user'].unique(), all_dates], names=['user', 'dt'])).reset_index()\nresult['val'] = result['val'].fillna(0)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\ndate_range = pd.date_range(start=min_date, end=max_date)\nexpanded_df = pd.DataFrame({'dt': date_range, 'user': df['user'].unique(), 'val': 233})\nresult = pd.concat([df, expanded_df]).sort_values(by=['user', 'dt']).reset_index(drop=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nusers = df['user'].unique()\ndates = pd.date_range(start=min_date, end=max_date)\nresult = pd.DataFrame({'dt': dates.repeat(len(users)), 'user': users.tolist() * len(dates)})\nresult = result.merge(df.groupby('user')['val'].max().reset_index(), on='user')\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nusers = df['user'].unique()\ndates = pd.date_range(start=min_date, end=max_date)\nexpanded_df = pd.DataFrame({'dt': dates.repeat(len(users)), 'user': users.tolist() * len(dates)})\nresult = pd.merge(expanded_df, df, on=['dt', 'user'], how='left').fillna(method='ffill')\n",
        "\ndf['name'] = pd.factorize(df['name'])[0] + 1\n",
        "\ndf['a'] = df.groupby('name').ngroup() + 1\n",
        "\n    df['name'] = pd.factorize(df['name'])[0] + 1\n    ",
        "\ndf['ID'] = df.groupby('name').ngroup() + 1\ndf.drop('name', axis=1, inplace=True)\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n\nresult = pd.melt(df, id_vars=['user'], var_name='others', value_name='value')\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\nresult = df.loc[df['c'] > 0.5, columns]\n",
        "\nresult = df.loc[df['c'] > 0.45, columns]\n",
        "\n    result = df[df['c'] > 0.5][columns].values\n    ",
        "\n    result = df[df['c'] > 0.5][columns]\n    result['sum'] = result.sum(axis=1)\n    ",
        "def f(df, columns=['b', 'e']):\n    result = df[df['c'] > 0.5][columns]\n    return result",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['diff'] = df['date'].diff().dt.days\ndf = df[df['diff'] > X]\nresult = df.drop(columns='diff')\n",
        "\ndf['date'] = pd.to_datetime(df['date'])  # Convert 'date' column to datetime format\ndf = df.sort_values('date')  # Sort dataframe by 'date' column\n\nfiltered_dates = [df.iloc[0]]  # Initialize list with the first row of the dataframe\n\nfor i in range(1, len(df)):\n    if (df.iloc[i]['date'] - filtered_dates[-1]['date']) > timedelta(weeks=X):\n        filtered_dates.append(df.iloc[i])\n\ndf = pd.DataFrame(filtered_dates)  # Create a new dataframe with the filtered dates\n",
        "\nfrom datetime import datetime, timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 145.99, 146.73, 171.10]})\n\nX = 52\n\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort dataframe by date\ndf = df.sort_values('date')\n\n# Initialize a list to store the filtered rows\nfiltered_rows = []\n\n# Iterate over each row in the dataframe\nfor index, row in df.iterrows():\n    # Check if the current row is the first row or if it is not within X weeks of the previous row\n    if index == 0 or (row['date'] - df.loc[index-1, 'date']).days > X*7:\n        # Add the current row to the filtered rows list\n        filtered_rows.append(row)\n\n# Create a new dataframe with the filtered rows\nresult = pd.DataFrame(filtered_rows)\n\n# Convert date column back to string format\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\n\n# Reset the index of the dataframe\nresult = result.reset_index(drop=True)\n\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 4).sum()\n",
        "\nresult = df[::-1].rolling(3).mean().dropna()[::-1]\n",
        "\nresult = []\ni = 0\nwhile i < len(df):\n    if i % 5 < 3:\n        result.append(df.loc[i:i+2, 'col1'].sum())\n        i += 3\n    else:\n        result.append(df.loc[i:i+1, 'col1'].mean())\n        i += 2\n",
        "\nresult = pd.concat([df.groupby(df.index // 3)['col1'].sum(), df.groupby(df.index // 2)['col1'].mean()]).sort_index().reset_index(drop=True)\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, pd.NA).fillna(method='ffill').fillna(method='bfill')\n",
        "\ndf['number'] = df['duration'].str.extract('(\\d+)')\ndf['time'] = df['duration'].str.extract('(\\D+)')\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\ndf['numer'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ndf['time_day'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\n    df['number'] = df.duration.str.extract('(\\d+)')\n    df['time'] = df.duration.str.extract('(\\D+)')\n    df['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n    ",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\n",
        "\nresult = np.where(np.any(df1[columns_check_list] != df2[columns_check_list], axis=1), True, False)\n",
        "\nresult = [all(df1[column] == df2[column]) for column in columns_check_list]\n",
        "\ndf.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1, inplace=True)\n",
        "\ndf.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1, inplace=True)\n",
        "\n    df.reset_index(inplace=True)\n    df['date'] = pd.to_datetime(df['date'])\n    output = df.values\n    ",
        "\n    df.index = pd.to_datetime(df.index.get_level_values(0))\n    df = df.swaplevel().sort_index()\n    ",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\nresult = df.pivot(index=['Variable', 'Country', 'year'], columns='Variable', values='value').reset_index()\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(by=['year'], ascending=False)\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nresult = df[abs(df[columns]) < 1].dropna()\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nresult = df[abs(df[columns]) > 1].dropna()\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\ndf[columns] = df[columns].abs()\ndf.columns = df.columns.str.replace('Value_', '')\nresult = df[(df[columns] > 1).any(axis=1)]\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT', '<', regex=True)\n",
        "\n    df = df.replace('&AMP;', '&', regex=True)\n    ",
        "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[-1] if ' ' in x else None)\ndf.loc[df['first_name'].notnull(), 'first_name'] = df.loc[df['first_name'].notnull(), 'first_name'].apply(lambda x: x.split(' ')[0])\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].str.split(' ').str[1]\ndf.loc[df['1_name'].notnull(), '1_name'] = df['1_name'].str.split(' ').str[0]\ndf.loc[df['1_name'].isnull(), '1_name'] = df['name']\ndf = df.drop(columns=['name'])\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first name'] = df['name'].apply(lambda x: x.split(' ')[0] if validate_single_space_name(x) else x)\ndf['middle_name'] = df['name'].apply(lambda x: x.split(' ')[1] if validate_single_space_name(x) else None)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[-1] if validate_single_space_name(x) else None)\ndf.drop('name', axis=1, inplace=True)\n",
        "\nresult = pd.merge(df2, df1, on='Timestamp', how='left')\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n",
        "\nerror_values = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        error_values.append(row['Field1'])\n",
        "\nresult = []\nfor index, row in df.iterrows():\n    if isinstance(row['Field1'], int):\n        result.append(row['Field1'])\n",
        "\n    result = []\n    for index, row in df.iterrows():\n        if not isinstance(row['Field1'], int):\n            result.append(row['Field1'])\n    ",
        "\ntotal = df.sum(axis=1)\ndf.iloc[:, 1:] = df.iloc[:, 1:].div(total, axis=0)\n",
        "\ntotal = df.sum(axis=0)\ndf_percentage = df.copy()\ndf_percentage.iloc[:, 1:] = df_percentage.iloc[:, 1:].div(total)\n",
        "# [Missing Code]\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\n",
        "\n    result = df.loc[test].drop_duplicates()\n    ",
        "\nfrom scipy.spatial import distance\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Begin of Missing Code\ndf2 = pd.DataFrame(columns=['car', 'nearest_neighbour', 'euclidean_distance'])\nfor t in df['time'].unique():\n    df_t = df[df['time'] == t]\n    for i, row in df_t.iterrows():\n        car = row['car']\n        x = row['x']\n        y = row['y']\n        df_t['distance'] = df_t.apply(lambda r: distance.euclidean((x, y), (r['x'], r['y'])), axis=1)\n        df_t_sorted = df_t.sort_values(by='distance')\n        nearest_neighbour = df_t_sorted.iloc[1]['car']\n        euclidean_distance = df_t_sorted.iloc[1]['distance']\n        df2 = df2.append({'car': car, 'nearest_neighbour': nearest_neighbour, 'euclidean_distance': euclidean_distance}, ignore_index=True)\n# End of Missing Code\n\nresult = df2\n",
        "\nfrom scipy.spatial import distance\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Begin of Missing Code\ndf2 = pd.DataFrame(columns=['time', 'car', 'farmost_neighbour', 'euclidean_distance'])\n\nfor t in df['time'].unique():\n    df_t = df[df['time'] == t]\n    for c in df_t['car'].unique():\n        df_c = df_t[df_t['car'] == c]\n        distances = distance.cdist(df_c[['x', 'y']], df_t[df_t['car'] != c][['x', 'y']], metric='euclidean')\n        max_distance = distances.max()\n        max_distance_index = distances.argmax()\n        farmost_neighbour = df_t[df_t['car'] != c].iloc[max_distance_index]['car']\n        df2 = df2.append({'time': t, 'car': c, 'farmost_neighbour': farmost_neighbour, 'euclidean_distance': max_distance}, ignore_index=True)\n\ndf2['euclidean_distance'] = df2['euclidean_distance'].round(6)\n# End of Missing Code\n\nresult = df2\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \",\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \"-\".join(row.dropna()), axis=1)\n",
        "\ndf_altered = df.sample(frac=0.2, random_state=0)\ndf_altered['Quantity'] = 0\ndf.update(df_altered)\n",
        "\ndf_altered = df.sample(frac=0.2, random_state=0)\ndf_altered['ProductId'] = 0\ndf.update(df_altered)\n",
        "\ndf['Quantity'] = df.groupby('UserId')['Quantity'].apply(lambda x: x.sample(frac=0.2, random_state=0).replace(x, 0))\n",
        "\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int).cumsum() - 1\n",
        "\ndf['index_original'] = df.index\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    df['index_original'] = df.index\n    duplicate = df.loc[duplicate_bool == True]\n    result = duplicate\n    ",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\ndf['index_original'] = duplicate_bool.astype(int)\n",
        "\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').map({True: df.index[-1]})\n",
        "\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\n",
        "\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\nresult = df.groupby(['Sp','Value']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.query(\"Category in @filter_list\")\n",
        "\nresult = df.query(\"Category != @filter_list\")\n",
        "\nresult = pd.melt(df, value_vars=[tuple(level) for level in df.columns])\n",
        "\nresult = pd.melt(df, value_vars=[tuple(col) for col in df.columns])\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].apply(lambda x: 0 if x < 0 else x)\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('r')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].nunique() == df[col2].nunique():\n                result.append(f'{col1} {col2} many-to-many')\n            elif df[col1].nunique() > df[col2].nunique():\n                result.append(f'{col1} {col2} one-to-many')\n            else:\n                result.append(f'{col1} {col2} many-to-one')\n        else:\n            result.append(f'{col1} {col2} one-to-one')\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].nunique() == df[col2].nunique():\n                result.append(f'{col1} {col2} many-2-many')\n            elif df[col1].nunique() > df[col2].nunique():\n                result.append(f'{col1} {col2} one-2-many')\n            else:\n                result.append(f'{col1} {col2} many-2-one')\n        else:\n            result.append(f'{col1} {col2} one-2-one')\n",
        "\nresult = df.corr().applymap(lambda x: 'one-to-one' if x == 1 else 'one-to-many' if x == -1 else 'many-to-one' if x == 0 else 'many-to-many')\n",
        "\nresult = df.corr().applymap(lambda x: 'one-2-one' if x == 1 else 'one-2-many' if x == -1 else 'many-2-one' if x == 0 else 'many-2-many')\n",
        "\ndf = df.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last')\n",
        "\nresult = pd.to_numeric(s.astype(str).str.replace(',',''), errors='coerce')\n",
        "\nresult = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0))['Survived'].mean()\n",
        "\nresult = df.groupby(((df['Survived'] > 0) | (df['Parch'] > 0)).map({True: 'Has Family', False: 'No Family'}))['SibSp'].mean()\n",
        "\nresult = df.groupby([(df['SibSp'] == 1) & (df['Parch'] == 1), \n                     (df['SibSp'] == 0) & (df['Parch'] == 0), \n                     (df['SibSp'] == 0) & (df['Parch'] == 1), \n                     (df['SibSp'] == 1) & (df['Parch'] == 0)])['Survived'].mean()\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\nstdMeann = lambda x: pd.Series([np.mean(x), np.std(x)], index=['mean', 'std'])\nresult = df.groupby('a')['b'].apply(stdMeann)\n\n",
        "\n\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = df.groupby('b').a.apply(stdMeann)\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\n# Begin of Missing Code\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: np.exp(x) / np.sum(np.exp(x)))\ndf['min-max'] = df.groupby('a')['b'].transform(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n# End of Missing Code\n\nresult = df\n",
        "\nresult = df.loc[:, (df != 0).any(axis=0)].loc[(df != 0).any(axis=1)]\n",
        "\nresult = df.loc[(df.sum(axis=1) != 0), (df.sum(axis=0) != 0)]\n",
        "\nresult = df.loc[(df.max(axis=1) <= 2), (df.max(axis=0) <= 2)]\n",
        "\ndf[df > 2] = 0\nresult = df\n",
        "\nresult = s.sort_values(ascending=True)\n",
        "\ndf = pd.DataFrame({'index': s.index, '1': s.values})\ndf = df.sort_values(by=['1', 'index'])\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\n",
        "\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\nresult = df.groupby(['Sp','Value']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Date'])\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Date'])\n",
        "\n    df['Date'] = df['Member'].map(dict)\n    ",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Val'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Val'].transform('count')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Date'].transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val'])['Date'].transform('count')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Date'].transform('count')\ndf['Count_w'] = df.groupby(df['Date'].dt.weekday.rename('weekday'))['Date'].transform('count')\ndf['Count_Val'] = df.groupby(['Val'])['Date'].transform('count')\n",
        "\nresult1 = df[df == 0].groupby('Date').count()\nresult2 = df[df != 0].groupby('Date').count()\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: x[['B', 'C']].apply(lambda y: y % 2 == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: x[['B', 'C']].apply(lambda y: y % 2 != 0).sum())\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D'], index=['B'], aggfunc=np.sum)\nresult['E'] = pd.pivot_table(df, values=['E'], index=['B'], aggfunc=np.mean)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\n\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=1)\n\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2')\n\nresult = result.compute()\n",
        "\n\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=1)\n\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2').reset_index(drop=True)\n",
        "\n\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=1)\n\nresult = df.assign(var2=df.var2.str.split('-')).explode('var2').reset_index(drop=True)\n",
        "\n\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\n",
        "\n\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'medi', 'row']] = df['row'].str.split(' ', 2, expand=True)\n",
        "\ndf = df.set_index('Name')  # Set 'Name' column as the index\ndf = df.replace(0, pd.NA)  # Replace 0 with NaN\ndf = df.cumsum(axis=1)  # Calculate cumulative sum along each row\ndf = df.div(df.count(axis=1), axis=0)  # Divide by the count of non-null values along each row\ndf = df.fillna(0)  # Replace NaN with 0\ndf = df.reset_index()  # Reset the index to default\n",
        "\ndf = df.set_index('Name')  # Set 'Name' column as the index\ndf = df.apply(lambda x: x[::-1].cumsum()[::-1].mask(x == 0, 0))  # Calculate cumulative sum from end to head and replace 0s with 0\ndf = df.reset_index()  # Reset the index to default\n",
        "\n    df.iloc[:, 1:] = df.iloc[:, 1:].cumsum(axis=1)\n    df.iloc[:, 1:] = df.iloc[:, 1:].div((df.iloc[:, 1:] != 0).cumsum(axis=1))\n    ",
        "\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.replace(0, pd.NA))\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.cumsum().div(x.notna().cumsum()))\n",
        "\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\ndf.loc[0, 'Label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = (df['arrival_time'].shift(-1) - df['departure_time']).dt.total_seconds()\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['Duration'] = pd.to_datetime(df['departure_time'].shift(-1)) - pd.to_datetime(df['arrival_time'])\ndf['Duration'] = df['Duration'].dt.total_seconds()\n",
        "\nresult = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\n",
        "\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n",
        "\nresult = df[df['key2'].str.endswith(\"e\")].groupby('key1').size().reset_index(name='count')\n",
        "\nmin_result = df.index.min()\nmax_result = df.index.max()\n",
        "\nmode_result = statistics.mode(df.index)\nmedian_result = statistics.median(df.index)\n",
        "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n",
        "\nresult = df.loc[df.groupby(\"item\")[\"diff\"].idxmin()]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n",
        "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]\n    ",
        "\n\n# Calculate the number of NaN values to fill with '0' and '1'\nnum_nan = df['Column_x'].isnull().sum()\nnum_fill_0 = math.floor(num_nan / 2)\nnum_fill_1 = math.ceil(num_nan / 2)\n\n# Fill the first half of NaN values with '0'\ndf['Column_x'].fillna(0, limit=num_fill_0, inplace=True)\n\n# Fill the remaining NaN values with '1'\ndf['Column_x'].fillna(1, limit=num_fill_1, inplace=True)\n\n",
        "\nnum_nan = df['Column_x'].isnull().sum()\nnum_fill_0 = int(num_nan * 0.3)\nnum_fill_05 = int(num_nan * 0.3)\nnum_fill_1 = num_nan - num_fill_0 - num_fill_05\n\ndf['Column_x'].fillna(0, limit=num_fill_0, inplace=True)\ndf['Column_x'].fillna(0.5, limit=num_fill_05, inplace=True)\ndf['Column_x'].fillna(1, limit=num_fill_1, inplace=True)\n",
        "\nnum_zeros = int(df['Column_x'].isnull().sum() / 2)\nnum_ones = int(df['Column_x'].isnull().sum() / 2)\n\ndf['Column_x'].fillna(0, limit=num_zeros, inplace=True)\ndf['Column_x'].fillna(1, limit=num_ones, inplace=True)\n",
        "\nresult = pd.DataFrame(np.array(list(zip(a.values.flatten(), b.values.flatten()))).reshape(a.shape), columns=a.columns)\n",
        "\nresult = pd.concat([a, b, c], axis=1).apply(tuple, axis=1).to_frame()\nresult.columns = ['one', 'two']\n",
        "\nresult = pd.concat([a, b], axis=1).apply(tuple, axis=1).apply(pd.Series)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.reset_index(drop=True)\nresult.loc[result['id'].duplicated(), ['city', 'district']] = None\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)\nresult = result.fillna('')\nresult = result[['id', 'city', 'district', 'date', 'value']]\n",
        "\nresult = pd.concat([df1, df2], axis=0).sort_values(['id', 'date']).reset_index(drop=True)\nresult[['city', 'district']] = result.groupby('id')[['city', 'district']].ffill()\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_x'].update(result['B_y'])\nresult = result.drop(columns=['B_y'])\nresult = result.rename(columns={'B_x': 'B'})\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_x'].fillna(result['B_y'], inplace=True)\nresult.drop(columns=['B_y'], inplace=True)\nresult.rename(columns={'B_x': 'B'}, inplace=True)\n",
        "\nresult = pd.merge(C, D, how='left', on='A').fillna(C)\nresult['dulplicated'] = result['B_x'] != result['B_y']\nresult['B'] = result['B_y'].fillna(result['B_x'])\nresult = result[['A', 'B', 'dulplicated']]\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist()).rename('amount-time-tuple')\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['amount', 'time']].values.tolist()[::-1])\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index)\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index, columns=[0, 1, 2, 3])\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = df.columns[df.columns.str.contains(s) & ~df.columns.str.match(s)].tolist()\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = df['codes'].apply(pd.Series)\n",
        "\nresult = df['codes'].apply(pd.Series)\n",
        "\nresult = df['codes'].apply(pd.Series)\n",
        "\nresult = [item for sublist in df['col1'] for item in sublist]\n",
        "\nresult = ','.join(','.join(map(str, lst[::-1])) for lst in df['col1'])\n",
        "\nresult = ','.join([str(item) for sublist in df['col1'] for item in sublist])\n",
        "\ndf['Time'] = df['Time'].dt.floor('2min')\ndf = df.groupby('Time').mean().reset_index()\n",
        "\ndf['Time'] = df['Time'].dt.floor('3min')\ndf = df.groupby('Time').sum().reset_index()\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert 'TIME' column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)  # Add 'RANK' column based on 'ID' and 'TIME' columns\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert 'TIME' column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)  # Add 'RANK' column based on 'TIME' column and grouped by 'ID'\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert TIME column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)  # Add RANK column by ranking TIME for each ID\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')  # Format TIME column as desired\n\nresult = df\n",
        "\nresult = df[filt[df.index.get_level_values('a')]]\n",
        "\nresult = df[filt[df.index.get_level_values('a')]]\n",
        "\nresult = df.loc[[0, 8]].isnull().any()\n",
        "\nresult = df.columns[(df.iloc[0] == df.iloc[8]) | (df.iloc[0].isnull() & df.iloc[8].isnull())]\n",
        "\nresult = df.columns[df.iloc[[0, 8]].isnull().any()]\n",
        "\nresult = []\nfor col in df.columns:\n    if df.loc[0, col] != df.loc[8, col]:\n        result.append((df.loc[0, col], df.loc[8, col]))\n",
        "\nts = df.set_index('Date')['Value']\n",
        "\nresult = df.stack().reset_index(drop=True).to_frame().T\n",
        "\nresult = df.stack().reset_index(drop=True).to_frame().T\nresult.columns = [f'{col}_{i}' for i, col in enumerate(result.columns)]\n",
        "\ndf['dogs'] = df['dogs'].round(2)\n",
        "\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\nresult = df.sort_index(level='time')\n",
        "\nresult = df.sort_values(by=['VIM', 'time'], ascending=[True, True])\n",
        "\n\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\nhd2_from = '2020-02-18 15:30:00'\nhd2_till = '2020-02-18 21:59:00'\n\nresult = df[(df.index < hd1_from) | (df.index > hd1_till)]\nresult = result[(result.index < hd2_from) | (result.index > hd2_till)]\n\n",
        "\nresult = df.copy()\nresult['Day of Week'] = result.index.strftime('%d-%b-%Y %A')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 0', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 1', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 2', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 3', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 4', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 5', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 6', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace('Jan', 'January')\nresult['Day of Week'] = result['Day of Week'].str.replace('Feb', 'February')\nresult['Day of Week'] = result['Day of Week'].str.replace('Mar', 'March')\nresult['Day of Week'] = result['Day of Week'].str.replace('Apr', 'April')\nresult['Day of Week'] = result['Day of Week'].str.replace('May', 'May')\nresult['Day of Week'] = result['Day of Week'].str.replace('Jun', 'June')\nresult['Day of Week'] = result['Day of Week'].str.replace('Jul', 'July')\nresult['Day of Week'] = result['Day of Week'].str.replace('Aug', 'August')\nresult['Day of Week'] = result['Day of Week'].str.replace('Sep', 'September')\nresult['Day of Week'] = result['Day of Week'].str.replace('Oct', 'October')\nresult['Day of Week'] = result['Day of Week'].str.replace('Nov', 'November')\nresult['Day of Week'] = result['Day of Week'].str.replace('Dec', 'December')\n",
        "\nresult = corr[corr > 0.3].stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\n",
        "\nresult = corr[corr > 0.3].stack().reset_index().iloc[:, [0, 2]]\nresult.columns = [0, 3]\nresult = result.set_index([0, 3]).squeeze()\n",
        "\ndf.columns = list(df.columns[:-1]) + ['Test']\n",
        "\ndf.columns = ['Test'] + list(df.columns[1:])\n",
        "\ndf['frequent'] = df.mode(axis=1).iloc[:, 0]\ndf['freq_count'] = df.sum(axis=1)\n",
        "\ndf['frequent'] = df.mode(axis=1).iloc[:, 0]\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n",
        "\nfrequent_values = df.apply(lambda row: row.value_counts().index.tolist(), axis=1)\nfreq_count = df.apply(lambda row: row.value_counts().values.tolist(), axis=1)\ndf['frequent'] = frequent_values\ndf['freq_count'] = freq_count\n",
        "\nresult = df.groupby([\"id1\",\"id2\"]).mean().reset_index()\n",
        "\nresult = df.replace('NULL', 0).groupby([\"id1\",\"id2\"]).mean()\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum')\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum', 'b_col']], on='EntityNum')\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nresult = [list(filter(lambda x: not np.isnan(x), sublist)) for sublist in x]\n",
        "\nb = np.eye(np.max(a)+1)[a]\n",
        "\nb = np.eye(np.max(a)+1)[a]\n",
        "\nb = np.eye(a.max() - a.min() + 1)[a - a.min()]\n",
        "\nb = np.zeros((len(a), len(a)))\nfor i, val in enumerate(a):\n    index = np.argsort(a).tolist().index(i)\n    b[i][index] = 1\n",
        "\nb = np.eye(np.max(a)+1)[a.flatten()]\nb = b.reshape(a.shape + (b.shape[1],))\n",
        "\nresult = np.percentile(a, p)\n",
        "\nB = np.reshape(A, (-1, ncol))\n",
        "\nB = np.reshape(A, (nrow, -1))\n",
        "\nB = np.reshape(A, (-1, ncol))\n",
        "\nB = np.reshape(A[-(len(A)//ncol)*ncol:], (-1, ncol))\n",
        "\nresult = np.roll(a, shift)\nresult[:shift] = np.nan\nresult[shift:] = a[:len(a)-shift]\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nnp.random.seed(0)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape, order='F')\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    result = np.unravel_index(np.argmax(a), a.shape)\n    ",
        "\nresult = np.unravel_index(np.argsort(a, axis=None)[-2], a.shape)\n",
        "\na = a[:, ~np.isnan(a).any(axis=0)]\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.sin(np.radians(degree))\n",
        "\nresult = np.cos(np.radians(degree))\n",
        "\nresult = 0 if np.sin(np.deg2rad(number)) > np.sin(number) else 1\n",
        "\nresult = np.arcsin(value) * 180 / np.pi\n",
        "\nresult = np.pad(A, (0, length - len(A)), mode='constant')\n",
        "\nresult = np.pad(A, (0, length - len(A)), mode='constant')\n",
        "\na = np.power(a, power)\n",
        "\n    result = np.power(a, power)\n    ",
        "\nresult = Fraction(numerator, denominator)\n",
        "\n    result = np.gcd(numerator, denominator)\n    numerator = numerator // result\n    denominator = denominator // result\n    ",
        "\nresult = Fraction(numerator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, np.maximum(b, c))\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\nresult = np.diag(np.fliplr(a))\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\nresult = X.flatten().tolist()\n",
        "\nresult = X.flatten(order='C')\n",
        "\n    result = X.flatten().tolist()\n    ",
        "\nresult = X.flatten(order='F')\n",
        "\nresult = np.array([int(digit) for digit in mystr])\n",
        "\nresult = np.cumsum(a[:, col] * multiply_number)\n",
        "\nresult = np.cumsum(a[row] * multiply_number)\n",
        "\nresult = np.prod(a[row] / divide_number)\n",
        "\nresult = np.linalg.qr(a)[0]\n",
        "\nresult = a.shape[1]\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a[~np.isnan(a)], b[~np.isnan(b)], equal_var=False)\n",
        "\nt_statistic = (amean - bmean) / np.sqrt((avar/anobs) + (bvar/bnobs))\ndegrees_of_freedom = anobs + bnobs - 2\np_value = 2 * (1 - scipy.stats.t.cdf(np.abs(t_statistic), degrees_of_freedom))\n",
        "\noutput = np.setdiff1d(A, B, axis=0)\n",
        "\noutput = np.concatenate((np.setdiff1d(A, B), np.setdiff1d(B, A)))\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)[::-1]\nc = b[sort_indices]\n",
        "\nsum_a = np.sum(a, axis=(1, 2))\nsorted_indices = np.argsort(sum_a)\nresult = b[sorted_indices]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0, 2], axis=1)\n",
        "\nresult = np.delete(a, del_col, axis=1)\n",
        "\na = np.insert(a, pos, element)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\n    a = np.insert(a, pos, element)\n    ",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\nresult = np.copy(array_of_arrays)\n",
        "\nresult = np.all(a == a[0], axis=0)\n",
        "\nresult = np.all(a == a[:, 0][:, np.newaxis], axis=0)\n",
        "\n    result = np.all(a[0] == a[1:], axis=0)\n    ",
        "\nX, Y = np.meshgrid(x, y)\nZ = np.cos(X)**4 + np.sin(Y)**2\nresult = simps(simps(Z, y), x)\n",
        "\n    X, Y = np.meshgrid(x, y)\n    Z = np.cos(X)**4 + np.sin(Y)**2\n    result = simps(simps(Z, y), x)\n    ",
        "\nresult = np.sort(grades) / np.sum(grades)\n",
        "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)(eval)\n",
        "\necdf = np.cumsum(grades / np.sum(grades))\nlow = np.min(grades[ecdf < threshold])\nhigh = np.max(grades[ecdf < threshold])\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\na_np = a.numpy()\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[-N:][::-1]\n",
        "\nresult = np.linalg.matrix_power(A, n)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = a[:a.shape[0]//patch_size*patch_size, :a.shape[1]//patch_size*patch_size].reshape(-1, patch_size, patch_size)\n",
        "\nresult = np.reshape(a, (h, w))\n",
        "\nresult = np.array([a[i:i+patch_size, j:j+patch_size] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])\n",
        "\nresult = a[:, low:high+1]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:, low:high]\n",
        "\na = np.array(eval(string))\n",
        "\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\n    result = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n    ",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nresult = np.ravel_multi_index(index, dims, order='F') + 1\n",
        "\nresult = np.ravel_multi_index(index, dims, order='C')\n",
        "\nvalues = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\nresult = np.maximum.reduceat(a, np.unique(index))\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\nresult = np.zeros(np.max(index)+1)\nfor i in range(len(a)):\n    if index[i] >= 0:\n        result[index[i]] = min(result[index[i]], a[i])\n",
        "\nz = np.vectorize(elementwise_function)(x, y)\n",
        "\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\nresult = np.pad(a, ((abs(low_index), high_index-a.shape[0]+1), (abs(low_index), high_index-a.shape[1]+1)), mode='constant')\n",
        "\nresult = x[x >= 0]\n",
        "\nresult = x[np.iscomplex(x)]\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_max = np.max(bin_data, axis=1)\n",
        "\nbin_data = np.array([data[:, i:i+bin_size] for i in range(0, data.shape[1], bin_size)])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nn = len(data)\nnum_bins = n // bin_size\nbin_data = np.split(data[-n:], num_bins)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nn = len(data[0])\nnum_bins = n // bin_size\nbin_data = np.array([data[:, i*bin_size:(i+1)*bin_size] for i in range(num_bins-1, -1, -1)])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nnum_rows, num_cols = data.shape\nnum_bins = num_cols // bin_size\nbin_data = np.zeros((num_rows, num_bins), dtype=object)\nfor i in range(num_rows):\n    for j in range(num_bins):\n        start = num_cols - (j+1)*bin_size\n        end = num_cols - j*bin_size\n        bin_data[i, j] = tuple(data[i, start:end])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\ndef smoothclamp(x):\n    return np.clip(3 * x**2 - 2 * x**3, x_min, x_max)\n",
        "\ndef smoothclamp(x, N):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0, 1)\n    result = t ** N * (3 - 2 * t)\n    result = result * (x_max - x_min) + x_min\n    return result\n",
        "\nresult = np.correlate(a, np.roll(b[::-1], 1), mode='valid')\n",
        "\nresult = df.values.reshape(4, 15, 5)\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\nresult = np.unpackbits(a[:, np.newaxis], axis=1)[:, -m:]\n",
        "\nresult = np.unpackbits(np.uint8(a.reshape(-1,1)), axis=1)[:, -m:]\n",
        "\nresult = np.unpackbits(a[:, np.newaxis], axis=1)[:, -m:]\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 3*std, mean + 3*std)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 2*std, mean + 2*std)\n",
        "\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3 * std, mean + 3 * std)\n    ",
        "\nmean = np.mean(a)\nstd = np.std(a)\nthreshold = mean + (2 * std)\nresult = np.abs(a - mean) > threshold\n",
        "\nprob = np.percentile(DataArray, percentile)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[1,:] = 0\na[:,0] = 0\n",
        "\nmask = a == np.amax(a, axis=1)[:, np.newaxis]\n",
        "\nmask = np.equal(a, np.min(a, axis=1)[:, np.newaxis])\n",
        "\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nresult = np.einsum('ij,ik->jik', X, X)\n",
        "\nX = np.sqrt(np.sum(Y, axis=2))\n",
        "\nis_contained = np.isin(number, a)\n",
        "\nC = np.setdiff1d(A, B)\n",
        "\nC = np.intersect1d(A, B)\n",
        "\nC = np.intersect1d(A, B)\n",
        "\nresult = len(a) + 1 - rankdata(a).astype(int)\n",
        "\nresult = len(a) - rankdata(a).astype(int) - 1\n",
        "\n    ranks = rankdata(a)\n    result = len(a) - ranks + 1\n    ",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\nresult = a[:, second, :][:, :, third]\n",
        "\narr = np.zeros((20,10,10,2))\n",
        "\nresult = X / np.sum(np.abs(X), axis=1, keepdims=True)\n",
        "\nresult = X / np.linalg.norm(X, axis=1, ord=2, keepdims=True)\n",
        "\nresult = np.divide(X, np.max(np.abs(X), axis=1, keepdims=True))\n",
        "\nresult = np.select(df['a'].astype(str).str.contains(target), choices, default=np.nan)\n",
        "\nresult = cdist(a, a)\n",
        "\nresult = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        result[i, j] = np.linalg.norm(a[i] - a[j])\n",
        "\nresult = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        result[i, j] = np.linalg.norm(a[i] - a[j])\n",
        "\nA = [float(x) for x in A]\nAVG = np.mean(A)\n",
        "\nNA = np.asarray(A, dtype=float)\nAVG = np.mean(NA, axis=0)\n",
        "\nA = [eval(x) for x in A]\n",
        "\nresult = np.delete(a, np.where(np.diff(a) == 0))\n",
        "\nresult = np.unique(a[np.nonzero(a)])\n",
        "\ndata = {'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()}\ndf = pd.DataFrame(data)\n",
        "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ",
        "\ndata = {'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()}\ndf = pd.DataFrame(data)\ndf['maximum'] = df.max(axis=1)\n",
        "\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        result.append(a[i:i+size[0], j:j+size[1]])\n",
        "\nresult = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        window = a[max(0, i-size[0]//2):min(a.shape[0], i+size[0]//2+1), max(0, j-size[1]//2):min(a.shape[1], j+size[1]//2+1)]\n        result.append(window)\n",
        "\nresult = np.nanmean(a)\n",
        "\n    result = np.mean(a.real)\n    ",
        "\nresult = Z[..., -1:]\n",
        "\nresult = a[-1:, ...]\n",
        "\nresult = c in CNTS\n",
        "\nresult = any(np.array_equal(c, arr) for arr in CNTS)\n",
        "\nf = intp.interp2d(np.arange(4), np.arange(4), a, kind='linear')\nresult = f(x_new, y_new)\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n",
        "\ni = np.diag(i)\n",
        "\na[np.nonzero(~np.eye(a.shape[0], dtype=bool))] = 0\n",
        "\nt0 = pd.to_datetime(start)\ntf = pd.to_datetime(end)\nseries = pd.date_range(start=t0, end=tf, periods=n)\nresult = pd.DatetimeIndex(series)\n",
        "\nresult = np.where((x == a) & (y == b))[0][0] if np.sum((x == a) & (y == b)) > 0 else -1\n",
        "\nresult = np.where((x == a) & (y == b))[0]\n",
        "\nresult = np.polyfit(x, y, 2)\n",
        "\ncoefficients = np.polyfit(x, y, degree)\nresult = coefficients[::-1]\n",
        "\ndf = df.apply(lambda x: x - a)\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.flatten().reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(a.shape[0], -1)).reshape(a.shape)\n",
        "\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n",
        "\nfor i in range(arr.shape[0]):\n    mask = arr[i] < n1[i]\n    mask2 = arr[i] >= n2[i]\n    mask3 = mask ^ mask2\n    arr[i][mask] = 0\n    arr[i][mask3] = arr[i][mask3] + 5\n    arr[i][~mask2] = 30\n",
        "\nresult = np.count_nonzero(np.isclose(s1, s2))\n",
        "\nresult = np.count_nonzero(np.isnan(s1) != np.isnan(s2))\n",
        "\nresult = all(np.array_equal(a[i], a[i+1]) for i in range(len(a)-1))\n",
        "\nresult = all(np.isnan(arr).all() for arr in a)\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), constant_values=element)\n",
        "\n    result = np.zeros(shape)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n    ",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\na = a.reshape(a.shape[0]//3, 3)\n",
        "\nresult = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]].squeeze()\n",
        "\nresult = a[:,:,1] * b\n",
        "\nresult = a[np.arange(b.shape[0])[:, None], np.arange(b.shape[1]), b]\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b])\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b])\n",
        "\nresult = df.loc[(df['a'] > 1) & (df['a'] <= 4), 'b'].tolist()\nresult = [np.nan if x not in result else x for x in df['b']]\n",
        "\nresult = im[1:4, 1:5]\n",
        "\nresult = A[np.ix_((A != 0).any(1), (A != 0).any(0))]\n",
        "\nresult = im[1:-1, 1:-1]\n",
        "\nresult = im[1:-1, 1:-1]\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\ndef my_map_func(i):\n  return [[i, i+1, i+2]]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n",
        "\n    def my_map_func(i):\n        return [i, i+1, i+2]\n    \n    ds = tf.data.Dataset.from_tensor_slices(input)\n    ds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\n    result = []\n    iterator = ds.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    with tf.compat.v1.Session() as sess:\n        for _ in range(9):\n            result.append(sess.run(next_element))\n    return result\n    ",
        "\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length)\npadded_mask = tf.pad(mask, [[0, 0], [8 - max_length, 0]])\nresult = tf.cast(padded_mask, dtype=tf.int32)\n",
        "\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length)\npadded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_length]])\nresult = tf.cast(padded_mask, dtype=tf.int32)\n",
        "\nmax_length = tf.reduce_max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\nresult = tf.pad(mask, paddings=[[0, 0], [8 - max_length, 0]])\n",
        "\n    max_length = max(lengths)\n    padded_length = 8\n    mask = tf.sequence_mask(lengths, maxlen=padded_length, dtype=tf.int32)\n    result = tf.pad(mask, [[0, 0], [0, padded_length - max_length]])\n    ",
        "\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length+1, dtype=tf.float32)\nresult = tf.pad(mask, [[0, 0], [0, 8-max_length-1]])\n",
        "\nresult = tf.transpose(tf.reshape(tf.tile(a, [tf.size(b)]), [-1, tf.size(b)]))\nresult = tf.concat([result, tf.reshape(tf.tile(b, [tf.size(a)]), [-1, tf.size(a)])], axis=1)\n",
        "\n    result = tf.stack(tf.meshgrid(a, b), axis=-1)\n    ",
        "\nresult = tf.squeeze(a, axis=2)\n",
        "\nresult = tf.expand_dims(a, axis=2)\n",
        "\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n",
        "\n    squared_diff = tf.square(tf.subtract(A, B))\n    result = tf.reduce_sum(squared_diff, axis=1)\n    ",
        "\nresult = tf.gather_nd(x, tf.stack((y, z), axis=1))\n",
        "\nm = tf.gather_nd(x, tf.stack((row, col), axis=1))\n",
        "\n    m = tf.gather_nd(x, tf.stack((y, z), axis=1))\n    ",
        "\nresult = tf.einsum('bik,bjk->bij', A, B)\n",
        "\nresult = tf.matmul(A, tf.transpose(B, perm=[0, 2, 1]))\n",
        "\nresult = [s.decode('utf-8') for s in x]\n",
        "\n    result = [s.decode('utf-8') for s in x]\n    ",
        "\nmask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\nsum_values = tf.reduce_sum(x, axis=-2)\ncount_nonzero = tf.reduce_sum(mask, axis=-2)\nresult = tf.divide(sum_values, count_nonzero)\n",
        "\nmask = tf.cast(tf.not_equal(x, 0), tf.float32)\nsum_values = tf.reduce_sum(x, axis=-2)\ncount_values = tf.reduce_sum(mask, axis=-2)\nresult = sum_values / count_values\n",
        "\n    mask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\n    sum_values = tf.reduce_sum(x, axis=-2)\n    count_nonzero = tf.reduce_sum(mask, axis=-2)\n    result = tf.divide(sum_values, count_nonzero)\n    ",
        "\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    result = tf.argmax(a, axis=1)\n    ",
        "\nresult = tf.argmin(a, axis=0)\n",
        "\ntf.saved_model.save(model, \"export/1\")\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=6, dtype=tf.int32)\n",
        "\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)  # Set random seed\n    result = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)  # Generate 10 random integers\n    return result",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\nresult, _ = scipy.optimize.curve_fit(lambda x, A, B, C: A * np.exp(B * x) + C, x, y, p0=p0)\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\nresult = stats.ks_2samp(x, y)\nresult = result[1] < alpha\n",
        "\nresult = optimize.minimize(lambda x: ((x[0]+x[1]-x[2])-2)**2 + ((3*x[0]-x[1]-x[2]))**2 + sin(x[1]) + cos(x[1]) + 4, initial_guess)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores, loc=mu, scale=sigma)\n",
        "\nz_scores = scipy.stats.norm.ppf(p_values)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nresult = sa.multiply(sb)\n",
        "\n    result = sA.multiply(sB)\n    ",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n",
        "\ndata_rot = rotate(data_orig, angle)\nxrot = int(x0 * np.cos(np.deg2rad(angle)) - y0 * np.sin(np.deg2rad(angle)))\nyrot = int(x0 * np.sin(np.deg2rad(angle)) + y0 * np.cos(np.deg2rad(angle)))\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, 'uniform')\n",
        "\nresult = stats.kstest(times, \"uniform\")\n",
        "\nresult = stats.kstest(times, 'uniform')\nresult = result.pvalue >= 0.05\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2)\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)\nresult = col_ind.tolist()\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)\nresult = col_ind.tolist()\n",
        "\nb.setdiag(0)\n",
        "\nlabels, num_regions = ndimage.label(img > threshold)\nresult = num_regions\n",
        "\nlabels, num_regions = ndimage.label(img < threshold)\nresult = num_regions\n",
        "\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nexample_img /= img.max()\ndef f(img = example_img):\n    threshold = 0.75\n    labeled_array, num_features = ndimage.label(img > threshold)\n    result = num_features\n    return result\n",
        "\nlabels, num_features = ndimage.label(img > threshold)\ncenters = ndimage.center_of_mass(img, labels, range(1, num_features+1))\nresult = [((0, 0), center) for center in centers]\n",
        "\nM = M + M.T\n",
        "\n    sA = sA + sA.T\n    ",
        "\nstructure = np.array([[1, 1, 1],\n                     [1, 0, 1],\n                     [1, 1, 1]])\nfiltered_square = scipy.ndimage.binary_opening(square, structure=structure)\nsquare = square - filtered_square\n",
        "\n\n# Find connected components in the image\nlabels, num_labels = scipy.ndimage.label(square)\n\n# Get the size of each connected component\nsizes = scipy.ndimage.sum(square, labels, range(1, num_labels+1))\n\n# Find the indices of the connected components with size equal to 1\nisolated_indices = np.where(sizes == 1)[0]\n\n# Set the isolated connected components to 0\nfor index in isolated_indices:\n    square[labels == index+1] = 0\n",
        "\nmean = col.mean()\nstandard_deviation = col.std()\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nMedian = np.median(col.data)\nMode = np.bincount(col.data).argmax()\n",
        "\ndef fourier(x, *coefficients):\n    result = 0\n    for i in range(len(coefficients)):\n        result += coefficients[i] * np.cos((i+1) * np.pi / tau * x)\n    return result\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*(degree))\n",
        "\nresult = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean')\n",
        "\nresult = scipy.spatial.distance.cdist(np.argwhere(example_array > 0), np.argwhere(example_array > 0), metric='cityblock')\n",
        "\n    # Calculate the coordinates of each patch\n    patches = np.unique(example_array)\n    patch_coords = []\n    for patch in patches:\n        patch_coords.append(np.argwhere(example_array == patch))\n    \n    # Calculate pairwise Euclidean distances between patches\n    distances = []\n    for i in range(len(patch_coords)):\n        for j in range(i+1, len(patch_coords)):\n            dist = scipy.spatial.distance.cdist(patch_coords[i], patch_coords[j], 'euclidean')\n            distances.append([patches[i], patches[j], np.min(dist)])\n    \n    result = np.array(distances)\n    ",
        "\ntck = interpolate.splrep(x[:, 0], y[:, 0], k=2, s=4)\nresult = interpolate.splev(x_val, tck, der=0)\n",
        "\nresult = ss.anderson_ksamp([x1, x2, x3, x4])\nstatistic = result.statistic\ncritical_values = result.critical_values\nsignificance_level = result.significance_level\n",
        "\nresult = ss.anderson_ksamp([x1, x2])\n",
        "\n\ndef tau1(x):\n    y = np.array(A['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\n\ndf['AB'] = df['B'].rolling(3).apply(tau1)\ndf['AC'] = df['C'].rolling(3).apply(tau1)\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(df['C'].rolling(3).apply(lambda y: tau1(x))))\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = block_diag(*a)\n",
        "\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    ",
        "\nmean = np.mean(a)\nstd = np.std(a)\nkurtosis_result = np.mean(((a - mean) / std) ** 4)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\n",
        "\nresult = scipy.interpolate.interp2d(x, y, z)(s, t)\n",
        "\nresult = scipy.interpolate.interp2d(x, y, z)(s, t)\n",
        "\n\nresult = np.zeros(len(extraPoints), dtype=int)\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    result[i] = region_index\n\n",
        "\n\nresult = np.zeros(len(extraPoints), dtype=int)\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    result[i] = region_index\n\n",
        "\nmax_size = max(len(vector) for vector in vectors)\npadded_vectors = [np.pad(vector, (0, max_size - len(vector)), mode='constant') for vector in vectors]\nresult = sparse.csr_matrix(padded_vectors)\n",
        "\nb = nd.median_filter(a, 3, origin=0.5)\n",
        "\nresult = M[row, column]\n",
        "\nresult = M[row, column].tolist()\n",
        "\nnew_array = scipy.interpolate.interp1d(x, array, axis=0)(x_new)\n",
        "\nprob, _ = scipy.integrate.quad(NDfx, -np.inf, x)\n",
        "\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    prob = P\n",
        "\nresult = sf.dct(np.eye(N))\n",
        "\nresult = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\n",
        "\nresult = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        result[i,j] = scipy.stats.binom.pmf(j, i, p)\n",
        "\nresult = df.apply(stats.zscore, axis=1)\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\nresult = pd.DataFrame()\nresult['data'] = df\nresult['zscore'] = stats.zscore(df)\n",
        "\nresult = pd.DataFrame()\nresult['data'] = df\nresult['zscore'] = df.apply(lambda x: np.round(stats.zscore(x), 3))\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\nmid = np.array([[shape[0] / 2, shape[1] / 2]])\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), mid)\n",
        "\nmid = np.array([shape[0]//2, shape[1]//2])\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), np.array([mid]))\n",
        "\n    rows, cols = shape\n    y, x = np.indices(shape)\n    mid = np.array([(rows-1)/2, (cols-1)/2])\n    result = distance.cdist(np.dstack((y, x)), mid)\n    ",
        "\nresult = scipy.ndimage.zoom(x, np.min(np.array(shape) / np.array(x.shape)), order=1)\n",
        "\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\n",
        "\nout = scipy.optimize.minimize(residual, x0, args=(a, y), method='L-BFGS-B', bounds=[(lb, None) for lb in x_lower_bounds])\n",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 + t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\nfor t in range(4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\nresult, error = scipy.integrate.quad(lambda x: 2*c*x, low, high)\n",
        "\n    def integrand(x):\n        return 2 * c * x\n    \n    result, error = scipy.integrate.quad(integrand, low, high)\n    ",
        "\nV = V + sparse.dok_matrix((V.shape[0], V.shape[1]), dtype=np.float64) + x\n",
        "# [Missing Code]\nV.data += x",
        "# [Missing Code]\nV.data += x\nV.data += y",
        "\n#csc sparse matrix\nsa = sa.tocsc()\n#iterate through columns\nfor Col in range(sa.shape[1]):\n   Column = sa[:,Col].data\n   List = [x**2 for x in Column]\n   #get the column length\n   Len = math.sqrt(sum(List))\n   #normalize the column\n   sa[:,Col] = (1/Len) * Column\n",
        "\nsa = sa.tocsc()  # convert the matrix to CSC format for efficient column access\nfor Col in range(sa.shape[1]):\n    Column = sa[:, Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    sa[:, Col] = (1 / Len) * Column  # update the original column of the matrix\n",
        "\na = np.where(a > 0, 1, 0)\n",
        "\na = np.where(a > 0, 1, 0)\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(dist_matrix, axis=0)\nresult = closest_indices\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(dist_matrix, axis=1)\nresult = data[closest_indices]\n",
        "\ndistances = scipy.spatial.distance.cdist(data, centroids)\nresult = np.argpartition(distances, k, axis=0)[:k]\n",
        "\nresult = []\nfor x, b in zip(xdata, bdata):\n    root = fsolve(eqn, x0=0.5, args=(x, b))\n    result.append(root)\n",
        "\nresult = []\nfor x, a in zip(xdata, adata):\n    def eqn(b):\n        return x + 2*a - b**2\n    root = fsolve(eqn, x0=0.5)\n    result.append(root)\nresult = np.array(result)\n",
        "\nresult = stats.kstest(sample_data, lambda x: integrate.quad(lambda y: bekkers(y, estimated_a, estimated_m, estimated_d), range_start, x)[0])\n",
        "\nresult = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d))\nresult = result.pvalue >= 0.05\n",
        "\nintegral_df = df.groupby(pd.Grouper(freq='5S')).apply(lambda x: integrate.trapz(x['A'], x.index))\n",
        "\nresult = scipy.interpolate.griddata(x, y, eval)\n",
        "\ndef multinomial_likelihood(weights):\n    probabilities = weights / np.sum(weights)\n    log_likelihood = np.sum(np.log(probabilities) * a['A1'])\n    return -log_likelihood\n\ninitial_weights = np.ones(12) / 12\nresult = sciopt.minimize(multinomial_likelihood, initial_weights, method='Nelder-Mead')\nweights = result.x\n",
        "\nresult = sciopt.minimize(e, x0=[0.5, 0.7], args=(x, y), bounds=[(0.5, 1.5), (0.7, 1.8)])\n",
        "\nresult = signal.argrelextrema(arr, np.less_equal, order=n)[0]\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if (j >= n and j < arr.shape[1] - n) and (arr[i, j] <= arr[i, j-n:j+n+1]).all():\n            result.append([i, j])\n",
        "\nnumeric_columns = df.select_dtypes(include=[np.number]).columns\ndf = df[(np.abs(stats.zscore(df[numeric_columns])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n    df = pd.DataFrame(data.data, columns=data.feature_names)\n    ",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.DataFrame(mlb.fit_transform(df['Col3']), columns=mlb.classes_)\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n",
        "\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.concat([df, pd.DataFrame(mlb.fit_transform(df['Col4']), columns=mlb.classes_)], axis=1)\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.DataFrame(mlb.fit_transform(df.iloc[:, -1]), columns=mlb.classes_)\ndf_out = pd.concat([df.iloc[:, :-1], df_out], axis=1)\n",
        "\ndf_out = pd.get_dummies(df.iloc[:, -1].apply(pd.Series).stack()).sum(level=0)\n",
        "\ncalibrated_model = CalibratedClassifierCV(svmmodel, cv=5, method='sigmoid')\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_test)\n",
        "\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n",
        "\ndf_transformed = pd.DataFrame(transform_output.toarray())\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\n    transform_output = transform_output.toarray()\n    df_transformed = pd.DataFrame(transform_output)\n    result = pd.concat([df, df_transformed], axis=1)\n    ",
        "\nsteps = clf.named_steps()\ndel steps['poly']\n",
        "\nclf.steps.pop(1)\n",
        "\nsteps = clf.named_steps\ndel steps['pOly']\n",
        "\nsteps = clf.named_steps()\ndel steps['poly']\n",
        "\nclf.steps.pop(1)\n",
        "\nsteps = clf.named_steps\ndel steps['pOly']\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY, **fit_params)\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test))\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test)[:, 1])\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nclf = GridSearchCV(bc, param_grid)\nclf.fit(X_train, y_train)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X.reshape(-1, 1), y)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X.reshape(-1, 1), y)\n",
        "\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\ndef prePro(text):\n    return text.lower()\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()] # Get the selected column names\n\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nselected_features = model.get_support()\ncolumn_names = X.columns[selected_features]\n",
        "\nselected_features = model.get_support(indices=True)\ncolumn_names = X.columns[selected_features]\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\ncolumn_names = X.columns[model.get_support()]\n",
        "\nkm.fit(X)\nclosest_50_samples = X[np.argsort(np.linalg.norm(X - km.cluster_centers_[p], axis=1))[:50]]\n",
        "\nfrom sklearn.cluster import KMeans\n\np, X = load_data()\nassert type(X) == np.ndarray\n\nkm = KMeans()\nkm.fit(X)\n\n# Get the cluster centers\ncenters = km.cluster_centers_\n\n# Get the distances between each sample and the p^th center\ndistances = np.linalg.norm(X - centers[p], axis=1)\n\n# Get the indices of the 50 samples closest to the p^th center\nclosest_indices = np.argsort(distances)[:50]\n\n# Get the 50 samples closest to the p^th center\nclosest_50_samples = X[closest_indices]\n\n",
        "\nkm.fit(X)\nclosest_100_samples = X[np.argsort(np.linalg.norm(X - km.cluster_centers_[p], axis=1))[:100]]\n",
        "\nfrom sklearn.cluster import KMeans\n\np, X = load_data()\nassert type(X) == np.ndarray\n\nkm = KMeans()\n\ndef get_samples(p, X, km):\n    km.fit(X)\n    cluster_centers = km.cluster_centers_\n    distances = np.linalg.norm(X - cluster_centers[p], axis=1)\n    closest_indices = np.argsort(distances)[:50]\n    closest_samples = X[closest_indices]\n    return closest_samples\n\nclosest_50_samples = get_samples(p, X, km)\n",
        "\nX_train = pd.get_dummies(X_train)\n",
        "\nX_train = pd.get_dummies(X_train)\n",
        "\nsvm = SVR(kernel='rbf')\nsvm.fit(X, y)\npredict = svm.predict(X)\n",
        "\nregressor = SVR(kernel='rbf')\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\n\n# fit, then predict X\nregressor = SVR(kernel='poly', degree=2)\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T).toarray()\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T)\n    return cosine_similarities_of_queries\ncosine_similarities_of_queries = solve(queries, documents)\n",
        "\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n",
        "\nnew_f = pd.get_dummies(pd.DataFrame(f).stack()).sum(level=0)\n",
        "\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n",
        "\n    new_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n    ",
        "\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(simM)\n",
        "\ndistance_matrix = 1 - np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\ndistance_matrix = 1 - np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\ndistance_matrix = 1 - np.array(simM)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b|['\\\"?!]\")\ntransformed_text = vectorizer.fit_transform([text])\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the dataset into training set and testing set\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=0)\n\n# Splitting the training set into x_train and y_train\nx_train = train_data.iloc[:, :-1]\ny_train = train_data.iloc[:, -1]\n\n# Splitting the testing set into x_test and y_test\nx_test = test_data.iloc[:, :-1]\ny_test = test_data.iloc[:, -1]\n",
        "\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the dataset into training set and testing set\ntrain_data, test_data = train_test_split(dataset, test_size=0.4, random_state=42)\n\n# Splitting the training set into x_train and y_train\nx_train = train_data.iloc[:, :-1]\ny_train = train_data.iloc[:, -1]\n\n# Splitting the testing set into x_test and y_test\nx_test = test_data.iloc[:, :-1]\ny_test = test_data.iloc[:, -1]\n",
        "\n    from sklearn.model_selection import train_test_split\n    \n    # Splitting the dataset into training set and testing set\n    train_data, test_data = train_test_split(data, test_size=0.2)\n    \n    # Splitting the training set into x_train and y_train\n    x_train = train_data.iloc[:, :-1]\n    y_train = train_data.iloc[:, -1]\n    \n    # Splitting the testing set into x_test and y_test\n    x_test = test_data.iloc[:, :-1]\n    y_test = test_data.iloc[:, -1]\n    \n    ",
        "\nf1 = df['mse'].values\nX = f1.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\nselector = LinearSVC(penalty='l1', dual=False)\nselector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.coef_[0] != 0]\n",
        "\nfeatureSelector = LinearSVC(penalty='l1', dual=False)\nfeatureSelector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.coef_[0] != 0]\n",
        "\n    featureSelector = LinearSVC(penalty='l1', dual=False)\n    featureSelector.fit(X, y)\n    selected_feature_indices = featureSelector.coef_.nonzero()[1]\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n    ",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nfrom sklearn.linear_model import LinearRegression\n\ndf1 = load_data()\n\nslopes = np.array([]) # blank array to store slopes\n\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] # removes NaN values for each column to apply sklearn function\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis=0)\n\n",
        "\nfrom sklearn.linear_model import LinearRegression\n\ndf1 = load_data()\n\nslopes = []\nfor col in df1.columns:\n    if col != 'Time':\n        df2 = df1[~np.isnan(df1[col])]\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        slopes.append(m)\n\n",
        "\nlabel_encoder = LabelEncoder()\ndf['Sex'] = label_encoder.fit_transform(df['Sex'])\ntransformed_df = df['Sex']\n",
        "\nlabel_encoder = LabelEncoder()\ndf['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n    label_encoder = LabelEncoder()\n    transformed_df = df.copy()\n    transformed_df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    ",
        "\nElasticNet = linear_model.ElasticNet() # create an ElasticNet instance\nElasticNet.fit(X_train, y_train) # fit data\ntraining_set_score = ElasticNet.score(X_train, y_train) # calculate R^2 for training set\ntest_set_score = ElasticNet.score(X_test, y_test) # calculate R^2 for test set\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a.reshape(-1, 1)).reshape(a.shape)\n    ",
        "\npredict = clf.predict(b)\n",
        "\nnew_X = np.array(X)\n",
        "\nnew_X = np.array(X)\n",
        "\nnew_X = np.array(X)\n",
        "\nX = dataframe.iloc[:, 1:-1].astype(float)\ny = dataframe.iloc[:, -1]\n",
        "\nX = dataframe.iloc[:, 1:-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n",
        "\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n",
        "\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=0.8, shuffle=False)\n    ",
        "\ncols = ['X2', 'X3']\ndf[cols + '_scale'] = df.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncols = myData.columns[2:4]\nmyData[['new_A2', 'new_A3']] = myData.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_).sort_values(by='mean_fit_time')\n",
        "\njoblib.dump(fitted_model, \"sklearn_model\")\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndf = load_data()\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n"
    ],
    "Pytorch": [
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\nembedding_weights = torch.FloatTensor(word2vec.wv.vectors)\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights)\nembedded_input = embedding_layer(input_Tensor)\n",
        "\n    weights = torch.FloatTensor(word2vec.wv.vectors)\n    embedding = torch.nn.Embedding.from_pretrained(weights)\n    embedded_input = embedding(input_Tensor)\n    ",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_logical]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\n    C = B[:, A_log.nonzero().squeeze()]\n    ",
        "\nC = B[:, A_log]\n",
        "\nC = B.index_select(1, idx)\n",
        "\nx_tensor = torch.from_numpy(np.array(x_array.tolist()))\n",
        "\nx_tensor = torch.tensor(x_array)\n",
        "\n\nx_array = load_data()\n\ndef Convert(a):\n    t = torch.from_numpy(np.array(a.tolist()))\n    return t\n\nx_tensor = Convert(x_array)\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nmask = torch.zeros(len(lens), max(lens))\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\n\nlens = load_data()\n\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\n\nmask = get_mask(lens)\n",
        "\nTensor_3D = torch.unsqueeze(Tensor_2D, dim=2)\n",
        "\n    result = torch.diag_embed(t)\n    ",
        "\nab = torch.cat((a, b.unsqueeze(0)), dim=0)\n",
        "\nab = torch.cat((a, b), dim=0)\n",
        "\n    if a.shape[0] == 1:\n        ab = torch.stack((a, b), 0)\n    else:\n        ab = torch.cat((a, b.unsqueeze(0)), 0)\n    ",
        "\nfor i in range(a.size(0)):\n    a[i, lengths[i]:, :] = 0\n",
        "\na[:, lengths.long():, :] = 2333\n",
        "\nfor i in range(a.size(0)):\n    a[i, :int(lengths[i]), :] = 0\n",
        "\nfor i in range(len(lengths)):\n    a[:, :int(lengths[i]), :] = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n    ",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, ids.unsqueeze(2)).squeeze(1)\n",
        "\n_, y = torch.max(softmax_output, dim=1, keepdim=True)\n",
        "\n_, y = torch.max(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.max(softmax_output, dim=1, keepdim=True)\n",
        "\n    _, y = torch.max(softmax_output, dim=1)\n    ",
        "\n    _, y = torch.max(softmax_output, dim=1)\n    ",
        "\nn, c, w, z = images.size()\nlog_p = F.log_softmax(images, dim=1)\nlog_p = log_p.permute(0, 2, 3, 1).contiguous().view(-1, c)\nlog_p = log_p[labels.view(n, w, z, 1).repeat(1, 1, 1, c) >= 0]\nlog_p = log_p.view(-1, c)\n\nmask = labels >= 0\nlabels = labels[mask]\n\nloss = F.nll_loss(log_p, labels.view(-1))\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = np.count_nonzero(A != B)\n",
        "\n\nA, B = load_data()\n\ndef Count(A, B):\n    cnt_equal = np.sum(A == B)\n    return cnt_equal\n\ncnt_equal = Count(A, B)\n",
        "\nx = A.shape[0] // 2\ncnt_equal = np.sum(A[-x:] == B[-x:])\n",
        "\nx = A.shape[0] // 2\ncnt_not_equal = np.sum(A[-x:] != B[-x:])\n",
        "\na_split = torch.split(a, chunk_dim, dim=3)\ntensors_31 = [a_split[i] for i in range(len(a_split))]\n",
        "\na_split = torch.chunk(a, a.shape[2] - chunk_dim + 1, dim=2)\ntensors_31 = [a_split[i] for i in range(a.shape[2] - chunk_dim + 1)]\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = sign_x * min\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = sign_x * max\n",
        "\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_values = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = sign_x * min_values\n    ",
        "\noutput = MyNet(input)\nconfidence_score = torch.nn.functional.softmax(output, dim=1)\n",
        "\nresult = torch.cat((a[:, :-1], (a[:, -1] + b[:, 0]) / 2, b[:, 1:]), dim=1)\n",
        "\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\ndef solve(a, b):\n    # Get the number of rows in the tensors\n    num_rows = a.shape[0]\n    \n    # Get the number of columns in each tensor\n    num_cols_a = a.shape[1]\n    num_cols_b = b.shape[1]\n    \n    # Create a new tensor with the correct shape\n    result = torch.zeros((num_rows, num_cols_a + num_cols_b - 1))\n    \n    # Copy the first columns of 'a' to the result tensor\n    result[:, :num_cols_a] = a\n    \n    # Copy the last columns of 'b' to the result tensor\n    result[:, -num_cols_b:] = b\n    \n    # Calculate the average of the overlapping column\n    avg_column = (a[:, -1] + b[:, 0]) / 2\n    \n    # Copy the average column to the result tensor\n    result[:, num_cols_a-1] = avg_column\n    \n    return result\n\nresult = solve(a, b)\n",
        "\nnew = torch.zeros((t.shape[0]+2, t.shape[1], t.shape[2]))\nnew[1:-1, :, :] = t\n",
        "\nnew = torch.zeros((t.shape[0]+2, t.shape[1]+2))\nnew[1:-1, 1:-1] = t\nresult = new\n",
        "\nnew = torch.tensor([[-1, -1, -1, -1]])\nresult = torch.cat((new, t, new), dim=1)\n",
        "\nresult = torch.matmul(data, W.unsqueeze(1)).squeeze()\n"
    ]
}