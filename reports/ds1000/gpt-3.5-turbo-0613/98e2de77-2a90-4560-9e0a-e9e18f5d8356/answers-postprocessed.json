{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\ndf = df.iloc[List]\nresult = len(df[df['Type'] != df.reset_index()['index']])\n",
        "\nfor column in df.columns:\n    value_counts = df[column].value_counts()\n    values_to_change = value_counts[value_counts >= 2].index.tolist()\n    df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n",
        "\nfor column in df.columns:\n    counts = df[column].value_counts()\n    values_to_change = counts[counts >= 3].index.tolist()\n    df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n",
        "\n    for column in df.columns:\n        value_counts = df[column].value_counts()\n        values_to_change = value_counts[value_counts >= 2].index.tolist()\n        df[column] = df[column].apply(lambda x: x if x in values_to_change else 'other')\n",
        "\nvalue_counts_qu1 = pd.value_counts(df.Qu1)\nvalue_counts_qu2 = pd.value_counts(df.Qu2)\nvalue_counts_qu3 = pd.value_counts(df.Qu3)\n\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if value_counts_qu1[x] < 3 else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if value_counts_qu2[x] < 2 else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if value_counts_qu3[x] < 2 else x)\n\nresult = df\n",
        "\nvalue_counts_qu1 = pd.value_counts(df.Qu1)\nvalue_counts_qu2 = pd.value_counts(df.Qu2)\nvalue_counts_qu3 = pd.value_counts(df.Qu3)\n\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x not in value_counts_qu1[value_counts_qu1 >= 3].index and x != 'apple' else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if x not in value_counts_qu2[value_counts_qu2 >= 2].index else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x not in value_counts_qu3[value_counts_qu3 >= 2].index else x)\n\nresult = df\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result.append(df[df['keep_if_dup'] == 'Yes'])\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first') | (df['drop_if_dup'] == 'No')\n",
        "\nresult = df.drop_duplicates(subset='url', keep='last')\nresult = result[result['keep_if_dup'] == 'Yes']\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\n\nresult = {}\nfor index, row in df.iterrows():\n    current_dict = result\n    for col in df.columns[:-1]:\n        value = row[col]\n        if value not in current_dict:\n            current_dict[value] = {}\n        current_dict = current_dict[value]\n    current_dict[df.columns[-1]] = row[df.columns[-1]]\n\nprint(result)\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\ndef extract_key_value_pairs(message):\n    pairs = re.findall(r'(\\w+):\\s*(\\w+|none)', message)\n    return dict(pairs)\n\ndf['message'] = df['message'].apply(extract_key_value_pairs)\nresult = pd.concat([df.drop('message', axis=1), df['message'].apply(pd.Series)], axis=1)\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\ndf.loc[~df['product'].isin(products), 'score'] = df.loc[~df['product'].isin(products), 'score'] * 10\n",
        "\ndf.loc[df['product'].isin(products[0] + products[1]), 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'].apply(lambda x: (x - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min()))\n",
        "\ndf['category'] = df.idxmax(axis=1)\n",
        "\ndf['category'] = df.apply(lambda row: row.idxmax(), axis=1)\n",
        "\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\nmask = (df['Date'] >= '2019-01-17') & (df['Date'] <= '2019-02-20')\nresult = df.loc[mask, 'Date'].dt.strftime('%d-%b-%Y %A')\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf.iloc[0]['#1'] = df.iloc[-1]['#1']\n",
        "\ndf = df.shift(-1)\ndf.iloc[-1] = df.iloc[0]\n",
        "\ndf = df.shift(1, axis=0)\ndf.iloc[0] = df.iloc[-1]\ndf = df.shift(-1, axis=1)\n",
        "\ndf['#1'] = df['#1'].shift(-1)\ndf.iloc[-1, 0] = df.iloc[0, 0]\n",
        "\ndf.rename(columns=lambda x: x + 'X', inplace=True)\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = [col + 'X' if not col.endswith('X') else 'X' + col for col in df.columns]\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\"})\nresult = result.join(df.filter(like='val').mean())\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\"})\nresult = result.join(df.filter(like='val').sum())\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\"})\nfor column in df.columns:\n    if column.startswith('val') and column.endswith('2'):\n        result[column] = df.groupby('group')[column].mean()\n    else:\n        result[column] = df.groupby('group')[column].sum()\n",
        "\nresult = df.loc[row_list, column_list].mean()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.loc[row_list, column_list].sum()\nresult = result[result < result.max()]\n",
        "\nresult = df.apply(pd.Series.value_counts).stack().astype(float)\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor column in df.columns:\n    result += \"---- \" + column + \" ---\\n\"\n    result += str(df[column].value_counts()) + \"\\n\"\nprint(result)\n",
        "\nresult = df.iloc[0:1].append(df.iloc[1:2], ignore_index=True)\n",
        "\nresult = df.iloc[0:1].append(df.iloc[1:2], ignore_index=True)\n",
        "\nresult = df.ffill(axis=1).bfill(axis=1)\n",
        "\nresult = df.ffill(axis=1).bfill(axis=1)\n",
        "\nresult = df.fillna(method='ffill')\n",
        "\ndf.loc[df['value'] < thresh] = df.loc[df['value'] < thresh].sum()\n",
        "\ndf.loc['X'] = df.loc[df['value'] >= thresh].mean()\ndf = df.loc[df['value'] < thresh]\n",
        "\ndf.loc[(df['value'] < section_left) | (df['value'] > section_right), 'lab'] = 'X'\ndf = df.groupby('lab').mean()\n",
        "\nresult = df.copy()\nresult.columns = result.columns + \"_inv\"\nresult = pd.concat([df, 1/df], axis=1)\n",
        "\nresult = pd.concat([df, np.exp(df)], axis=1)\nresult.columns = df.columns.tolist() + ['exp_' + col for col in df.columns]\n",
        "\nresult = df.copy()\nresult = result.join(1 / result, rsuffix='_inv')\n",
        "\nresult = df.copy()\nresult = pd.concat([result, pd.DataFrame(np.exp(-result[['A', 'B']]).apply(lambda x: 1/(1+x)).values, columns=['sigmoid_A', 'sigmoid_B'])], axis=1)\n",
        "\nmin_idx = df.idxmin()\nresult = df.apply(lambda x: x[:min_idx[x.name]].idxmax())\n",
        "\nmin_idx = df.idxmin()\nresult = df.idxmax().where(df.columns != min_idx)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\ndf_expanded = pd.DataFrame({'dt': date_range})\ndf_expanded = df_expanded.merge(df, on='dt', how='left').fillna(0)\nresult = df_expanded[['dt', 'user', 'val']]\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nall_dates = pd.date_range(start=min_date, end=max_date)\nresult = df.set_index(['user', 'dt']).reindex(pd.MultiIndex.from_product([df['user'].unique(), all_dates], names=['user', 'dt'])).reset_index()\nresult['val'] = result['val'].fillna(0)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\nall_dates = pd.date_range(start=min_date, end=max_date)\nexpanded_df = pd.DataFrame({'dt': all_dates, 'user': df['user'].unique(), 'val': 233})\nresult = pd.concat([df, expanded_df]).sort_values(by=['user', 'dt']).reset_index(drop=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nusers = df['user'].unique()\ndates = pd.date_range(start=min_date, end=max_date)\nresult = pd.DataFrame({'dt': dates.repeat(len(users)), 'user': users.tolist() * len(dates)})\nresult = result.merge(df.groupby('user')['val'].max().reset_index(), on='user')\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nusers = df['user'].unique()\ndates = pd.date_range(start=min_date, end=max_date)\nexpanded_df = pd.DataFrame({'dt': dates.repeat(len(users)), 'user': users.tolist() * len(dates)})\nresult = pd.merge(expanded_df, df, on=['dt', 'user'], how='left').fillna(method='ffill')\n",
        "\ndf['name'] = pd.factorize(df['name'])[0] + 1\n",
        "\ndf['a'] = df.groupby('name').ngroup() + 1\n",
        "\n    df['name'] = pd.factorize(df['name'])[0] + 1\n",
        "\ndf['ID'] = df.groupby('name').ngroup() + 1\ndf.drop('name', axis=1, inplace=True)\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n\nresult = pd.melt(df, id_vars=['user'], var_name='others', value_name='value')\nprint(result)\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\nresult = df.loc[df['c'] > 0.5, columns]\n",
        "\nresult = df.loc[df['c'] > 0.45, columns]\n",
        "\n    result = df[df['c'] > 0.5][columns].values\n",
        "\n    result = df[df['c'] > 0.5][columns]\n    result['sum'] = result.sum(axis=1)\n",
        "\n    result = df[df['c'] > 0.5][columns]\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['diff'] = df['date'].diff().dt.days\ndf = df[df['diff'] > X]\nresult = df.drop(columns='diff')\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['date_diff'] = df['date'].diff()\ndf = df[df['date_diff'] > timedelta(weeks=X)]\nresult = df.drop(columns=['date_diff'])\n",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 145.99, 146.73, 171.10]})\n\nX = 52\n\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort dataframe by date\ndf = df.sort_values('date')\n\n# Initialize a list to store the filtered rows\nfiltered_rows = []\n\n# Iterate over each row in the dataframe\nfor index, row in df.iterrows():\n    # Check if the current row is the first row or if it is not within X weeks of the previous row\n    if index == 0 or (row['date'] - df.loc[index-1, 'date']).days > X*7:\n        # Add the current row to the filtered rows list\n        filtered_rows.append(row)\n\n# Create a new dataframe with the filtered rows\nresult = pd.DataFrame(filtered_rows)\n\n# Convert date column back to string format\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\n\n# Reset the index of the dataframe\nresult = result.reset_index(drop=True)\n\nprint(result)\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 4).sum()\n",
        "\nresult = df[::-1].rolling(3).mean()[::-1].iloc[2::3]\n",
        "\nresult = []\ni = 0\nwhile i < len(df):\n    if i % 5 < 3:\n        result.append(df.loc[i:i+2, 'col1'].sum())\n        i += 3\n    else:\n        result.append(df.loc[i:i+1, 'col1'].mean())\n        i += 2\n",
        "\nresult = pd.concat([df.groupby(df.index // 3)['col1'].sum(), df.groupby(df.index // 2)['col1'].mean()]).sort_index().reset_index(drop=True)\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, pd.NA).fillna(method='ffill').fillna(method='bfill')\n",
        "\ndf['number'] = df['duration'].str.extract('(\\d+)')\ndf['time'] = df['duration'].str.extract('(\\D+)')\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\ndf['numer'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ndf['time_day'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\n    df['number'] = df['duration'].str.extract('(\\d+)')\n    df['time'] = df['duration'].str.extract('(\\D+)')\n    df['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\n",
        "\nresult = np.where(np.any(df1[columns_check_list] != df2[columns_check_list], axis=1), True, False)\n",
        "\nresult = np.where(np.all(df1[columns_check_list] == df2[columns_check_list], axis=1), True, False)\n",
        "\ndf.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1, inplace=True)\n",
        "\ndf.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1, inplace=True)\n",
        "\n    df.reset_index(inplace=True)\n    df['date'] = pd.to_datetime(df['date'])\n    output = df.values\n",
        "\n    df.index = pd.to_datetime(df.index.get_level_values(0))\n    df = df.swaplevel().sort_index()\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\nresult = df.pivot(index=['Variable', 'Country', 'year'], columns='Variable', values='value').reset_index()\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(by=['year'], ascending=False)\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nresult = df[abs(df[columns]) < 1].dropna()\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nresult = df[abs(df[columns]) > 1].dropna()\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value_')]\ndf = df[df[columns].abs() > 1].dropna()\ndf.columns = [col.replace('Value_', '') for col in df.columns]\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT', '<', regex=True)\n",
        "\n    df = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[-1] if ' ' in x else None)\ndf.loc[df['first_name'].notnull(), 'first_name'] = df.loc[df['first_name'].notnull(), 'first_name'].apply(lambda x: x.split(' ')[0])\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].str.split(' ').str[1]\ndf.loc[df['1_name'].notnull(), '1_name'] = df['1_name'].str.split(' ').str[0]\ndf.loc[df['1_name'].isnull(), '1_name'] = df['name']\ndf = df.drop(columns=['name'])\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first name'] = df['name'].apply(lambda x: x.split(' ')[0] if validate_single_space_name(x) else x)\ndf['middle_name'] = df['name'].apply(lambda x: x.split(' ')[1] if validate_single_space_name(x) else None)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[-1] if validate_single_space_name(x) else None)\ndf.drop('name', axis=1, inplace=True)\n",
        "\nresult = pd.merge(df2, df1, on='Timestamp', how='left')\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n",
        "\nerror_values = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        error_values.append(row['Field1'])\n",
        "\nresult = []\nfor index, row in df.iterrows():\n    if isinstance(row['Field1'], int):\n        result.append(row['Field1'])\n",
        "\n    result = []\n    for index, row in df.iterrows():\n        if not isinstance(row['Field1'], int):\n            result.append(row['Field1'])\n",
        "\ntotal = df.sum(axis=1)\ndf.iloc[:, 1:] = df.iloc[:, 1:].div(total, axis=0)\n",
        "\ntotal = df.sum(axis=0)\ndf.iloc[:, 1:] = df.iloc[:, 1:].div(total)\n",
        "# [Missing Code]\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\n",
        "\n    result = df.loc[test].drop_duplicates()\n",
        "\nimport pandas as pd\nfrom scipy.spatial import distance\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Begin of Missing Code\ndf2 = pd.DataFrame(columns=['car', 'nearest_neighbour', 'euclidean_distance'])\nfor t in df['time'].unique():\n    df_t = df[df['time'] == t]\n    for i, row in df_t.iterrows():\n        car = row['car']\n        x = row['x']\n        y = row['y']\n        df_t['distance'] = df_t.apply(lambda r: distance.euclidean((x, y), (r['x'], r['y'])), axis=1)\n        df_t_sorted = df_t.sort_values(by='distance')\n        nearest_neighbour = df_t_sorted.iloc[1]['car']\n        euclidean_distance = df_t_sorted.iloc[1]['distance']\n        df2 = df2.append({'car': car, 'nearest_neighbour': nearest_neighbour, 'euclidean_distance': euclidean_distance}, ignore_index=True)\n# End of Missing Code\n\nresult = df2\nprint(result)\n",
        "\nimport pandas as pd\nfrom scipy.spatial import distance\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Begin of Missing Code\ndf2 = pd.DataFrame(columns=['time', 'car', 'farmost_neighbour', 'euclidean_distance'])\n\nfor t in df['time'].unique():\n    df_t = df[df['time'] == t]\n    for c in df_t['car'].unique():\n        df_c = df_t[df_t['car'] == c]\n        distances = distance.cdist(df_c[['x', 'y']], df_t[df_t['car'] != c][['x', 'y']], metric='euclidean')\n        max_distance = distances.max()\n        max_distance_index = distances.argmax()\n        farmost_neighbour = df_t[df_t['car'] != c].iloc[max_distance_index]['car']\n        df2 = df2.append({'time': t, 'car': c, 'farmost_neighbour': farmost_neighbour, 'euclidean_distance': max_distance}, ignore_index=True)\n\ndf2['euclidean_distance'] = df2['euclidean_distance'].round(6)\n# End of Missing Code\n\nresult = df2\nprint(result)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \",\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \"-\".join(row.dropna()), axis=1)\n",
        "\ndf_altered = df.sample(frac=0.2, random_state=0)\ndf_altered['Quantity'] = 0\ndf.update(df_altered)\n",
        "\ndf_altered = df.sample(frac=0.2, random_state=0)\ndf_altered['ProductId'] = 0\ndf.update(df_altered)\n",
        "\ndf['Quantity'] = df.groupby('UserId')['Quantity'].apply(lambda x: x.sample(frac=0.2, random_state=0).replace(x, 0))\n",
        "\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int).cumsum() - 1\n",
        "\ndf['index_original'] = df.index\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    df['index_original'] = df.index\n    result = df.loc[duplicate_bool == True]\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\ndf['index_original'] = duplicate_bool.astype(int)\n",
        "\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').map({True: df.index[-1]})\n",
        "\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\n",
        "\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\nresult = df.groupby(['Sp','Value']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.query(\"Category in @filter_list\")\n",
        "\nresult = df.query(\"Category != @filter_list\")\n",
        "\nresult = pd.melt(df, value_vars=[tuple(level) for level in df.columns])\n",
        "\nresult = pd.melt(df, value_vars=[tuple(col) for col in df.columns])\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].apply(lambda x: 0 if x < 0 else x)\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('r')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = []\ncolumns = df.columns\n\nfor i in range(len(columns)):\n    for j in range(len(columns)):\n        if i != j:\n            if df[columns[i]].nunique() == df[columns[j]].nunique():\n                result.append(f\"{columns[i]} {columns[j]} many-to-many\")\n            elif df[columns[i]].nunique() > df[columns[j]].nunique():\n                result.append(f\"{columns[i]} {columns[j]} one-to-many\")\n            else:\n                result.append(f\"{columns[i]} {columns[j]} many-to-one\")\n        else:\n            result.append(f\"{columns[i]} {columns[j]} one-to-one\")\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].nunique() == df[col2].nunique():\n                result.append(f'{col1} {col2} many-2-many')\n            elif df[col1].nunique() > df[col2].nunique():\n                result.append(f'{col1} {col2} one-2-many')\n            else:\n                result.append(f'{col1} {col2} many-2-one')\n        else:\n            result.append(f'{col1} {col2} one-2-one')\n",
        "\nresult = df.corr().applymap(lambda x: 'many-to-many' if x > 0.7 else 'one-to-many' if x > 0.3 else 'many-to-one' if x < -0.3 else 'one-to-one')\n",
        "\nresult = df.corr().applymap(lambda x: 'one-2-one' if x == 1 else 'one-2-many' if x == -1 else 'many-2-one' if x == 0 else 'many-2-many')\n",
        "\ndf = df.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last')\n",
        "\nresult = pd.to_numeric(s.astype(str).str.replace(',',''), errors='coerce')\n",
        "\nresult = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0))['Survived'].mean()\n",
        "\nresult = df.groupby(((df['Survived'] > 0) | (df['Parch'] > 0)).map({True: 'Has Family', False: 'No Family'}))['SibSp'].mean()\n",
        "\nresult = df.groupby([(df['SibSp'] == 1) & (df['Parch'] == 1), \n                     (df['SibSp'] == 0) & (df['Parch'] == 0), \n                     (df['SibSp'] == 0) & (df['Parch'] == 1), \n                     (df['SibSp'] == 1) & (df['Parch'] == 0)])['Survived'].mean()\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\nimport numpy as np\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\nstdMeann = lambda x: pd.Series([np.mean(x), np.std(x)], index=['mean', 'std'])\nresult = df.groupby('a')['b'].apply(stdMeann)\n\nprint(result)\n",
        "\nimport numpy as np\n\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = df.groupby('b').a.apply(stdMeann)\n\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\n# Begin of Missing Code\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: np.exp(x) / np.sum(np.exp(x)))\ndf['min-max'] = df.groupby('a')['b'].transform(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n# End of Missing Code\n\nresult = df\nprint(result)\n",
        "\nresult = df.loc[:, (df != 0).any(axis=0)].loc[(df != 0).any(axis=1)]\n",
        "\nresult = df.loc[(df.sum(axis=1) != 0), (df.sum(axis=0) != 0)]\n",
        "\nresult = df.loc[(df.max(axis=1) <= 2), (df.max(axis=0) <= 2)]\n",
        "\ndf[df > 2] = 0\nresult = df\n",
        "\nresult = s.sort_values(ascending=True)\n",
        "\ndf = pd.DataFrame({'index': s.index, '1': s.values})\ndf = df.sort_values(by=['1', 'index'])\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\n",
        "\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\nresult = df.groupby(['Sp','Value']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Date'])\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Date'])\n",
        "\n    df['Date'] = df['Member'].map(dict)\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Val'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Val'].transform('count')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Date'].transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val'])['Date'].transform('count')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Date'].transform('count')\ndf['Count_w'] = df.groupby(df['Date'].dt.weekday.rename('weekday'))['Date'].transform('count')\ndf['Count_Val'] = df.groupby('Val')['Date'].transform('count')\n",
        "\nresult1 = df[df == 0].groupby('Date').count()\nresult2 = df[df != 0].groupby('Date').count()\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: x[['B', 'C']].apply(lambda y: y % 2 == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: x[['B', 'C']].apply(lambda y: y % 2 != 0).sum())\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D'], index=['B'], aggfunc=np.sum)\nresult['E'] = pd.pivot_table(df, values=['E'], index=['B'], aggfunc=np.mean)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=1)\n\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2')\n\nresult = result.compute()\n",
        "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=1)\n\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2').reset_index(drop=True)\n",
        "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=1)\n\nresult = df.assign(var2=df.var2.str.split('-')).explode('var2').reset_index(drop=True)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\nprint(result)\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'medi', 'row']] = df['row'].str.split(' ', 2, expand=True)\n",
        "\ndf = df.set_index('Name')  # Set 'Name' column as the index\ndf = df.replace(0, pd.NA)  # Replace 0 with NaN\ndf = df.cumsum(axis=1)  # Calculate cumulative sum along each row\ndf = df.div(df.count(axis=1), axis=0)  # Calculate average by dividing by the count of non-null values along each row\ndf = df.fillna(0)  # Replace NaN with 0\ndf = df.reset_index()  # Reset the index to default\n",
        "\ndf = df.set_index('Name')  # Set 'Name' column as the index\ndf = df.apply(lambda x: x[::-1].cumsum()[::-1].mask(x == 0, 0))  # Calculate cumulative sum from end to head and replace 0s with 0\ndf = df.reset_index()  # Reset the index to default\n",
        "\n    result = df.copy()\n    result.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x[x != 0].cumsum() / (x != 0).cumsum(), axis=1)\n",
        "\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.replace(0, pd.NA))\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.cumsum().div(x.notna().cumsum()))\n",
        "\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\ndf.loc[0, 'Label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = (df['arrival_time'].shift(-1) - df['departure_time']).dt.total_seconds()\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['Duration'] = pd.to_datetime(df['departure_time'].shift(-1)) - pd.to_datetime(df['arrival_time'])\ndf['Duration'] = df['Duration'].dt.total_seconds()\n",
        "\nresult = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\n",
        "\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n",
        "\nresult = df[df['key2'].str.endswith(\"e\")].groupby('key1').size().reset_index(name='count')\n",
        "\nmin_result = df.index.min()\nmax_result = df.index.max()\n",
        "\nmode_result = df.index.mode()[0]\nmedian_result = statistics.median(df.index)\n",
        "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n",
        "\nresult = df.loc[df.groupby(\"item\")[\"diff\"].idxmin()]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n",
        "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1).str[0]\n",
        "\nimport math\n\n# Calculate the number of NaN values to fill with '0' and '1'\nnum_nan = df['Column_x'].isnull().sum()\nnum_fill_0 = math.floor(num_nan / 2)\nnum_fill_1 = math.ceil(num_nan / 2)\n\n# Fill the first half of NaN values with '0'\ndf['Column_x'].fillna(0, limit=num_fill_0, inplace=True)\n\n# Fill the remaining NaN values with '1'\ndf['Column_x'].fillna(1, inplace=True)\n\nresult = df\nprint(result)\n",
        "\nnum_nan = df['Column_x'].isnull().sum()\nnum_fill_0 = int(num_nan * 0.3)\nnum_fill_05 = int(num_nan * 0.3)\nnum_fill_1 = num_nan - num_fill_0 - num_fill_05\n\ndf['Column_x'].fillna(0, limit=num_fill_0, inplace=True)\ndf['Column_x'].fillna(0.5, limit=num_fill_05, inplace=True)\ndf['Column_x'].fillna(1, limit=num_fill_1, inplace=True)\n",
        "\nnum_zeros = int(df['Column_x'].isnull().sum() / 2)\nnum_ones = int(df['Column_x'].isnull().sum() / 2)\n\ndf['Column_x'].fillna(0, limit=num_zeros, inplace=True)\ndf['Column_x'].fillna(1, limit=num_ones, inplace=True)\n",
        "\nresult = pd.DataFrame(np.array(list(zip(a.values.flatten(), b.values.flatten()))).reshape(a.shape), columns=a.columns)\n",
        "\nresult = pd.concat([a, b, c], axis=1).apply(tuple, axis=1).to_frame()\nresult.columns = ['one', 'two']\n",
        "\nresult = pd.concat([a, b], axis=1).apply(tuple, axis=1)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.reset_index(drop=True)\nresult.loc[result['id'].duplicated(), ['city', 'district']] = None\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)\nresult = result.fillna('')\nresult = result[['id', 'city', 'district', 'date', 'value']]\n",
        "\nresult = pd.concat([df1, df2], axis=0).sort_values(['id', 'date']).reset_index(drop=True)\nresult[['city', 'district']] = result.groupby('id')[['city', 'district']].ffill()\nprint(result)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_x'].update(result['B_y'])\nresult = result.drop(columns=['B_y'])\nresult = result.rename(columns={'B_x': 'B'})\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_x'].fillna(result['B_y'], inplace=True)\nresult.drop(columns=['B_y'], inplace=True)\nresult.rename(columns={'B_x': 'B'}, inplace=True)\n",
        "\nresult = pd.merge(C, D, how='left', on='A').fillna(C)\nresult['dulplicated'] = result['B_x'] != result['B_y']\nresult['B'] = result['B_y'].fillna(result['B_x'])\nresult = result[['A', 'B', 'dulplicated']]\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['amount', 'time']].values.tolist()[::-1])\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index)\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index, columns=[0, 1, 2, 3])\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = df.columns[df.columns.str.contains(s) & ~df.columns.str.match(s)].tolist()\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = df['codes'].apply(pd.Series)\n",
        "\nresult = df['codes'].apply(pd.Series)\n",
        "\nresult = df['codes'].apply(pd.Series)\n",
        "\nresult = [item for sublist in df['col1'] for item in sublist]\n",
        "\nresult = ','.join(','.join(map(str, lst[::-1])) for lst in df['col1'])\n",
        "\nresult = ','.join([str(item) for sublist in df['col1'] for item in sublist])\n",
        "\ndf['Time'] = df['Time'].dt.floor('2min')\ndf = df.groupby('Time').mean().reset_index()\n",
        "\ndf['Time'] = df['Time'].dt.floor('3min')\ndf = df.groupby('Time').sum().reset_index()\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert 'TIME' column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)  # Add 'RANK' column based on the ranking of 'TIME' within each 'ID' group\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert 'TIME' column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)  # Add 'RANK' column based on 'TIME' column and grouped by 'ID'\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])  # Convert TIME column to datetime format\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)  # Add RANK column by ranking TIME for each ID\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')  # Format TIME column as desired\n\nresult = df\nprint(result)\n",
        "\nresult = df[filt[df.index.get_level_values('a')]]\n",
        "\nresult = df[filt[df.index.get_level_values('a')]]\n",
        "\nresult = df.loc[[0, 8]].isnull().any()\n",
        "\nresult = df.columns[(df.iloc[0] == df.iloc[8]) | (df.iloc[0].isnull() & df.iloc[8].isnull())]\n",
        "\nresult = df.columns[df.iloc[[0, 8]].isnull().any()]\n",
        "\nresult = []\nfor col in df.columns:\n    if df.loc[0, col] != df.loc[8, col]:\n        result.append((df.loc[0, col], df.loc[8, col]))\n",
        "\nts = df.set_index('Date')['Value']\n",
        "\nresult = df.stack().reset_index(drop=True)\n",
        "\nresult = df.stack().reset_index(drop=True).to_frame().T\nresult.columns = [f'{col}_{i}' for i, col in enumerate(result.columns)]\n",
        "\ndf['dogs'] = df['dogs'].round(2)\n",
        "\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\nresult = df.sort_index(level='time')\n",
        "\nresult = df.sort_values(by=['VIM', 'time'], ascending=[True, True])\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\nhd2_from = '2020-02-18 15:30:00'\nhd2_till = '2020-02-18 21:59:00'\n\nresult = df[(df.index < hd1_from) | (df.index > hd1_till)]\nresult = result[(result.index < hd2_from) | (result.index > hd2_till)]\n\nprint(result)\n",
        "\nresult = df.copy()\nresult['Day of Week'] = result.index.strftime('%d-%b-%Y %A')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 0', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 1', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 2', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 3', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 4', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 5', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace(' 6', ' ')\nresult['Day of Week'] = result['Day of Week'].str.replace('Jan', 'January')\nresult['Day of Week'] = result['Day of Week'].str.replace('Feb', 'February')\nresult['Day of Week'] = result['Day of Week'].str.replace('Mar', 'March')\nresult['Day of Week'] = result['Day of Week'].str.replace('Apr', 'April')\nresult['Day of Week'] = result['Day of Week'].str.replace('May', 'May')\nresult['Day of Week'] = result['Day of Week'].str.replace('Jun', 'June')\nresult['Day of Week'] = result['Day of Week'].str.replace('Jul', 'July')\nresult['Day of Week'] = result['Day of Week'].str.replace('Aug', 'August')\nresult['Day of Week'] = result['Day of Week'].str.replace('Sep', 'September')\nresult['Day of Week'] = result['Day of Week'].str.replace('Oct', 'October')\nresult['Day of Week'] = result['Day of Week'].str.replace('Nov', 'November')\nresult['Day of Week'] = result['Day of Week'].str.replace('Dec', 'December')\n",
        "\nresult = corr[corr > 0.3].stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\n",
        "\nresult = corr[corr > 0.3].stack().reset_index().iloc[:, [0, 2]]\nresult.columns = [0, 3]\nresult = result.set_index([0, 3]).squeeze()\n",
        "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\n",
        "\ndf.columns = ['Test'] + list(df.columns[1:])\n",
        "\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n",
        "\ndf['frequent'] = df.mode(axis=1).iloc[:, 0]\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n",
        "\nfrequent_values = df.apply(lambda row: row.value_counts().index.tolist(), axis=1)\nfreq_count = df.apply(lambda row: row.value_counts().values.tolist(), axis=1)\ndf['frequent'] = frequent_values\ndf['freq_count'] = freq_count\n",
        "\nresult = df.groupby([\"id1\",\"id2\"]).mean().reset_index()\n",
        "\nresult = df.replace('NULL', 0).groupby([\"id1\",\"id2\"]).mean()\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum')\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum', 'b_col']], on='EntityNum')\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nresult = [list(filter(lambda x: not np.isnan(x), sublist)) for sublist in x]\n",
        "\nb = np.eye(np.max(a)+1)[a]\n",
        "\nb = np.eye(np.max(a)+1)[a]\n",
        "\nb = np.eye(a.max() - a.min() + 1)[a - a.min()]\n",
        "\nb = np.zeros((len(a), len(a)))\nfor i, val in enumerate(a):\n    index = np.argsort(a).tolist().index(i)\n    b[i][index] = 1\n",
        "\nb = np.eye(np.max(a)+1)[a.flatten()]\nb = b.reshape(a.shape + (b.shape[1],))\n",
        "\nresult = np.percentile(a, p)\n",
        "\nB = np.reshape(A, (-1, ncol))\n",
        "\nB = np.reshape(A, (nrow, -1))\n",
        "\nB = np.reshape(A, (-1, ncol))\n",
        "\nB = np.reshape(A[-(len(A)//ncol)*ncol:], (-1, ncol))\n",
        "\nresult = np.roll(a, shift)\nresult[:shift] = np.nan\nresult[shift:] = a[:len(a)-shift]\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nnp.random.seed(0)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape, order='F')\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    result = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argsort(a, axis=None)[-2], a.shape)\n",
        "\na = a[:, ~np.isnan(a).any(axis=0)]\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.sin(np.radians(degree))\n",
        "\nresult = np.cos(np.radians(degree))\n",
        "\nresult = 0 if np.sin(np.deg2rad(number)) > np.sin(number) else 1\n",
        "\nresult = np.arcsin(value) * 180 / np.pi\n",
        "\nresult = np.pad(A, (0, length - len(A)), mode='constant')\n",
        "\nresult = np.pad(A, (0, length - len(A)), mode='constant')\n",
        "\na = np.power(a, power)\n",
        "\n    result = np.power(a, power)\n",
        "\nresult = Fraction(numerator, denominator)\n",
        "\n    result = np.gcd(numerator, denominator)\n    numerator = numerator // result\n    denominator = denominator // result\n",
        "\nresult = Fraction(numerator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, np.maximum(b, c))\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\nresult = np.diag(np.fliplr(a))\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[diagonal[::-1]]\n",
        "\nresult = X.flatten().tolist()\n",
        "\nresult = X.flatten(order='C')\n",
        "\n    result = X.flatten().tolist()\n",
        "\nresult = X.flatten(order='F')\n",
        "\nresult = np.array([int(digit) for digit in mystr])\n",
        "\nresult = np.cumsum(a[:, col] * multiply_number)\n",
        "\nresult = np.cumsum(a[row] * multiply_number)\n",
        "\nresult = np.prod(a[row] / divide_number)\n",
        "\nresult = np.linalg.qr(a)[0]\n",
        "\nresult = a.shape[1]\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a[~np.isnan(a)], b[~np.isnan(b)], equal_var=False)\n",
        "\nt_statistic = (amean - bmean) / np.sqrt((avar/anobs) + (bvar/bnobs))\ndegrees_of_freedom = anobs + bnobs - 2\np_value = 2 * (1 - scipy.stats.t.cdf(np.abs(t_statistic), degrees_of_freedom))\n",
        "\noutput = np.setdiff1d(A, B, axis=0)\n",
        "\noutput = np.concatenate((np.setdiff1d(A, B), np.setdiff1d(B, A)))\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)[::-1]\nc = b[sort_indices]\n",
        "\nsum_a = np.sum(a, axis=(1, 2))\nsorted_indices = np.argsort(sum_a)\nresult = b[sorted_indices]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0, 2], axis=1)\n",
        "\nresult = np.delete(a, del_col, axis=1)\n",
        "\na = np.insert(a, pos, element)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\n    a = np.insert(a, pos, element)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\nresult = np.copy(array_of_arrays)\n",
        "\nresult = np.all(a == a[0], axis=0)\n",
        "\nresult = np.all(a == a[:, 0][:, np.newaxis], axis=0)\n",
        "\n    result = np.all(a[0] == a[1:], axis=0)\n",
        "\nX, Y = np.meshgrid(x, y)\nZ = np.cos(X)**4 + np.sin(Y)**2\nresult = simps(simps(Z, y), x)\n",
        "\n    # Create a 2D grid of x and y values\n    X, Y = np.meshgrid(x, y)\n    \n    # Evaluate the function at each point on the grid\n    Z = np.cos(X)**4 + np.sin(Y)**2\n    \n    # Calculate the integral using Simpson's rule\n    result = simps(simps(Z, y), x)\n",
        "\nresult = np.sort(grades) / np.sum(grades)\nresult = np.cumsum(result)\n",
        "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)(eval)\n",
        "\necdf = np.cumsum(grades / np.sum(grades))\nlow = np.min(grades[ecdf < threshold])\nhigh = np.max(grades[ecdf < threshold])\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\na_np = a.numpy()\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[-N:][::-1]\n",
        "\nresult = np.linalg.matrix_power(A, n)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = a[:a.shape[0]//patch_size*patch_size, :a.shape[1]//patch_size*patch_size].reshape(-1, patch_size, patch_size)\n",
        "\nresult = np.reshape(a, (h, w))\n",
        "\nresult = np.array([a[i:i+patch_size, j:j+patch_size] for i in range(0, a.shape[0]-patch_size+1, patch_size) for j in range(0, a.shape[1]-patch_size+1, patch_size)])\n",
        "\nresult = a[:, low:high+1]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:, low:high]\n",
        "\na = np.array(eval(string))\n",
        "\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\n    result = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nresult = np.ravel_multi_index(index, dims, order='F') + 1\n",
        "\nresult = np.ravel_multi_index(index, dims, order='C')\n",
        "\nvalues = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\nresult = np.maximum.reduceat(a, np.unique(index))\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\nresult = np.zeros(np.max(index)+1)\nfor i in range(len(a)):\n    if index[i] >= 0:\n        result[index[i]] = min(result[index[i]], a[i])\n",
        "\nz = np.vectorize(elementwise_function)(x, y)\n",
        "\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\nresult = np.pad(a, ((abs(low_index), high_index-a.shape[0]+1), (abs(low_index), high_index-a.shape[1]+1)), mode='constant')\n",
        "\nresult = x[x >= 0]\n",
        "\nresult = x[np.iscomplex(x)]\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_max = np.max(bin_data, axis=1)\n",
        "\nbin_data = np.array([data[:, i:i+bin_size] for i in range(0, data.shape[1], bin_size)])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nn = len(data)\nnum_bins = n // bin_size\nbin_data = np.split(data[-n:], num_bins)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nn = len(data[0])\nnum_bins = n // bin_size\nbin_data = np.array([data[:, i*bin_size:(i+1)*bin_size] for i in range(num_bins-1, -1, -1)])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nnum_rows, num_cols = data.shape\nnum_bins = num_cols // bin_size\nbin_data = np.zeros((num_rows, num_bins), dtype=object)\nfor i in range(num_rows):\n    for j in range(num_bins):\n        start = num_cols - (j+1)*bin_size\n        end = num_cols - j*bin_size\n        bin_data[i, j] = tuple(data[i, start:end])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\ndef smoothclamp(x):\n    return np.clip(3 * x**2 - 2 * x**3, x_min, x_max)\n",
        "\ndef smoothclamp(x, N):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0, 1)\n    result = t ** N * (3 - 2 * t)\n    result = result * (x_max - x_min) + x_min\n    return result\n",
        "\nresult = np.correlate(a, np.roll(b[::-1], 1), mode='valid')\n",
        "\nresult = df.values.reshape(4, 15, 5)\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\nresult = np.unpackbits(a[:, np.newaxis], axis=1)[:, -m:]\n",
        "\nresult = np.unpackbits(np.uint8(a.reshape(-1,1)), axis=1)[:, -m:]\n",
        "\nresult = np.unpackbits(a[:, np.newaxis], axis=1)[:, -m:]\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 3*std, mean + 3*std)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 2*std, mean + 2*std)\n",
        "\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3 * std, mean + 3 * std)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nthreshold = mean + (2 * std)\nresult = np.abs(a - mean) > threshold\n",
        "\nprob = np.percentile(DataArray, percentile)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[1,:] = 0\na[:,0] = 0\n",
        "\nmask = a == np.amax(a, axis=1)[:, np.newaxis]\n",
        "\nmask = np.equal(a, np.min(a, axis=1)[:, np.newaxis])\n",
        "\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nresult = np.einsum('ij,ik->jik', X, X)\n",
        "\nX = np.sqrt(np.sum(Y, axis=2))\n",
        "\nis_contained = np.isin(number, a)\n",
        "\nC = np.setdiff1d(A, B)\n",
        "\nC = np.intersect1d(A, B)\n",
        "\nC = np.intersect1d(A, B)\n",
        "\nresult = len(a) + 1 - rankdata(a).astype(int)\n",
        "\nresult = len(a) - rankdata(a).astype(int) - 1\n",
        "\n    result = len(a) + 1 - rankdata(a).astype(int)\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\nresult = a[:, second, :][:, :, third]\n",
        "\narr = np.zeros((20,10,10,2))\n",
        "\nresult = np.divide(X, np.linalg.norm(X, ord=1, axis=1).reshape(-1, 1))\n",
        "\nresult = X / np.linalg.norm(X, axis=1, ord=2, keepdims=True)\n",
        "\nresult = np.divide(X, np.max(np.abs(X), axis=1, keepdims=True))\n",
        "\nresult = np.select(df['a'].astype(str).str.contains(target), choices, default=np.nan)\n",
        "\nresult = cdist(a, a)\n",
        "\nresult = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        result[i, j] = np.linalg.norm(a[i] - a[j])\n",
        "\nresult = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        result[i, j] = np.linalg.norm(a[i] - a[j])\n",
        "\nA = [float(x) for x in A]\nAVG = np.mean(A)\n",
        "\nNA = np.array([float(x) if x != 'inf' else np.inf for x in NA])\nAVG = np.mean(NA, axis=0)\n",
        "\nA = [eval(x) for x in A]\n",
        "\nresult = np.delete(a, np.where(np.diff(a) == 0))\n",
        "\nresult = np.unique(a[np.nonzero(a)])\n",
        "\ndata = {'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()}\ndf = pd.DataFrame(data)\n",
        "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n",
        "\ndata = {'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()}\ndf = pd.DataFrame(data)\ndf['maximum'] = df[['lat', 'lon', 'val']].max(axis=1)\n",
        "\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        result.append(a[i:i+size[0], j:j+size[1]])\n",
        "\nresult = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        window = a[max(0, i-size[0]//2):min(a.shape[0], i+size[0]//2+1), max(0, j-size[1]//2):min(a.shape[1], j+size[1]//2+1)]\n        result.append(window)\n",
        "\nresult = np.nanmean(a)\n",
        "\n    result = np.nanmean(a)\n",
        "\nresult = Z[..., -1:]\n",
        "\nresult = a[-1:, ...]\n",
        "\nresult = c in CNTS\n",
        "\nresult = any(np.array_equal(c, arr) for arr in CNTS)\n",
        "\nf = intp.interp2d(np.arange(4), np.arange(4), a, kind='linear')\nresult = f(x_new, y_new)\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n",
        "\ni = np.diag(i)\n",
        "\na[np.nonzero(~np.eye(a.shape[0], dtype=bool))] = 0\n",
        "\nt0 = pd.to_datetime(start)\ntf = pd.to_datetime(end)\nseries = pd.date_range(start=t0, end=tf, periods=n)\nresult = pd.DatetimeIndex(series)\n",
        "\nresult = np.where((x == a) & (y == b))[0][0] if np.sum((x == a) & (y == b)) > 0 else -1\n",
        "\nresult = np.where((x == a) & (y == b))[0]\n",
        "\nresult = np.polyfit(x, y, 2)\n",
        "\nresult = np.polyfit(x, y, degree)\n",
        "\ndf = df.apply(lambda x: x - a)\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.flatten().reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(a.shape[0], -1)).reshape(a.shape)\n",
        "\nmask = arr < -10\nmask2 = arr >= 15\narr[mask] = 0\narr[mask2] = 30\narr[~(mask | mask2)] += 5\n",
        "\nfor i in range(arr.shape[0]):\n    mask = arr[i] < n1[i]\n    mask2 = arr[i] >= n2[i]\n    mask3 = mask ^ mask2\n    arr[i][mask] = 0\n    arr[i][mask3] = arr[i][mask3] + 5\n    arr[i][~mask2] = 30\n",
        "\nresult = np.count_nonzero(np.isclose(s1, s2))\n",
        "\nresult = np.count_nonzero(np.isnan(s1) != np.isnan(s2))\n",
        "\nresult = all(np.array_equal(a[i], a[i+1]) for i in range(len(a)-1))\n",
        "\nresult = all(np.isnan(arr).all() for arr in a)\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), constant_values=element)\n",
        "\n    result = np.zeros(shape)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\na = a.reshape(a.shape[0]//3, 3)\n",
        "\nresult = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]].squeeze()\n",
        "\nresult = a[:,:,1] * b\n",
        "\nresult = a[np.arange(b.shape[0])[:, None], np.arange(b.shape[1]), b]\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b])\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b])\n",
        "\nresult = df.loc[(df['a'] > 1) & (df['a'] <= 4), 'b'].tolist()\nresult = [np.nan if x not in result else x for x in df['b']]\n",
        "\nresult = im[1:4, 1:5]\n",
        "\nresult = A[np.ix_((A != 0).any(1), (A != 0).any(0))]\n",
        "\nresult = im[1:-1, 1:-1]\n",
        "\nresult = im[1:-1, 1:-1]\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n",
        "\nplt.minorticks_on()\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\nplt.minorticks_on()\n",
        "\nplt.minorticks_on()\nplt.tick_params(axis='x', which='both', bottom=True, top=True)\n",
        "\nline_styles = ['-', '--', '-.', ':']\nfor style in line_styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style)\nplt.show()\n",
        "\nline_styles = ['-', '--', '-.', ':']\nfor style in line_styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style)\nplt.show()\n",
        "\nplt.plot(x, y, marker='D', linestyle='-', linewidth=1)\n",
        "\nplt.plot(x, y, marker='D', linewidth=2)\n",
        "\nax.set_ylim(0, 40)\n",
        "\nplt.axvspan(2, 4, facecolor='red', alpha=0.5)\n",
        "\nx = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n",
        "\nx = np.array([0, 1])\ny = np.array([0, 2])\n\nplt.plot(x, y)\nplt.show()\n",
        "\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df)\nplt.show()\n",
        "\nsns.set(style=\"whitegrid\")\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot')\nplt.show()\n",
        "\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(data=df, x='x', y='y')\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', markersize=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20)\n",
        "\nl.set_markerfacecolor(l.get_markerfacecolor() + (0.2,))\n",
        "\nl.set_markeredgecolor('black')\n",
        "\nl.set_color('red')\nl.set_markerfacecolor('red')\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(np.arange(0, 2*np.pi+1, 2*np.pi/10))\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", ha=\"right\", x=1)\n",
        "\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
        "\nplt.title(\"\\n\".join(myTitle[i:i+20] for i in range(0, len(myTitle), 20)))\n",
        "\nplt.gca().invert_yaxis()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nplt.plot(x, color='blue')\nplt.plot(y, color='orange')\nplt.plot(z, color='green')\nplt.fill_between(range(len(x)), x, color='blue', alpha=0.3)\nplt.fill_between(range(len(y)), y, color='orange', alpha=0.3)\nplt.fill_between(range(len(z)), z, color='green', alpha=0.3)\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolors='black', facecolors='blue')\nplt.show()\n",
        "\nplt.xticks(np.arange(10))\nplt.yticks(np.arange(0, 2.1, 0.5))\n",
        "\nplt.ticklabel_format(style='plain', axis='y')\n",
        "\nax.axhline(1, linestyle='dashed', color='red')\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\nax1.plot(x, y1)\nax1.set_ylabel('y1')\n\nax2.plot(x, y2)\nax2.set_ylabel('y2')\n\nplt.xlabel('x')\n\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\nax1.plot(x, y1)\nax1.set_ylabel('y1')\n\nax2.plot(x, y2)\nax2.set_xlabel('x')\nax2.set_ylabel('y2')\n\nsns.despine(ax=ax1)\nsns.despine(ax=ax2)\n\nplt.show()\n",
        "\nplt.xlabel(\"\")\n",
        "\nplt.xticks([])\n",
        "\nplt.xticks([3, 4])\nplt.grid(axis='x')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y')\n\nplt.xticks([1, 2])\nplt.grid(axis='x')\n",
        "\n\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n\n# show grids\nplt.grid(True)\n\nplt.show()\n",
        "\nplt.legend(loc=\"lower right\")\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), tight_layout=True)\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\nplt.clf()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10, 20)\nz = np.arange(10)\n\n\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\n\nplt.legend()\nplt.show()\n",
        "\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xticks(np.arange(10), x)\nplt.tick_params(axis='x', which='both', pad=20)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.yaxis.tick_right()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.tick_params(axis='y', labelleft=True, labelright=False)\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='green')\nplt.show()\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='green')\nplt.show()\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\nplt.show()\n",
        "\nplt.bar(df[\"celltype\"], df[\"s1\"])\nplt.xticks(rotation=0)\nplt.xlabel(\"celltype\")\nplt.show()\n",
        "\n\nplt.bar(df[\"celltype\"], df[\"s1\"])\nplt.xticks(rotation=45)\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1\")\nplt.title(\"Bar Plot of s1\")\n\nplt.show()\n\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.xticks(color=\"red\")\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.axhline(0, color='red')\n",
        "\nplt.plot(x, y)\nplt.xticks(fontsize=10, rotation='vertical')\n",
        "\nx_values = [0.22058956, 0.33088437, 2.20589566]\nfor x in x_values:\n    plt.axvline(x=x, color='r', linestyle='--')\n",
        "\n\nfig, ax = plt.subplots()\nim = ax.imshow(rand_mat)\n\n# Set the tick labels on top\nax.xaxis.tick_top()\n\n# Create colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\n\n# Set the tick labels and invert the y-axis labels\nax.set_xticks(numpy.arange(len(xlabels)))\nax.set_yticks(numpy.arange(len(ylabels)))\nax.set_xticklabels(xlabels)\nax.set_yticklabels(ylabels[::-1])\n\n# Rotate the tick labels and set their alignment\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations\nfor i in range(len(ylabels)):\n    for j in range(len(xlabels)):\n        text = ax.text(j, i, round(rand_mat[i, j], 2), ha=\"center\", va=\"center\", color=\"w\")\n\nplt.show()\n\n",
        "\nfrom matplotlib import rc\n\nrc(\"mathtext\", default=\"regular\")\n\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n# copy the code of the above plot and edit it to have legend for all three curves in the two subplots\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax2.legend(loc=1)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title(\"Y\")\naxs[1].plot(x, y)\naxs[1].set_title(\"Y\")\nplt.show()\n",
        "\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\nplt.show()\n",
        "\nplt.scatter(a, b)\nfor i, txt in enumerate(c):\n    plt.annotate(txt, (a[i], b[i]))\nplt.xlabel('a')\nplt.ylabel('b')\nplt.title('Scatter plot of a over b')\nplt.show()\n",
        "\nplt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.title(\"Legend\", fontweight=\"bold\")\nplt.show()\n",
        "\nplt.hist(x, edgecolor='black', linewidth=1.2)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.plot(x, y)\nax1.set_title('Subplot 1')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\nax2.plot(x, y)\nax2.set_title('Subplot 2')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\n\nplt.tight_layout()\nplt.show()\n",
        "\nplt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n",
        "\nplt.hist([x, y], bins=10, label=['x', 'y'])\nplt.legend()\nplt.show()\n",
        "\nx = [a, c]\ny = [b, d]\n\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nim1 = ax1.imshow(x, cmap='viridis')\nax1.set_title('X')\n\nim2 = ax2.imshow(y, cmap='plasma')\nax2.set_title('Y')\n\nfig.colorbar(im1, ax=[ax1, ax2])\nplt.show()\n",
        "\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n\nax1.plot(x, y)\nax1.set_title('Y')\n\nax2.plot(a, z)\nax2.set_title('Z')\n\nfig.suptitle('Y and Z')\n\nplt.show()\n",
        "\nx = [point[0] for point in points]\ny = [point[1] for point in points]\n\nplt.plot(x, y)\nplt.yscale('log')\nplt.xlabel('X')\nplt.ylabel('Y (log scale)')\nplt.title('Line Plot')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Plot of y over x\", fontsize=20)\nplt.xlabel(\"x\", fontsize=18)\nplt.ylabel(\"y\", fontsize=16)\nplt.show()\n",
        "\nax.plot(x, y)\nax.set_xticks(x)\nax.set_xticklabels(x+1)\n",
        "\n\nfor line, color in zip(lines, c):\n    plt.plot([line[0][0], line[1][0]], [line[0][1], line[1][1]], color=color)\n\nplt.show()\n\n",
        "\nplt.loglog(x, y)\nplt.xticks([1, 10, 100], [1, 10, 100])\nplt.yticks([1, 10, 100], [1, 10, 100])\nplt.show()\n",
        "\n\nplt.plot(df.index, df['A'], marker='o', label='A')\nplt.plot(df.index, df['B'], marker='o', label='B')\nplt.plot(df.index, df['C'], marker='o', label='C')\nplt.plot(df.index, df['D'], marker='o', label='D')\n\nplt.legend()\nplt.show()\n\n",
        "\nplt.hist(data, bins=10, density=True)\nplt.gca().yaxis.set_major_formatter(plt.PercentFormatter(1))\nplt.yticks(np.arange(0, 1.1, 0.1))\n",
        "\nplt.plot(x, y, marker='o', alpha=0.5)\n",
        "\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\nax1.plot(x, y, label='y')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\nax2.plot(z, a, label='a')\nax2.set_xlabel('z')\nax2.set_ylabel('a')\n\nfig.legend(loc='center')\n\nplt.show()\n\n",
        "\n\nfig, axes = plt.subplots(2, 1, figsize=(8, 10))\n\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axes[0])\naxes[0].set_title(\"Regression plot of bill_depth_mm over bill_length_mm\")\n\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axes[1])\naxes[1].set_title(\"Regression plot of flipper_length_mm over bill_length_mm\")\n\nplt.tight_layout()\nplt.show()\n\n",
        "\nlabels = [str(i) if i != 2 else \"second\" for i in range(1, 10)]\nplt.xticks(range(1, 10), labels)\n",
        "\nplt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n",
        "\nextra_ticks = [2.1, 3, 7.6]\nexisting_ticks = plt.xticks()[0]\nnew_ticks = np.concatenate((existing_ticks, extra_ticks))\nplt.xticks(new_ticks)\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.xticks(rotation=-60, va=\"top\")\n",
        "\nplt.xticks(alpha=0.5)\n",
        "\nplt.margins(x=0, y=0.1)\n",
        "\nplt.margins(x=0.02, y=0.0)\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\nfig.suptitle(\"Figure\")\n\naxs[0].plot(x, y)\naxs[0].set_title(\"Subplot 1\")\n\naxs[1].plot(x, y)\naxs[1].set_title(\"Subplot 2\")\n\nplt.show()\n",
        "\ndf.plot(kind='line')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n",
        "\nplt.scatter(x, y, marker='|', hatch='/', edgecolor='black', linewidth=1)\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolors='none', marker='o', hatch='|')\n",
        "\nplt.scatter(x, y, marker='*', hatch='*')\nplt.show()\n",
        "\nplt.scatter(x, y, s=100, marker='*', hatch='|')\n",
        "\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.imshow(data, extent=[1, 5, 1, 4], cmap='hot')\nplt.colorbar()\nplt.show()\n",
        "\nplt.stem(x, y, orientation='horizontal')\nplt.show()\n",
        "\n\n# Get the keys and values from dictionary d\nkeys = list(d.keys())\nvalues = list(d.values())\n\n# Get the colors for each bar by looking up the color in dictionary c\ncolors = [c[key] for key in keys]\n\n# Create a bar plot\nplt.bar(keys, values, color=colors)\n\n# Show the plot\nplt.show()\n\n",
        "\nplt.axvline(x=3, color='r', linestyle='-', label='cutoff')\nplt.legend()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, polar=True)\nax.bar(labels, height)\n",
        "\nfig, ax = plt.subplots()\nax.pie(data, labels=l, wedgeprops=dict(width=0.4))\nax.set(aspect=\"equal\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(True, linestyle='--', color='blue')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\n\nax.minorticks_on()\nax.grid(which='minor', linestyle='dashed', color='gray')\nax.grid(which='major', visible=False)\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.title(\"Activities\")\nplt.legend(labels, loc=\"best\")\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title(\"Activities\")\nplt.legend(labels, loc=\"best\")\nplt.axis('equal')\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', linestyle='-', color='blue', markerfacecolor='none', markeredgecolor='blue', alpha=0.5)\n",
        "\nplt.axvline(x=55, color=\"green\")\n",
        "\nbar_width = 0.35\nindex = np.arange(len(blue_bar))\n\nplt.bar(index, blue_bar, bar_width, label='Blue', color='blue')\nplt.bar(index + bar_width, orange_bar, bar_width, label='Orange', color='orange')\n\nplt.xlabel('Group')\nplt.ylabel('Height')\nplt.title('Comparison of Blue and Orange Bars')\nplt.xticks(index + bar_width/2, ('A', 'B', 'C'))\nplt.legend()\n\nplt.show()\n",
        "\n\n",
        "\nplt.scatter(x, y, c=y, cmap='Spectral')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(0, 10, 1))\nplt.show()\n",
        "\ng = sns.factorplot(x=\"sex\", y=\"bill_length_mm\", col=\"species\", data=df, kind=\"bar\", sharey=False)\nplt.show()\n",
        "\ncircle = plt.Circle((0.5, 0.5), 0.2, color='blue')\nfig, ax = plt.subplots()\nax.add_artist(circle)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\mathbf{\\phi}$', fontsize=16)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(markerscale=0.1)\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\nplt.show()\n",
        "\nplt.legend(ncol=2)\n",
        "\nplt.legend()\nplt.plot(x[3], y[3], marker=\"o\", markersize=10, color=\"red\")\nplt.plot(x[7], y[7], marker=\"o\", markersize=10, color=\"blue\")\n",
        "\nplt.imshow(data, cmap='hot')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r\"$\\bf{Figure}$ 1\")\n",
        "\nsns.pairplot(df, x_vars='x', y_vars='y', hue='id')\nplt.legend().remove()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n",
        "\nplt.scatter(x, y)\nplt.axis('auto')\n",
        "\nplt.scatter(x, y, color='red', edgecolor='black')\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax in axs.flatten():\n    ax.plot(x, y)\nplt.show()\n",
        "\nplt.hist(x, bins=5, range=(0, 10), rwidth=2)\nplt.show()\n",
        "\nplt.plot(x, y, 'b-', label='Data')\nplt.fill_between(x, y-error, y+error, color='gray', alpha=0.5, label='Error')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n",
        "\nplt.axhline(0, color='white')\nplt.axvline(0, color='white')\n",
        "\nax.errorbar(box_position, box_height, yerr=box_errors, fmt=\"none\", ecolor=c)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\nax1.plot(x, y)\nax1.set_title(\"Y\")\n\nax2.plot(a, z)\nax2.set_title(\"Z\")\nax2.title.set_position([0.5, 1.05])\n\nplt.tight_layout()\nplt.show()\n",
        "\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xticks(np.arange(0, 10, 2))\n        axs[i, j].set_yticks(np.arange(0, 10, 2))\n\nplt.show()\n",
        "\nplt.figure(figsize=(8, 8))\nplt.matshow(d)\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(8, 6))\nax.axis('off')\ntable = ax.table(cellText=df.values, colLabels=df.columns, loc='center')\ntable.auto_set_font_size(False)\ntable.set_fontsize(14)\ntable.scale(1.2, 1.2)\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.xaxis.set_ticks_position('both')\nax.yaxis.set_ticks_position('both')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.xaxis.set_ticks_position('both')\nax.yaxis.set_ticks_position('left')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(x)\nplt.tick_params(axis='x', which='both', bottom=False, top=False)\nplt.show()\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"swarm\")\ng.set_titles(\"Group: {col_name}\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"strip\")\ng.set_axis_labels(\"Exercise Time\", \"Pulse\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"swarm\")\ng.set_axis_labels(\"\", \"\")\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\n",
        "\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, legend=False)\nplt.show()\n",
        "\n\ng = sns.FacetGrid(df, row=\"b\", height=3, aspect=3)\ng.map(sns.pointplot, \"a\", \"c\", color=\"steelblue\", ci=None)\ng.set_xticks(np.arange(0, 31, 2))\ng.set_xticklabels(np.arange(0, 31, 1))\nplt.show()\n\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\n\nax.view_init(azim=100, elev=50)\n\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\n",
        "\ngs = gridspec.GridSpec(nrow, ncol, wspace=0.0, hspace=0.0)\n\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = plt.subplot(gs[i, j])\n        ax.imshow(x, cmap='gray')\n        ax.axis('off')\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\ndef my_map_func(i):\n  return [[i, i+1, i+2]]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n",
        "\n    ds = tf.data.Dataset.from_tensor_slices(input)\n    ds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n    result = []\n    iterator = ds.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    with tf.compat.v1.Session() as sess:\n        while True:\n            try:\n                result.append(sess.run(next_element))\n            except tf.errors.OutOfRangeError:\n                break\n",
        "\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length)\npadded_mask = tf.pad(mask, [[0, 0], [8 - max_length, 0]])\nresult = tf.cast(padded_mask, dtype=tf.int32)\n",
        "\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length)\npadded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_length]])\nresult = tf.cast(padded_mask, dtype=tf.int32)\n",
        "\nmax_length = tf.reduce_max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\nresult = tf.pad(mask, paddings=[[0, 0], [8 - max_length, 0]])\n",
        "\n    max_length = max(lengths)\n    padded_length = 8\n    mask = tf.sequence_mask(lengths, maxlen=padded_length, dtype=tf.int32)\n    result = tf.pad(mask, [[0, 0], [0, padded_length - max_length]])\n",
        "\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_length+1, dtype=tf.float32)\nresult = tf.pad(mask, [[0, 0], [0, 8-max_length-1]])\n",
        "\nresult = tf.transpose(tf.reshape(tf.tile(a, [tf.size(b)]), [tf.size(b), tf.size(a)])) + tf.reshape(tf.tile(b, [tf.size(a)]), [tf.size(a), tf.size(b)])\n",
        "\n    result = tf.stack(tf.meshgrid(a, b), axis=-1)\n",
        "\nresult = tf.squeeze(a, axis=2)\n",
        "\nresult = tf.expand_dims(a, axis=2)\n",
        "\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n",
        "\n    result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n",
        "\nresult = tf.gather_nd(x, tf.stack((y, z), axis=1))\n",
        "\nm = tf.gather_nd(x, tf.stack((row, col), axis=1))\n",
        "\n    result = tf.gather_nd(x, tf.stack((y, z), axis=1))\n",
        "\nresult = tf.einsum('bik,bjk->bij', A, B)\n",
        "\nresult = tf.matmul(A, tf.transpose(B, perm=[0, 2, 1]))\n",
        "\nresult = [s.decode('utf-8') for s in x]\n",
        "\n    result = [s.decode('utf-8') for s in x]\n",
        "\nmask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\nsum_values = tf.reduce_sum(x, axis=-2)\ncount_nonzero = tf.reduce_sum(mask, axis=-2)\nresult = tf.divide(sum_values, count_nonzero)\n",
        "\nmask = tf.cast(tf.not_equal(x, 0), tf.float32)\nsums = tf.reduce_sum(x, axis=-2)\ncounts = tf.reduce_sum(mask, axis=-2)\nresult = sums / counts\n",
        "\n    mask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\n    sum_values = tf.reduce_sum(x, axis=-2)\n    count_nonzero = tf.reduce_sum(mask, axis=-2)\n    result = tf.divide(sum_values, count_nonzero)\n",
        "import tensorflow as tf\n\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n\nprint(result)",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    result = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmin(a, axis=0)\n",
        "\ntf.saved_model.save(model, \"export/1\")\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=6, dtype=tf.int32)\n",
        "\nimport tensorflow as tf\n\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform([10], minval=1, maxval=5, dtype=tf.int32)\n    return result\n",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\nresult, _ = scipy.optimize.curve_fit(lambda x, A, B, C: A * np.exp(B * x) + C, x, y, p0=p0)\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\nresult = stats.ks_2samp(x, y)\nresult = result[1] < alpha\n",
        "\nresult = optimize.minimize(lambda x: ((x[0]+x[1]-x[2])-2)**2 + ((3*x[0]-x[1]-x[2]))**2 + sin(x[1]) + cos(x[1]) + 4, initial_guess)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores, loc=mu, scale=sigma)\n",
        "\nz_scores = scipy.stats.norm.ppf(p_values)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nresult = sa.multiply(sb)\n",
        "\n    result = sA.multiply(sB)\n",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n",
        "\ndata_rot = rotate(data_orig, angle)\nxrot = int(x0 * np.cos(np.deg2rad(angle)) - y0 * np.sin(np.deg2rad(angle)))\nyrot = int(x0 * np.sin(np.deg2rad(angle)) + y0 * np.cos(np.deg2rad(angle)))\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, 'uniform')\n",
        "\n    result = stats.kstest(times, 'uniform')\n",
        "\nresult = stats.kstest(times, 'uniform')\nresult = result.pvalue >= 0.05\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2)\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)\nresult = col_ind.tolist()\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)\nresult = col_ind.tolist()\n",
        "\nb.setdiag(0)\n",
        "\nlabels, num_regions = ndimage.label(img > threshold)\nresult = num_regions\n",
        "\nlabels, num_regions = ndimage.label(img < threshold)\nresult = num_regions\n",
        "\n    labeled_array, num_features = ndimage.label(img > threshold)\n    result = num_features\n",
        "\nlabels, num_features = ndimage.label(img > threshold)\ncom = ndimage.center_of_mass(img, labels, range(1, num_features+1))\nresult = [((x-0)**2 + (y-0)**2)**0.5 for x, y in com]\n",
        "\nM = M + M.T\n",
        "\n    sA = sA + sA.T\n",
        "\nstructure = np.array([[1, 1, 1],\n                     [1, 0, 1],\n                     [1, 1, 1]])\nfiltered_square = scipy.ndimage.binary_opening(square, structure=structure)\nsquare = square - filtered_square\n",
        "\nimport scipy.ndimage\n\n# Find connected components in the image\nlabels, num_labels = scipy.ndimage.label(square)\n\n# Loop through each label\nfor label in range(1, num_labels+1):\n    # Find the coordinates of the label\n    coords = np.argwhere(labels == label)\n    \n    # Check if the label is completely surrounded by zeros\n    if np.all(square[coords[:, 0], coords[:, 1]] == 0):\n        # Set the label to zero\n        square[coords[:, 0], coords[:, 1]] = 0\n",
        "\nmean = col.mean()\nstandard_deviation = col.std()\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nMedian = np.median(col.data)\nMode = np.argmax(np.bincount(col.data))\n",
        "\ndef fourier(x, *coefficients):\n    result = 0\n    for i in range(len(coefficients)):\n        result += coefficients[i] * np.cos((i+1) * np.pi / tau * x)\n    return result\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*(degree))\n",
        "\nresult = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean')\n",
        "\nresult = scipy.spatial.distance.cdist(np.argwhere(example_array > 0), np.argwhere(example_array > 0), metric='cityblock')\n",
        "\n    result = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(example_array.nonzero()))\n",
        "\ntck = interpolate.splrep(x[:, 0], y[:, 0], k=2, s=4)\ny_extrapolated = interpolate.splev(x_val, tck, der=0)\nresult = np.array([y_extrapolated])\nfor i in range(1, 5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k=2, s=4)\n    y_extrapolated = interpolate.splev(x_val, tck, der=0)\n    result = np.append(result, [y_extrapolated], axis=0)\n",
        "\nresult = ss.anderson_ksamp([x1, x2, x3, x4])\nstatistic = result.statistic\ncritical_values = result.critical_values\nsignificance_level = result.significance_level\n",
        "\nresult = ss.anderson_ksamp([x1, x2])\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\n\ndef tau1(x):\n    y = np.array(A['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\n\ndf['AB'] = df['B'].rolling(3).apply(tau1)\ndf['AC'] = df['C'].rolling(3).apply(tau1)\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(df['C'].rolling(3).apply(lambda y: tau1(x))))\nprint(df)\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = block_diag(*a)\n",
        "\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nkurtosis_result = np.mean(((a - mean) / std) ** 4)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\n",
        "\nresult = scipy.interpolate.interp2d(x, y, z)(s, t)\n",
        "\n    interp_func = scipy.interpolate.interp2d(x[:,0], y[0,:], z, kind='cubic')\n    result = interp_func(s, t).flatten()\n",
        "\nimport numpy as np\n\nresult = np.zeros(len(extraPoints), dtype=int)\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    result[i] = region_index\n\n",
        "\nimport numpy as np\n\nresult = np.zeros(len(extraPoints), dtype=int)\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    result[i] = region_index\n\n",
        "\nmax_size = max(len(vector) for vector in vectors)\npadded_vectors = [np.pad(vector, (0, max_size - len(vector)), mode='constant') for vector in vectors]\nresult = sparse.csr_matrix(padded_vectors)\n",
        "\nb = nd.median_filter(a, 3, origin=0.5)\n",
        "\nresult = M[row, column]\n",
        "\nresult = M[row, column].tolist()\n",
        "\nnew_array = scipy.interpolate.interp1d(x, array, axis=0)(x_new)\n",
        "\nprob, _ = scipy.integrate.quad(NDfx, -np.inf, x)\n",
        "\n    prob = scipy.integrate.quad(NDfx, -np.inf, x, args=(u, o2))[0]\n",
        "\nresult = sf.dct(np.eye(N))\n",
        "\nresult = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\n",
        "\nresult = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        result[i,j] = scipy.stats.binom.pmf(j, i, p)\n",
        "\nresult = df.apply(stats.zscore, axis=1)\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\nresult = pd.DataFrame()\nresult['data'] = df\nresult['zscore'] = stats.zscore(df)\n",
        "\nresult = pd.DataFrame()\nresult['data'] = df\nresult['zscore'] = df.apply(lambda x: np.round(stats.zscore(x), 3))\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\nmid = np.array([[shape[0] / 2, shape[1] / 2]])\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), mid)\n",
        "\nmid = np.array([shape[0]//2, shape[1]//2])\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), np.array([mid]))\n",
        "\n    rows, cols = shape\n    y, x = np.indices(shape)\n    mid = np.array([(rows-1)/2, (cols-1)/2])\n    result = distance.cdist(np.dstack((y, x)), mid)\n",
        "\nresult = scipy.ndimage.zoom(x, np.min(np.array(shape) / np.array(x.shape)), order=1)\n",
        "\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\n",
        "\nout = scipy.optimize.minimize(residual, x0, args=(a, y), method='L-BFGS-B', bounds=[(lb, None) for lb in x_lower_bounds])\n",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 + t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\nfor t in range(4):\n    def const(x, t=t):\n        return x[t]\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\nresult, error = scipy.integrate.quad(lambda x: 2*c*x, low, high)\n",
        "\n    def integrand(x):\n        return 2 * c * x\n    \n    result, error = scipy.integrate.quad(integrand, low, high)\n",
        "# [Missing Code]\nV = V + sparse.dok_matrix((V.shape[0], V.shape[1]), dtype=np.float64) + x\n",
        "\nV.data += x\n",
        "# [Missing Code]\nV.data += x\nV.data += y",
        "\nsa = sa.tocsc()\nfor Col in range(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    sa[:,Col] = (1/Len) * Column\n",
        "\nsa = sa.tocsc()  # convert the matrix to CSC format for efficient column access\nfor Col in range(sa.shape[1]):\n    Column = sa[:, Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    sa[:, Col] = (1 / Len) * Column  # update the original column of the matrix\n",
        "\na = np.where(a > 0, 1, 0)\n",
        "\na = np.where(a > 0, 1, 0)\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(dist_matrix, axis=0)\nresult = closest_indices\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(dist_matrix, axis=1)\nresult = data[closest_indices]\n",
        "\ndistances = scipy.spatial.distance.cdist(data, centroids)\nresult = np.argpartition(distances, k, axis=0)[:k]\n",
        "\nresult = []\nfor x, b in zip(xdata, bdata):\n    root = fsolve(eqn, x0=0.5, args=(x, b))\n    result.append(root)\n",
        "\nresult = []\nfor x, a in zip(xdata, adata):\n    def eqn(b):\n        return x + 2*a - b**2\n    root = fsolve(eqn, x0=0.5)\n    result.append(root)\nresult = np.array(result)\n",
        "\nresult = stats.kstest(sample_data, lambda x: integrate.quad(lambda y: bekkers(y, estimated_a, estimated_m, estimated_d), range_start, x)[0])\n",
        "\nresult = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d))\nresult = result.pvalue >= 0.05\n",
        "\nintegral_df = df.groupby(pd.Grouper(freq='5S')).apply(lambda x: integrate.trapz(x['A'], x.index))\n",
        "\nresult = scipy.interpolate.griddata(x, y, eval)\n",
        "\ndef multinomial_likelihood(weights):\n    probabilities = weights / np.sum(weights)\n    log_likelihood = np.sum(np.log(probabilities) * a['A1'])\n    return -log_likelihood\n\ninitial_weights = np.ones(12) / 12\nresult = sciopt.minimize(multinomial_likelihood, initial_weights, method='Nelder-Mead')\nweights = result.x\n",
        "\nresult = sciopt.minimize(e, x0=[0.5, 0.7], args=(x, y), bounds=[(0.5, 1.5), (0.7, 1.8)])\n",
        "\nresult = signal.argrelextrema(arr, np.less_equal, order=n)[0]\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if (j >= n and j < arr.shape[1] - n) and (arr[i, j] <= arr[i, j-n:j+n+1]).all():\n            result.append([i, j])\nprint(result)\n",
        "\ndf = df.select_dtypes(include=[np.number])\ndf = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n    result = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.DataFrame(mlb.fit_transform(df['Col3']), columns=mlb.classes_)\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n",
        "\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.concat([df, pd.DataFrame(mlb.fit_transform(df['Col4']), columns=mlb.classes_)], axis=1)\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.DataFrame(mlb.fit_transform(df.iloc[:, -1]), columns=mlb.classes_)\ndf_out = pd.concat([df.iloc[:, :-1], df_out], axis=1)\n",
        "\ndf_out = pd.get_dummies(df.iloc[:, -1].apply(pd.Series).stack()).sum(level=0)\n",
        "\ncalibrated_model = CalibratedClassifierCV(svmmodel, cv=5, method='sigmoid')\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_test)[:, 1]\n",
        "\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n",
        "\ndf_transformed = pd.DataFrame(transform_output.toarray())\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\n    transform_output = transform_output.toarray()\n    df_transformed = pd.DataFrame(transform_output)\n    result = pd.concat([df, df_transformed], axis=1)\n",
        "\ndel clf.named_steps['poly']\n",
        "\ndel clf.named_steps['reduce_poly']\n",
        "\ndel clf.named_steps['pOly']\n",
        "\nclf.steps.insert(1, ('new_step', SomeTransformer()))\n",
        "\nclf.steps.insert(1, ('new_step', SVC()))\n",
        "\nsteps = clf.named_steps\nsteps.insert(2, ('t1919810', PCA()))\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY, **fit_params)\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test))\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.extend(logreg.predict_proba(X_test)[:, 1])\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nclf = GridSearchCV(bc, param_grid)\nclf.fit(X_train, y_train)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X.reshape(-1, 1), y)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X.reshape(-1, 1), y)\n",
        "\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\ndef prePro(text):\n    return text.lower()\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()] # Get the selected column names\n\nprint(column_names)\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nselected_features = model.get_support()\ncolumn_names = X.columns[selected_features]\n",
        "\nselected_features = model.get_support(indices=True)\ncolumn_names = X.columns[selected_features]\n",
        "\nselected_features = X.columns[model.get_support()]\ncolumn_names = selected_features.tolist()\n",
        "\nkm.fit(X)\nclosest_50_samples = X[np.argsort(np.linalg.norm(X - km.cluster_centers_[p], axis=1))[:50]]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\np, X = load_data()\nassert type(X) == np.ndarray\n\nkm = KMeans()\nkm.fit(X)\n\n# Get the cluster centers\ncenters = km.cluster_centers_\n\n# Get the distances between each sample and the p^th center\ndistances = np.linalg.norm(X - centers[p], axis=1)\n\n# Get the indices of the 50 samples closest to the p^th center\nclosest_indices = np.argsort(distances)[:50]\n\n# Get the 50 samples closest to the p^th center\nclosest_50_samples = X[closest_indices]\n\nprint(closest_50_samples)\n",
        "\nkm.fit(X)\nclosest_100_samples = X[np.argsort(np.linalg.norm(X - km.cluster_centers_[p], axis=1))[:100]]\n",
        "\n    km.fit(X)\n    cluster_centers = km.cluster_centers_\n    distances = np.linalg.norm(X - cluster_centers[p], axis=1)\n    closest_indices = np.argsort(distances)[:50]\n    samples = X[closest_indices]\n",
        "\nX_train = pd.get_dummies(X_train)\n",
        "\nX_train = pd.get_dummies(X_train)\n",
        "\nsvm = SVR(kernel='rbf')\nsvm.fit(X, y)\npredict = svm.predict(X)\n",
        "\nregressor = SVR(kernel='rbf')\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\n\n# fit, then predict X\nregressor = SVR(kernel='poly', degree=2)\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf_matrix.T).toarray()\n",
        "\n    query_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T)\n",
        "\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n",
        "\nnew_f = pd.get_dummies(pd.DataFrame(f).stack()).sum(level=0)\n",
        "\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n",
        "\n    new_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n",
        "\nnew_features = pd.get_dummies(pd.DataFrame(features).stack()).sum(level=0)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(simM)\n",
        "\ndistance_matrix = 1 - np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\ndistance_matrix = 1 - np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\ndistance_matrix = 1 - np.array(simM)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b|['\\\"?!]\")\ntransformed_text = vectorizer.fit_transform([text])\n",
        "\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
        "\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nf1 = df['mse'].values\nX = f1.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\nselector = LinearSVC(penalty='l1', dual=False)\nselector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.coef_[0] != 0]\n",
        "\nfeatureSelector = LinearSVC(penalty='l1', dual=False)\nfeatureSelector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.coef_[0] != 0]\n",
        "\n    featureSelector = LinearSVC(penalty='l1', dual=False)\n    featureSelector.fit(X, y)\n    selected_feature_indices = featureSelector.coef_.nonzero()[1]\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndf1 = load_data()\n\nslopes = np.array([]) # blank array to store slopes\n\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] # removes NaN values for each column to apply sklearn function\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis=0)\n\nprint(slopes)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndf1 = load_data()\n\nslopes = []\nfor col in df1.columns:\n    if col != 'Time':\n        df2 = df1[~np.isnan(df1[col])]\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        slopes.append(m)\n\nprint(slopes)\n",
        "\nlabel_encoder = LabelEncoder()\ndf['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\nlabel_encoder = LabelEncoder()\ndf['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n    label_encoder = LabelEncoder()\n    transformed_df = df.copy()\n    transformed_df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\nElasticNet = linear_model.ElasticNet() # create an ElasticNet instance\nElasticNet.fit(X_train, y_train) # fit data\n\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a.reshape(-1, 1)).reshape(a.shape)\n",
        "# [Missing Code]\npredict = clf.predict(b)\n",
        "\nnew_X = np.array(X)\n",
        "\nnew_X = np.array(X)\n",
        "\nnew_X = np.array(X)\n",
        "\nX = dataframe.iloc[:, 1:-1].astype(float)\ny = dataframe.iloc[:, -1]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\ndataframe = dataframe.drop([\"Name\"], axis=1)\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\n\nX = dataframe.iloc[:, 1:-1].astype(float)\ny = dataframe.iloc[:, -1]\n\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\npredict = logReg.predict(X)\nprint(predict)\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n",
        "\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfeatures_dataframe = load_data()\n\ndef solve(features_dataframe):\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort_values(by=\"date\")\n    test_dataframe = test_dataframe.sort_values(by=\"date\")\n    return train_dataframe, test_dataframe\n\ntrain_dataframe, test_dataframe = solve(features_dataframe)\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\ncols = ['X2', 'X3']\ndf[cols + '_scale'] = df.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncols = myData.columns[2:4]\nmyData[['new_A2', 'new_A3']] = myData.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_).sort_values(by='mean_fit_time')\n",
        "\njoblib.dump(fitted_model, \"sklearn_model\")\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndf = load_data()\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\nprint(cosine_similarity_matrix)\n"
    ],
    "Pytorch": [
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\nembedding_weights = word2vec.wv.vectors\nembedding_weights = np.vstack((np.zeros((1, embedding_weights.shape[1])), embedding_weights))\nembedding_weights = torch.tensor(embedding_weights)\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights)\nembedded_input = embedding_layer(input_Tensor)\n",
        "\n    # Get the embedding weights from the gensim Word2Vec model\n    embedding_weights = word2vec.wv.vectors\n\n    # Create a PyTorch embedding layer with the same number of embeddings and embedding dimension\n    embedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_weights))\n\n    # Embed the input tensor using the embedding layer\n    embedded_input = embedding_layer(torch.LongTensor(input_Tensor))\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_logical]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\n    C = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B.index_select(1, idx)\n",
        "\nx_tensor = torch.from_numpy(np.array(x_array.tolist()))\n",
        "\nx_tensor = torch.tensor(x_array)\n",
        "\nimport pandas as pd\nimport torch\nimport numpy as np\nx_array = load_data()\ndef Convert(a):\n    t = torch.from_numpy(np.array(a.tolist()))\n    return t\nx_tensor = Convert(x_array)\nprint(x_tensor)\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nmask = torch.zeros(len(lens), max(lens))\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\n\nlens = load_data()\n\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\n\nmask = get_mask(lens)\nprint(mask)\n",
        "\nTensor_3D = torch.unsqueeze(Tensor_2D, dim=2)\n",
        "\n    result = torch.diag_embed(t)\n",
        "\nab = torch.cat((a, b.unsqueeze(0)), dim=0)\n",
        "\nab = torch.cat((a, b), dim=0)\n",
        "\n    ab = torch.cat((a, b.unsqueeze(0)), dim=0)\n",
        "\nfor i in range(a.shape[0]):\n    a[i, lengths[i]:, :] = 0\n",
        "\na[:, lengths.long():, :] = 2333\n",
        "\nfor i in range(a.size(0)):\n    a[i, :int(lengths[i]), :] = 0\n",
        "\nfor i in range(a.size(0)):\n    a[i, :int(lengths[i]), :] = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, torch.unsqueeze(ids, 2)).squeeze()\n",
        "\n_, y = torch.max(softmax_output, dim=1, keepdim=True)\n",
        "\n_, y = torch.max(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.max(softmax_output, dim=1, keepdim=True)\n",
        "\n    _, y = torch.max(softmax_output, dim=1)\n",
        "\n    _, y = torch.max(softmax_output, dim=1)\n",
        "\nn, c, w, z = images.size()\nlog_p = F.log_softmax(images, dim=1)\nlog_p = log_p.permute(0, 2, 3, 1).contiguous().view(-1, c)\nlog_p = log_p[labels.view(n, w, z, 1).repeat(1, 1, 1, c) >= 0]\nlog_p = log_p.view(-1, c)\n\nmask = labels >= 0\ntarget = labels[mask]\nloss = F.nll_loss(log_p, target.view(-1), size_average=False)\nloss /= mask.data.sum()\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = np.count_nonzero(A != B)\n",
        "\n    cnt_equal = np.sum(A == B)\n",
        "\nx = A.shape[0] // 2\ncnt_equal = np.sum(A[-x:] == B[-x:])\n",
        "\nx = A.shape[0] // 2\ncnt_not_equal = np.sum(A[-x:] != B[-x:])\n",
        "\na_split = torch.split(a, chunk_dim, dim=3)\ntensors_31 = [a_split[i] for i in range(len(a_split))]\n",
        "\na_split = torch.chunk(a, chunk_dim, dim=2)\ntensors_31 = [a_split[i] for i in range(chunk_dim)]\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = min * sign_x * sign_y\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = sign_x * max\n",
        "\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_values = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = sign_x * min_values\n",
        "\nconfidence_score = torch.nn.functional.softmax(output, dim=1)\n",
        "\nresult = torch.cat((a[:, :-1], (a[:, -1] + b[:, 0]) / 2, b[:, 1:]), dim=1)\n",
        "\n    result = torch.cat((a[:, :-1], (a[:, -1] + b[:, 0]) / 2, b[:, 1:]), dim=1)\n",
        "\nnew = torch.zeros(1, t.shape[1]+2)\nresult = torch.cat([new, t, new], dim=0)\n",
        "\nnew = torch.zeros((t.shape[0]+2, t.shape[1]+2))\nnew[1:-1, 1:-1] = t\nresult = new\n",
        "\nnew = torch.tensor([[-1, -1, -1, -1]])\nresult = torch.cat((new, t, new), dim=1)\n",
        "\nresult = torch.matmul(data, W.unsqueeze(1)).squeeze()\n"
    ]
}