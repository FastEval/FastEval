{
    "Pandas": [
        "\nresult = df.reindex(List)\n",
        "df_new = df.reindex(List)\nresult = sum(df_new['Type'] != df['Type'])\n",
        "\nfor col in df.columns:\n    s = df[col].value_counts()\n    df[col] = df[col].apply(lambda x: x if s[x] >= 2 else 'other')\nresult = df\n",
        "\nfor col in df.columns:\n    counts = df[col].value_counts()\n    mask = df[col].map(counts) < 3\n    df.loc[mask, col] = 'other'\nresult = df\n",
        "\nfor col in df.columns:\n    mask = df[col].map(df[col].value_counts()) < 2\n    df.loc[mask, col] = 'other'\nresult = df\n",
        "\nfor col in df.columns:\n    if col == 'Qu1':\n        s = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: x if s[x] >= 3 else 'other')\n    else:\n        s = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: x if s[x] >= 2 else 'other')\nresult = df\n",
        "\nfor col in ['Qu1', 'Qu2', 'Qu3']:\n    if col == 'Qu1':\n        s = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: x if (s[x] >= 3 or x == 'apple') else 'other')\n    else:\n        s = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: x if s[x] >= 2 else 'other')\nresult = df\n",
        "\nresult = pd.concat([df[df['keep_if_dup'] == 'Yes'], df[df['keep_if_dup'] == 'No'].drop_duplicates(subset='url', keep='first')])\n",
        "\ndf_yes = df[df['drop_if_dup'] == 'Yes'].drop_duplicates(subset='url', keep='first')\ndf_no = df[df['drop_if_dup'] == 'No']\nresult = pd.concat([df_yes, df_no]).sort_index()\n",
        "\ndf1 = df[df['keep_if_dup'] == 'Yes']\ndf2 = df[df['keep_if_dup'] == 'No'].drop_duplicates(subset='url', keep='last')\nresult = pd.concat([df1, df2]).sort_index()\n",
        "\ndef nested_dict(df):\n    if len(df.columns) == 1:\n        if df.values.size == 1: return df.values[0][0]\n        return df.values.squeeze()\n    grouped = df.groupby(df.columns[0])\n    d = {k: nested_dict(g.iloc[:,1:]) for k,g in grouped}\n    return d\nresult = nested_dict(df)\n",
        "df['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\nresult = df\n",
        "df['datetime'] = df['datetime'].dt.tz_localize(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf = df.sort_values(by='datetime')\n",
        "df['datetime'] = df['datetime'].dt.tz_localize(None)\ndf = df.sort_values(by='datetime')\n",
        "\ndef parse_message(message):\n    message = message.strip('[]')\n    pairs = message.split(',')\n    result = {}\n    for pair in pairs:\n        key, value = pair.split(':')\n        result[key.strip()] = value.strip() if value.strip() else 'none'\n    return result\ndf['message'] = df['message'].apply(parse_message)\nresult = pd.concat([df.drop(['message'], axis=1), df['message'].apply(pd.Series)], axis=1)\n",
        "df.loc[df['product'].isin(products), 'score'] *= 10",
        "df.loc[~df['product'].isin(products), 'score'] *= 10",
        "\nfor product_range in products:\n    df.loc[df['product'].between(product_range[0], product_range[1]), 'score'] *= 10\n",
        "\nmask = df['product'].isin(products)\ndf.loc[mask, 'score'] = (df.loc[mask, 'score'] - df.loc[mask, 'score'].min()) / (df.loc[mask, 'score'].max() - df.loc[mask, 'score'].min())\n",
        "\ndf['category'] = df.apply(lambda x: df.columns[x.idxmax()], axis=1)\n",
        "df['category'] = df.eq(0).dot(df.columns)",
        "df['category'] = df.apply(lambda row: [col for col in df.columns if row[col]==1], axis=1)",
        "df['Date'] = df['Date'].dt.strftime('%b-%Y')",
        "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\nstart_date = '2019-01-17'\nend_date = '2019-02-20'\nmask = (df['Date'] > start_date) & (df['Date'] <= end_date)\ndf = df.loc[mask]\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\n",
        "df['#1'] = df['#1'].shift(1)\ndf['#1'].iloc[0] = df['#1'].iloc[-1]",
        "df['#1'] = df['#1'].shift(-1)\ndf['#1'].fillna(df['#1'].iloc[0], inplace=True)",
        "\ndf['#1'] = df['#1'].shift(1)\ndf['#1'].iloc[0] = df['#1'].iloc[-1]\ndf['#2'] = df['#2'].shift(-1)\ndf['#2'].iloc[-1] = df['#2'].iloc[0]\n",
        "\nfrom sklearn.metrics import r2_score\nimport numpy as np\nmin_r2 = np.inf\nmin_df = None\nfor i in range(len(df)):\n    df['#1'] = df['#1'].shift(-1)\n    df.iloc[0, 0] = df.iloc[-1, 0]\n    r2 = r2_score(df['#1'], df['#2'])\n    if r2 < min_r2:\n        min_r2 = r2\n        min_df = df.copy()\ndf = min_df\n",
        "df.columns = [col + 'X' for col in df.columns]",
        "df.columns = 'X' + df.columns\n",
        "df.columns = ['X' + col if not col.endswith('X') else col for col in df.columns]\n",
        "agg_dict = {col: 'mean' for col in df.columns if 'val' in col}\nagg_dict['group_color'] = 'first'\nresult = df.groupby('group').agg(agg_dict)\n",
        "agg_dict = {col: 'sum' for col in df.columns if 'val' in col}\nagg_dict['group_color'] = 'first'\nresult = df.groupby('group').agg(agg_dict)\n",
        "\nagg_dict = {col: 'mean' if col.endswith('2') else 'sum' for col in df.columns if col not in ['group', 'group_color']}\nagg_dict['group_color'] = 'first'\nresult = df.groupby('group').agg(agg_dict)\n",
        "result = df.loc[row_list, column_list].mean()",
        "result = df.loc[row_list, column_list].sum()\n",
        "result = df.loc[row_list, column_list].sum()\nresult = result.drop(result.idxmax())\n",
        "\nresult = df.apply(lambda x: x.value_counts()).stack().sort_index(level=1)\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor col in df:\n    result += \"---- \" + col + \" ---\\n\"\n    result += str(df[col].value_counts()) + \"\\n\"\n",
        "\ndf.columns = df.iloc[0]\ndf = df.reindex(df.index.drop(0)).reset_index(drop=True)\nresult = df\n",
        "\ndf.columns = df.iloc[0].fillna('') + df.iloc[1].fillna('')\ndf = df.iloc[2:]\nresult = df.reset_index(drop=True)\n",
        "\nresult = df.apply(lambda x: pd.Series(x.dropna().values.tolist() + x[x.isnull()].values.tolist()), axis=1)\n",
        "\nresult = df.apply(lambda x: pd.Series(np.concatenate([x[x.isnull()].values, x[x.notnull()].values])), axis=1)\n",
        "\nresult = df.apply(lambda s: pd.Series(s.dropna().values, index=s.index[:len(s.dropna())]), axis=0)\n",
        "\ndf_above_thresh = df[df['value'] >= thresh]\ndf_below_thresh = df[df['value'] < thresh]\ndf_below_thresh = df_below_thresh.sum()\ndf_below_thresh.name = 'X'\nresult = df_above_thresh.append(df_below_thresh)\n",
        "\ndf_below_thresh = df[df['value'] < thresh]\ndf_above_thresh = df[df['value'] >= thresh]\navg_above_thresh = df_above_thresh['value'].mean()\nresult = df_below_thresh.append(pd.DataFrame({'value': avg_above_thresh}, index=['X']))\n",
        "\ndf_in_section = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\ndf_out_section = df[(df['value'] < section_left) | (df['value'] > section_right)]\naverage_out_section = df_out_section['value'].mean()\ndf_out_section = pd.DataFrame({'value': [average_out_section]}, index=['X'])\nresult = pd.concat([df_in_section, df_out_section])\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    result['inv_'+col] = 1/df[col]\n",
        "\nimport numpy as np\nfor col in df.columns:\n    df['exp_'+col] = np.exp(df[col])\nresult = df\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    result['inv_'+col] = 1/df[col].replace(0, float('inf'))\n",
        "\nfor col in df.columns:\n    df['sigmoid_'+col] = 1 / (1 + np.exp(-df[col]))\nresult = df\n",
        "\nresult = df.where(df.values == df.expanding().max().values).idxmax()\nresult = result.where(result <= df.idxmin())\n",
        "\nresult = df.where(df.ge(df.min()).cumsum().gt(0)).idxmax()\n",
        "\ndf.set_index('dt', inplace=True)\ndf = df.groupby('user').resample('D').first()\ndf['user'].fillna(method='ffill', inplace=True)\ndf['val'].fillna(0, inplace=True)\ndf.reset_index(inplace=True)\nresult = df\n",
        "\ndf.set_index('dt', inplace=True)\ndf = df.groupby('user').resample('D').first().reset_index()\ndf['val'].fillna(0, inplace=True)\nresult = df\n",
        "\ndf.set_index('dt', inplace=True)\ndf = df.groupby('user').resample('D').asfreq()\ndf['val'].fillna(233, inplace=True)\ndf.reset_index(inplace=True)\nresult = df\n",
        "\ndf = df.set_index('dt').groupby('user').resample('D').first().reset_index()\ndf['val'] = df.groupby('user')['val'].transform(lambda x: x.ffill().fillna(x.max()))\nresult = df\n",
        "\ndf = df.sort_values(['user', 'dt'])\ndf['val'] = df.groupby('user')['val'].transform('max')\nr = pd.date_range(start=df.dt.min(), end=df.dt.max())\ndf = df.set_index('dt').reindex(r).fillna(method='ffill').rename_axis('dt').reset_index()\ndf['dt'] = df['dt'].dt.strftime('%d-%b-%Y')\nresult = df\n",
        "df['name'] = df['name'].astype('category')\ndf['name'] = df['name'].cat.codes\nresult = df\n",
        "df['a'] = df.groupby('name').cumcount() + 1\nresult = df\n",
        "\ndf['name'] = df['name'].astype('category')\ndf['name'] = df['name'].cat.codes\nresult = df\n",
        "df['ID'] = (df['name'] + df['a'].astype(str)).astype('category').cat.codes\nresult = df.drop(['name', 'a'], axis=1)\n",
        "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "df = df.melt(id_vars='user', var_name='others', value_name='value')\n",
        "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.dropna(subset=['value'])\ndf = df.sort_values(by=['user'])\n",
        "result = df.loc[df['c'] > 0.5, columns]",
        "result = df.loc[df['c'] > 0.45, columns].values\n",
        "result = df.loc[df['c'] > 0.5, columns].values\n",
        "\n    result = df[df['c'] > 0.5][columns]\n    result['sum'] = result[columns].sum(axis=1)\n    ",
        "result = df.loc[df['c'] > 0.5, columns]",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['diff'] = df['date'].diff().dt.days\nresult = df[df['diff'].isna() | (df['diff'] > X)]\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['diff'] = df['date'].diff().dt.days\nresult = df[df['diff'] > X*7].drop('diff', axis=1)\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\ndf['diff'] = df['date'].diff().dt.days\ndf['diff'] = df['diff'].fillna(0)\ndf['diff'] = df['diff'].apply(lambda x: x/7)\nresult = df[df['diff'] >= X]\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\ndf['group'] = df.index // 3\nresult = df.groupby('group')['col1'].mean().reset_index(drop=True)\n",
        "\ndf['group'] = df.index // 4\nresult = df.groupby('group').sum()\n",
        "\ndf = df.iloc[::-1]\nresult = df.rolling(3).mean().iloc[::-1]\nresult = result.dropna()\n",
        "\nresult = pd.DataFrame()\nresult['col1'] = [df['col1'][i:i+3].sum() if i%5<3 else df['col1'][i:i+2].mean() for i in range(0, len(df), 3 if i%5<3 else 2)]\n",
        "\ndf = df[::-1].reset_index(drop=True)\ngrouped = df.groupby(df.index // 3)\nresult = grouped['col1'].agg(['sum', 'mean']).stack().reset_index(drop=True)\nresult = result[::-1].reset_index(drop=True)\n",
        "\ndf['A'] = df['A'].replace(0, pd.np.nan)\ndf['A'] = df['A'].fillna(method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, pd.np.nan)\ndf['A'] = df['A'].fillna(method='bfill')\n",
        "\ndf['A'] = df['A'].replace(0, pd.np.nan)\ndf['A'] = df['A'].fillna(method='ffill').where(df['A'].fillna(method='ffill') >= df['A'].fillna(method='bfill'), df['A'].fillna(method='bfill'))\n",
        "df['number'] = df['duration'].str.extract('(\\d+)').astype(int)\ndf['time'] = df['duration'].str.extract('([a-zA-Z]+)')\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})",
        "df['number'] = df['duration'].str.extract('(\\d+)', expand=False)\ndf['time'] = df['duration'].str.extract('([a-zA-Z]+)', expand=False)\ndf['time_day'] = df['time'].map({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\n    df['number'] = df['duration'].str.extract('(\\d+)').astype(int)\n    df['time'] = df['duration'].str.extract('([a-zA-Z]+)')\n    df['time_days'] = df['time'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    ",
        "df['number'] = df['duration'].str.extract('(\\d+)').astype(int)\ndf['time'] = df['duration'].str.extract('([a-zA-Z]+)')\ndf['time_day'] = df['time'].map({'year': 365, 'month': 30, 'week': 7, 'day': 1})\ndf['time_day'] = df['time_day'] * df['number']\n",
        "\nresult = (df1[columns_check_list] != df2[columns_check_list]).any(axis=1).tolist()\n",
        "\nresult = (df1[columns_check_list] == df2[columns_check_list]).all(axis=1).tolist()\n",
        "\ndf.reset_index(level=1, inplace=True)\ndf['date'] = pd.to_datetime(df['date'])\ndf.set_index('date', append=True, inplace=True)\n",
        "\ndf.reset_index(inplace=True)\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf.set_index(['name', 'datetime'], inplace=True)\n",
        "\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.values\n    ",
        "\ndf.index = pd.MultiIndex.from_tuples([(i[1], pd.to_datetime(i[0])) for i in df.index.values])\ndf = df.swaplevel(0, 1)\n",
        "\ndf = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf.columns.name = None\n",
        "\ndf = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(['Country', 'Variable', 'year'], ascending=[True, True, False])\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf.columns.name = None\n",
        "value_cols = [col for col in df.columns if 'Value' in col]\ndf = df[df[value_cols].abs().max(axis=1) < 1]",
        "\nvalue_cols = [col for col in df.columns if 'Value' in col]\ndf = df[df[value_cols].abs().max(axis=1) > 1]\n",
        "\nvalue_cols = [col for col in df.columns if 'Value' in col]\ndf = df[df[value_cols].abs().max(axis=1) > 1]\ndf.columns = [col.replace('Value_', '') for col in df.columns]\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT;', '<', regex=True)\n",
        "\nfor col in df.columns:\n    df[col] = df[col].replace('&AMP;', '&', regex=True)\nresult = df\n",
        "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\ndf = df.replace({'&AMP;': '&'}, regex=True)\ndf['A'] = df['A'].replace({'1 & 1': '1 & 1 = 1', '1 & 0': '1 & 0 = 0'}, regex=True)\ndf['C'] = df['C'].replace({'0 & 0': '0 & 0 = 0'}, regex=True)\n",
        "\ndef split_name(name):\n    parts = name.split()\n    if len(parts) == 2:\n        return pd.Series((parts[0], parts[1]))\n    else:\n        return pd.Series((name, None))\ndf[['first_name', 'last_name']] = df['name'].apply(split_name)\n",
        "\ndef split_name(name):\n    parts = name.split()\n    if len(parts) == 2:\n        return pd.Series([parts[0], parts[1]])\n    else:\n        return pd.Series([name, None])\ndf[['1_name', '2_name']] = df['name'].apply(split_name)\n",
        "\ndf['name_split'] = df['name'].str.split(' ')\ndf['first_name'] = df['name_split'].apply(lambda x: x[0])\ndf['middle_name'] = df['name_split'].apply(lambda x: ' '.join(x[1:-1]) if len(x) > 2 else np.nan)\ndf['last_name'] = df['name_split'].apply(lambda x: x[-1] if len(x) > 1 else np.nan)\ndf.drop(columns=['name', 'name_split'], inplace=True)\n",
        "\ndf1.set_index('Timestamp', inplace=True)\ndf2.set_index('Timestamp', inplace=True)\nresult = df2.join(df1, how='left')\n",
        "\ndf1.set_index('Timestamp', inplace=True)\ndf2.set_index('Timestamp', inplace=True)\nresult = df1.join(df2, how='left')\n",
        "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2']>50 and row['col3']>50 else row['col1']+row['col2']+row['col3'], axis=1)\n",
        "\nerrors = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        errors.append(row['Field1'])\nresult = errors\n",
        "\nresult = [x for x in df['Field1'] if isinstance(x, int)]\n",
        "\nresult = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        result.append(row['Field1'])\n",
        "df.set_index('cat', inplace=True)\ndf = df.div(df.sum(axis=1), axis=0)\ndf.reset_index(inplace=True)",
        "df.iloc[:, 1:] = df.iloc[:, 1:].div(df.iloc[:, 1:].sum(), axis='columns')\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "result = df.drop(test)\n",
        "\nresult = df.loc[test].drop_duplicates()\n",
        "\nfrom scipy.spatial import distance_matrix\ndef nearest_neighbour(df):\n    dist_matrix = distance_matrix(df[['x', 'y']].values, df[['x', 'y']].values)\n    np.fill_diagonal(dist_matrix, np.inf)\n    nearest_neighbour = np.argmin(dist_matrix, axis=1)\n    euclidean_distance = np.min(dist_matrix, axis=1)\n    df['nearest_neighbour'] = df.iloc[nearest_neighbour]['car'].values\n    df['euclidean_distance'] = euclidean_distance\n    return df\ndf = df.groupby('time').apply(nearest_neighbour)\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\nfrom itertools import combinations\ndef get_farthest_neighbour(group):\n    if len(group) == 1:\n        return pd.DataFrame({'car': group['car'], 'farthest_neighbour': [None], 'euclidean_distance': [None]})\n    else:\n        dist_matrix = pdist(group[['x', 'y']])\n        dist_matrix = squareform(dist_matrix)\n        np.fill_diagonal(dist_matrix, -1)\n        farthest_neighbour = np.argmax(dist_matrix, axis=1)\n        euclidean_distance = np.max(dist_matrix, axis=1)\n        return pd.DataFrame({'car': group['car'], 'farthest_neighbour': group['car'].iloc[farthest_neighbour], 'euclidean_distance': euclidean_distance})\ndf = df.groupby('time').apply(get_farthest_neighbour).reset_index()\n",
        "\ndf['keywords_all'] = df.apply(lambda row: ','.join(row.dropna().astype(str)), axis=1)\n",
        "\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().astype(str)), axis=1)\n",
        "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda row: '-'.join(row.dropna().values[::-1]), axis=1)\n",
        "\nsample = df.sample(frac=0.2, random_state=0)\ndf.loc[sample.index, 'Quantity'] = 0\n",
        "\nsample = df.sample(frac=0.2, random_state=0)\ndf.loc[sample.index, 'ProductId'] = 0\n",
        "\nfor user in df['UserId'].unique():\n    user_rows = df[df['UserId'] == user]\n    sample_rows = user_rows.sample(frac=0.2, random_state=0)\n    df.loc[sample_rows.index, 'Quantity'] = 0\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nfirst_duplicate_index = df.drop_duplicates(subset=['col1','col2'], keep='first').reset_index()\nfirst_duplicate_index = first_duplicate_index.rename(columns={'index':'index_original'})\nresult = pd.merge(duplicate, first_duplicate_index, on=['col1','col2'], how='left')\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.apply(lambda row: df[(df['col1'] == row['col1']) & (df['col2'] == row['col2'])].index[-1], axis=1)\nresult = duplicate\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.groupby(['col1','col2']).cumcount()\n    result = duplicate\n    ",
        "\ndf['index_original'] = df.duplicated(subset=['col1','col2', '3col'], keep='first').groupby(df[['col1','col2', '3col']].apply(tuple)).cumcount()\nresult = df[df['index_original'] != 0]\n",
        "\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep=False).groupby(df[['col1','col2']].apply(tuple,1)).cumsum()\ndf['index_original'] = df['index_original'].map(df[df.duplicated(subset=['col1','col2'], keep=False)].groupby(['col1','col2']).apply(lambda x: x.index[-1]))\nresult = df[df.duplicated(subset=['col1','col2'], keep='first')]\n",
        "\nidx = df.groupby(['Sp','Mt'])['count'].transform(max) == df['count']\nresult = df[idx]\n",
        "\nresult = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\n",
        "\nidx = df.groupby(['Sp','Mt'])['count'].transform(min) == df['count']\nresult = df[idx]\n",
        "\nresult = df[df.groupby(['Sp','Value'])['count'].transform(max) == df['count']]\n",
        "\nresult = df[df['Category'].isin(filter_list)]\n",
        "result = df[~df['Category'].isin(filter_list)]",
        "\nresult = pd.melt(df, value_vars=df.columns.tolist())\n",
        "\ndf.columns = df.columns.map('_'.join)\ndf = df.reset_index().melt(id_vars='index', var_name=['variable_0', 'variable_1', 'variable_2'])\nresult = df.sort_values(['variable_0', 'variable_1', 'variable_2']).drop('index', axis=1).reset_index(drop=True)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].clip(lower=0)\n",
        "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum(skipna=False))\n",
        "\nresult = df.groupby('r')['v'].apply(lambda x: np.sum(x) if not x.isnull().any() else np.nan)\n",
        "\nresult = df.groupby('l')['v'].apply(lambda x: np.sum(x) if not x.isnull().any() else np.nan).reset_index()\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].nunique() == df[col2].nunique():\n                if df[col1].nunique() == len(df):\n                    result.append(f'{col1} {col2} one-to-one')\n                else:\n                    result.append(f'{col1} {col2} many-to-many')\n            elif df[col1].nunique() > df[col2].nunique():\n                result.append(f'{col1} {col2} one-to-many')\n            else:\n                result.append(f'{col1} {col2} many-to-one')\n",
        "\ndef relationship_type(df, col1, col2):\n    if df[col1].is_unique and df[col2].is_unique:\n        return 'one-2-one'\n    elif df[col1].is_unique and not df[col2].is_unique:\n        return 'one-2-many'\n    elif not df[col1].is_unique and df[col2].is_unique:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            result.append(f'{col1} {col2} {relationship_type(df, col1, col2)}')\n",
        "\ndef relationship_type(df, col1, col2):\n    if df[col1].is_unique and df[col2].is_unique:\n        return 'one-to-one'\n    elif df[col1].is_unique and not df[col2].is_unique:\n        return 'one-to-many'\n    elif not df[col1].is_unique and df[col2].is_unique:\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            result.loc[col1, col2] = relationship_type(df, col1, col2)\n",
        "\ndef relationship_type(df, col1, col2):\n    if df[col1].is_unique and df[col2].is_unique:\n        return 'one-2-one'\n    elif df[col1].is_unique and not df[col2].is_unique:\n        return 'one-2-many'\n    elif not df[col1].is_unique and df[col2].is_unique:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            result.loc[col1, col2] = relationship_type(df, col1, col2)\n",
        "\n# sort by bank column in descending order so that NaN values will be at the end\ndf = df.sort_values(by='bank', ascending=False)\n# drop duplicates based on firstname, lastname, email and keep the first one which will be the one with bank account\nresult = df.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')\n",
        "\nresult = pd.to_numeric(s.str.replace(',',''), errors='coerce')\n",
        "\ndf['Family'] = ['Has Family' if x > 0 or y > 0 else 'No Family' for x, y in zip(df['SibSp'], df['Parch'])]\nresult = df.groupby('Family')['Survived'].mean()\n",
        "\ndf['Group'] = ['Has Family' if x > 0 or y > 0 else 'No Family' for x, y in zip(df['Survived'], df['Parch'])]\nresult = df.groupby('Group')['SibSp'].mean()\n",
        "df['Family'] = pd.np.select(\n    [\n        (df['SibSp'] == 1) & (df['Parch'] == 1), \n        (df['SibSp'] == 0) & (df['Parch'] == 0), \n        (df['SibSp'] == 0) & (df['Parch'] == 1), \n        (df['SibSp'] == 1) & (df['Parch'] == 0)\n    ], \n    [\n        'Has Family', \n        'No Family', \n        'New Family', \n        'Old Family'\n    ], \n    default='Unknown'\n)\nresult = df.groupby('Family')['Survived'].mean()",
        "\ndf = df.sort_values(['cokey', 'A'])\ndf.set_index(['cokey'], inplace=True)\nresult = df\n",
        "\ndf = df.sort_values(['cokey', 'A'], ascending=[True, False])\nresult = df.set_index(['cokey', df.groupby('cokey').cumcount()])\n",
        "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])\n",
        "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])",
        "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])",
        "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\nresult = df.groupby('a').b.agg(['mean', 'std'])\n",
        "\nresult = df.groupby('b').agg({'a':['mean','std']})\n",
        "\nimport numpy as np\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\ndf['softmax'] = df.groupby('a')['b'].transform(softmax)\ndf['min-max'] = df.groupby('a')['b'].transform(min_max)\n",
        "\nresult = df.loc[:, (df != 0).any(axis=0)]\nresult = result.loc[(result != 0).any(axis=1)]\n",
        "\nresult = df.loc[:, (df != 0).any(axis=0)]\nresult = result.loc[(result != 0).any(axis=1)]\n",
        "\nresult = df.loc[:, (df.max() < 3)]\nresult = result.loc[(result.max(axis=1) < 3)]\n",
        "\nresult = df.mask(df == 2, 0)\n",
        "result = s.sort_values().sort_index(kind='mergesort')",
        "\ndf = s.sort_values().reset_index()\ndf.columns = ['index', 1]\ndf = df.sort_values(by=[1, 'index'])\n",
        "\nresult = df[pd.to_numeric(df['A'], errors='coerce').notnull()]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nidx = df.groupby(['Sp','Mt'])['count'].transform(max) == df['count']\nresult = df[idx]\n",
        "\nresult = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\n",
        "\nresult = df[df.groupby(['Sp','Mt'])['count'].transform(min) == df['count']]\n",
        "\nresult = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Date'])\n",
        "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'].fillna('17/8/1926', inplace=True)\n",
        "\n    df['Date'] = df['Member'].map(dict).fillna(df['Date'])\n    result = df\n    ",
        "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17/8/1926')\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\n",
        "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.dt.to_period('M').map(df.groupby(df.Date.dt.to_period('M')).size())\ndf['Count_y'] = df.Date.dt.to_period('Y').map(df.groupby(df.Date.dt.to_period('Y')).size())",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby('Date')['Date'].transform('count')\ndf['Count_m'] = df.groupby(df['Date'].dt.to_period('M'))['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.to_period('Y'))['Date'].transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val'])['Val'].transform('count')\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby('Date')['Date'].transform('count')\ndf['Count_m'] = df.groupby(df['Date'].dt.to_period('M'))['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year)['Date'].transform('count')\ndf['Count_w'] = df.groupby(df['Date'].dt.week)['Date'].transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val'])['Val'].transform('count')\n",
        "df['B_zero'] = (df['B'] == 0)\ndf['C_zero'] = (df['C'] == 0)\ndf['B_non_zero'] = (df['B'] != 0)\ndf['C_non_zero'] = (df['C'] != 0)\nresult1 = df.groupby('Date')[['B_zero', 'C_zero']].sum().astype(int)\nresult2 = df.groupby('Date')[['B_non_zero', 'C_non_zero']].sum().astype(int)\nresult1.columns = ['B', 'C']\nresult2.columns = ['B', 'C']\n",
        "\ndf['B_even'] = df['B'] % 2 == 0\ndf['C_even'] = df['C'] % 2 == 0\ndf['B_odd'] = df['B'] % 2 == 1\ndf['C_odd'] = df['C'] % 2 == 1\nresult1 = df.groupby('Date')[['B_even', 'C_even']].sum().astype(int)\nresult1.columns = ['B', 'C']\nresult2 = df.groupby('Date')[['B_odd', 'C_odd']].sum().astype(int)\nresult2.columns = ['B', 'C']\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], \n                        aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], \n                        aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = df.groupby('B').agg({'D': np.max, 'E': np.min})\n",
        "\ndf = df.reset_index().rename(columns={'index':'id'})\ndf['var2'] = df['var2'].str.split(',')\nresult = df.explode('var2')\n",
        "\nddf = dd.from_pandas(df, npartitions=2)\nddf['var2'] = ddf['var2'].str.split(',')\nresult = ddf.explode('var2').compute()\n",
        "\ndf['var2'] = df['var2'].str.split('-')\nresult = df.explode('var2').reset_index(drop=True)\n",
        "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df[\"str\"].apply(count_special_char)",
        "\ndef count_alpha_chars(string):\n    alpha_chars = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            alpha_chars = alpha_chars + 1\n    return alpha_chars\ndf[\"new\"] = df['str'].apply(count_alpha_chars)\n",
        "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)",
        "df[['fips', 'row']] = df['row'].str.split(' ', expand=True)",
        "df[['fips','medi','row']] = df['row'].str.split(' ', 2, expand=True)\ndf['fips'] = df['fips'].str.zfill(5)\ndf['medi'] = df['medi'].str.zfill(5)\ndf['row'] = df['row'].str.zfill(5)",
        "\ndf.set_index('Name', inplace=True)\ndf = df.mask(df == 0).expanding(1).mean().fillna(0)\ndf.reset_index(inplace=True)\n",
        "\ndf.set_index('Name', inplace=True)\ndf = df.iloc[:, ::-1]\ndf = df.mask(df == 0).expanding(axis=1).mean().round(2)\ndf = df.iloc[:, ::-1]\ndf.reset_index(inplace=True)\n",
        "\ndf = df.set_index('Name')\ndf = df.mask(df == 0)\nresult = df.expanding(1).mean().round(3)\n",
        "\ndf.set_index('Name', inplace=True)\ndf = df.iloc[:, ::-1]\ndf = df.mask(df == 0)\ndf = df.expanding(axis=1).mean()\ndf = df.iloc[:, ::-1]\n",
        "df['Label'] = (df['Close'].diff() > 0).astype(int)\ndf['Label'].iloc[0] = 1\n",
        "df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['label'].iloc[0] = 1\n",
        "df['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['label'].iloc[0] = 1\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\ndf['Duration'] = df['arrival_time'].shift(-1) - df['departure_time']\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\ndf['Duration'] = (df['arrival_time'].shift(-1) - df['departure_time']).dt.total_seconds()\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\ndf['Duration'] = (df['arrival_time'].shift(-1) - df['departure_time']).dt.total_seconds()\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['arrival_time'] = df['arrival_time'].replace('NaT', 'NaN')\ndf['departure_time'] = df['departure_time'].replace('NaT', 'NaN')\n",
        "\ndf['count'] = df['key2'] == 'one'\nresult = df.groupby('key1')['count'].sum().reset_index()\n",
        "\nresult = df[df['key2'] == 'two'].groupby(['key1']).size().reset_index(name='count')\n",
        "\ndf['endswith_e'] = df['key2'].apply(lambda x: x.endswith('e'))\nresult = df.groupby('key1')['endswith_e'].sum().reset_index()\nresult.columns = ['key1', 'count']\n",
        "\ndf.index = pd.to_datetime(df.index)\nmax_result = df.index.max()\nmin_result = df.index.min()\n",
        "\ndf.index = pd.to_datetime(df.index)\nmode_result = df.index.mode()[0]\nmedian_result = df.index.median()\n",
        "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n",
        "\nresult = df.loc[df.groupby(\"item\")[\"diff\"].idxmin()]\n",
        "df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]) if \"_\" in x else x)\n",
        "df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n",
        "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', 1).str[0]\nresult = df\n",
        "\nnan_indexes = df[df['Column_x'].isna()].index\nhalf = len(nan_indexes) // 2\ndf.loc[nan_indexes[:half], 'Column_x'] = 0\ndf.loc[nan_indexes[half:], 'Column_x'] = 1\n",
        "\nnan_indexes = df[df['Column_x'].isna()].index\nnan_count = len(nan_indexes)\nfirst_third = int(nan_count * 0.3)\nsecond_third = int(nan_count * 0.6)\ndf.loc[nan_indexes[:first_third], 'Column_x'] = 0\ndf.loc[nan_indexes[first_third:second_third], 'Column_x'] = 0.5\ndf.loc[nan_indexes[second_third:], 'Column_x'] = 1\n",
        "\nnan_count = df['Column_x'].isna().sum()\nhalf_count = nan_count // 2\ndf.loc[df['Column_x'].isna(), 'Column_x'] = [0]*half_count + [1]*(nan_count-half_count)\n",
        "\nresult = pd.DataFrame({col: list(zip(a[col], b[col])) for col in a.columns})\n",
        "\ndataframes = [a, b, c]\nresult = pd.concat([df.stack() for df in dataframes], axis=1).apply(tuple, axis=1).unstack()\n",
        "\n# Align the dataframes\na_aligned, b_aligned = a.align(b, fill_value=np.nan)\n# Combine the dataframes into tuples\nresult = a_aligned.combine(b_aligned, lambda s1, s2: list(zip(s1, s2)))\n",
        "\ndf['bins'] = pd.cut(df.views, bins)\nresult = df.groupby(['username', 'bins']).size().unstack().fillna(0)\n",
        "\ndf['bins'] = pd.cut(df.views, bins)\nresult = df.groupby(['username', 'bins']).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack().fillna(0)\n",
        "\nresult = pd.DataFrame({'text': [', '.join(df['text'].values)]})\n",
        "\nresult = pd.DataFrame({'text': ['-'.join(df['text'].tolist())]}, index=[0])\n",
        "\nresult = pd.DataFrame({'text': [', '.join(df['text'][::-1])]})\n",
        "\nresult = pd.Series(df['text'].str.cat(sep=', '))\n",
        "\nresult = pd.Series({'text': '-'.join(df['text'][::-1])})\n",
        "\ndf2 = df2.merge(df1[['id', 'city', 'district']], on='id', how='left')\nresult = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "\ndf1['date'] = pd.to_datetime(df1['date']).dt.strftime('%d-%b-%Y')\ndf2['date'] = pd.to_datetime(df2['date']).dt.strftime('%d-%b-%Y')\ndf = pd.concat([df1, df2], axis=0, ignore_index=True)\ndf = df.merge(df1[['id', 'city', 'district']], on='id', how='left')\ndf = df.sort_values(['id', 'date'], ascending=[True, False]).reset_index(drop=True)\nresult = df\n",
        "\ndf2 = df2.merge(df1[['id', 'city', 'district']], on='id', how='left')\nresult = pd.concat([df1, df2], axis=0).sort_values(['id', 'date'])\n",
        "\nresult = C.set_index('A').combine_first(D.set_index('A')).reset_index()\nresult['B'] = result['B'].astype(int)\n",
        "\nmerged = pd.merge(C, D, on='A', how='left')\nmerged['B'] = merged['B_x'].where(merged['B_x'].notnull(), merged['B_y'])\nresult = merged[['A', 'B']]\n",
        "\nresult = pd.concat([C.set_index('A'), D.set_index('A')], axis=0).reset_index().drop_duplicates(subset='A', keep='last')\nresult['dulplicated'] = result.duplicated(subset='A', keep=False)\nresult = result.sort_index()\n",
        "\ndf = df.sort_values(['user', 'time'])\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: list(map(list, x.values)))\n",
        "df = df.sort_values(['user', 'time'])\ndf['amount-time-tuple'] = list(zip(df.time, df.amount))\nresult = df.groupby('user')['amount-time-tuple'].apply(list)\n",
        "\ndf = df.sort_values(['user', 'time'], ascending=[True, False])\ndf['amount-time-tuple'] = list(zip(df.amount, df.time))\nresult = df.groupby('user')['amount-time-tuple'].apply(list)\n",
        "df = pd.DataFrame(series.tolist(), index=series.index)",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index).reset_index()\ndf.columns = ['name'] + list(df.columns[1:])\n",
        "result = [col for col in df.columns if s in col]",
        "\nresult = [col for col in df.columns if s in col]\n",
        "\nspike_cols = [col for col in df.columns if s in col]\nfor i, col in enumerate(spike_cols, 1):\n    df.rename(columns={col: f'spike{i}'}, inplace=True)\nresult = df\n",
        "\nresult = pd.DataFrame(df['codes'].to_list(), columns=['code_0', 'code_1', 'code_2'])\n",
        "\nresult = pd.DataFrame(df['codes'].to_list(), columns=['code_1', 'code_2', 'code_3'])\n",
        "\ndf['codes'] = df['codes'].apply(lambda x: sorted(x))\nresult = pd.DataFrame(df['codes'].to_list(), columns=['code_1', 'code_2', 'code_3'])\n",
        "\ndf['col1'] = df['col1'].apply(literal_eval)\nresult = [item for sublist in df['col1'].tolist() for item in sublist]\n",
        "\nresult = ','.join([str(i) for sublist in df['col1'] for i in reversed(sublist)])\n",
        "\nresult = ','.join(map(str, [elem for sublist in df['col1'] for elem in sublist]))\n",
        "\ndf.set_index('Time', inplace=True)\ndf = df.resample('2T').mean()\ndf.reset_index(inplace=True)\n",
        "\ndf.set_index('Time', inplace=True)\ndf = df.resample('3T').sum()\ndf.reset_index(inplace=True)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n",
        "\nresult = df[df.index.get_level_values('a').map(filt)]\n",
        "result = df[filt[df.index.get_level_values('a')]]",
        "\nresult = df.columns[df.loc[0] != df.loc[8]]\n",
        "\nresult = df.loc[0].eq(df.loc[8])\nresult = result[result].index\n",
        "\nresult = df.columns[df.loc[0] != df.loc[8]].tolist()\n",
        "\nrow0 = df.iloc[0]\nrow8 = df.iloc[8]\nresult = [(x, y) for x, y in zip(row0, row8) if x != y or (pd.isnull(x) and not pd.isnull(y)) or (not pd.isnull(x) and pd.isnull(y))]\n",
        "\nts = pd.Series(df['Value'].values, index=df['Date'])\n",
        "\ndf.columns = [f\"{col}_{i+1}\" for i in range(df.shape[0]) for col in df.columns]\ndf = df.T.reset_index(drop=True).T\n",
        "\ndf = df.T\ndf.columns = [f'{col}_{i}' for i in range(len(df.columns)) for col in df.index]\ndf = df.T\ndf = df.iloc[0:1, :]\n",
        "\ndf = df.round(2)\n",
        "\ndf = df.round(2)\n",
        "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "df['Avg'] = df[list_of_my_columns].mean(axis=1)",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\nresult = df.sort_index(level='time')\n",
        "result = df.sort_values('VIM', kind='mergesort')\n",
        "\nresult = df[~df.index.strftime('%Y-%m-%d').isin(['2020-02-17', '2020-02-18'])]\n",
        "\n# Remove the dates\ndates_to_remove = ['2020-02-17', '2020-02-18']\ndf = df[~df.index.strftime('%Y-%m-%d').isin(dates_to_remove)]\n# Add day of week\ndf['Day of Week'] = df.index.strftime('%d-%b-%Y %A')\nresult = df\n",
        "\ncorr = corr.where(np.triu(np.ones(corr.shape)).astype(np.bool))\ncorr = corr.stack().reset_index()\ncorr.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult = corr[corr['Pearson Correlation Coefficient'] > 0.3]\n",
        "result = corr[corr > 0.3].stack().dropna()",
        "df.columns = ['A' if x!=max(df.columns.get_loc(x)) else 'Test' for x in df.columns]\nresult = df\n",
        "df.columns = ['Test' if x==df.columns[0] else x for x in df.columns]\nresult = df\n",
        "\ndf['frequent'] = df.mode(axis=1)[0]\ndf['freq_count'] = df.apply(lambda row: list(row).count(row['frequent']), axis=1)\n",
        "df['frequent'] = df.mode(axis=1)[0]\ndf['freq_count'] = df.apply(lambda row: list(row).count(row['frequent']), axis=1)",
        "df['frequent'] = df.apply(lambda x: x.mode().tolist(), axis=1)\ndf['freq_count'] = df['frequent'].apply(lambda x: len(x))",
        "\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\n",
        "\ndf['bar'] = df['bar'].replace('NULL', 0)\ndf['bar'] = df['bar'].astype(float)\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum')\n",
        "result = pd.merge(df_a, df_b[['EntityNum', 'b_col']], on='EntityNum')\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "x = x[~np.isnan(x)]",
        "x[np.isnan(x)] = np.inf",
        "\nresult = [[elem for elem in row if not np.isnan(elem)] for row in x]\n",
        "b = np.zeros((a.size, a.max()+1))\nb[np.arange(a.size),a] = 1",
        "b = np.zeros((a.size, a.max()+1))\nb[np.arange(a.size),a] = 1",
        "\nb = np.zeros((a.size, a.max()-a.min()+1))\nb[np.arange(a.size), a-a.min()] = 1\n",
        "\nunique_a = np.unique(a)\nb = (unique_a[:, None] == a).astype(int)\n",
        "b = np.zeros((a.size, a.max()+1), dtype=int)\nb[np.arange(a.size), a.ravel()] = 1\nb = b.reshape(*a.shape, -1)",
        "\nresult = np.percentile(a, p)\n",
        "B = A.reshape(-1, ncol)",
        "B = A.reshape(nrow, -1)",
        "nrow = len(A) // ncol\nB = A[:nrow*ncol].reshape(nrow, ncol)\n",
        "\nnrow = len(A) // ncol\nA = A[-nrow*ncol:]\nB = A.reshape(nrow, ncol)[::-1]\n",
        "\nresult = np.empty_like(a)\nif shift > 0:\n    result[:shift] = np.nan\n    result[shift:] = a[:-shift]\nelif shift < 0:\n    result[shift:] = np.nan\n    result[:shift] = a[-shift:]\nelse:\n    result = a\n",
        "\ndef shift_array(arr, num, fill_value=np.nan):\n    result = np.empty_like(arr)\n    if num > 0:\n        result[:num] = fill_value\n        result[num:] = arr[:-num]\n    elif num < 0:\n        result[num:] = fill_value\n        result[:num] = arr[-num:]\n    else:\n        result = arr\n    return result\nresult = shift_array(a, shift)\n",
        "\nresult = np.empty_like(a)\nfor i, s in enumerate(shift):\n    if s > 0:\n        result[i, s:] = a[i, :-s]\n        result[i, :s] = np.nan\n    elif s < 0:\n        result[i, :s] = a[i, -s:]\n        result[i, s:] = np.nan\n    else:\n        result[i] = a[i]\n",
        "\nnp.random.seed(0)\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nnp.random.seed(0)\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n",
        "result = np.unravel_index(np.argmax(a, axis=None), a.shape)",
        "result = np.unravel_index(np.argmin(a, axis=None), a.shape)",
        "result = np.unravel_index(np.argmax(a, axis=None), a.shape, order='F')",
        "result = np.unravel_index(np.argmax(a, axis=None), a.shape)",
        "\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
        "\nflat_indices = np.argpartition(a.flatten(), -2)[-2]\nresult = np.unravel_index(flat_indices, a.shape)\n",
        "mask = np.all(np.isnan(a), axis=0)\na = a[:, ~mask]",
        "a = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(a.argmin(), a.shape)\n",
        "result = np.unravel_index(a.argmax(), a.shape)\n",
        "\nresult = np.argwhere(a == np.min(a)).tolist()\n",
        "\nresult = np.sin(np.radians(degree))\n",
        "\nradians = np.deg2rad(degree)\nresult = np.cos(radians)\n",
        "\nif np.sin(np.deg2rad(number)) > np.sin(number):\n    result = 1\nelse:\n    result = 0\n",
        "\nresult = np.degrees(np.arcsin(value))\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant')\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant')\n",
        "\na = np.power(a, power)\n",
        "\nresult = np.power(a, power)\n",
        "\nfrom fractions import Fraction\nresult = Fraction(numerator, denominator)\nresult = (result.numerator, result.denominator)\n",
        "\nfrom fractions import Fraction\nresult = Fraction(numerator, denominator)\nresult = (result.numerator, result.denominator)\n",
        "\nfrom math import gcd\nif denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    g = gcd(numerator, denominator)\n    result = (numerator//g, denominator//g)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(np.maximum(a, b), c)\n",
        "\nresult = np.fliplr(a).diagonal()\n",
        "\nresult = np.fliplr(a).diagonal()\n",
        "\nresult = np.array([a[i, i] for i in range(a.shape[0])], [a[i, a.shape[0]-i-1] for i in range(a.shape[0])])\n",
        "\nresult = []\nfor i in range(a.shape[1]):\n    result.append(a.diagonal(i))\nfor i in range(1, a.shape[0]):\n    result.append(a.diagonal(-i))\nresult = np.array(result)\n",
        "\nresult = X.flatten().tolist()\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nresult = X.flatten(order='F').tolist()\n",
        "\nresult = np.array(list(map(int, list(mystr))))\n",
        "\na[:, col] *= multiply_number\nresult = np.cumsum(a[:, col])\n",
        "\na[row] = a[row] * multiply_number\nresult = np.cumsum(a[row])\n",
        "\na[row] = a[row] / divide_number\nresult = np.prod(a[row])\n",
        "\nu, indices = np.unique(a, axis=0, return_index=True)\nresult = a[sorted(indices)]\n",
        "result = a.shape[1]\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\n# Remove nans from the data\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n# Perform the t-test\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\n# calculate the standard deviation\nastd = np.sqrt(avar)\nbstd = np.sqrt(bvar)\n# calculate the standard error\nse = np.sqrt(astd**2/anobs + bstd**2/bnobs)\n# calculate the t statistic\nt_stat = (amean - bmean) / se\n# calculate the degrees of freedom\ndf = anobs + bnobs - 2\n# calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(np.abs(t_stat), df))\n",
        "output = [x for x in A.tolist() if x not in B.tolist()]\n",
        "\nA1 = np.array([tuple(i) for i in A])\nB1 = np.array([tuple(i) for i in B])\nA1_rows = set(map(tuple, A1))\nB1_rows = set(map(tuple, B1))\nsym_diff = np.array(list(A1_rows.symmetric_difference(B1_rows)))\noutput = np.array([x for x in sym_diff if tuple(x) in A1] + [x for x in sym_diff if tuple(x) in B1])\n",
        "indices = np.ogrid[[slice(i) for i in a.shape]]\nindices[0] = np.argsort(a, axis=0)\nc = b[tuple(indices)]\n",
        "sort_indices = np.argsort(a, axis=0)\nixgrid = np.ogrid[[slice(x) for x in a.shape]]\nixgrid[0] = sort_indices\nc = b[ixgrid]\n",
        "indices = np.argsort(a, axis=0)[::-1]\nc = np.take_along_axis(b, indices, axis=0)\n",
        "indices = np.argsort(a.sum(axis=(1,2)))\nresult = b[indices]\n",
        "a = np.delete(a, 2, 1)\n",
        "a = np.delete(a, 2, 0)",
        "a = np.delete(a, [0, 2], axis=1)\n",
        "\ndel_col = del_col[del_col < a.shape[1]]\nresult = np.delete(a, del_col, axis=1)\n",
        "a = np.insert(a, pos, element)",
        "a = np.insert(a, pos, element, axis=0)",
        "a = np.insert(a, pos, element)",
        "for i in range(len(pos)):\n    a = np.insert(a, pos[i], element[i], axis=0)",
        "\nimport copy\nresult = copy.deepcopy(array_of_arrays.tolist())\n",
        "\nresult = np.all(a == a[0,:])\n",
        "\nresult = np.all(a == a[:,0][:,None])\n",
        "\nresult = np.all(a[1:] == a[:-1])\n",
        "\nfrom scipy import integrate\nX, Y = np.meshgrid(x, y)\nZ = (np.cos(X))**4 + (np.sin(Y))**2\nresult = integrate.simps(integrate.simps(Z, x), y)\n",
        "result = (np.cos(x)**4 + np.sin(y)**2)\n",
        "\ndef ecdf(data):\n    \"\"\" Compute ECDF \"\"\"\n    x = np.sort(data)\n    n = x.size\n    y = np.arange(1, n+1) / n\n    return(x,y)\nx, y = ecdf(grades)\nresult = dict(zip(x, y))\n",
        "\ndef ecdf(data):\n    \"\"\" Compute ECDF \"\"\"\n    x = np.sort(data)\n    n = x.size\n    y = np.arange(1, n+1) / n\n    return(x,y)\nx, y = ecdf(grades)\nresult = np.interp(eval, x, y)\n",
        "\ndef ecdf(x):\n    x = np.sort(x)\n    n = x.size\n    y = np.arange(1, n+1) / n\n    return(x,y)\nx, y = ecdf(grades)\nlow = x[np.argmax(y < threshold)]\nhigh = x[np.argmax(y >= threshold)]\nif high < low:\n    high = np.max(x)\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\na_np = a.numpy()\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[-N:][::-1]\n",
        "\nfrom scipy.linalg import fractional_matrix_power\nresult = fractional_matrix_power(A, n)\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\nresult = np.array(result)\n",
        "\nresult = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        result.append(a[i:i+2, j:j+2])\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\nresult = np.array(result)\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        if i + patch_size <= a.shape[0] and j + patch_size <= a.shape[1]:\n            result.append(a[i:i+patch_size, j:j+patch_size])\nresult = np.array(result)\n",
        "\nresult = a.reshape(h, w)\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size])\nresult = [x for x in result if x.shape == (patch_size, patch_size)]\n",
        "result = a[:, low:high]",
        "result = a[low:high]\n",
        "\nresult = a[:, low:min(high, a.shape[1])]\n",
        "\na = np.array(eval(string))\n",
        "\nbase = 10\nlog_min = np.log(min)/np.log(base)\nlog_max = np.log(max)/np.log(base)\nresult = np.power(base, np.random.uniform(log_min, log_max, n))\n",
        "\nbase = 10\nlog_min = np.log10(min)\nlog_max = np.log10(max)\nresult = np.power(base, np.random.uniform(log_min, log_max, n))\n",
        "\nlog_min = np.log(min)\nlog_max = np.log(max)\nresult = np.exp(np.random.uniform(log_min, log_max, n))\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty(shape=(3,0))\n",
        "result = np.ravel_multi_index(index, dims)\n",
        "\nresult = np.ravel_multi_index(index, dims)\n",
        "\nvalues = np.zeros((2,3), dtype='int32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\ndf['b'] = df['b'].astype('float32')\ndf['c'] = df['c'].astype('float32')\n",
        "result = np.bincount(accmap, weights=a)\n",
        "\nresult = np.bincount(index, weights=a).astype(int)\n",
        "\naccmap[accmap<0] += len(a)\nunique, inverse = np.unique(accmap, return_inverse=True)\nresult = np.bincount(inverse, weights=a)\n",
        "\nresult = np.minimum.reduceat(a, np.r_[np.unique(index, return_index=True)[1], len(a)])\n",
        "\nx = np.array(x)\ny = np.array(y)\nz = np.vectorize(elementwise_function)(x, y)\n",
        "\nresult = np.random.choice(len(lista_elegir), samples, p=probabilit)\nresult = [lista_elegir[i] for i in result]\n",
        "\ndef pad_slice(a, low_index, high_index):\n    pad_width = max(-low_index, high_index - a.shape[0] + 1, 0)\n    a_padded = np.pad(a, pad_width, mode='constant')\n    slice_ = a_padded[low_index + pad_width:high_index + pad_width, low_index + pad_width:high_index + pad_width]\n    return slice_\nresult = pad_slice(a, low_index, high_index)\n",
        "\nresult = x[x >= 0]\n",
        "\nresult = x[np.iscomplex(x)]\n",
        "\ndata = data[:len(data)//bin_size*bin_size]\nbin_data = data.reshape(-1, bin_size)\nbin_data_mean = bin_data.mean(axis=1)\n",
        "\ndata = data[:len(data)//bin_size*bin_size]\nbin_data = np.split(data, len(data)//bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\ndata = data[::-1] # reverse the array\nbin_data = np.array_split(data, len(data)//bin_size) # split the array into bins\nbin_data_mean = [np.mean(bin) for bin in bin_data if len(bin) == bin_size] # calculate the mean of each bin\n",
        "\nbin_data = np.array([row[-bin_size::-bin_size][::-1] for row in data])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nn_rows, n_cols = data.shape\nn_bins = n_cols // bin_size\nbin_data = data[:, -n_bins*bin_size:].reshape(n_rows, n_bins, bin_size)\nbin_data_mean = bin_data.mean(axis=2)\n",
        "\ndef smoothclamp(x, x_min=0, x_max=1):\n    x = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    return (3 - 2 * x) * x * x * (x_max - x_min) + x_min\n",
        "\ndef smoothclamp(x, x_min=0, x_max=1, N=1):\n    x = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    return x_min + (x_max - x_min) * (x**N * (N+1 - N*x))\n",
        "\nfrom scipy.linalg import circulant\n# Create a circulant matrix with b\nc = circulant(b)\n# Perform dot product of a and each row of c\nresult = np.dot(c, a)\n",
        "\nresult = df.values.reshape((4, 15, 5))\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\nresult = ((a[:, None] & (1 << np.arange(m))) > 0).astype(int)\nresult = result[:, ::-1]\n",
        "\nresult = np.zeros((len(a), m), dtype=np.uint8)\nfor i in range(len(a)):\n    result[i] = np.array(list(np.binary_repr(np.abs(a[i]), m)), dtype=np.uint8)\n",
        "\nbinary_a = ((a[:, None] & (1 << np.arange(m))) > 0).astype(int)\nresult = np.bitwise_xor.reduce(binary_a)\n",
        "\nmean = np.mean(a)\nstd_dev = np.std(a)\nresult = (mean - 3*std_dev, mean + 3*std_dev)\n",
        "\nmu = np.mean(a)\nsigma = np.std(a)\nresult = (mu - 2*sigma, mu + 2*sigma)\n",
        "\nmu = np.mean(a)\nsigma = np.std(a)\nresult = (mu - 3*sigma, mu + 3*sigma)\n",
        "\nmu, sigma = a.mean(), a.std()\nresult = (a < mu - 2 * sigma) | (a > mu + 2 * sigma)\n",
        "\nimport numpy.ma as ma\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\nprob = ma.median(masked_data)\n",
        "a[zero_rows, :] = 0\na[:, zero_cols] = 0",
        "a[zero_rows, :] = 0\na[:, zero_cols] = 0",
        "\na[1,:] = 0\na[:,0] = 0\n",
        "\nmask = (a == np.max(a, axis=1)[:, None])\n",
        "\nmask = a == np.min(a, axis=1)[:, None]\n",
        "\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nresult = np.einsum('ij,ik->jik', X, X)\n",
        "\nX = np.array([np.linalg.cholesky(Y[i]) for i in range(Y.shape[0])]).T\n",
        "is_contained = number in a\n",
        "\nC = np.setdiff1d(A, B)\n",
        "\nC = A[np.isin(A, B)]\n",
        "\nC = A[np.logical_or(np.logical_and(A > B[0], A < B[1]), np.logical_and(A > B[1], A < B[2]))]\n",
        "\nresult = len(a) - rankdata(a).astype(int) + 1\n",
        "\nresult = len(a) - rankdata(a, method='ordinal') + 1\n",
        "\n    result = len(a) - rankdata(a).astype(int) + 1\n",
        "dists = np.dstack((x_dists, y_dists))",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "result = a[:, second][:, :, third]\n",
        "\narr = np.zeros((20,10,10,2))\n",
        "\nl1 = np.abs(X).sum(axis=1)\nresult = X / l1[:, None]\n",
        "\nnorm = np.linalg.norm(X, ord=2, axis=1, keepdims=True)\nresult = X/norm\n",
        "\nnorms = np.abs(X).max(axis=1)\nresult = X / norms[:, None]\n",
        "conditions = [df['a'].astype(str).str.contains(target)]\ndf['result'] = np.select(conditions, choices, default=np.nan)",
        "\nfrom scipy.spatial import distance_matrix\nresult = distance_matrix(a, a)\n",
        "\nfrom scipy.spatial import distance\nresult = distance.cdist(a, a, 'euclidean')\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\nresult = squareform(pdist(a, 'euclidean'))\n",
        "\nNA = NA.astype(float)\nAVG = np.mean(NA, axis=0)\n",
        "\nNA = np.asarray(A, dtype=np.float)\nAVG = np.mean(NA, axis=0)\n",
        "\nNA = np.array([np.inf if i=='np.inf' else float(i) for i in A])\nAVG = np.mean(NA[~np.isinf(NA)], axis=0)\n",
        "\nmask = np.concatenate(([True], a[1:] != a[:-1], [True]))\nresult = a[mask]\nresult = result[result != 0]\n",
        "\nmask = np.concatenate(([False], a[1:] != a[:-1]))\nresult = a[mask]\nresult = result[result != 0]\n",
        "\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n",
        "\n    lat = lat.flatten()\n    lon = lon.flatten()\n    val = val.flatten()\n    df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\n",
        "\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df[['lat', 'lon', 'val']].max(axis=1)\n",
        "\nresult = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        r = np.arange(max(0,i-size[0]//2),min(a.shape[0],i+size[0]//2+1))\n        c = np.arange(max(0,j-size[1]//2),min(a.shape[1],j+size[1]//2+1))\n        result.append(a[np.ix_(r,c)])\n",
        "\nresult = []\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        result.append(a[max(0, i - size[0] // 2):min(a.shape[0], i + size[0] // 2 + 1),\n                        max(0, j - size[1] // 2):min(a.shape[1], j + size[1] // 2 + 1)])\n",
        "\nreal_parts = np.mean(a.real)\nimag_parts = np.mean(a.imag)\nresult = complex(real_parts, imag_parts)\n",
        "\nreal_parts = np.real(a)\nimag_parts = np.imag(a)\nreal_mean = np.mean(real_parts[np.isfinite(real_parts)])\nimag_mean = np.mean(imag_parts[np.isfinite(imag_parts)])\nresult = real_mean + imag_mean*1j\n",
        "\nslicer = [slice(None)] * Z.ndim\nslicer[-1] = slice(-1, None)\nresult = Z[tuple(slicer)]\n",
        "\nslices = [slice(None)] * a.ndim\nslices[0] = slice(-1, None)\nresult = a[tuple(slices)]\n",
        "\nresult = any((c == x).all() for x in CNTS)\n",
        "\nresult = any((np.array_equal(c, cnt) for cnt in CNTS))\n",
        "\nf = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\n",
        "df = pd.DataFrame(data)\ndf[name] = df.groupby('D')['Q'].cumsum()",
        "\ni = np.diag(i)\n",
        "\na *= np.eye(a.shape[0])\n",
        "\nstart_u = pd.to_datetime(start).value//10**9\nend_u = pd.to_datetime(end).value//10**9\nresult = pd.to_datetime(np.linspace(start_u, end_u, n), unit='s')\n",
        "\nresult = np.where((x == a) & (y == b))\nif len(result[0]) > 0:\n    result = result[0][0]\nelse:\n    result = -1\n",
        "\nresult = np.where((x == a) & (y == b))[0]\n",
        "\nx = np.array(x)\ny = np.array(y)\np = np.polyfit(x, y, 2)\nresult = p.tolist()\n",
        "\nX = np.vander(x, degree + 1)\nresult = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "\ndf = df.apply(lambda x: x-a[x.name], axis=1)\n",
        "\nresult = np.einsum('...k,kl->...l', A, B)\n",
        "\nscaler = MinMaxScaler()\na = a.reshape(-1, 1)\nresult = scaler.fit_transform(a)\nresult = result.reshape(2, 2)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(arr.T).T\n",
        "\nresult = []\nfor matrix in a:\n    scaler = MinMaxScaler()\n    scaler.fit(matrix)\n    result.append(scaler.transform(matrix))\nresult = np.array(result)\n",
        "\nmask1 = arr < -10\nmask2 = arr >= 15\narr[mask1] = 0\narr[mask2] = 30\narr[~mask1 & ~mask2] += 5\n",
        "\nfor i in range(arr.shape[0]):\n    mask = arr[i] < n1[i]\n    mask2 = arr[i] >= n2[i]\n    arr[i][mask] = 0\n    arr[i][mask2] = 30\n    arr[i][~mask & ~mask2] += 5\n",
        "\ntolerance = 1e-10\nresult = np.nonzero(np.abs(s1 - s2) > tolerance)[0].shape[0]\n",
        "\n# Use np.isclose to compare s1 and s2 with a tolerance\nmask = np.isclose(s1, s2, equal_nan=True)\n# Count the number of False in mask, which are the truly different elements in s1 and s2\nresult = np.count_nonzero(~mask)\n",
        "result = all((np.array_equal(a[0], x) for x in a))\n",
        "result = all(np.isnan(x).any() for x in a)",
        "\npad_shape = ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1]))\nresult = np.pad(a, pad_width=pad_shape, mode='constant', constant_values=0)\n",
        "\npad_shape = ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1]))\nresult = np.pad(a, pad_width=pad_shape, mode='constant', constant_values=0)\n",
        "\npad_width = ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1]))\nresult = np.pad(a, pad_width, mode='constant', constant_values=element)\n",
        "\npad_shape = ((0, shape[0] - arr.shape[0]), (0, shape[1] - arr.shape[1]))\nresult = np.pad(arr, pad_shape, 'constant')\n",
        "\npad_rows = shape[0] - a.shape[0]\npad_cols = shape[1] - a.shape[1]\npad_top = pad_rows // 2\npad_bottom = pad_rows - pad_top\npad_left = pad_cols // 2\npad_right = pad_cols - pad_left\nresult = np.pad(a, ((pad_top, pad_bottom), (pad_left, pad_right)), 'constant', constant_values=0)\n",
        "a = a.reshape(a.shape[0]//3,3)\n",
        "\nresult = np.choose(b, a.transpose(2,0,1))\n",
        "\nresult = np.take_along_axis(a, np.expand_dims(b, axis=-1), axis=-1).squeeze()\n",
        "\nN, M = b.shape\nI, J = np.ogrid[:N,:M]\nresult = a[I, J, b]\n",
        "\nN, M, T = a.shape\nindices = np.ogrid[:N, :M]\nresult = a[indices[0], indices[1], b].sum()\n",
        "\nN, M, T = a.shape\nindices = np.indices((N, M))\nresult = np.sum(a[indices[0], indices[1], b] * np.arange(T) != b)\n",
        "\ndf['b'] = np.where((df['a'] > 1) & (df['a'] <= 4), df['b'], np.nan)\nresult = df['b'].tolist()\n",
        "\nmask = np.any(im, axis=0)\nmask2 = np.any(im, axis=1)\nresult = im[np.ix_(mask2, mask)]\n",
        "\nrows = np.any(A, axis=1)\ncols = np.any(A, axis=0)\nrmin, rmax = np.where(rows)[0][[0, -1]]\ncmin, cmax = np.where(cols)[0][[0, -1]]\nresult = A[rmin:rmax+1, cmin:cmax+1]\n",
        "\nwhile np.any(im[0]):\n    im = im[1:]\nwhile np.any(im[:,0]):\n    im = np.delete(im,0,1)\nwhile np.any(im[-1]):\n    im = im[:-1]\nwhile np.any(im[:,-1]):\n    im = np.delete(im,-1,1)\nresult = im\n",
        "\nmask = im > 0\nrows = np.flatnonzero(np.any(mask, axis=1))\ncols = np.flatnonzero(np.any(mask, axis=0))\nresult = im[rows.min():rows.max()+1, cols.min():cols.max()+1]\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label='x-y')\nplt.legend()\nplt.show()\n",
        "\nplt.gca().yaxis.set_minor_locator(plt.AutoMinorLocator())\nplt.grid(which='minor', axis='y', linestyle=':', linewidth=0.5)\n",
        "\nplt.minorticks_on()\n",
        "\nplt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\nplt.minorticks_on()\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=True)\n",
        "\nline_styles = ['-', '--', '-.', ':']\nfor style in line_styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style)\nplt.show()\n",
        "\nline_styles = ['-', '--', '-.', ':']\nfor style in line_styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style)\nplt.show()\n",
        "\nplt.plot(x, y, 'd-', markersize=5)\nplt.show()\n",
        "\nplt.plot(x, y, 'D-', markersize=10)\nplt.show()\n",
        "\nax.set(ylim=(0, 40))\n",
        "\nplt.axvspan(2, 4, color='red', alpha=0.5)\n",
        "\nplt.plot([0, 1], [0, 2])\nplt.show()\n",
        "\nplt.plot([0, 1], [0, 2])\nplt.show()\n",
        "\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df)\nplt.show()\n",
        "\nsns.set_style(\"whitegrid\")\nplt.plot(x, y)\nplt.show()\n",
        "\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': x, 'y': y})\n# Use seaborn to create a line plot\nsns.lineplot(data=df, x='x', y='y')\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', linewidth=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize='20')\nplt.show()\n",
        "\nplt.setp(l, markerfacecolor=(1, 1, 1, 0.2))\n",
        "\nplt.setp(l, markeredgecolor='black', markeredgewidth=1)\n",
        "\nl.set_color('red')\nl.set_markerfacecolor('red')\nl.set_markeredgecolor('red')\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(np.arange(0, max(x), 2))\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H, cmap='viridis')\nplt.colorbar()\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", loc='right')\nplt.show()\n",
        "\nplt.xticks(rotation=90)\n",
        "\nplt.title(\"\\n\".join([myTitle[i:i+30] for i in range(0, len(myTitle), 30)]))\n",
        "\nplt.scatter(x, y)\nplt.gca().invert_yaxis()\nplt.show()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nplt.plot(z, color='blue', linewidth=5)\nplt.plot(y, color='green', linewidth=7)\nplt.plot(x, color='red', linewidth=9)\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolors='black', facecolors='blue')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(min(x), max(x)+1, 1.0))\nplt.yticks(np.arange(min(y), max(y)+1, 1.0))\nplt.show()\n",
        "\nplt.ticklabel_format(style='plain', axis='y')\n",
        "\nax.lines[0].set_linestyle(\"--\")\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, sharex=True)\nfig.suptitle('Sine and Cosine Plots')\naxs[0].plot(x, y1, 'tab:orange')\naxs[0].set_title('Sine')\naxs[1].plot(x, y2, 'tab:blue')\naxs[1].set_title('Cosine')\nfor ax in axs.flat:\n    ax.set(xlabel='x', ylabel='y')\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()\nplt.show()\n",
        "\nfig, axs = plt.subplots(2)\nfig.suptitle('Sin and Cos Functions')\n# Plotting sin function\naxs[0].plot(x, y1, 'tab:orange')\naxs[0].set_title('sin(x)')\naxs[0].spines['top'].set_visible(False)\naxs[0].spines['right'].set_visible(False)\naxs[0].spines['bottom'].set_visible(False)\naxs[0].spines['left'].set_visible(False)\n# Plotting cos function\naxs[1].plot(x, y2, 'tab:blue')\naxs[1].set_title('cos(x)')\naxs[1].spines['top'].set_visible(False)\naxs[1].spines['right'].set_visible(False)\naxs[1].spines['bottom'].set_visible(False)\naxs[1].spines['left'].set_visible(False)\nplt.show()\n",
        "\nplt.xlabel('')\n",
        "\nplt.xticks([], [])\n",
        "\nplt.xticks([3, 4])\nplt.grid(axis='x')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y')\n",
        "\nplt.yticks([3, 4])\nplt.xticks([1, 2])\nplt.grid(axis='y', which='both', linestyle='-', color='grey')\nplt.grid(axis='x', which='both', linestyle='-', color='grey')\n",
        "\nplt.grid(True)\n",
        "\nplt.legend(loc='lower right')\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n# Adjust the subplot padding\nplt.tight_layout()\nplt.show()\nplt.clf()\n",
        "\nplt.legend()\nplt.show()\n",
        "\nax.xaxis.tick_top()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", labelpad=20)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks([]) # this line hides the xticks\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().yaxis.tick_right()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.yticks(np.arange(min(y), max(y)+1, 1))\nplt.gca().yaxis.tick_left()\nplt.gca().yaxis.set_label_position(\"right\")\nplt.show()\n",
        "\njoint = sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='g')\njoint.plot_joint(plt.scatter, c='g', s=30, linewidth=1, marker=\"+\")\njoint.ax_marg_x.hist(tips['total_bill'], color=\"b\", alpha=.6)\njoint.ax_marg_y.hist(tips['tip'], color=\"b\", alpha=.6, orientation=\"horizontal\")\nplt.show()\n",
        "\njoint = sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='b')\njoint.plot_joint(plt.scatter, c='b', s=30, linewidth=1, marker=\"+\")\njoint.ax_joint.collections[0].set_color('g')\nplt.show()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg')\nplt.show()\n",
        "\ndf.set_index('celltype').plot(kind='bar', rot=0)\nplt.xlabel('celltype')\nplt.ylabel('value')\nplt.show()\n",
        "\ndf.set_index('celltype').plot(kind='bar')\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color='red')\nplt.tick_params(axis='x', colors='red')\nplt.show()\n",
        "\nplt.plot(x, y, color='red')\nplt.xlabel('X')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(fontsize=10, rotation='vertical')\nplt.show()\n",
        "\nx_values = [0.22058956, 0.33088437, 2.20589566]\nfor x in x_values:\n    plt.axvline(x=x, color='r')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nheatmap = ax.imshow(rand_mat, cmap='hot', interpolation='nearest')\n# Move x-axis to top and invert y-axis\nax.xaxis.tick_top()\nax.invert_yaxis()\n# Set the x-axis and y-axis labels\nax.set_xticklabels(xlabels)\nax.set_yticklabels(ylabels)\n# Show the plot\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111)\nline1, = ax.plot(time, Swdown, \"-\", label=\"Swdown\")\nline2, = ax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nline3, = ax2.plot(time, temp, \"-r\", label=\"temp\")\n# Create a legend for the whole figure\nlines = [line1, line2, line3]\nlabels = [l.get_label() for l in lines]\nax.legend(lines, labels, loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n# Plotting on the first subplot\naxs[0].plot(x, y)\naxs[0].set_title('Y')\n# Plotting on the second subplot\naxs[1].plot(x, y)\naxs[1].set_title('Y')\nplt.show()\n",
        "\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", size=30)\nplt.show()\n",
        "\nplt.scatter(a, b)\nfor i, txt in enumerate(c):\n    plt.annotate(txt, (a[i], b[i]))\nplt.show()\n",
        "\nplt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", title_fontsize='bold')\nplt.show()\n",
        "\nplt.hist(x, edgecolor='black', linewidth=1.2)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw={'width_ratios': [3, 1]})\nax1.plot(x, y)\nax2.plot(x, y)\nplt.show()\n",
        "\nplt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\nplt.legend(loc='upper right')\nplt.show()\n",
        "\n# Create a dataframe from x and y\ndf = pd.DataFrame({'x': x, 'y': y})\n# Plot the grouped histograms\ndf.plot.hist(alpha=0.5, bins=10)\nplt.show()\n",
        "\n# calculate the slope and intercept of the line\nslope = (d - b) / (c - a)\nintercept = b - slope * a\n# create a range of x values\nx = range(0, 6)\n# calculate the corresponding y values\ny = [slope * i + intercept for i in x]\n# plot the line\nplt.plot(x, y)\n# set the xlim and ylim\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True)\n# Plotting the first colormap\nc1 = axs[0].imshow(x)\naxs[0].set_title('x')\n# Plotting the second colormap\nc2 = axs[1].imshow(y)\naxs[1].set_title('y')\n# Creating a colorbar\nfig.colorbar(c1, ax=axs, location='right', shrink=0.6)\nplt.show()\n",
        "\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n",
        "\nfig, axs = plt.subplots(2)\nfig.suptitle('Y and Z')\naxs[0].plot(x, y)\naxs[0].set_title('Y over X')\naxs[1].plot(a, z)\naxs[1].set_title('Z over A')\nplt.show()\n",
        "\nx = [point[0] for point in points]\ny = [point[1] for point in points]\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n",
        "\nax.plot(x, y)\nax.set_xticks(np.arange(1, 11))\nax.set_yticks(np.arange(1, 11))\nplt.show()\n",
        "\nfor i in range(len(lines)):\n    line = lines[i]\n    color = c[i]\n    plt.plot([line[0][0], line[1][0]], [line[0][1], line[1][1]], color=color)\nplt.show()\n",
        "\nplt.loglog(x, y)\nplt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: int(y)))\nplt.show()\n",
        "\nplt.figure(figsize=(10,6))\nfor column in df.columns:\n    plt.plot(df.index, df[column], marker='o', label=column)\nplt.legend()\nplt.show()\n",
        "\n# Make a histogram of data and renormalize the data to sum up to 1\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=7)\n# Format the y tick labels into percentage and set y tick labels as 10%, 20%, etc.\nplt.gca().yaxis.set_major_formatter(mticker.PercentFormatter(1))\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markersize=8, markerfacecolor='blue', alpha=0.5, linestyle='solid', linewidth=2, color='blue')\nplt.show()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n# Plot y over x\naxs[0].plot(x, y, label='y')\n# Plot a over z\naxs[1].plot(z, a, label='a')\n# Set labels for subplots\naxs[0].set_title('y')\naxs[1].set_title('a')\n# Create a figure-level legend\nfig.legend(loc='upper right')\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 1, sharex=False)\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\nplt.tight_layout()\nplt.show()\n",
        "\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nlabels[1] = 'second'\nax.set_xticklabels(labels)\n",
        "\nplt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()\n",
        "\nextra_ticks = [2.1, 3, 7.6]\nplt.xticks(list(plt.xticks()[0]) + extra_ticks)\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.yticks(rotation=-60)\nplt.xticks(va='top')\n",
        "\nfor label in plt.gca().get_xticklabels():\n    label.set_alpha(0.5)\n",
        "\nplt.margins(x=-0.05, y=0.1)\n",
        "\nplt.margins(x=0.1, y=0)\nplt.ylim(bottom=0)\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\nfig.suptitle('Figure')\naxs[0].plot(x, y)\naxs[1].plot(x, y)\nplt.show()\n",
        "\ndf.plot(kind='line')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='o', s=100, hatch='||||')\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolors='none', hatch='|')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='*')\nplt.show()\n",
        "\nplt.scatter(x, y, s=100, marker='*')\nplt.scatter(x, y, s=100, marker='|')\nplt.show()\n",
        "\nplt.imshow(data, extent=[0, 10, 0, 10], origin='lower')\nplt.colorbar()\nplt.xlim(1, 5)\nplt.ylim(1, 4)\nplt.show()\n",
        "\nplt.stem(y, x, orientation='horizontal')\nplt.show()\n",
        "\nplt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n",
        "\nplt.axvline(x=3, color='r', linestyle='-', label='cutoff')\nplt.legend()\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, polar=True)\ntheta = [0, 1]\nbars = ax.bar(theta, height, width=0.5, bottom=0.0)\n# Set the rotation of the labels\nax.set_xticks(theta)\nax.set_xticklabels(labels)\nplt.show()\n",
        "\nplt.pie(data, labels = l, wedgeprops=dict(width=0.4))\nplt.gca().set_aspect(\"equal\")  # To ensure pie is drawn as a circle.\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(color='blue', linestyle='--', linewidth=0.5)\nplt.show()\n",
        "\nplt.plot(x, y)\n# Turn minor ticks on\nplt.minorticks_on()\n# Show gray dashed minor grid lines\nplt.grid(which='minor', linestyle='--', color='gray')\n# Do not show any major grid lines\nplt.grid(which='major', linestyle='', color='')\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.gca().set_prop_cycle(None) # Reset the color cycle\nfor text in plt.gca().texts:\n    text.set_fontweight('bold') # Make the labels bold\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.gca().set_prop_cycle(None) # Reset the color cycle\nfor text in plt.gca().texts: # For each label (percentage is also a label)\n    text.set_fontweight('bold') # Make the text bold\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markerfacecolor='none', markeredgecolor='blue', linestyle='-')\nplt.show()\n",
        "\nplt.axvline(55, color='green')\n",
        "\n# Specify the positions of the blue bars on the x-axis\nr1 = np.arange(len(blue_bar))\n# Specify the positions of the orange bars on the x-axis\nr2 = [x + 0.25 for x in r1]\n# Plot bars\nplt.bar(r1, blue_bar, color='b', width=0.25, edgecolor='grey', label='blue')\nplt.bar(r2, orange_bar, color='orange', width=0.25, edgecolor='grey', label='orange')\n# Adding xticks\nplt.xlabel('groups', fontweight='bold')\nplt.xticks([r + 0.125 for r in range(len(blue_bar))], ['A', 'B', 'C'])\nplt.legend()\nplt.show()\n",
        "\nfig, axs = plt.subplots(2)\n# Plot y over x in the first subplot\naxs[0].plot(x, y, label='y over x')\n# Plot z over a in the second subplot\naxs[1].plot(a, z, label='z over a')\n# Put a legend on the first subplot\naxs[0].legend()\nplt.show()\n",
        "\nplt.scatter(x, y, c=y, cmap='Spectral')\nplt.colorbar(label='y-value')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(min(x), max(x)+1, 1))\nplt.show()\n",
        "\ng = sns.factorplot(x=\"sex\", y=\"bill_length_mm\", col=\"species\", data=df, kind=\"bar\", sharey=False)\n",
        "\ncircle = plt.Circle((0.5, 0.5), 0.2, fill = False)\nfig, ax = plt.subplots()\nax.add_artist(circle)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\boldsymbol{\\phi}$', fontsize=20)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.1)\nplt.show()\n",
        "\nplt.plot(x, y, label='Line')\nplt.legend(handlelength=0.3)\nplt.show()\n",
        "\nplt.legend(ncol=2)\nplt.show()\n",
        "\nplt.legend()\nplt.scatter([x[3], x[7]], [y[3], y[7]], color='red')  # add two markers on the line\nplt.show()\n",
        "\nplt.imshow(data, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\bf{Figure}$' + ' 1')  # Using LaTeX to bold the word \"Figure\"\nplt.show()\n",
        "\nsns.pairplot(df, x_vars=[\"x\"], y_vars=[\"y\"], hue=\"id\", plot_kws={'legend': False})\nplt.legend([],[], frameon=False)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n",
        "\nplt.scatter(x, y, clip_on=False)\nplt.show()\n",
        "\nplt.scatter(x, y, color='red', edgecolor='black')\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor i in range(2):\n    for j in range(2):\n        axs[i, j].plot(x, y)\nplt.show()\n",
        "\nplt.hist(x, bins=5, range=(0,10))\nplt.show()\n",
        "\nplt.plot(x, y, 'k-')\nplt.fill_between(x, y-error, y+error, alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\nplt.show()\n",
        "\nplt.axhline(0, color='white') # y=0 line\nplt.axvline(0, color='white') # x=0 line\n",
        "\nfor i in range(len(box_position)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], color=c[i], fmt='o')\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title('Y')\naxs[1].plot(a, z)\naxs[1].set_title('Z', y=1.05)\nplt.show()\n",
        "\nfig, axs = plt.subplots(4, 4, figsize=(5,5))\nplt.subplots_adjust(hspace=0.5, wspace=0.5) # Adjust the spacing between subplots\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xticks(x)\n        axs[i, j].set_yticks(y)\nplt.show()\n",
        "\nplt.figure(figsize=(8, 8))\nplt.matshow(d, fignum=1)\nplt.show()\n",
        "\nfig, ax = plt.subplots(1, 1)\ntable_data = []\nfor row in df.itertuples():\n    table_data.append(row[1:])\ntable = ax.table(cellText=table_data, colLabels=df.columns, loc='center')\ntable.set_fontsize(14)\ntable.scale(1, 1.5)\nax.axis('off')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True, labeltop=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='x', which='both', bottom=True, top=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='x', which='both', length=0) # hide the x axis ticks\nplt.show()\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\n# Change the subplots titles\ntitles = [\"Group: Fat\", \"Group: No Fat\"]\nfor ax, title in zip(g.axes.flat, titles):\n    ax.set_title(title)\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.set_axis_labels(\"Exercise Time\", \"Pulse\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.set_axis_labels(\"\", \"\")\ng.set_titles(\"{col_name}\")\n",
        "\nplt.plot(x, y, label='y')\nplt.legend(fontsize=8)\nplt.show()\n",
        "\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.plot(x, y, label='y')\nlegend = plt.legend(frameon = 1)\nframe = legend.get_frame()\nframe.set_edgecolor('none')\nplt.show()\n",
        "\nplt.figure(figsize=(10,6))\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend()\nplt.title('Plot of sin(t), cos(t), and sin(t) + cos(t)')\nplt.xlabel('t')\nplt.ylabel('Value')\nplt.grid(True)\nplt.show()\n",
        "\nstrip_plot = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df)\nstrip_plot.legend_.remove()\nplt.show()\n",
        "\ng = sns.FacetGrid(df, row=\"b\")\ng.map(sns.pointplot, \"a\", \"c\")\n# Set xticks with intervals of 1\nfor ax in g.axes.flat:\n    ax.set_xticks(np.arange(0, 10, 1))\n# Set xtick labels with intervals of 2\nfor ax in g.axes.flat:\n    ax.set_xticklabels(np.arange(1, 11, 2))\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([]) # Hide x-axis tick labels\nplt.yticks([]) # Hide y-axis tick labels\nplt.show()\n",
        "\ngs = gridspec.GridSpec(nrow, ncol, wspace=0.0, hspace=0.0)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = plt.subplot(gs[i, j])\n        ax.imshow(x, cmap='viridis')\n        ax.set_xticks([])\n        ax.set_yticks([])\nplt.show()\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nlabels_tensor = tf.constant(labels)\nresult = tf.one_hot(labels_tensor, depth=10, dtype=tf.int32)\n",
        "\nlabels_tensor = tf.constant(labels, dtype=tf.int32)\none_hot_labels = tf.one_hot(labels_tensor, depth=10, dtype=tf.int32)\nresult = tf.math.subtract(1, one_hot_labels)\n",
        "\nlabels_tensor = tf.constant(labels, dtype=tf.int32)\nresult = tf.one_hot(labels_tensor, depth=10, on_value=0, off_value=1, dtype=tf.int32)\nresult = tf.reverse(result, axis=[1])\n",
        "\nlabels = tf.constant(labels, dtype=tf.int32)\nresult = tf.one_hot(labels, depth=10)\n",
        "\nlabels_tensor = tf.constant(labels, dtype=tf.int32)\none_hot = tf.one_hot(labels_tensor, depth=10)\nresult = tf.cast(tf.math.logical_not(one_hot), tf.int32)\n",
        "\ndef my_map_func(i):\n  return [i, i+1, i+2]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64, tf.int64, tf.int64]\n))\nds = ds.flat_map(tf.data.Dataset.from_tensor_slices)\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n",
        "\n    def my_map_func(i):\n        return [i, i+1, i+2]\n    ds = tf.data.Dataset.from_tensor_slices(input)\n    ds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\n    iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n    result = []\n    with tf.compat.v1.Session() as sess:\n        for _ in range(9):\n            result.append(sess.run(iterator.get_next()))\n    ",
        "\nlengths = tf.constant(lengths)\nmax_length = 8\nresult = tf.sequence_mask(lengths, max_length)\nresult = tf.cast(result, tf.int32)\n",
        "\nlengths = tf.constant(lengths)\nmax_length = 8\nresult = tf.sequence_mask(lengths, max_length)\nresult = tf.cast(result, tf.int32)\nresult = 1 - result\n",
        "\nlengths = tf.constant(lengths)\nmax_length = 8\nrange_tensor = tf.range(max_length)\nmask = tf.less(range_tensor, lengths[:, None])\nresult = tf.cast(mask, tf.float32)\n",
        "\n    lengths = tf.constant(lengths)\n    max_length = tf.constant(8)\n    range_tensor = tf.range(max_length)\n    mask = tf.less(range_tensor, lengths[:, None])\n    result = tf.cast(mask, tf.int32)\n",
        "\nlengths = tf.constant(lengths, dtype=tf.int32)\ntotal_length = 8\nmask = tf.sequence_mask(lengths, total_length)\nresult = tf.cast(mask, tf.float32)\nresult = tf.reverse(result, axis=[1])\n",
        "\na = tf.expand_dims(a, 1)\nb = tf.expand_dims(b, 0)\nresult = tf.stack(tf.meshgrid(a, b), -1)\n",
        "\n    a = tf.expand_dims(a, -1)\n    b = tf.expand_dims(b, 0)\n    result = tf.stack(tf.meshgrid(a, b), -1)\n",
        "\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\nresult = tf.expand_dims(a, axis=2)\n",
        "\nresult = tf.reshape(a, [1, 50, 100, 1, 512])\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\ndiff = tf.subtract(a, b)\nsquared_diff = tf.square(diff)\nresult = tf.reduce_sum(squared_diff, axis=1)\n",
        "\ndiff = tf.square(tf.subtract(a, b))\nresult = tf.reduce_sum(diff, axis=0)\n",
        "\n    diff = tf.subtract(A, B)\n    square_diff = tf.square(diff)\n    result = tf.reduce_sum(square_diff, axis=1)\n    ",
        "\nindices = tf.stack([y, z], axis=1)\nresult = tf.gather_nd(x, indices)\n",
        "indices = tf.stack([row, col], axis=-1)\nresult = tf.gather_nd(x, indices)\n",
        "\n    indices = tf.stack([y, z], axis=-1)\n    result = tf.gather_nd(x, indices)\n    ",
        "\nA = tf.transpose(A, perm=[1, 0, 2])\nB = tf.transpose(B, perm=[1, 0, 2])\nresult = tf.tensordot(A, B, axes=[[2], [2]])\nresult = tf.transpose(result, perm=[1, 2, 0])\n",
        "\nresult = tf.tensordot(A, B, axes=[[2], [2]])\n",
        "\nx_tensor = tf.constant(x)\nresult = tf.strings.unicode_decode(x_tensor, 'UTF-8')\n",
        "\n    tensor_x = tf.constant(x)\n    result = tf.strings.unicode_decode(tensor_x, 'UTF-8')\n",
        "\nnon_zero_mask = tf.cast(tf.not_equal(x, 0), tf.float32)\nnon_zero_count = tf.reduce_sum(non_zero_mask, axis=-2, keepdims=True)\nnon_zero_sum = tf.reduce_sum(x, axis=-2, keepdims=True)\nresult = non_zero_sum / non_zero_count\nresult = tf.squeeze(result, axis=-2)\n",
        "\nmask = tf.cast(tf.not_equal(x, 0), tf.float32)\nmean = tf.reduce_sum(x, axis=-2) / tf.reduce_sum(mask, axis=-2)\nmean = tf.where(tf.math.is_nan(mean), tf.zeros_like(mean), mean)\nvariance = tf.reduce_sum(mask * (x - mean[..., None, :]) ** 2, axis=-2) / tf.reduce_sum(mask, axis=-2)\nresult = tf.where(tf.math.is_nan(variance), tf.zeros_like(variance), variance)\n",
        "\nnon_zero_mask = tf.cast(tf.not_equal(x, 0), tf.float32)\nnon_zero_count = tf.reduce_sum(non_zero_mask, axis=-2, keepdims=True)\nnon_zero_sum = tf.reduce_sum(x * non_zero_mask, axis=-2, keepdims=True)\nresult = non_zero_sum / non_zero_count\n",
        "\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B)).numpy()\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=0)\n",
        "result = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmin(a, axis=0)\n",
        "\ntf.saved_model.save(model, 'my_model')\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n",
        "\ntf.random.set_seed(seed_x)\nresult = 2 + tf.random.uniform(shape=[114], minval=0, maxval=4, dtype=tf.int32)\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nlogx = np.log(x)\nresult = np.polyfit(logx, y, 1)\n",
        "\nlogx = np.log(x)\np = np.polyfit(logx, y, 1)\nresult = np.flip(p)\n",
        "\ndef func(x, A, B, C):\n    return A * np.exp(B * x) + C\npopt, pcov = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array(popt)\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\nD, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n",
        "def f(x):\n    a, b, c = x\n    return ((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4\nresult = optimize.minimize(f, initial_guess)\n",
        "\np_values = scipy.stats.norm.sf(abs(z_scores))\n",
        "\np_values = scipy.stats.norm.sf(np.array(z_scores))\n",
        "\nz_scores = [-scipy.stats.norm.ppf(p) for p in p_values]\n",
        "\ns = np.exp(mu)\nscale = np.exp(stddev**2)\ndist = stats.lognorm(s=s, scale=scale)\nresult = dist.cdf(x)\n",
        "\n# Create a lognormal distribution with the given parameters\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\n# Calculate the expected value and median of the distribution\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nsb = sparse.csr_matrix(np.array([0,1,2]).reshape(1,-1))\nresult = sa.multiply(sb)\n",
        "\n    sB = sB.transpose()\n    result = sA.multiply(sB)\n    ",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ndata_rot = rotate(data_orig, angle)\ncenter = np.array(data_orig.shape) / 2. - 0.5\noriginal = np.array([x0, y0])\nrelative = original - center\nrotated = np.dot(relative, [[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]])\nxrot, yrot = rotated + center\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, 'uniform', args=(0, T))\n",
        "\n    times = np.array(times)\n    times = times / T\n    result = stats.kstest(times, 'uniform')\n",
        "\n# Normalize the times to the range [0, 1]\ntimes = np.array(times) / T\n# Perform the Kolmogorov-Smirnov test\nD, p_value = stats.kstest(times, 'uniform')\n# If the p-value is greater than 0.05, we cannot reject the null hypothesis that the data is uniformly distributed\nresult = p_value > 0.05\n",
        "Feature = sparse.hstack((c1, c2))\n",
        "Feature = sparse.hstack((c1, c2))\n",
        "Feature = sparse.vstack((c1, c2))\n#print(Feature)",
        "distances = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(distances)\nresult = result[1]\n",
        "distances = scipy.spatial.distance.cdist(points1, points2, 'cityblock')\nresult = scipy.optimize.linear_sum_assignment(distances)\nresult = result[1]\n",
        "\nb.setdiag(0)\nb.eliminate_zeros()\n",
        "\nlabeled_array, num_features = ndimage.label(img > threshold)\nresult = num_features\n",
        "\nimg = img < threshold\nlabel_im, nb_labels = ndimage.label(img)\nresult = nb_labels\n",
        "labeled_array, num_features = ndimage.label(img > threshold)\nresult = num_features\n",
        "\nlabeled, nr_objects = ndimage.label(img > threshold) \nregions = ndimage.find_objects(labeled)\nresult = []\nfor region in regions:\n    center_of_mass = ndimage.center_of_mass(img, labeled, region)\n    distance = np.sqrt(center_of_mass[0]**2 + center_of_mass[1]**2)\n    result.append(distance)\n",
        "\nM = (M + M.T)/2\nM = M.tolil()\n",
        "\n    sA = sA + sA.T - np.diag(sA.diagonal())\n",
        "\neroded_square = scipy.ndimage.binary_erosion(square)\nreconstructed_square = scipy.ndimage.binary_dilation(eroded_square)\nindex = np.logical_and(square, np.logical_not(reconstructed_square))\nsquare[index] = 0\n",
        "\nstructure = np.ones((3, 3))\nlabels, n = scipy.ndimage.label(square, structure)\ncounts = np.bincount(labels.ravel())\nsquare[labels == np.where(counts == 1)[0]] = 0\n",
        "\nmean = col.sum() / col.shape[0]\nstandard_deviation = np.sqrt((col.power(2).sum() / col.shape[0]) - (mean ** 2))\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nfrom scipy import stats\n# Convert the sparse matrix to a 1D numpy array\ncol_arr = col.toarray().flatten()\n# Calculate the median\nMedian = np.median(col_arr)\n# Calculate the mode\nMode = stats.mode(col_arr)[0][0]\n",
        "\ndef fourier(x, *a):\n    ret = a[0] * np.cos(np.pi / tau * x)\n    for deg in range(1, len(a)):\n        ret += a[deg] * np.cos((deg+1) * np.pi / tau * x)\n    return ret\np0 = [1.0] * degree\npopt, pcov = curve_fit(fourier, z, Ua, p0=p0)\n",
        "\n# Get unique IDs\nunique_ids = np.unique(example_array)\nunique_ids = unique_ids[unique_ids != 0]\n# Get coordinates of each cell for each unique ID\ncoords = {uid: np.column_stack(np.where(example_array == uid)) for uid in unique_ids}\n# Calculate pairwise distances and get minimum\nresult = np.zeros((len(unique_ids), len(unique_ids)))\nfor i, uid1 in enumerate(unique_ids):\n    for j, uid2 in enumerate(unique_ids):\n        if uid1 != uid2:\n            result[i, j] = np.min(scipy.spatial.distance.cdist(coords[uid1], coords[uid2]))\n",
        "\n# Get unique IDs\nunique_ids = np.unique(example_array)\n# Initialize result array\nresult = np.zeros((len(unique_ids), len(unique_ids)))\n# Calculate pairwise Manhattan distances\nfor i, id1 in enumerate(unique_ids):\n    for j, id2 in enumerate(unique_ids):\n        if id1 != id2:\n            y1, x1 = np.where(example_array == id1)\n            y2, x2 = np.where(example_array == id2)\n            distances = scipy.spatial.distance.cdist(np.column_stack((y1, x1)), np.column_stack((y2, x2)), metric='cityblock')\n            result[i, j] = np.min(distances)\n",
        "\n    unique_ids = np.unique(example_array)\n    unique_ids = unique_ids[unique_ids != 0]  # Exclude 0 if it exists\n    result = np.zeros((len(unique_ids), len(unique_ids)))\n    for i, id1 in enumerate(unique_ids):\n        for j, id2 in enumerate(unique_ids):\n            if i != j:\n                coords1 = np.array(np.where(example_array == id1)).T\n                coords2 = np.array(np.where(example_array == id2)).T\n                result[i, j] = np.min(scipy.spatial.distance.cdist(coords1, coords2))\n    return result\n",
        "\nresult = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    result[i] = interpolate.splev(x_val, tck, der = 0, ext = 0)\n",
        "statistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])",
        "\nresult = ss.anderson_ksamp([x1, x2])\nresult = result.significance_level < 0.05\n",
        "\ndef rolling_kendall(df, window):\n    result = pd.DataFrame()\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                result[col1+col2] = df[[col1, col2]].rolling(window).apply(lambda x: stats.kendalltau(x[col1], x[col2])[0], raw=False)\n    return pd.concat([df, result], axis=1)\ndf = rolling_kendall(df, 3)\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = block_diag(*a)\n",
        "result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\nmean = np.mean(a)\nstd_dev = np.std(a)\nkurtosis_result = np.mean((a - mean) ** 4) / (std_dev ** 4)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=True, bias=False)\n",
        "\ninterpolated = scipy.interpolate.interp2d(x, y, z, kind='cubic')\nresult = interpolated(s, t)\n",
        "interp = scipy.interpolate.interp2d(x, y, z, kind='cubic')\nresult = interp(s, t)\n",
        "\nresult = []\nfor extraPoint in extraPoints:\n    dist = scipy.spatial.distance.cdist([extraPoint], points)\n    result.append(dist.argmin())\n",
        "\ndef voronoi_finite_polygons_2d(vor, radius=None):\n    if vor.points.shape[1] != 2:\n        raise ValueError(\"Requires 2D input\")\n    new_regions = []\n    new_vertices = vor.vertices.tolist()\n    center = vor.points.mean(axis=0)\n    if radius is None:\n        radius = vor.points.ptp().max()*2\n    for region in vor.regions:\n        if not -1 in region:\n            new_regions.append(region)\n            continue\n        poly = [vor.vertices[i] for i in region if i != -1]\n        hull = scipy.spatial.ConvexHull(poly)\n        for i in range(hull.nsimplex):\n            t = hull.transform[i,:2]\n            t0 = np.dot(t,hull.points[hull.vertices[i],:]) - 1\n            t = np.linalg.norm(t, axis=0)\n            r = np.array([-t[1], t[0]])\n            dt0 = np.dot(r,center)\n            if dt0 < 0:\n                r *= -1\n            far_point = np.dot(t,r)*radius + center\n            new_vertices.append(far_point.tolist())\n            region.append(len(new_vertices) - 1)\n        new_regions.append(region)\n    return new_regions, np.asarray(new_vertices)\ndef point_in_hull(point, hull, tolerance=1e-12):\n    return all((np.dot(eq[:-1], point) + eq[-1] <= tolerance) for eq in hull.equations)\ndef points_in_voronoi(vor, points):\n    regions, vertices = voronoi_finite_polygons_2d(vor)\n    hulls = [scipy.spatial.ConvexHull(vertices[region]) for region in regions]\n    return np.array([next((i for i, hull in enumerate(hulls) if point_in_hull(point, hull)), -1) for point in points])\nresult = points_in_voronoi(vor, extraPoints)\n",
        "\nresult = sparse.vstack([sparse.csr_matrix(np.pad(v, (0, max_vector_size - len(v)))) for v in vectors])\n",
        "\nb = scipy.ndimage.median_filter(a, size=(3,3), mode='constant', cval=0.0, origin=(0,1))\n",
        "result = M[row, column]",
        "\nresult = [M[i, j] for i, j in zip(row, column)]\n",
        "\nnew_array = scipy.interpolate.interp1d(x, array, axis=0)(x_new)\n",
        "\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner, _ = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nprob = NormalDistro(u,o2,x)\n",
        "\ndev = abs((x-u)/o2)\nP_inner, _ = scipy.integrate.quad(NDfx,-dev,dev)\nP_outer = 1 - P_inner\nprob = P_inner + P_outer/2\n",
        "\ndef dctmtx(N):\n    x,y = np.meshgrid(range(N), range(N))\n    D = np.sqrt(2.0/N) * np.cos(np.pi * (2*x+1) * y / (2.0 * N))\n    D[0] /= np.sqrt(2)\n    return D\nresult = dctmtx(N)\n",
        "\n# The diags function takes the diagonals in the order they are given, \n# so we need to reverse the order of the diagonals in the matrix.\nmatrix = matrix[::-1]\nresult = sparse.diags(matrix, [-1, 0, 1], (5, 5)).toarray()\n",
        "\nresult = np.zeros((N+1, N+1))\ni = np.arange(N+1).reshape(-1, 1)\nj = np.arange(N+1)\nmask = j <= i\nresult[mask] = scipy.stats.binom.pmf(j[mask], i[mask], p)\n",
        "\nresult = df.apply(stats.zscore, axis=1)\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\ndf_zscore = df.apply(stats.zscore, axis=1)\ndf_zscore.index = df.index + ' zscore'\nresult = pd.concat([df, df_zscore]).sort_index()\n",
        "\ndf_zscore = df.apply(stats.zscore)\ndf_zscore.index = df.index + ' zscore'\ndf.index = df.index + ' data'\nresult = pd.concat([df, df_zscore]).sort_index()\nresult = result.round(3)\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, np.array(starting_point), np.array(direction))\n",
        "\ny, x = np.indices(shape)\ncenter = np.array([(shape[0]-1)/2, (shape[1]-1)/2])\nresult = distance.cdist(np.column_stack([y.ravel(), x.ravel()]), center.reshape(1, -1)).reshape(shape)\n",
        "\ncenter = np.array([(x - shape[0] // 2, y - shape[1] // 2) for x in range(shape[0]) for y in range(shape[1])])\nresult = distance.cdist(center, [center[shape[0] * shape[1] // 2]], 'cityblock').reshape(shape)\n",
        "\n    y, x = np.indices(shape)\n    center = np.array(shape) / 2.\n    result = distance.cdist(np.column_stack([y.ravel(), x.ravel()]), center.reshape(1, -1)).reshape(shape)\n",
        "\nzoom = [shape_dim / x_dim for shape_dim, x_dim in zip(shape, x.shape)]\nresult = scipy.ndimage.zoom(x, zoom, order=1)\n",
        "\ndef func(x):\n    return np.sum((a.dot(x ** 2) - y) ** 2)\nout = scipy.optimize.minimize(func, x0)\n",
        "\ndef func(x, a, y):\n    return np.sum((a.dot(x ** 2) - y) ** 2)\nout = scipy.optimize.minimize(func, x0, args=(a, y), method='L-BFGS-B', bounds=scipy.optimize.Bounds(x_lower_bounds, np.inf))\n",
        "def dN1_dt(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])",
        "def dN1_dt(t, N1):\n    if 0 < t < 2*np.pi:\n        return -100 * N1 + t - np.sin(t)\n    else:\n        return -100 * N1 + 2*np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "def dN1_dt(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\nfor t in range (4):\n    cons.append({'type':'ineq', 'fun': lambda x, t=t: x[t]})\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\nresults = []\nfor i in range(c):\n    result, error = scipy.integrate.quad(lambda x: 2*x*i, low, high)\n    results.append(result)\n",
        "result, error = scipy.integrate.quad(lambda x: 2*c*x, low, high)\n",
        "\nfor key in V.keys():\n    V[key] += x\n",
        "\nV.data += x\n",
        "\nV.data += x\nV.data += y\n",
        "for Col in range(sa.shape[1]):\n    Column = sa[:,Col].copy()\n    List = [x**2 for x in Column.data]\n    Len = math.sqrt(sum(List))\n    sa[:,Col] = sa[:,Col] / Len\n",
        "\nfor i in range(sa.shape[1]):\n    col = sa[:, i].toarray()\n    col_len = np.linalg.norm(col)\n    sa[:, i] = sa[:, i] / col_len\n",
        "a = (a > 0).astype(int)",
        "a = (a > 0).astype(int)",
        "\ndistances = scipy.spatial.distance.cdist(centroids, data)\nresult = np.argmin(distances, axis=1)\n",
        "\ndistances = scipy.spatial.distance.cdist(centroids, data)\nclosest = np.argmin(distances, axis=1)\nresult = data[closest]\n",
        "\ndistances = scipy.spatial.distance.cdist(centroids, data)\nsorted_indices = np.argsort(distances, axis=1)\nresult = sorted_indices[:, k-1]\n",
        "result = []\nfor x, b in zip(xdata, bdata):\n    result.append(fsolve(lambda a: eqn(x, a, b), x0=0.5))\nresult = np.array(result)\n",
        "\nresult = []\nfor x, a in zip(xdata, adata):\n    roots = fsolve(lambda b: eqn(x, a, b), x0=[0.5])\n    roots.sort()\n    result.append(roots)\nresult = np.array(result)\n",
        "\ndef cdf(x):\n    return integrate.quad(lambda x: bekkers(x, estimated_a, estimated_m, estimated_d), range_start, x)[0]\nresult = sp.stats.kstest(sample_data, cdf)\n",
        "\n# Define the cumulative distribution function\ndef cdf_bekkers(x, a, m, d):\n    return integrate.quad(lambda x: bekkers(x, a, m, d), range_start, x)[0]\n# Calculate the cumulative distribution function values for the sample data\ncdf_sample_data = [cdf_bekkers(x, estimated_a, estimated_m, estimated_d) for x in sample_data]\n# Perform the Kolmogorov-Smirnov test\nD, p_value = sp.stats.kstest(cdf_sample_data, 'uniform')\n# Check if the null hypothesis can be rejected at the 95% confidence level\nresult = p_value < 0.05\n",
        "\ndf['Time'] = pd.to_datetime(df['Time'], format='%Y-%m-%d-%H:%M:%S')\ndf.set_index('Time', inplace=True)\ndf['integral'] = df.rolling('25S').apply(lambda x: integrate.trapz(x, x.index.astype(int)), raw=False)\nintegral_df = df['integral']\n",
        "\ngrid_z0 = scipy.interpolate.griddata(x, y, eval, method='linear')\nresult = grid_z0[0]\n",
        "\ndef multinomial(params):\n    return -np.sum(a['A1']*np.log(params))\nx0 = np.ones(12)/12\nbounds = [(0.001, 1) for _ in range(12)]\nconstraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\nresult = sciopt.minimize(multinomial, x0, bounds=bounds, constraints=constraints)\nweights = result.x\n",
        "\nresult = sciopt.minimize(e, [0.5, 0.5], bounds=[(0.5, 1.5), (0.7, 1.8)], args=(x,y))\n",
        "\nresult = signal.argrelextrema(arr, np.less_equal, order=n)\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(n, arr.shape[1]-n):\n        if all(arr[i][j] <= arr[i][k] for k in range(j-n, j+n+1)):\n            result.append([i, j])\n",
        "\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\ndf = df[(np.abs(stats.zscore(df[numeric_cols])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])",
        "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n",
        "DataFrame(data.data, columns=data.feature_names)\n",
        "\ndf = pd.DataFrame(data=data.data, columns=data.feature_names)\nresult = df\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ndf_out = df.join(pd.DataFrame(mlb.fit_transform(df.pop('Col3')),\n                          columns=mlb.classes_,\n                          index=df.index))\n",
        "\n# get all unique values in the list of lists\nunique_values = pd.unique(df['Col3'].explode())\n# create new columns for each unique value\nfor value in unique_values:\n    df[value] = np.where(df['Col3'].apply(lambda x: value in x), 1, 0)\ndf_out = df\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ndf_out = df.join(pd.DataFrame(mlb.fit_transform(df.pop('Col4')),\n                          columns=mlb.classes_,\n                          index=df.index))\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ndf_out = df.join(pd.DataFrame(mlb.fit_transform(df.pop(df.columns[-1])),\n                          columns=mlb.classes_,\n                          index=df.index))\n",
        "\n# Get the unique elements in the last column\nunique_elements = pd.unique(df[df.columns[-1]].explode())\n# Create a new dataframe with the unique elements as columns\ndf_out = pd.DataFrame(columns=unique_elements)\n# Iterate over the rows of the original dataframe\nfor index, row in df.iterrows():\n    # For each unique element, check if it is in the list in the last column of the current row\n    # If it is, set the corresponding cell in the new dataframe to 0, otherwise set it to 1\n    for element in unique_elements:\n        df_out.loc[index, element] = 0 if element in row[df.columns[-1]] else 1\n# Concatenate the original dataframe (without the last column) with the new dataframe\ndf_out = pd.concat([df[df.columns[:-1]], df_out], axis=1)\n",
        "\nsvmmodel.fit(X, y)\ncalibrated_clf = CalibratedClassifierCV(svmmodel, cv=5)\ncalibrated_clf.fit(X, y)\nproba = calibrated_clf.predict_proba(x_test)\n",
        "\nfrom sklearn.calibration import CalibratedClassifierCV\nmodel = svm.LinearSVC()\ncalibrated_clf = CalibratedClassifierCV(model, method='sigmoid', cv=5)\ncalibrated_clf.fit(X, y)\nproba = calibrated_clf.predict_proba(x_predict)\n",
        "\n# Convert the sparse matrix to a dense numpy array\ndense_output = transform_output.toarray()\n# Convert the numpy array to a dataframe\ndf_transformed = pd.DataFrame(dense_output)\n# Concatenate the original dataframe with the transformed dataframe\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\n# Convert the csr_matrix to a DataFrame\ndf_transformed = pd.DataFrame.sparse.from_spmatrix(transform_output)\n# Merge the transformed DataFrame with the original DataFrame\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\n    transform_output_df = pd.DataFrame(transform_output.toarray())\n    result = pd.concat([df, transform_output_df], axis=1)\n",
        "del clf.steps[1]\n",
        "del clf.steps[1]\n",
        "del clf.named_steps['pOly']\n",
        "clf.steps.insert(1, ('poly', PolynomialFeatures()))\n",
        "clf.steps.insert(1, ('new_step', SVC()))\n",
        "\nsteps = list(clf.steps)\nsteps.insert(1, ('t1919810', PCA()))\nclf = Pipeline(steps)\n",
        "\nfit_params = {\"early_stopping_rounds\":42, \n              \"eval_metric\" : \"mae\", \n              \"eval_set\" : [(testX, testY)]}\nmodel = xgb.XGBRegressor()\nparamGrid = {'max_depth': [3, 5, 7], 'min_child_weight': [1, 3, 5]}\ncv = TimeSeriesSplit(n_splits=5)\ngridsearch = GridSearchCV(estimator=model, \n                          param_grid=paramGrid, \n                          cv=cv, \n                          verbose=1, \n                          fit_params=fit_params)\ngridsearch.fit(trainX, trainY)\n",
        "\nmodel = xgb.XGBRegressor()\nparamGrid = {'max_depth': [3, 5, 7], 'n_estimators': [100, 500, 1000]}\nfit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]}\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=1, iid=False)\ngridsearch.fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train, test in cv:\n    logreg.fit(X[train], y[train])\n    proba.extend(logreg.predict_proba(X[test]))\nproba = np.array(proba)\n",
        "\nproba = []\nfor train, test in cv:\n    logreg.fit(X[train], y[train])\n    proba.extend(logreg.predict_proba(X[test]))\nproba = np.array(proba)\n",
        "\n# run regression model\nmodel = SomeRegressionModel()\nmodel.fit(scaled)\n# check score\nscore = model.score(scaled)\n# predict t'\npredicted = model.predict(scaled)\n# check predicted t' with real time value(inverse StandardScaler)\ninversed = scaler.inverse_transform(predicted)\n",
        "\nfrom sklearn.linear_model import LinearRegression\n# Separate features and target\nX = scaled[:, 1:]\ny = scaled[:, 0]\n# Fit the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n# Predict\npredictions = model.predict(X)\n# Inverse transform the predictions\ninversed = scaler.inverse_transform(np.column_stack((predictions, X)))\n",
        "model_name = model.__class__.__name__",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf = pipe.named_steps[\"tf_idf\"]\ntf_idf_out = tf_idf.fit_transform(data)\n",
        "\npipe.fit(data.test)\ntf_idf_out = pipe.named_steps['tf_idf'].transform(data.test)\n",
        "\npipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)\n",
        "\nclf = GridSearchCV(bc, param_grid)\nclf = clf.fit(X_train, y_train)\n",
        "\nX = X.reshape(-1, 1)\nX_test = X_test.reshape(-1, 1)\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nregressor.fit(X, y)\n",
        "\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\nX_test = X_test.reshape(-1, 1)\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y.ravel())\n",
        "\ndef preprocess(s):\n    return s.upper()\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\ndef prePro(text):\n    return text.lower()\ntfidf = TfidfVectorizer(preprocessor=prePro)\n",
        "\nscaled_data = preprocessing.scale(data)\ndf_out = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n",
        "\nscaled_data = preprocessing.scale(data)\ndf_out = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nmask = model.get_support() #list of booleans for selected features\nnew_features = [] # The list of your K best features\nfor bool, feature in zip(mask, X.columns):\n    if bool:\n        new_features.append(feature)\ncolumn_names = new_features\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nfeature_idx = model.get_support()\ncolumn_names = X.columns[feature_idx]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nfeature_idx = model.get_support()\ncolumn_names = X.columns[feature_idx]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nmask = model.get_support() #list of booleans for selected features\nnew_features = [] # The list of your K best features\nfor bool, feature in zip(mask, X.columns):\n    if bool:\n        new_features.append(feature)\ncolumn_names = new_features\n",
        "\nkm.fit(X)\ndistances = km.transform(X)\ndistances_to_p = distances[:, p]\nclosest_50_indices = np.argsort(distances_to_p)[:50]\nclosest_50_samples = X[closest_50_indices]\n",
        "\nkm.fit(X)\ndistances = km.transform(X)[:, p]\nclosest_50_indices = np.argsort(distances)[:50]\nclosest_50_samples = X[closest_50_indices]\n",
        "\nkm.fit(X)\ndistances = km.transform(X)\ndistances_to_p = distances[:, p]\nclosest_100_indices = np.argsort(distances_to_p)[:100]\nclosest_100_samples = X[closest_100_indices]\n",
        "\nkm.fit(X)\ndistances = km.transform(X)[:, p]\nindices = np.argsort(distances)[:50]\nsamples = X[indices]\n",
        "\n# Convert categorical variable to matrix and merge back with original training data\nX_train = pd.get_dummies(X_train, columns=[0])\n",
        "\n# Convert categorical variable to dummy variables\nX_train = pd.get_dummies(X_train, columns=[0])\n",
        "\nfrom sklearn.svm import SVR\nmodel = SVR(kernel='rbf')\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\n# create a SVM regression model with a gaussian kernel\nmodel = SVR(kernel='rbf')\n# fit the model with the data\nmodel.fit(X, y)\n# predict the output for the same data\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\n# create a SVM regression model with polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n# fit the model with the data\nmodel.fit(X, y)\n# predict the output for the same data\npredict = model.predict(X)\n",
        "\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarities_of_queries = []\nfor query in queries:\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = cosine_similarity(query_tfidf, tfidf.transform(documents))\n    cosine_similarities_of_queries.append(cosine_similarities[0])\ncosine_similarities_of_queries = np.array(cosine_similarities_of_queries)\n",
        "\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarities_of_queries = []\nfor query in queries:\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = cosine_similarity(query_tfidf, tfidf.transform(documents))\n    cosine_similarities_of_queries.append(cosine_similarities)\ncosine_similarities_of_queries = np.array(cosine_similarities_of_queries)\n",
        "\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarities_of_queries = []\nfor query in queries:\n    query_vector = tfidf.transform([query])\n    cosine_similarities = cosine_similarity(query_vector, tfidf.transform(documents))\n    cosine_similarities_of_queries.append(cosine_similarities[0])\ncosine_similarities_of_queries = np.array(cosine_similarities_of_queries)\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\nnew_features = pd.DataFrame(new_features,columns=mlb.classes_)\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nnew_f = pd.DataFrame(mlb.fit_transform(f),columns=mlb.classes_, index=['r'+str(i+1) for i in range(len(f))])\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\nnew_features = pd.DataFrame(new_features,columns=mlb.classes_)\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\nnew_features = pd.DataFrame(new_features,columns=mlb.classes_)\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nnew_features = pd.DataFrame(mlb.fit_transform(features), columns=mlb.classes_, index=['r'+str(i+1) for i in range(len(features))])\n",
        "\n# Create an object of AgglomerativeClustering\nagg_clustering = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n# Fit the data and predict the cluster labels\ncluster_labels = agg_clustering.fit_predict(data_matrix)\n",
        "\n# Create a model and fit it\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\nmodel.fit(data_matrix)\n# Get cluster assignments\ncluster_labels = model.labels_.tolist()\n",
        "\n# Convert the similarity matrix to a distance matrix\ndistM = 1 - np.array(simM)\n# Perform hierarchical clustering\ncluster = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = cluster.fit_predict(distM)\n",
        "\nZ = scipy.cluster.hierarchy.linkage(data_matrix, 'ward')\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, 2, criterion='maxclust')\n",
        "\nZ = scipy.cluster.hierarchy.linkage(data_matrix, 'ward')\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, 2, criterion='maxclust')\n",
        "\nZ = scipy.cluster.hierarchy.linkage(simM, 'ward')\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, 2, criterion='maxclust')\n",
        "\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\")\ntransformed_text = vectorizer.fit_transform([text])\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the dataset into features and target variable\nX = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the data into features and target\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the dataset into training and testing sets\ntrain_set, test_set = train_test_split(dataset, test_size=0.4, random_state=42)\n# Split the training set into x (all columns except the last one) and y (the last column)\nx_train = train_set.iloc[:, :-1]\ny_train = train_set.iloc[:, -1]\n# Split the testing set into x (all columns except the last one) and y (the last column)\nx_test = test_set.iloc[:, :-1]\ny_test = test_set.iloc[:, -1]\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\ntrain, test = train_test_split(data, test_size=0.2, random_state=42)\n# Split the training set into x (features) and y (target)\nx_train = train.iloc[:, :-1]\ny_train = train.iloc[:, -1]\n# Split the testing set into x (features) and y (target)\nx_test = test.iloc[:, :-1]\ny_test = test.iloc[:, -1]\n",
        "\nf1 = df['mse'].values\nf1 = f1.reshape(-1,1)\nkmeans = KMeans(n_clusters=2).fit(f1)\nlabels = kmeans.predict(f1)\n",
        "\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n",
        "\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\nmodel = sklearn.feature_selection.SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[model.get_support()]\n",
        "\n# Create a LinearSVC model with l1 penalty\nmodel = LinearSVC(penalty='l1', dual=False)\nmodel.fit(X, y)\n# Get the coefficients of the model\ncoef = model.coef_\n# Get the indices of the features selected by the model\nselected_indices = np.where(coef != 0)[1]\n# Get the names of the selected features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_indices]\n",
        "\n    lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n    model = sklearn.feature_selection.SelectFromModel(lsvc, prefit=True)\n    X_new = model.transform(X)\n    selected_features = model.get_support(indices=True)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_features]\n",
        "\nvocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'}\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n",
        "\nvocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'}\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nX = np.where(X==0, 1, X)\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nX = np.where(X>0,1,X)\n",
        "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m[0,0])\n",
        "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m[0,0])\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ntransformed_df = df\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ntransformed_df = df\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ntransformed_df = df\n",
        "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nscaler = MinMaxScaler()\nreshaped = np_array.reshape(-1, 1)\ntransformed = scaler.fit_transform(reshaped)\ntransformed = transformed.reshape(np_array.shape)\n",
        "\nscaler = MinMaxScaler()\nreshaped = np_array.reshape(-1,1)\ntransformed = scaler.fit_transform(reshaped)\ntransformed = transformed.reshape(np_array.shape)\n",
        "\nscaler = MinMaxScaler()\nreshaped = a.reshape(-1,1)\nscaled = scaler.fit_transform(reshaped)\nnew_a = scaled.reshape(a.shape)\n",
        "\nclose_buy1 = close.values[:-1]\nm5 = ma50.values[:-1]\nm10 = ma100.values[:-1]\nma20 = ma200.values[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict(b)\n",
        "\ndf = pd.DataFrame(X, columns=['feature1', 'feature2'])\ndf['feature1'] = df['feature1'].astype('category').cat.codes\nnew_X = df.values\n",
        "\n# Convert the list to a DataFrame\ndf = pd.DataFrame(X, columns=['feature1', 'feature2'])\n# Use LabelEncoder to convert string values to numeric\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n# Apply LabelEncoder to the DataFrame\nnew_X = df.apply(le.fit_transform)\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\nnew_X = new_X.astype(float)\n",
        "\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].values.astype(float)\ny = dataframe.iloc[:, -1].values\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\ntrain_size = int(len(features_dataframe) * 0.2)\ntrain_dataframe = features_dataframe[:train_size]\ntest_dataframe = features_dataframe[train_size:]\n",
        "\ntrain_size = int(0.8 * len(features_dataframe))\ntrain_dataframe = features_dataframe.sort_values(by=['date']).iloc[train_size:]\ntest_dataframe = features_dataframe.sort_values(by=['date']).iloc[:train_size]\n",
        "\ntrain_size = int(len(features_dataframe) * 0.2)\ntrain_dataframe = features_dataframe[:train_size]\ntest_dataframe = features_dataframe[train_size:]\n",
        "\nfor col in ['X2', 'X3']:\n    df[col + '_scale'] = df.groupby('Month')[col].transform(lambda x: scaler.fit_transform(x.values.reshape(-1,1)))\n",
        "\nfor col in ['A2', 'A3']:\n    myData['new_' + col] = myData.groupby('Month')[col].transform(lambda x: scaler.fit_transform(x.values.reshape(-1,1)).flatten())\n",
        "\ncount = CountVectorizer(lowercase = False, token_pattern = r\"(?u)\\b\\w\\w+\\b|@[\\w]+|#[\\w]+\")\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase = False, token_pattern = '[^\\s]+')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n",
        "\n# Convert to Dataframe\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n",
        "\n# Convert to Dataframe\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n# Sort by mean_fit_time\nfull_results = full_results.sort_values(by='mean_fit_time')\n",
        "\njoblib.dump(fitted_model, 'sklearn_model.pkl')\n",
        "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = (1 - sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix))\n"
    ],
    "Pytorch": [
        "\nfor g in optim.param_groups:\n    g['lr'] = 0.001\n",
        "\nfor g in optim.param_groups:\n    g['lr'] = 0.001\n",
        "\nfor g in optim.param_groups:\n    g['lr'] = 0.0005\n",
        "\n# To change the learning rate of the optimizer, you can use the following code:\nfor g in optim.param_groups:\n    g['lr'] = 0.0005\n# As for choosing the learning rate, it depends on the situation. \n# If the loss on the training set is increasing, it means that the learning rate might be too high. \n# In this case, you should try a smaller learning rate like 0.0005. \n# On the other hand, if the training process is very slow, you might want to try a larger learning rate like 0.05.\n",
        "\nweights = torch.FloatTensor(word2vec.wv.vectors)\nembedding = torch.nn.Embedding.from_pretrained(weights)\nembedded_input = embedding(input_Tensor)\n",
        "\nweights = torch.FloatTensor(word2vec.wv.vectors)\nembedding = torch.nn.Embedding.from_pretrained(weights)\nembedded_input = embedding(input_Tensor)\n",
        "\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n",
        "\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n",
        "\nx = x.numpy()\npx = pd.DataFrame(x)\n",
        "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log.nonzero().squeeze()]",
        "\nA_logical = A_logical.nonzero().squeeze()\nC = B[:, A_logical]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nA_log = A_log.bool()\nC = B[:, A_log]\n",
        "\nA_log = A_log.nonzero().squeeze()\nC = B[:, A_log]\n",
        "\nA_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\n# Convert ByteTensor to LongTensor for indexing\nA_log = A_log.nonzero().squeeze()\n# Use the LongTensor for indexing\nC = B[:, A_log]\n",
        "\nC = torch.index_select(B, 1, idx)\n",
        "\nx_tensor = torch.Tensor(np.vstack(x_array).astype(np.float32))\n",
        "\nx_tensor = torch.from_numpy(np.vstack(x_array).astype(np.float))\n",
        "\nt = torch.tensor([np.array(a[i], dtype=np.float32) for i in range(len(a))])\n",
        "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len).long()\nfor idx, length in enumerate(lens):\n    mask[idx, :length] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.arange(max_len).expand(len(lens), max_len) < torch.LongTensor(lens).unsqueeze(1)\n",
        "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len).long()\nfor idx, length in enumerate(lens):\n    mask[idx, :length] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len).long()\nfor idx, length in enumerate(lens):\n    mask[idx, :length] = 1\n",
        "\nTensor_3D = torch.stack([torch.diag(Tensor_2D[i]) for i in range(Tensor_2D.shape[0])])\n",
        "\nresult = []\nfor i in range(t.shape[0]):\n    result.append(torch.diag(t[i]))\nresult = torch.stack(result)\n",
        "\nb = b.unsqueeze(0)\nab = torch.cat((a, b), 0)\n",
        "\nab = torch.cat((a, b), 0)\n",
        "\nb = b.unsqueeze(0)\nab = torch.cat((a, b), 0)\n",
        "\nfor i, length in enumerate(lengths):\n    a[i, length:, :] = 0\n",
        "\nfor i, length in enumerate(lengths):\n    a[i, length:, :] = 2333\n",
        "\nfor i, length in enumerate(lengths):\n    a[i, :length, :] = 0\n",
        "for i, length in enumerate(lengths):\n    a[i, :length, :] = 2333",
        "\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "new_tensors = torch.stack(list)\n",
        "\ntt = torch.stack(lt)\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[torch.arange(len(t)), torch.from_numpy(idx)]\n",
        "\nresult = t[torch.arange(t.size(0)), torch.from_numpy(idx)]\n",
        "\nidx = torch.from_numpy(idx)\nresult = torch.gather(t, 1, idx.view(-1,1)).squeeze()\n",
        "\nresult = x.gather(1, ids.unsqueeze(-1).expand(-1, -1, x.size(-1))).squeeze(1)\n",
        "\nresult = x.gather(1, ids.unsqueeze(-1).expand(-1, -1, x.size(2))).squeeze(1)\n",
        "\nids = torch.tensor(ids)\nx = torch.tensor(x)\nresult = torch.zeros(ids.shape[0], x.shape[2])\nfor i in range(ids.shape[0]):\n    for j in range(ids.shape[1]):\n        if ids[i][j] == 1:\n            result[i] = x[i][j]\n",
        "\n_, y = torch.max(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.max(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.min(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.max(softmax_output, dim=1)\ny = y.view(-1, 1)\n",
        "\n    _, y = torch.min(softmax_output, dim=1)\n    ",
        "\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, h, w), target: (n, h, w)\n    n, c, h, w = input.size()\n    # log_p: (n, c, h, w)\n    log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*h*w, c)\n    log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n    log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c).view(-1, c) >= 0]\n    log_p = log_p.view(-1, c)\n    # target: (n*h*w,)\n    mask = target >= 0\n    target = target[mask]\n    loss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False)\n    if size_average:\n        loss /= mask.data.sum()\n    return loss\nimages = Variable(torch.randn(5, 7, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(7))\nloss = cross_entropy2d(images, labels)\n",
        "\ncnt_equal = torch.sum(A == B).item()\n",
        "\ncnt_equal = torch.sum(A == B).item()\n",
        "\ncnt_not_equal = torch.sum(A != B).item()\n",
        "\ncnt_equal = torch.sum(A == B).item()\n",
        "\nx = A.shape[0] // 2\ncnt_equal = torch.sum(A[-x:] == B[-x:]).item()\n",
        "\nx = A.shape[0] // 2\ncnt_not_equal = torch.sum(A[-x:] != B[-x:]).item()\n",
        "\ntensors_31 = [a[..., i:i+chunk_dim, :] for i in range(a.shape[3] - chunk_dim + 1)]\n",
        "\ntensors_31 = [a[:,:,i:i+chunk_dim,:,:] for i in range(a.shape[2]-chunk_dim+1)]\n",
        "\nmask = mask.unsqueeze(2).expand_as(clean_input_spectrogram)\noutput = torch.where(mask == 1, clean_input_spectrogram, output)\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_abs = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(torch.abs(x) == min_abs, sign_x * min_abs, sign_y * min_abs)\n",
        "\n# Compute the signs of x and y\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\n# Compute the absolute values of x and y\nabs_x = torch.abs(x)\nabs_y = torch.abs(y)\n# Compute the maximum absolute values\nmax_abs = torch.max(abs_x, abs_y)\n# Compute the signs of the maximum absolute values\nsign_max_abs = torch.where(abs_x >= abs_y, sign_x, sign_y)\n# Multiply the signs with the maximum absolute values\nsigned_max = sign_max_abs * max_abs\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_abs = torch.min(torch.abs(x), torch.abs(y))\nsign_min = torch.where(torch.abs(x) == min_abs, sign_x, sign_y)\nsigned_min = sign_min * min_abs\n",
        "\ndef predict_allCharacters(input):\n    output = MyNet(input)\n    softmax = torch.nn.Softmax(dim=1)\n    output = softmax(output)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\nconfidence_score = predict_allCharacters(input)\n",
        "\n# Create a new tensor by concatenating 'a' and 'b' along the second axis (columns)\nc = torch.cat((a, b), dim=1)\n# Calculate the average of the overlapping columns\navg_col = (a[:, -1] + b[:, 0]) / 2\n# Replace the overlapping columns in 'c' with the average column\nc[:, 2] = avg_col\nresult = c\n",
        "\n    overlap = (a[:,-1] + b[:,0]) / 2\n    result = torch.cat((a[:,:-1], overlap.unsqueeze(1), b[:,1:]), dim=1)\n",
        "\n# Create a tensor of zeros with the same shape as t\nzeros = torch.zeros_like(t)\n# Concatenate the zeros tensor and t tensor along the second dimension\nresult = torch.cat((zeros, t, zeros), dim=1)\n# Add zeros at the beginning and end of the tensor\nresult = torch.cat((torch.zeros(1, result.shape[1], result.shape[2]), result, torch.zeros(1, result.shape[1], result.shape[2])), dim=0)\n",
        "\n# Create a tensor of zeros with the same size as t\nzeros = torch.zeros_like(t)\n# Create a tensor of zeros with the same size as t but with an additional row and column\npadded_zeros = torch.zeros(t.size(0), t.size(1)+2, t.size(2)+2)\n# Copy the original tensor t into the center of the padded_zeros tensor\npadded_zeros[:, 1:-1, 1:-1] = t\n# Concatenate the zeros tensor and the padded_zeros tensor along the first dimension\nresult = torch.cat((zeros, padded_zeros, zeros), dim=1)\n",
        "\nnew = torch.full((1, 4, 4), -1)\nnew[:, 1:-1, 1:-1] = t\nresult = new\n",
        "\nW = W.view(hid_dim, 1)\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\n"
    ]
}