{
    "Pandas": [
        "shuffled_indices = List[df.index]\nshuffled_df = df.iloc[shuffled_indices]\nresult = shuffled_df",
        "\nshuffled_df = df.iloc[List]\ncount = 0\nfor index, row in shuffled_df.iterrows():\n    if row['Type'] != df['Type'].iloc[index]:\n        count += 1\nprint(count)\n",
        "\nvalue_counts_result = pd.value_counts(df.Qu1)\ngreater_or_equal_two = value_counts_result[value_counts_result >= 2]\nto_replace = greater_or_equal_two.index\nto_replace = to_replace[to_replace != 'egg']\nto_replace = to_replace + ['other']\ndf.loc[df.Qu1.isin(to_replace), 'Qu1'] = 'other'\nvalue_counts_result = pd.value_counts(df.Qu2)\ngreater_or_equal_two = value_counts_result[value_counts_result >= 2]\nto_replace = greater_or_equal_two.index\nto_replace = to_replace[to_replace != 'egg']\nto_replace = to_replace + ['other']\ndf.loc[df.Qu2.isin(to_replace), 'Qu2'] = 'other'\n",
        "\nvalue_counts_result = pd.value_counts(df.Qu1)\ngreater_than_3_count = value_counts_result[value_counts_result >= 3].index.tolist()\nnew_values = ['other'] * len(greater_than_3_count)\nnew_values[greater_than_3_count] = ['cheese']\ndf.Qu1 = df.Qu1.map(new_values)\n",
        "\n    value_counts = pd.value_counts(df.Qu1)\n    value_counts_greater_or_equal_2 = value_counts[value_counts >= 2]\n    value_counts_greater_or_equal_2_values = value_counts_greater_or_equal_2.index.tolist()\n    \n    # Replace values in Qu1 column\n    df.loc[df.Qu1.isin(value_counts_greater_or_equal_2_values), 'Qu1'] = 'other'\n    \n    # Replace values in Qu2 column\n    df.loc[df.Qu2.isin(['apple', 'sausage']), 'Qu2'] = 'other'\n    \n    ",
        "\nvalue_counts_result = pd.value_counts(df.Qu1)\ngreater_or_equal_3 = value_counts_result[value_counts_result >= 3]\nvalue_to_replace = greater_or_equal_3.index[greater_or_equal_3.index != 'cheese']\ndf.loc[df.Qu1.isin(value_to_replace), 'Qu1'] = 'other'\nvalue_counts_result = pd.value_counts(df.Qu2)\ngreater_or_equal_2 = value_counts_result[value_counts_result >= 2]\nvalue_to_replace = greater_or_equal_2.index[greater_or_equal_2.index != 'banana']\ndf.loc[df.Qu2.isin(value_to_replace), 'Qu2'] = 'other'\nvalue_counts_result = pd.value_counts(df.Qu3)\ngreater_or_equal_2 = value_counts_result[value_counts_result >= 2]\nvalue_to_replace = greater_or_equal_2.index[greater_or_equal_2.index != 'cheese']\ndf.loc[df.Qu3.isin(value_to_replace), 'Qu3'] = 'other'\n",
        "\n# Replace values in Qu1 with 'other' if count is 3 or greater, except for 'apple'\napple_count = df['Qu1'].value_counts().loc['apple']\napple_keep = apple_count < 3\ndf.loc[apple_keep, 'Qu1'] = 'apple'\ndf.loc[~apple_keep, 'Qu1'] = 'other'\n# Replace values in Qu2 and Qu3 with 'other' if count is 2 or greater\nother_count = 2\ndf.loc[df['Qu2'].value_counts() >= other_count, 'Qu2'] = 'other'\ndf.loc[df['Qu3'].value_counts() >= other_count, 'Qu3'] = 'other'\n# Output the result\nprint(df)\n",
        "\nkeep_if_dup_dupes = df.groupby('url')['keep_if_dup'].first()\ndf = df[~df['url'].isin(keep_if_dup_dupes[keep_if_dup_dupes == 'Yes'].index)]\ndf = df.drop_duplicates(subset='url', keep='first')\nprint(result)",
        "\ndf = df.groupby(['url', 'drop_if_dup']).first()\nprint(result)",
        "\nkeep_if_dup_dupes = df.groupby('url')['keep_if_dup'].transform('max')\ndf = df[~df.duplicated(['url', 'keep_if_dup'], keep='first')]\ndf = df[df.keep_if_dup == keep_if_dup_dupes]\n",
        "\nresult = {}\nfor name, group in df.groupby(\"name\"):\n    temp = {}\n    for col, value in zip(group.columns, group.values):\n        temp[col] = value\n    result[name] = temp\n",
        "\n# Convert to UTC\nresult = df.dt.tz_localize('UTC')\n# Extract datetime without timezone info\nresult['datetime'] = result['datetime'].dt.tz_convert('UTC').dt.strftime('%Y-%m-%d %H:%M:%S')\n# Convert back to pandas datetime object\nresult['datetime'] = pd.to_datetime(result['datetime'])\nprint(result)",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\nexample_df['datetime'] = pd.to_datetime(example_df['datetime'])\ndef f(df=example_df):\n    # Get the timezone offset from the datetime column\n    tz_offset = df['datetime'].dt.tz.tz_localize(None).tz_convert('UTC').tz_localize(None).tz.zone\n    # Remove the timezone offset from the datetime column\n    df['datetime'] = df['datetime'].dt.tz_convert('UTC').dt.tz_localize(None).replace(tz_offset, '')\n    # Return the modified dataframe\n    return df\n",
        "\n# Convert datetime column to UTC\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf['datetime'] = pd.to_datetime(df['datetime'].dt.tz_localize(None))\n# Sort by datetime column\ndf = df.sort_values('datetime')\n# Format datetime column\ndf['datetime'] = pd.to_datetime(df['datetime']).dt.strftime('%Y-%m-%d %H:%M:%S')\nresult = df\nprint(result)",
        "\n# Get the timezone offset\ntimezone = df['datetime'].dt.tz.tz_localize(None).tz_convert('UTC')\n# Remove the timezone offset from the datetime column\ndf['datetime'] = df['datetime'].dt.tz_convert('UTC') - timezone\n# Sort the datetime column in ascending order\ndf = df.sort_values('datetime')\n# Print the result\nprint(df)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\n# Extract key-value pairs from the 'message' column\nkey_value_pairs = df['message'].str.extractall(r'(\\w+):\\s*\"([^\"]*)\"')\n# Convert the extracted key-value pairs into a dataframe\nkey_value_df = pd.DataFrame(key_value_pairs, columns=['key', 'value'])\n# Merge the key-value dataframe with the original dataframe\ndf = pd.merge(df, key_value_df, on='name')\n# Expand the dataframe to include all possible columns\ndf = df.explode('message')\n# Print the resulting dataframe\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1066490, 1077784]\n# Multiply scores corresponding to products in the list by 10\nfor product in products:\n    df.loc[df['product'] == product, 'score'] *= 10\n# Display the modified dataframe\nresult = df\nprint(result)\n",
        "\nfor product in products:\n    result.loc[result['product'] == product, 'score'] *= 10\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [[1069104, 1069105], [1179159, 1179161]]\nfor index, row in df.iterrows():\n    if row['product'] in products:\n        df.loc[index, 'score'] *= 10\nresult = df\nprint(result)\n",
        "\nfor product in products:\n    mask = df['product'] == product\n    result[mask] = result[mask].multiply(df[product].max() - df[product].min(), axis=0) + df[product].min()\n",
        "# Convert binary columns to categorical column\nfor col in ['A', 'B', 'C', 'D']:\n    result[col + ' category'] = pd.Categorical(result[col], categories=['A', 'B', 'C', 'D'])\n# Drop original binary columns\nresult.drop(columns=['A', 'B', 'C', 'D'], inplace=True)\nprint(result)",
        "\ncategory = pd.get_dummies(df, columns=['A', 'B', 'C', 'D'])\nresult = pd.concat([result, category], axis=1)\nprint(result)",
        "\ncategory = df.columns[df.dtypes == 'object']\nresult = pd.concat([result, pd.Series(df[category].tolist(), name='category')], axis=1)\nprint(result)",
        "\ndf['Date'].dt.strftime('%B-%Y')",
        "# Extract the month name and year and day in a simple way\nresult['Date'] = pd.to_datetime(df['Date']).dt.strftime('%B-%d-%Y')\nprint(result)",
        "",
        "\ndf = df.shift(1, axis=0)\nresult = df.iloc[:-1, :]\n",
        "\nresult = df.shift(1, axis=0)\nresult.iloc[0, :] = df.iloc[-1, :]\nresult.iloc[-1, :] = df.iloc[0, :]\n",
        "\n# Shift the first row of the first column down 1 row\nfirst_col = df['#1']\nfirst_col.iloc[0] = first_col.iloc[1]\nfirst_col.iloc[1:] = first_col.iloc[2:]\n# Shift the last row of the first column to the first row, first column\nfirst_col = df['#1']\nfirst_col.iloc[-1] = first_col.iloc[-2]\nfirst_col.iloc[0:1] = first_col.iloc[1:2]\n# Shift the last row of the second column up 1 row\nsecond_col = df['#2']\nsecond_col.iloc[-1] = second_col.iloc[-2]\nsecond_col.iloc[0:1] = second_col.iloc[1:2]\n# Shift the first row of the second column down 1 row\nsecond_col = df['#2']\nsecond_col.iloc[0] = second_col.iloc[1]\nsecond_col.iloc[1:] = second_col.iloc[2:]\n# Combine the shifted columns back into the dataframe\nresult = pd.concat([first_col, second_col], axis=1)\n# Output the result\nprint(result)\n",
        "\n# Shift the first row of the first column down one row\ndf = df.iloc[1:]\ndf.iloc[0] = df.iloc[-1]\n# Calculate the R^2 values for the first and second columns\nr2_1 = df['#1'].rolling(window=2).corr()\nr2_2 = df['#2'].rolling(window=2).corr()\n# Find the index of the minimum R^2 value for each column\nmin_r2_1_idx = r2_1.min().index\nmin_r2_2_idx = r2_2.min().index\n# Use these indices to select the minimum R^2 values for each column\nmin_r2_1 = r2_1.min()[min_r2_1_idx]\nmin_r2_2 = r2_2.min()[min_r2_2_idx]\n# Use the minimum R^2 values to create a new DataFrame with the desired rows\nresult = df.loc[min_r2_1_idx, :]\nresult['#2'] = df.loc[min_r2_2_idx, '#2']\n# Output the result\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n# Add 'X' to the end of each column name\ndf.columns = ['HeaderAX', 'HeaderBX', 'HeaderCX']\n# Create a new DataFrame with the modified column names\nresult = pd.DataFrame(df.values, columns=df.columns)\nprint(result)\n",
        "\ndf.columns = ['X' + col for col in df.columns]\nresult = df\nprint(result)",
        "\n# Add new columns with \"X\" in the header\nfor col in df.columns:\n    if not col.endswith('X'):\n        df = df.rename(columns={col: f'{col}X'})\n# Rename all columns\ndf = df.rename(columns={'HeaderAX': 'HeaderA', 'HeaderBX': 'HeaderB', 'HeaderCX': 'HeaderC', 'HeaderX': 'XHeaderX'})\n",
        "\ncols = [col for col in df.columns if 'val' in col]\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\"})\nresult = result.reset_index()\nresult = result.merge(df[cols], on='group', how='left')\nresult['mean'] = result['val1'].mean() + result['val2'].mean() + result['val3'].mean()\n",
        "\ncols = [col for col in df.columns if 'val' in col]\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\"})\nresult = result.reset_index()\nresult = result.merge(df[cols], on='group')\nresult['sum'] = result['val1'] + result['val2'] + result[cols]\n",
        "\ncols = [col for col in df.columns if col.endswith('2')]\ngrouped = df.groupby('group')\nresult = pd.DataFrame({'group': grouped.groups, 'val32': grouped[cols].mean()})\n",
        "\nmean_df = df.mean(axis=0)\nresult = mean_df[column_list].loc[row_list]\nprint(result)",
        "\nsum_df = df.sum(axis=1)\nresult = sum_df[row_list, column_list]\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n# Calculate the sum of the specified columns for each row in the row_list\nresult = df.sum(axis=1, level=row_list)\n# Delete the largest value in each column\nresult = result.loc[result.idxmax(axis=1)]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n# Count the number of unique values in each column\ncounts = df.value_counts()\n# Create a Series that contains the value counts for each column\nresult = pd.Series(counts, index=df.columns)\n# Filter the Series to only include columns that have missing values\nmissing_cols = result[result.isna().any()].index\n# Get the value counts for the columns that have missing values\ncounts_missing = counts[missing_cols]\n# Merge the value counts for the columns with missing values with the original DataFrame\ndf['value_counts'] = pd.Series(counts_missing, index=df.columns)\n# Print the DataFrame\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n# Count the number of null values in each column\ncounts = df.isnull().sum()\n# Create a new Series with the counts and the column names\nresult = pd.Series(counts, index=df.columns)\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n# Get the value counts for each column\nvalue_counts = df.value_counts()\n# Create a string with the value counts for each column\nresult = \"---- id ----\\n\"\nresult += \"{}\\n\".format(value_counts['id'])\nresult += \"---- temp ----\\n\"\nresult += \"{}\\n\".format(value_counts['temp'])\nresult += \"---- name ----\\n\"\nresult += \"{}\\n\".format(value_counts['name'])\nprint(result)\n",
        "\n# Merge the first two rows\ndf = df.iloc[0:1,:] + df.iloc[1:2,:]\n# Drop the 'Unnamed: 1' column\ndf = df.drop('Unnamed: 1', axis=1)\n# Print the result\nprint(df)\n",
        "\n# Merge the first two rows\ndf = df.iloc[0:1,:] + df.iloc[1:,:]\n# Drop the 'Unnamed: 1' column\ndf = df.drop('Unnamed: 1', axis=1)\n# Print the result\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\n# Fill in the missing code here\ndf.fillna(method='ffill', inplace=True)\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[3,1,2],[1,2,np.nan],[2,np.nan,np.nan]],columns=['0','1','2'])\n# Fill in the missing code here\ndf.fillna(method='ffill', inplace=True)\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\n# Fill in the missing code here\ndf.fillna(df.mean(), inplace=True)\nprint(result)\n",
        "\nresult = df.groupby('lab').sum()\nresult.loc[result['value'] < thresh] = result.loc[result['value'] < thresh].sum()\n",
        "\n# Calculate the average value for each lab\ngrouped = df.groupby('lab')\navg_values = grouped['value'].mean()\n# Replace values above the threshold with the average value\nresult = df.loc[df['value'] > thresh, 'value'].replace(avg_values)\n# Merge the result with the original dataframe\ndf = pd.concat([df, result], axis=1)\n# Drop the 'value' column since it is no longer needed\ndf.drop('value', axis=1, inplace=True)\nprint(result)\n",
        "\ndef aggregate_rows(df, section_left, section_right):\n    left_mask = df.value >= section_left\n    right_mask = df.value <= section_right\n    mask = left_mask & right_mask\n    agg_df = df.loc[mask].groupby('lab').mean()\n    agg_df.index.name = 'lab'\n    return agg_df\n# Aggregate the rows whose value is in the given section\nagg_df = aggregate_rows(df, section_left, section_right)\n# Replace the rows in the original dataframe with the aggregated values\ndf.loc[mask] = agg_df.values\nresult = df\nprint(result)",
        "\nresult[\"inv_A\"] = 1 / df[\"A\"]\nresult[\"inv_B\"] = 1 / df[\"B\"]\n",
        "\nresult[\"exp_A\"] = df[\"A\"].map(lambda x: math.e ** x)\nresult[\"exp_B\"] = df[\"B\"].map(lambda x: math.e ** x)",
        "\nresult[\"inv_A\"] = 1 / df[\"A\"]\nresult[\"inv_B\"] = 1 / df[\"B\"]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n# Define a function to apply sigmoid function to each column\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n# Apply sigmoid function to each column and add the results to the dataframe\nfor col in df.columns:\n    df[f\"sigmoid_{col}\"] = df[col].apply(sigmoid)\nprint(result)\n",
        "\nmask = (df.idxmin() <= df.idx) & (df.idx < df.idx.max())\nresult = df.loc[mask, 'abc'].idxmax()\n",
        "\nmask = (df.idxmax() == df.idxmin())\nresult = df.loc[mask, :]\n",
        "# Fill in 0 for val column\ndf['val'] = df['val'].fillna(0)\n# Expand date column to include all dates\ndf = df.set_index('dt')\ndf = df.reindex(pd.date_range(df.index.min(), df.index.max()))\ndf = df.reset_index()\n# Add expanded date column to dataframe\ndf = pd.merge(df, df.set_index('dt')['dt'], on='dt')\n# Drop original date column\ndf = df.drop('dt', axis=1)\n# Output result\nprint(df)",
        "# Fill in 0 for val column\ndf['val'] = df['val'].fillna(0)\n# Expand date column to include all dates\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndates = pd.date_range(min_date, max_date)\ndf['dt'] = dates\n# Merge expanded date column with original dataframe\nresult = pd.merge(df, pd.DataFrame({'dt': dates, 'user': ['abc', 'abc', 'efg', 'efg'], 'val': [0, 0, 0, 0]}), on='dt')\n# Drop original date column\nresult = result.drop('dt', axis=1)\n# Output result\nprint(result)",
        "# Fill in the missing code here\n# Get the minimum and maximum dates from the 'dt' column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n# Create a new column with the expanded dates using pd.date_range\nnew_dates = pd.date_range(min_date, max_date)\n# Fill in the missing values in the 'val' column with 233\ndf['val'] = df['val'].fillna(233)\n# Merge the new dates and 'val' columns into the original dataframe\ndf = df.merge(new_dates, on='dt', how='left')\n# Drop the original 'dt' column\ndf.drop('dt', axis=1, inplace=True)\n# Print the result\nprint(df)",
        "# Fill in maximum value of val for each user\ndf.groupby('user')['val'].transform(max)\n# Expand date column to include all dates between min and max\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndf['dt'] = pd.date_range(min_date, max_date)\n# Fill in missing values with maximum value of val for each user and date\ndf = df.fillna(df.groupby(['user', 'dt'])['val'].transform(max))\n# Output result\nprint(df)",
        "\ndf['min_dt'] = df['dt'].min()\ndf['max_dt'] = df['dt'].max()\ndf['dt'] = pd.date_range(df['min_dt'], df['max_dt'], freq='D')\ndf = df.groupby(['user']).agg({'val': 'max'})\ndf['dt'] = pd.to_datetime(df['dt'])\n",
        "\n# Define a dictionary to map names to unique IDs\nname_to_id = {\n    'Aaron': 1,\n    'Brave': 2,\n    'David': 3\n}\n# Use the map to replace names with IDs in the 'name' column\ndf['name'] = df['name'].map(name_to_id)\n# Print the result\nprint(result)\n",
        "\n# Replace each 'a' column value with a unique ID\ndf['a'] = df['a'].map(lambda x: 'ID_{}'.format(x))\n# Print the resulting dataframe\nprint(df)\n",
        "\n    result = df.reset_index(drop=True)\n    result['id'] = result.index + 1\n    result.rename(columns={'name': 'name_orig'}, inplace=True)\n    result = result.set_index('name_orig')\n    result = result.join(df, on='name')\n    result = result.drop('name_orig', axis=1)\n    ",
        "\n# Create a new column called \"ID\" and set it to the value of the \"name\" column\ndf['ID'] = df['name']\n# Use the \"groupby\" method to group the rows by the \"ID\" column\ngrouped = df.groupby('ID')\n# Use the \"transform\" method to replace the \"name\" and \"a\" columns with unique IDs\ndf['b'] = grouped['b'].transform('sum')\ndf['c'] = grouped['c'].transform('sum')\n# Drop the \"name\" and \"a\" columns since they are no longer needed\ndf.drop(['name', 'a'], axis=1, inplace=True)\n# Print the result\nprint(df)\n",
        "# Repartition the date columns into two columns date and value\nresult = df.pivot_table(index='user', columns='date', values='value', fill_value=0)\n# Add a new column called 'date' with the original date values\nresult['date'] = df['01/12/15'] + df['02/12/15']\n# Drop the original '01/12/15' and '02/12/15' columns\nresult = result.drop(['01/12/15', '02/12/15'], axis=1)\n# Rename the new 'date' column to 'date'\nresult.rename(columns={'date': 'date'}, inplace=True)\n# Add the original 'someBool' column back to the result DataFrame\nresult = result.join(df['someBool'])\n# Print the result DataFrame\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n# Split the 'others' column into two columns: 'others' and 'value'\nothers_col = df['01/12/15'] + df['02/12/15']\ndf['others'] = others_col.str.split(' ', expand=True)[0]\ndf['value'] = others_col.str.split(' ', expand=True)[1]\n",
        "# Repartition the date columns into two columns date and value\nresult = result.pivot_table(index='user', columns='date', values='value', fill_value=None)\n# Add a new column someBool\nresult['someBool'] = df['someBool']\nprint(result)",
        "\nlocs = df.columns.get_loc(columns)\nresult = df[df.c > 0.5][locs]\nprint(result)",
        "# Select rows where column 'c' is greater than 0.45\nmask = df.c > 0.45\n# Get indices of selected rows\nindices = df.loc[mask].index\n# Extract selected columns from dataframe\nresult = df.loc[mask, columns]\n# Convert to numpy array\ntraining_set = np.array(result)\n",
        "\nimport pandas as pd\ndef f(df, columns=['b', 'e']):\n    # Select rows where column 'c' is greater than 0.5\n    locs = df.columns.get_loc('c')\n    mask = df['c'] > 0.5\n    rows = df.loc[mask, :]\n    \n    # Select columns 'b' and 'e'\n    result = rows[columns]\n    \n    return result\n",
        "\nimport pandas as pd\ndef f(df, columns=['b', 'e']):\n    # Get the indices of the columns to keep\n    locs = [df.columns.get_loc(_) for _ in columns]\n    \n    # Select the rows where column 'c' is greater than 0.5\n    result = df[df.c > 0.5][locs]\n    \n    # Compute and append the sum of the two columns\n    result['sum'] = result.iloc[:, 0] + result.iloc[:, 1]\n    \n    return result\n",
        "\ndef f(df, columns=['b', 'e']):\n    # Get the indices of the desired columns\n    locs = [df.columns.get_loc(_) for _ in columns]\n    \n    # Use iloc to select the desired rows and columns\n    result = df.iloc[df.c > 0.5, locs]\n    \n    return result\n",
        "\ndf = df.groupby('ID').agg({'date': 'min', 'close': 'min'})\ndf = df.reset_index()\ndf = df[df['date'] != df['date'].shift(1)]\ndf = df[~df.index.duplicated(keep='first')]\nX = 120\nresult = df[(df['date'] - df['date'].min()) < pd.Timedelta(days=X)]\nprint(result)\n[Instruction]\n",
        "\ndf = df.sort_values('date')\ndf = df.groupby('ID').agg({'date': 'first'})\ndf = df.reset_index()\ndf = df[~df.index.duplicated(keep='first')]\n",
        "\nobservation_time = 'D'\nobservation_period = X\ngrouped = df.groupby('ID')\nresult = pd.DataFrame()\nfor name, group in grouped:\n    date_range = pd.date_range(group.index[0], periods=observation_period, freq=observation_time)\n    overlapping = group.loc[group.index.isin(date_range)]\n    result = result.append(overlapping)\n",
        "\n# Define a function to bin the dataframe\ndef bin_dataframe(df, bin_size=3):\n    # Create a new column to store the binned values\n    df['col2'] = np.nan\n    \n    # Iterate over each row in the dataframe\n    for index, row in df.iterrows():\n        # Get the current value of col1\n        current_value = row['col1']\n        \n        # If we have reached the end of the dataframe, break out of the loop\n        if index + 1 == df.shape[0]:\n            break\n        \n        # If the current value is different from the previous value, add a new bin\n        if current_value != df.iloc[index - 1]['col1']:\n            # Calculate the bin size and the number of bins\n            bin_size = (current_value - df.iloc[index - 1]['col1']) // bin_size\n            num_bins = (df.shape[0] - index) // bin_size\n            \n            # Fill in the binned values for the current row\n            for i in range(num_bins):\n                df.iloc[index + i * bin_size, 'col2'] = (current_value - i * bin_size * bin_size) / bin_size\n    \n    # Return the modified dataframe\n    return df\n# Apply the binning function to the dataframe\nresult = bin_dataframe(df, bin_size=3)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\n# Create a new column that counts the number of rows in each group\ngroup_size = df.groupby(pd.Grouper(freq='3R'))['col1'].ngroup()\n# Create a new column that bins the data based on the group size\nbinned_df = df.assign(bin=group_size.astype('int'))\n# Create a new column that bins the data based on the group size\nbinned_df['bin'] = binned_df['bin'].astype('str') + '2'\n# Merge the bin column back into the original dataframe\nresult = pd.merge(df, binned_df, on='col1')\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1, 4]})\n# Create a new column to store the bin counts\ndf['bin'] = pd.cut(df['col1'], bins=4, right=False)\n# Group by the 'col1' column and sum the bin counts\nresult = df.groupby('col1')['bin'].sum()\n# Print the result\nprint(result)\n",
        "\n# Define a function to bin the dataframe\ndef bin_dataframe(df, bin_size=3):\n    # Get the number of rows in the dataframe\n    n_rows = df.shape[0]\n    \n    # Create an empty dataframe to store the binned data\n    binned_df = pd.DataFrame()\n    \n    # Loop through the rows from back to front\n    for i in range(n_rows-bin_size, -1, -1):\n        # Create a group of consecutive rows\n        group = df.iloc[i-bin_size:i, :]\n        \n        # Calculate the mean of the group\n        mean = group.mean()\n        \n        # Add the mean and the group size to the binned dataframe\n        binned_df = binned_df.append({'mean': mean, 'group_size': group.shape[0]}, ignore_index=True)\n    \n    return binned_df\n# Apply the binning function to the dataframe\nbinned_df = bin_dataframe(df, bin_size=3)\n# Print the binned dataframe\nprint(binned_df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n# bin the dataframe by 3 rows and then 2 rows\ngrouped = df.groupby(pd.Grouper(freq='3D', axis=1)) \\\n        .agg({'col1': ['sum', 'mean']})\n# concatenate the results into a new dataframe\nresult = pd.concat([grouped['sum'].reset_index(name='Sum'),\n                   grouped['mean'].reset_index(name='Avg')], axis=1)\n# print the result\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n# bin the dataframe by 3 rows\ngrouped = df.groupby(pd.Grouper(freq='3R'))\n# calculate the sum and average for each group\nresult = grouped.sum().reset_index()\nresult['avg'] = grouped.mean().reset_index()\n# concatenate the two dataframes\nfinal = pd.concat([df, result], axis=1)\n# print the final dataframe\nprint(final)\n",
        "\nimport pandas as pd\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n# Fill in the missing code here\ndf.fillna(method='ffill', inplace=True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n# Fill in the missing code here\ndf.fillna(method='ffill', inplace=True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n# Fill in the missing code here\ndf.fillna(method='ffill', inplace=True)\nresult = df\nprint(result)\n",
        "\ndf['number'] = df.duration.str.extract(r'\\d+', expand=False)\ndf['time_days'] = df.duration.str.extract(r'(year|month|week|day)', expand=False)\ndf['time_days'] = df['time_days'].astype(int)\n",
        "\ndf['numer'] = df.duration.str.extract(r'\\d+', expand=False)\ndf['time'] = df.duration.str.extract(r'\\w+', expand=False)\n",
        "\n    df['number'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\n    df['time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n# Extract numbers from duration column\ndf['numer'] = df.duration.str.extract(r'\\d+')\n# Separate time units from duration column\ndf['time'] = df.duration.str.extract(r'(?P<time_unit>[a-z]+)')\n# Convert time units to numeric values\ntime_dict = {'year': 365, 'month': 30, 'week': 7, 'day': 1}\ndf['time_num'] = df['time'].map(time_dict)\n# Multiply time_num by numer to get time in days\ndf['time_day'] = df['time_num'] * df['numer']\n# Combine columns into new dataframe\nresult = df[['duration', 'numer', 'time', 'time_num', 'time_day']]\nprint(result)\n",
        "\n# Create a boolean array for each column\ncolumn_arrays = [df1[column] != df2[column] for column in columns_check_list]\n# Use np.logical_or to check if any of the columns are not equal\nresult = np.logical_or.reduce(column_arrays)\n# Convert the boolean array to a list\nresult = result.tolist()\nprint(result)\n",
        "\ncheck = np.where((df1[columns_check_list] == df2[columns_check_list]).all(axis=1))[0]\nresult = [True] * len(columns_check_list)\nresult[check] = False\n",
        "\ndf.index.set_levels(df.index.levels[1], level=0, inplace=True)",
        "\ndf.index.set_levels(['name', pd.to_datetime(df.index.get_level_values('datetime'))], inplace=True)\n[Instruction]",
        "\nimport pandas as pd\ndef f(df):\n    # Parse the date index\n    date_index = pd.to_datetime(df.index)\n    # Convert the date index to a numpy array\n    date_array = date_index.values\n    # Extract the x and y columns from the dataframe\n    x = df[\"x\"].values\n    y = df[\"y\"].values\n    # Combine the date, x, and y arrays into a single numpy array\n    output = np.array([date_array, x, y])\n    return output\n",
        "df = df.set_index(['date', 'id'])\ndf = df.swaplevel(0, 1)\nreturn df",
        "\ndf = pd.melt(df, id_vars='Country', value_name='Var1', var_name='year')\ndf = df.rename(columns={'value': 'Var1', 'variable': 'Var2'})\n",
        "\n# Reverse the order of the 'year' column\ndf = df.sort_values('year', ascending=False)\n# Create a new column for each variable, keeping the 'year' column and reordering the variables\nresult = pd.melt(df, id_vars='Country', value_name='Var1', var_name='year')\nresult = result.sort_values(['Country', 'year'])\nresult['Var2'] = df['Variable'].str.slice(6)\nresult['Var3'] = df['Variable'].str.slice(12)\n",
        "\nmask = (df.abs() < 1).all(axis=1)\nresult = df[mask]\n",
        "\ncols = df.columns[df.columns.str.startswith('Value')]\nmask = (df[cols] >= 1) | (df[cols] <= -1)\nresult = df[mask]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_A': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n# Filter rows where absolute value of any column is more than 1\ncondition = (df['Value'] > 1) | (df['Value'] < -1)\ndf = df[condition]\n# Remove 'Value_' from each column\ndf.columns = [col.replace('Value_', '') for col in df.columns]\n# Print result\nresult = df\nprint(result)\n",
        "\nresult['Title'] = result['Title'].str.replace('&AMP;', '&')",
        "\nfor col in df.columns:\n    df[col] = df[col].str.replace('&LT;', '<')\nresult = df\nprint(result)",
        "\nresult = df.A.str.replace('&AMP;', '&')\nreturn result",
        "\nfor col in df.columns:\n    df[col] = df[col].str.replace('&AMP;', '&', regex=False)\n    df[col] = df[col].str.replace('&LT;', '<', regex=False)\n    df[col] = df[col].str.replace('&GT;', '>', regex=False)\nresult = df\nprint(result)",
        "\nresult = df.replace('&AMP;', '&', regex=True)\nprint(result)",
        "\nimport pandas as pd\ndef split_names(df):\n    df['first_name'] = df['name'].str.split(' ', expand=True)[0]\n    df['last_name'] = df['name'].str.split(' ', expand=True)[1]\n    return df\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Juan de la Cruz']})\nresult = split_names(df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Juan de la Cruz']})\ndef split_name(name):\n    if ' ' in name:\n        first, last = name.split(' ')\n        return first, last\n    else:\n        return name\ndf['1_name'] = df['name'].apply(split_name)\ndf['2_name'] = df['name'].fillna(df['1_name'])\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport re\ndef split_name(name):\n    name_parts = re.split(r'\\s{2,}', name)\n    if len(name_parts) == 1:\n        return name, None, None\n    elif len(name_parts) == 2:\n        return name_parts[0], name_parts[1], None\n    elif len(name_parts) == 3:\n        return name_parts[0], name_parts[1], name_parts[2]\n    else:\n        return None, None, None\ndef split_names(df):\n    name_df = df.copy()\n    name_df['first_name'] = name_df['name'].apply(lambda x: split_name(x)[0])\n    name_df['middle_name'] = name_df['name'].apply(lambda x: split_name(x)[1])\n    name_df['last_name'] = name_df['name'].apply(lambda x: split_name(x)[2])\n    return name_df\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\nresult = split_names(df)\nprint(result)\n",
        "\ndf1 = df1.set_index('Timestamp')\ndf2 = df2.set_index('Timestamp')\nresult = df2.join(df1, on='Timestamp')\n",
        "\ndf1 = df1.merge(df2, on='Timestamp')\n",
        "\ndef state_func(col1, col2, col3):\n    if col2 <= 50 and col3 <= 50:\n        return col1\n    else:\n        return max(col1, col2, col3)\ndf['state'] = df.apply(state_func, axis=1)\n",
        "\ndef state_calculation(col1, col2, col3):\n    if col2 + col3 > 50:\n        return col1\n    else:\n        return col1 + col2 + col3\ndf['state'] = df.apply(state_calculation, axis=1)",
        "\nfor index, row in df.iterrows():\n    try:\n        row[\"Field1\"] = int(row[\"Field1\"])\n    except ValueError:\n        errors.append(row[\"Field1\"])\n",
        "\nfor index, row in df.iterrows():\n    try:\n        if not pd.isnumeric(row[\"Field1\"]):\n            result = result.append({\"ID\": index, \"Field1\": int(row[\"Field1\"])}, ignore_index=True)\n    except:\n        pass\nprint(result)",
        "\nfor index, row in example_df.iterrows():\n    try:\n        row[\"Field1\"] = int(row[\"Field1\"])\n    except ValueError:\n        errors.append(row[\"Field1\"])\nreturn errors",
        "\ndf['val1'] /= df['val1'].sum()\ndf['val2'] /= df['val2'].sum()\ndf['val3'] /= df['val3'].sum()\ndf['val4'] /= df['val4'].sum()\n",
        "# Compute the percentage of each category for each column\nfor col in df.columns:\n    df[col] = df[col].sum() / df.sum()\n# Add the computed percentage to the original DataFrame\ndf['percentage'] = df.apply(lambda x: 100 * x[col] / x['sum'], axis=1)\n# Print the result\nprint(df)",
        "\nresult = df.loc[test]\nprint(result)\n",
        "\nresult = df.loc[test]\nprint(result)",
        "\nfor row in test:\n    df = df.drop(row)\n",
        "\nimport pandas as pd\ndef f(df, test):\n    # Extract a list of row names from the test list\n    row_names = [row for row in test if row in df.index]\n    \n    # Create a boolean mask for the rows in the dataframe that match the row names in the test list\n    mask = df.index.isin(row_names)\n    \n    # Use the boolean mask to select the rows from the dataframe\n    result = df[mask]\n    \n    return result\n",
        "\n# Calculate pairwise distances between cars\ndf[\"distance\"] = df.apply(lambda row: pd.Series([\n    pd.Series(row[\"x\"]).diff().abs().sum(),\n    pd.Series(row[\"y\"]).diff().abs().sum()\n]).sum(), axis=1)\n# Find nearest neighbour for each car\ndf[\"nearest_neighbour\"] = df.groupby(\"car\")[\"distance\"].idxmin()\n# Calculate Euclidean distance between each car and its nearest neighbour\ndf[\"euclidean_distance\"] = df.apply(lambda row: pd.Series([\n    pd.Series(row[\"x\"]).diff(row[\"nearest_neighbour\"][\"x\"])**2 + \n    pd.Series(row[\"y\"]).diff(row[\"nearest_neighbour\"][\"y\"])**2\n]).sum()**0.5, axis=1)\n# Combine results into single dataframe\nresult = df[[\"time\", \"x\", \"y\", \"car\", \"nearest_neighbour\", \"euclidean_distance\"]]\nprint(result)\n",
        "# Calculate the distance between each car and its farthest neighbour\ndf['farthest_neighbour'] = df.groupby('car')['x'].transform(lambda x: x.max())\ndf['distance'] = df.apply(lambda row: pd.Series([row['x'] - row['farthest_neighbour']]).abs().sum(), axis=1)\n# Calculate the average distance for each time point\nresult = df.groupby('time')['distance'].mean()\n# Merge the two dataframes\nresult = result.merge(df, on='time')\n# Drop the unnecessary columns\nresult = result.drop(['farthest_neighbour', 'x'], axis=1)\n# Print the result\nprint(result)",
        "\nkeywords = [\"keywords_0\", \"keywords_1\", \"keywords_2\", \"keywords_3\"]\nresult = df.apply(lambda row: \",\".join(row[keywords]), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\nresult = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\nresult[\"keywords_all\"] = result[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\nresult = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\nrandom_state = 0\nsample_size = int(len(df) * 0.2)\nsample_indexes = df.sample(n=sample_size, random_state=random_state).index\nresult = df.loc[sample_indexes, :]\nresult['Quantity'] = result['Quantity'].fillna(0)\nprint(result)",
        "\nrandom_state = 0\nsample_size = int(len(df) * 0.2)\nsample_indexes = df.sample(n=sample_size, random_state=random_state).index\nresult = df.loc[sample_indexes, :]\nresult['ProductId'] = result['ProductId'].fillna(0)\nprint(result)",
        "\nrandom_state = 0\nresult = df.sample(n=int(0.2 * len(df)), random_state=random_state)\nresult['Quantity'] = result['Quantity'].where(result['Quantity'] != 0, 0)\nindexes = df.index.intersection(result.index)\nresult = result[indexes]\nprint(result)",
        "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)",
        "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)",
        "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nreturn duplicate",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)",
        "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)",
        "\n# Group the DataFrame by 'Sp', 'Mt' columns\ngrouped = df.groupby(['Sp', 'Mt'])\n# Use 'max' function to get the row with the maximum 'count' value in each group\nresult = df[grouped.agg({'count': 'max'}).index]\n",
        "result = df.groupby(['Sp', 'Mt']).max()['count']",
        "\n# Group by 'Sp', 'Mt' columns and get the min value of 'count' column for each group\ngrouped = df.groupby(['Sp', 'Mt']).count()['count'].min()\n# Filter rows where 'count' equals to the min value of each group\nresult = df[df['count'] == grouped]\nprint(result)\n",
        "result = df.groupby(['Sp','Value']).max()['count']\nresult = result.reset_index()\nresult = result[result['count'] == df.groupby(['Sp','Value']).max()['count']]\nprint(result)",
        "\nfilter_list=['Foo','Bar']\ndf.query(\"Catergory==filter_list\")\n",
        "\nfilter_list=['Foo','Bar']\ndf.query(\"Category!=filter_list\")\nprint(result)",
        "\nvalue_vars = [tuple(x) for x in zip(*df.columns)]\npd.melt(df, value_vars=value_vars)",
        "\nvalue_vars = [(list(col1), list(col2), list(col3)) for col1, col2, col3 in zip(df.columns[0], df.columns[1], df.columns[2])]\nresult = pd.melt(df, id_vars=['variable_0', 'variable_1', 'variable_2'], value_vars=value_vars)\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n",
        "cumsum = df.groupby('id')['val'].cumsum()\nresult = pd.concat([df, cumsum], axis=1)\nprint(result)",
        "\ndf['cumsum'] = df.groupby('id').cummin()\nresult = result.reset_index()\nresult = result.rename(columns={'index': 'id'})\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\nresult = result.reset_index(drop=True)\nprint(result)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\nresult = result.join(df['cumsum'], on='id')\nprint(result)\n",
        "\ndf.groupby('l')['v'].sum(skipna=False)\n",
        "\ndf.groupby('r')['v'].sum(skipna=False)\n",
        "\ndf.groupby('l')['v'].sum().fillna(np.nan)\n",
        "\nresult = []\nfor col1, col2 in zip(df['Column1'], df['Column2']):\n    if col1.isin(col2).all():\n        result.append('Column1 {} Column2 one-to-one'.format(col1))\n    elif col1.isin(col2).any():\n        result.append('Column1 {} Column2 many-to-one'.format(col1))\n    elif col2.isin(col1).all():\n        result.append('Column2 {} Column1 many-to-one'.format(col2))\n    elif col2.isin(col1).any():\n        result.append('Column2 {} Column1 many-to-many'.format(col2))\n    else:\n        result.append('Column1 {} Column2 one-to-many'.format(col1))\nfor col1, col3 in zip(df['Column1'], df['Column3']):\n    if col1.isin(col3).all():\n        result.append('Column1 {} Column3 one-to-one'.format(col1))\n    elif col1.isin(col3).any():\n        result.append('Column1 {} Column3 many-to-one'.format(col1))\n    elif col3.isin(col1).all():\n        result.append('Column3 {} Column1 many-to-one'.format(col3))\n    elif col3.isin(col1).any():\n        result.append('Column3 {} Column1 many-to-many'.format(col3))\n    else:\n        result.append('Column1 {} Column3 one-to-many'.format(col1))\nfor col1, col4 in zip(df['Column1'], df['Column4']):\n    if col1.isin(col4).all():\n        result.append('Column1 {} Column4 one-to-one'.format(col1))\n    elif col1.isin(col4).any():\n        result.append('Column1 {} Column4 many-to-one'.format(col1))\n    elif col4.isin(col1).all():\n        result.append('Column4 {} Column1 many-to-one'.format(col4))\n    elif col4.isin(col1).any():\n        result.append('Column4 {} Column1 many-to-many'.format(col4))\n    else:\n        result.append('Column1 {} Column4 one-to-many'.format(col1))\nfor col1, col5 in zip(df['Column1'], df['Column5']):\n    if col1.isin(col5).all():\n        result.append('Column1 {} Column5 one-to-one'.format(col1))\n    elif col1.isin(col5).any():\n        result.append('Column1 {} Column5 many-to-one'.format(col1))\n    elif col5.isin(col1).all():\n        result.append('Column5 {} Column1 many-to-one'.format(col5))\n    elif col5.isin(col1).any():\n        result.append('Column5 {} Column1 many-to-many'.format(col5))\n    else:\n        result.append('Column1 {} Column5 one-to-many'.format(col1))\n",
        "\nrelation_types = ['one-to-one', 'one-to-many', 'many-to-one', 'many-to-many']\nresult = []\nfor i in range(len(df.columns)):\n    for j in range(i+1, len(df.columns)):\n        if df[df.columns[i]].isin(df[df.columns[j]]).sum() == len(df[df.columns[i]]):\n            result.append([df.columns[i], df.columns[j]])\nrelation_dict = {}\nfor r in result:\n    if r[0] not in relation_dict:\n        relation_dict[r[0]] = [relation_types[0]]\n    elif r[1] not in relation_dict[r[0]]:\n        relation_dict[r[0]].append(relation_types[1])\n    else:\n        relation_dict[r[0]].append(relation_types[2])\n        relation_dict[r[1]].append(relation_types[3])\n",
        "\nrelation_matrix = pd.DataFrame({\n    'Column1': df.Column1,\n    'Column2': df.Column2,\n    'Column3': df.Column3,\n    'Column4': df.Column4,\n    'Column5': df.Column5\n})\ndef get_relation_type(row):\n    if row.isnull().sum() == 0:\n        return 'one-to-one'\n    elif row.isnull().sum() == 1:\n        return 'one-to-many'\n    elif row.isnull().sum() == 2:\n        return 'many-to-one'\n    elif row.isnull().sum() == 3:\n        return 'many-to-many'\n    else:\n        return 'none'\nrelation_type = relation_matrix.apply(get_relation_type, axis=1)\nresult = pd.concat([relation_matrix, relation_type], axis=1)\n",
        "\nrelation_matrix = pd.DataFrame({\n    'Column1': df.Column1,\n    'Column2': df.Column2,\n    'Column3': df.Column3,\n    'Column4': df.Column4,\n    'Column5': df.Column5\n})\ndef get_relation_type(row):\n    if row.isnull().sum() == 0:\n        return 'one-2-one'\n    elif row.isnull().sum() == 1:\n        return 'one-2-many'\n    elif row.isnull().sum() == 2:\n        return 'many-2-one'\n    elif row.isnull().sum() == 3:\n        return 'many-2-many'\n    else:\n        return 'none'\nrelation_matrix['relation_type'] = relation_matrix.apply(get_relation_type, axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar'],\n                   'bank': [np.nan, 'abc', 'xyz']})\n# Sort the dataframe by the 'bank' column in ascending order\ndf_sorted = df.sort_values(by='bank')\n# Create a boolean mask to keep only the rows with non-nan values in the 'bank' column\nmask = df_sorted['bank'].notna()\n# Use the mask to filter the original dataframe and keep only the rows that match the mask\nresult = df.loc[mask]\nprint(result)\n",
        "\nresult = s.astype(float)\n",
        "\ngrouped = df.groupby(['SibSp', 'Parch'])\nmeans = grouped.mean()\nresult = pd.concat([means[means['SibSp'] > 0 | means['Parch'] > 0].reset_index(name='Has Family'),\n                    means[means['SibSp'] == 0 & means['Parch'] == 0].reset_index(name='No Family')], axis=1)\nprint(result)",
        "",
        "\ngrouped = df.groupby(['SibSp', 'Parch'])\nresult = pd.DataFrame({'Has Family': [], 'New Family': [], 'No Family': [], 'Old Family': []}, index=df.index)\nfor name, group in grouped:\n    if (group['SibSp'] == 1) & (group['Parch'] == 1):\n        result['Has Family'].append(group['Survived'].mean())\n    elif (group['SibSp'] == 0) & (group['Parch'] == 0):\n        result['No Family'].append(group['Survived'].mean())\n    elif (group['SibSp'] == 0) & (group['Parch'] == 1):\n        result['New Family'].append(group['Survived'].mean())\n    elif (group['SibSp'] == 1) & (group['Parch'] == 0):\n        result['Old Family'].append(group['Survived'].mean())\nprint(result)",
        "# Sort the dataframe by 'A' column\nresult = df.groupby('cokey').sort_values('A')\n# Print the result\nprint(result)",
        "# Sort the dataframe by 'cokey' and 'A' columns\nresult = df.groupby('cokey')['A','B'].sort_values(['cokey','A'])\n# Print the result\nprint(result)",
        "\n# Convert column headers to MultiIndex\ndf.columns = pd.MultiIndex.from_tuples(l)\n# Change column tuples to MultiIndex\ndf = df.set_index(['Caps', 'Lower'], drop=False)\nresult = df\nprint(result)",
        "\n# Convert the column headers to a MultiIndex\ncols = df.columns\nnew_cols = pd.MultiIndex.from_tuples(cols)\ndf.columns = new_cols\n# Change the column values to match the desired format\ndf = df.reset_index(drop=True)\ndf = df.set_index(['Caps', 'Middle', 'Lower'])\ndf = df.unstack()\nresult = df\n# Output the result\nprint(result)",
        "\n# Convert the column headers to a MultiIndex\ncols = df.columns\nnew_cols = pd.MultiIndex.from_tuples(cols)\ndf.columns = new_cols\n# Reorder the levels\ndf = df.reindex(columns=new_cols, level=0)\n# Remove the original column headers\ndf.columns = df.columns.droplevel(0)\n# Print the result\nprint(df)",
        "\nimport numpy as np\nimport pandas as pd\nnp.random.seed(123)\nbirds = np.random.choice(['African Swallow', 'Dead Parrot', 'Exploding Penguin'], size=int(5e4))\nsomeTuple = np.unique(birds, return_counts=True)\ndf = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult = pd.concat([result, df], ignore_index=True)\nprint(result)\n",
        "mean_grouped = df.groupby('a')['b'].mean()\nstd_grouped = df.groupby('a')['b'].std()\nresult = pd.Series({'mean': mean_grouped, 'std': std_grouped})\nprint(result)",
        "mean_grouped = df.groupby('b')['a'].mean()\nstd_grouped = df.groupby('b')['a'].std()\nresult = pd.Series({'mean': mean_grouped, 'std': std_grouped})\nprint(result)",
        "\ngrouped = df.groupby('a')\nfor name, group in grouped:\n    b = group['b']\n    softmax = pd.Series(b).softmax()\n    min_max = pd.Series(b).min().max()\n    result = result.append({'a': name, 'b': b, 'softmax': softmax, 'min-max': min_max}, ignore_index=True)\nprint(result)",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,1,0,1],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n# Remove rows and columns that sum to zero\nzero_sum = df.sum(axis=1) == 0\nzero_sum = zero_sum.any(axis=1)\ndf = df[~zero_sum]\n# Print result\nprint(df)\n",
        "\nresult = df.sum(axis=1) == 0\ndf = df[~result]\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n# Remove rows and columns with maximum value of 2\nmax_value = df.max()\nmask = df == max_value\ndf = df[~mask]\n# Print result\nprint(df)\n",
        "# Replace the maximum values of 2 with 0\ndf.where(df == 2, other=0, axis=1)\n[Instruction]",
        "\nsorted_index = s.sort_values(ascending=False)\nresult = pd.Series(sorted_index.values, index=sorted_index.index)\nprint(result)",
        "\nsorted_index = s.sort_values(ascending=False)\nresult = pd.DataFrame({'index': sorted_index.index, '1': sorted_index.values})\nprint(result)",
        "# Convert the 'A' column to integers\ndf['A'] = df['A'].astype(int)\n# Keep only the rows where 'A' is an integer or numeric\nresult = df[df['A'].isin([int, float])]\n# Print the result\nprint(result)",
        "\ndf = df[df['A'].astype(str) == df['A']]\nprint(result)",
        "\n# Group the DataFrame by 'Sp', 'Mt' columns\ngrouped = df.groupby(['Sp', 'Mt'])\n# Use 'max' function to get the maximum 'count' value for each group\nresult = df[df.groupby(['Sp', 'Mt'])['count'].transform('max')]\n# Print the result\nprint(result)\n",
        "result = df.groupby(['Sp', 'Mt']).max()['count']\n[Instruction]",
        "\n# Group by 'Sp', 'Mt' columns and get the min value of 'count' column for each group\ngrouped = df.groupby(['Sp', 'Mt']).count()['count'].min()\n# Filter rows where 'count' equals to the min value of 'count' for each group\nresult = df[df['count'] == grouped]\n# Print the result\nprint(result)\n",
        "result = df.groupby(['Sp','Value']).max()['count']\nresult = result.reset_index()\nresult = result[result['count'] == df.groupby(['Sp','Value']).max()['count']]\nprint(result)",
        "\nresult = df.merge(dict, left_on='Member', right_on='key', how='left')\n",
        "\n# Fill missing values in 'Member' column with 'None'\ndf['Member'] = df['Member'].fillna('None')\n# Use 'map' function to map values in 'dict' to another column 'Date'\nresult = df.assign(Date=df['Member'].map(dict))\nprint(result)\n",
        "\nresult = pd.DataFrame({'Member':example_df['Member'], 'Group':example_df['Group'], 'Date':example_df['Date'].map(dict)})\nreturn result\n",
        "\n# Fill missing values in 'Member' column with 'None'\ndf['Member'] = df['Member'].fillna('None')\n# Create a new column 'Date' by mapping values in 'Member' column to values in 'dict'\ndf['Date'] = df['Member'].map(dict).fillna(pd.to_datetime('17-Aug-1926'))\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n# Begin of Missing Code\ndf1 = df.groupby(['year', 'month']).agg({'count'})\nresult = pd.merge(df, df1, on=['year', 'month'])\n# End of Missing Code\nresult = result.reset_index()\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n# Begin of Missing Code\ndf1 = df.groupby(['Date', 'Val']).agg({'count': ['sum']})\nresult = pd.merge(df1, df, on=['Date', 'Val'])\n# End of Missing Code\nresult = result.pivot(index='Date', columns='Val', values='count')\nresult = result.reset_index()\nresult = result.pivot(index='year', columns='month', values='Count_Val')\nresult = result.reset_index()\nresult = result.pivot(index='year', columns='month', values='Count_m')\nresult = result.reset_index()\nresult = result.pivot(index='year', columns='month', values='Count_y')\nresult = result.reset_index()\nresult = result.pivot(index='year', columns='month', values='Count_d')\nresult = result.reset_index()\nresult = result.pivot(index='year', columns='month', values='Val')\nresult = result.reset_index()\nprint(result)\n",
        "\ndf1 = df.groupby(['Date', df['Date'].dt.year, df['Date'].dt.month]).agg({'count': ['sum']})\nresult = pd.concat([df, df1], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n# result1: zero\n# result2: non-zero\nzero = df.groupby(['Date']).sum()\nzero = zero.reset_index()\nzero.columns = ['Date', 'Zero_B', 'Zero_C']\nnon_zero = df.groupby(['Date']).sum()\nnon_zero = non_zero.reset_index()\nnon_zero.columns = ['Date', 'Non_Zero_B', 'Non_Zero_C']\n# print results\nprint(zero)\nprint(non_zero)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n# result1: even\nresult1 = df.groupby(['Date']).sum()\nresult1['B'] = result1['B'].astype(int)\nresult1['B'] = result1['B'].apply(lambda x: x % 2 == 0)\nresult1['C'] = result1['C'].astype(int)\nresult1['C'] = result1['C'].apply(lambda x: x % 2 == 0)\n# result2: odd\nresult2 = df.groupby(['Date']).sum()\nresult2['B'] = result2['B'].astype(int)\nresult2['B'] = result2['B'].apply(lambda x: x % 2 != 0)\nresult2['C'] = result2['C'].astype(int)\nresult2['C'] = result2['C'].apply(lambda x: x % 2 != 0)\nprint(result1)\nprint(result2)\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=['sum', 'mean'])\nprint(result)",
        "\naggfunc = {'D': np.sum, 'E': np.mean}\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=aggfunc)\nprint(result)",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=['max','min'])\n",
        "\ndf = df.map_partitions(lambda x: [y.split(',') for y in x])\nresult = dd.from_pandas(df, npartitions=2)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\n# Split the values in the 'var2' column into a list\nsplit_values = df['var2'].str.split(',', expand=True)\n# Create a new column with the split values\ndf['var2_split'] = split_values\n# Merge the split values back into the 'var2' column\ndf['var2'] = df['var2_split'].apply(lambda x: ','.join(x))\n",
        "\ndf_split = dd.from_pandas(df, npartitions=1)\ndf_split = df_split.explode('var2')\nresult = df_split.to_pandas()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n# Count the number of special characters in each string\ndef count_special_chars(string):\n    special_char_count = 0\n    for char in string:\n        if char.isalpha():\n            continue\n        else:\n            special_char_count += 1\n    return special_char_count\n# Apply the function to each string in the 'str' column and store the result in a new column called 'new'\ndf['new'] = df['str'].apply(count_special_chars)\n# Print the dataframe\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n# Count the number of non-alphabetic characters in each string\ndef count_non_alphabetic(string):\n    count = 0\n    for char in string:\n        if not char.isalpha():\n            count += 1\n    return count\n# Apply the function to each string in the 'str' column\ndf['new'] = df['str'].apply(count_non_alphabetic)\n# Print the result\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALABAMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n# Split the 'row' column into two columns, 'fips' and 'row'\nfips_col = df['row'].str[:3]\nrow_col = df['row'].str[4:]\n# Create a new dataframe with the two columns\nresult = pd.DataFrame({'fips': fips_col, 'row': row_col})\n# Concatenate the two dataframes\ndf = pd.concat([df, result], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'row': ['114 AAAAAA', '514 ENENEN',\n                           '1926 HAHAHA', '0817 O-O,O-O',\n                           '998244353 TTTTTT']})\n# Split the 'row' column into two columns, 'fips' and 'row'\ndf['fips'] = df['row'].str[:3]\ndf['row'] = df['row'].str[4:]\n# Display the result\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALAB AMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n# Split the 'row' column into three columns\nfips, medi, row = df['row'].str.split(' ', expand=True)\n# Assign the new columns to the DataFrame\ndf['fips'] = fips\ndf['medi'] = medi\ndf['row'] = row\n# Display the result\nresult = df\nprint(result)\n",
        "\ndf['2001'] = df['2001'].fillna(0)\ndf['2002'] = df['2002'].fillna(0)\ndf['2003'] = df['2003'].fillna(0)\ndf['2004'] = df['2004'].fillna(0)\ndf['2005'] = df['2005'].fillna(0)\ndf['2006'] = df['2006'].fillna(0)\ndf = df.astype(float)\ndf = df.groupby('Name').sum()\ndf = df.reset_index()\ndf['Cumulative Average'] = df['2001'] + (df['2002'] + (df['2003'] + (df['2004'] + (df['2005'] + df['2006']))) / 5)\n",
        "\n    df[\"Cumulative Average\"] = df[\"Cumulative Average\"].apply(lambda x: x.where(x != 0, x))\n    return df\nresult = calculate_cumulative_average(df)\nprint(result)",
        "\nresult = df.groupby('Name')['2001', '2002', '2003', '2004', '2005', '2006'].sum()\nresult = result.where(pd.notnull(result), 0)\nreturn result",
        "\ndf['2001'] = df['2001'].fillna(0)\ndf['2002'] = df['2002'].fillna(0)\ndf['2003'] = df['2003'].fillna(0)\ndf['2004'] = df['2004'].fillna(0)\ndf['2005'] = df['2005'].fillna(0)\ndf['2006'] = df['2006'].fillna(0)\ndf = df.astype(float)\ndf = df.groupby('Name').sum()\ndf = df.reset_index()\ndf = df.pivot(index='Name', columns='2001', values='2002')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\n",
        "\ndf['Label'] = 0\ndf.loc[0, 'Label'] = 1\ndf['Label'] = (df['Close'] - df['Close'].shift(1)) > 1\n",
        "\nlabel = 1\ndf['label'] = df.groupby('DateTime')['Close'].diff().apply(lambda x: [1, 0, -1][x > 0]).reset_index(name='label')\nresult = pd.concat([df, df['label']], axis=1)\nprint(result)",
        "\ndf['label'] = 1\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndiff = df['Close'].diff()\ndiff.fillna(0, inplace=True)\ndiff = diff.astype(int)\ndiff = diff.where(diff > 0, 1)\ndiff = diff.where(diff == 0, 0)\ndiff = diff.where(diff < 0, -1)\nresult = pd.concat([df, diff], axis=1)\nprint(result)",
        "\ndf['Duration'] = df.departure_time.iloc[1:] - df.arrival_time.iloc[:-1]\n",
        "\n# Calculate the difference in seconds between each row's departure time and the previous row's arrival time\ndf['Duration'] = df.groupby('id')['departure_time'].diff().dt.total_seconds()\n# Print the dataframe\nresult = df\nprint(result)\n",
        "\n# Convert arrival_time and departure_time to datetime64[ns] format\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], format='%d-%b-%Y %H:%M:%S')\n# Calculate the time difference in seconds between each row's departure time and the previous row's arrival time\ndf['Duration'] = df.groupby('id')['departure_time'].diff().dt.seconds\n# Convert the departure_time and Duration columns to the desired format\ndf['departure_time'] = df['departure_time'].dt.strftime('%Y-%m-%d %H:%M:%S')\ndf['Duration'] = df['Duration'].dt.strftime('%H:%M:%S')\n",
        "\n# Count the number of rows with key2 equal to 'one' for each unique value of key1\ncounts = df.groupby(['key1']).key2.eq('one').sum()\n# Merge the counts with the original dataframe\nresult = df.merge(counts, on='key1', how='left')\n# Print the result\nprint(result)\n",
        "\n# Count the number of rows with key2 equal to 'two' for each unique value of key1\ncounts = df.groupby(['key1']).key2.eq('two').sum()\n# Merge the counts with the original dataframe\nresult = df.merge(counts, left_on='key1', right_on='key1')\n",
        "\n# Count the number of rows where key2 ends with \"e\"\nmask = df['key2'].str.endswith(\"e\")\n# Group by key1 and count the rows with the mask\nresult = df.groupby('key1')[mask].size()\n# Print the result\nprint(result)\n",
        "max_date = df.index.max()\nmin_date = df.index.min()\nprint(max_date,min_date)",
        "\nmode_result = '2014-03-27'\nmedian_result = '2014-03-21'\n",
        "\ndf = df[(99 <= df['closing_price'] <= 101)]",
        "\ndf = df[df['closing_price'].notna()]\ndf = df[(df['closing_price'] < 99) & (df['closing_price'] > 101)]\n",
        "\ndf1 = df.groupby([\"item\", \"otherstuff\"], as_index=False)[\"diff\"].min()\nresult = pd.concat([df, df1], axis=1)\nprint(result)\n",
        "\n# Split the `SOURCE_NAME` column into a list of strings\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_')\n# Extract the last element of each string in the list\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str[-1]\n# Convert the `SOURCE_NAME` column back to a string\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].astype(str)\n# Remove any strings that do not contain an `_`\ndf = df[df['SOURCE_NAME'].str.contains('_')]\n# Extract the first element of each string in the list\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str[0]\n# Convert the `SOURCE_NAME` column back to a string\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].astype(str)\n# Drop the original `SOURCE_NAME` column\ndf = df.drop('SOURCE_NAME', axis=1)\n# Print the result\nprint(df)",
        "\n# Split the `SOURCE_NAME` column into a list of strings\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_')\n# Extract the last element of each string in the list\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str[-1]\n# Convert the `SOURCE_NAME` column back to a string\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].astype(str)\n# Remove any strings that do not contain an `_`\ndf = df[df['SOURCE_NAME'].str.contains('_')]\n# Extract the numeric part of each string in the `SOURCE_NAME` column\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.extract('(\\d+)')\n# Convert the extracted numbers to integers\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].astype(int)\n# Drop the original `SOURCE_NAME` column\ndf.drop('SOURCE_NAME', axis=1, inplace=True)\n# Print the result\nprint(df)",
        "\ndef f(df=example_df):\n    # split the column SOURCE_NAME into a list of strings using '_' as the delimiter\n    col = df['SOURCE_NAME']\n    split_col = col.str.split('_')\n    \n    # create a new column with the desired output\n    result = pd.Series(index=df.index, dtype=object)\n    result['SOURCE_NAME'] = split_col.str[0]\n    \n    # replace the original column with the new column\n    df = df.replace({'SOURCE_NAME': result['SOURCE_NAME']})\n    \n    return df",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n# Fill the first 50% of NaN values with '0' and the last 50% with '1'\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df.loc[df['Column_x'].isnull(), 'Column_x'].fillna(df['Column_x'].mode()[0], inplace= True)[0:int(len(df) / 2)]\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df.loc[df['Column_x'].isnull(), 'Column_x'].fillna(df['Column_x'].mode()[1], inplace= True)[int(len(df) / 2):]\n# Print the result\nresult = df\nprint(result)\n",
        "\n# Fill the first 30% of NaN values with '0', the middle 30% with '0.5', and the last with '1'\nmask = df['Column_x'].isna()\nnum_nan = mask.sum()\npercent_30 = int(0.3 * num_nan)\npercent_70 = int(0.7 * num_nan)\nmask_first = mask[:percent_30]\ndf.loc[mask_first, 'Column_x'] = 0\nmask_middle = mask[percent_30:percent_70]\ndf.loc[mask_middle, 'Column_x'] = 0.5\nmask_last = mask[percent_70:num_nan]\ndf.loc[mask_last, 'Column_x'] = 1\n",
        "\n# Fill NaN values with \"0\" or \"1\" so that the number of \"0\" is 50%(round down) and the number of \"1\" is 50%(round down)\n# Fill zeros first and then ones\ndef fill_nan(column_x):\n    n_zeros = int(len(column_x) / 2)\n    column_x[0:n_zeros] = 0\n    column_x[n_zeros:] = 1\ndf['Column_x'] = df['Column_x'].fillna(fill_nan, inplace= True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\n# Fill in the missing code here\na_b = pd.DataFrame([[(1, 5), (2, 6)],[(3, 7), (4, 8)]], columns=['one', 'two'])\nresult = pd.concat([a, b, a_b], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nc = pd.DataFrame(np.array([[9, 10],[11, 12]]), columns=['one', 'two'])\n# Fill in the missing code here\nresult = pd.concat([a, b, c], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8],[9, 10]]), columns=['one', 'two'])\n# Fill in the missing code here\nresult = pd.concat([a, b], axis=1)\nresult = result.fillna(value=np.nan)\nprint(result)\n",
        "\n# Group the DataFrame by bins and count the number of views for each user\ngrouped = df.groupby(pd.cut(df.views, bins))\nresult = pd.DataFrame({'username': df.username,\n                        'views': [bins[i] for i in range(len(bins) - 1)],\n                        'count': [grouped.get_group(i).size() for i in range(len(bins) - 1)]})\n# Merge the result with the original DataFrame\nresult = pd.merge(df, result, on='username')\n# Print the result\nprint(result)\n",
        "\ngroups = df.groupby(pd.cut(df.views, bins))\nresult = pd.crosstab(groups.index, groups.username, margins=True)\nprint(result)\n",
        "\n# Group the DataFrame by bins and count the number of views for each username\ngrouped = df.groupby(pd.cut(df.views, bins))\n# Create a new DataFrame with the counts and a column for the username\nresult = pd.DataFrame({'views': [], 'username': []})\nfor username, group in grouped:\n    # Get the counts for each bin\n    counts = group.views.value_counts()\n    # Add the counts and username to the result DataFrame\n    result = result.append({'views': counts, 'username': username}, ignore_index=True)\n# Print the result DataFrame\nprint(result)\n",
        "\nresult = \"\"\nfor i in range(len(df)):\n    result += df.iloc[i]['text'] + \", \"\nprint(result)",
        "\nresult = \"\"\nfor i in range(len(df)):\n    result += df.iloc[i]['text'] + \"-\"\nprint(result)",
        "\nresult = \"\"\nfor i in range(len(df)):\n    result += df.iloc[i]['text'] + \", \"\nresult = result[:-2]",
        "# Merge the rows into a single string using the ', ' separator\ntext_merged = df['text'].str.cat(sep=', ')\n# Convert the merged string into a Series\nresult['text'] = pd.Series(text_merged)\n# Print the result\nprint(result)",
        "\nresult = 'jkl-ghi-def-abc'",
        "\ndf1 = pd.concat([df1, df2], axis=0)\nresult = pd.merge(df1, df2, on=['id', 'date'])\n",
        "\ndf1 = pd.concat([df1, df2], axis=0)\n# Fill in the date column with the expected format\ndf1['date'] = pd.to_datetime(df1['date'])\ndf1['date'] = df1['date'].dt.strftime('%d-%b-%Y')\n# Group by id and fill in the city and district columns with the values from df1\ndf1 = df1.groupby('id').fillna(method='ffill')\n# Sort by date in ascending order\ndf1 = df1.sort_values('date')\n",
        "\ndf1 = pd.concat([df1, df2], axis=0)\n# Fill in the missing code here\ndf1.sort_values(['id', 'date'], ascending=[True, True], inplace=True)\n# Merge the two DataFrames based on the 'id' column\nmerged = pd.merge(df1, df2, on='id', how='outer')\n# Fill in the missing code here\nmerged.sort_values(['id', 'date', 'district', 'city'], ascending=[True, True, True, True], inplace=True)\n",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n# Merge the two DataFrames using an inner join, which will only keep rows that exist in both\n# DataFrames and overwrite the values in 'B_x' where they exist in 'B_y'\nresult = pd.merge(C, D, how='inner', on='A')\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n# Merge the two DataFrames using an inner join, which will only keep rows that exist in both\n# DataFrames and will fill in missing values with NaNs\nresult = pd.merge(C, D, how='inner', on='A')\n# Use the fillna method to replace the NaNs in the 'B_y' column with the values from the\n# 'B_x' column where 'B_x' is not NaN\nresult['B_y'] = result['B_y'].fillna(result['B_x'].notna())\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n# Merge the two DataFrames on the 'A' column, using an outer join\n# to preserve all rows from both DataFrames\nresult = pd.merge(C, D, how='outer', on='A')\n# Create a new column 'duplicated' that indicates whether the\n# values in the 'A' column are duplicated across the two DataFrames\nresult['duplicated'] = result.groupby('A')['B'].transform(lambda x: x.eq(x.shift()))\n# Sort the DataFrame by the 'A' column and drop the duplicate rows\nresult = result.sort_values('A').drop_duplicates()\n# Print the result\nprint(result)\n",
        "\nresult = df.groupby('user')[['time', 'amount']].apply(list).sort_values()\nprint(result)\n",
        "\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: list(zip(x['time'], x['amount'])))\nresult = pd.DataFrame(result)\nresult.sort_values(['time', 'amount'], inplace=True)\nprint(result)",
        "\nresult = df.groupby('user')[['time', 'amount']].apply(list).sort_values(by=['time', 'amount'])\nprint(result)\n",
        "\ndf_concatenated = pd.DataFrame(series.values.T, index=series.index)\nresult = pd.concat([series, df_concatenated], axis=1)\nprint(result)",
        "\ndf_concatenated = pd.DataFrame(series.values.T, columns=['0', '1', '2', '3'])\nresult = pd.concat([df_concatenated, series], axis=1)\nprint(result)",
        "\nresult = []\nfor col in df.columns:\n    if s in col:\n        result.append(col)\nprint(result)",
        "\npattern = re.compile(r'^.*-spike$')\nresult = df.columns[df.columns.str.match(pattern)]\nprint(result)",
        "\ncolumn_names = df.columns.tolist()\nspike_pattern = re.compile(r'^spike')\nspike_columns = [name for name in column_names if spike_pattern.match(name)]\nspike_numbers = []\nfor spike_column in spike_columns:\n    number_pattern = re.compile(r'\\d+')\n    number_match = number_pattern.search(spike_column)\n    if number_match:\n        spike_numbers.append(int(number_match.group()))\n    else:\n        spike_numbers.append(None)\nrenamed_df = df.rename(columns={spike_columns[0]: 'spike1',\n                                  spike_columns[1]: 'spike2'})\n",
        "\nfor i, code in enumerate(df['codes']):\n    if isinstance(code, list):\n        df['code_' + str(i + 1)] = code\n    else:\n        df['code_' + str(i + 1)] = np.nan\n",
        "\nfor index, row in df.iterrows():\n    codes = row['codes']\n    if isinstance(codes, list):\n        for i, code in enumerate(codes):\n            df.at[index, f'code_{i+1}'] = code\n    else:\n        df.at[index, 'code_1'] = codes\nprint(result)\n",
        "\nfor index, row in df.iterrows():\n    codes = row['codes']\n    if isinstance(codes, list):\n        for i, code in enumerate(codes):\n            df.at[index, f'code_{i+1}'] = code\n    else:\n        df.at[index, 'code_1'] = codes\nprint(result)\n",
        "\nids = df.loc[0:index, 'User IDs'].values.tolist()\nids = [list(map(int, x)) for x in ids]\nresult = pd.DataFrame({'col1': df['col1'], 'User IDs': ids})\nprint(result)\n",
        "\n# Convert the list column to a regular column\ndf['col1'] = df['col1'].apply(pd.Series)\n# Reverse the values in the 'col1' column\nids = str(df.loc[0:index, 'col1'].values.tolist()[::-1])\n# Convert the string back to a list\nids = list(ids)\n# Concatenate the list into a single string\nids = ','.join(ids)\nprint(ids)\n",
        "\nids = [str(x) for x in df['User IDs'].tolist()]\nids = ','.join(ids)\nprint(ids)\n",
        "\n# bin the times into 2 minute intervals\ngrouped = df.groupby(['Time'])\nbinned = grouped.resample('2T').mean()\n# average the values for each bin\nbinned['Value'] = binned['Value'].mean()\n# interpolate the values for each bin\nbinned = binned.interpolate()\n# convert back to datetime\nbinned['Time'] = pd.to_datetime(binned['Time'])\n# concatenate the original dataframe with the binned dataframe\nresult = pd.concat([df, binned], axis=1)\n# print the result\nprint(result)\n",
        "\n# bin the times into 3-minute intervals\ngrouped = df.groupby('Time')\nbinned = grouped.agg({'Value': 'sum'})\n# interpolate the binned values\ninterpolated = binned.resample('3T').interpolate()\n# concatenate the original dataframe with the interpolated values\nresult = pd.concat([df, interpolated], axis=1)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\nresult = result.reset_index()\nprint(result)",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\nresult = df\nprint(result)",
        "# Rank each group of ID by TIME column in ascending order\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n# Convert TIME column to datetime format\ndf['TIME'] = pd.to_datetime(df['TIME'])\n# Format TIME column as desired\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %H:%M:%S')\nresult = df\nprint(result)",
        "\nresult = df.loc[filt.index, :]\n",
        "\nresult = df.loc[filt.index]\n",
        "\ndf.columns[df.isnull().any()]\n",
        "\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\nnan_cols = df.columns[df.isna().sum() > 0]\ncols_equal = df[nan_cols].groupby(level=0).agg(lambda x: nan_cols[x.name].isin(nan_cols[x.name]))\nresult = df.loc[cols_equal, nan_cols].index.tolist()\n",
        "\ndef find_diff_cols(row0, row8):\n    diff_cols = []\n    for col in range(len(row0)):\n        if row0[col] != row8[col]:\n            diff_cols.append(col)\n    return diff_cols\ndiff_cols = find_diff_cols(df.iloc[0], df.iloc[8])\n",
        "\nresult = []\nfor col in df.columns:\n    if df[col].isnull().sum() > 0:\n        nan_values = df[df[col].isnull()].dropna().values\n        non_nan_values = df[~df[col].isnull()].values\n        diff_values = np.where(nan_values != non_nan_values, nan_values, np.nan)\n        result.extend(list(zip(diff_values, non_nan_values)))\nprint(result)\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\nresult = ts",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n# Convert to wide format\nwide = pd.melt(df, id_vars=['A'], value_name='value')\n# Group by index and sum values\nresult = wide.groupby(level=0).sum()\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n# Convert to a Series\nseries = df.iloc[0]\n# Create a new DataFrame with the single row\nresult = pd.DataFrame([series],index=['A_0','B_0','C_0','D_0','E_0',\n                                      'A_1','B_1', 'C_1', 'D_1', 'E_1',\n                                      'A_2','B_2', 'C_2', 'D_2', 'E_2'])\nprint(result)\n",
        "\n# Convert 'dogs' column to float\ndf['dogs'] = df['dogs'].astype(float)\n# Round 'dogs' column to 2 decimal places\ndf['dogs'] = df['dogs'].round(2)\n# Output the result\nresult = df\nprint(result)",
        "\n# Convert `pd.NAN` to `np.nan`\ndf = df.replace(pd.NAN, np.nan)\n# Round columns\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n# Replace `np.nan` back to `pd.NAN`\ndf = df.replace(np.nan, pd.NAN)\n# End of Missing Code\nresult = df\nprint(result)",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf[list_of_my_columns].mean(axis=1)\n",
        "\n# Calculate the average of the specified columns\navg_cols = [df[col] for col in list_of_my_columns]\nresult['Avg'] = avg_cols.mean(axis=1)\n# Calculate the minimum, maximum, and median of the specified columns\nmin_cols = [df[col].min() for col in list_of_my_columns]\nmax_cols = [df[col].max() for col in list_of_my_columns]\nmedian_cols = [df[col].median() for col in list_of_my_columns]\nresult['Min'] = min_cols\nresult['Max'] = max_cols\nresult['Median'] = median_cols\n",
        "\n# Sort the DataFrame by the time index in ascending order\nsorted_df = df.sort_index(level=0)\n# Keep the original order for elements with the same time index\nsorted_df.reset_index(inplace=True)\n",
        "\n# Sort the DataFrame by the VIM column in ascending order\nsorted_df = df.sort_values(by='VIM', ascending=True)\n# Keep the original order for elements with the same VIM value and time index\nsorted_df.reset_index(inplace=True)\nsorted_df.set_index(['VIM', 'time'], inplace=True)\n",
        "\ndf.drop(df.index[df.index.isin(['2020-02-17', '2020-02-18'])], inplace=True)\n",
        "\ndf.loc[df.index.isin(pd.date_range('2020-02-17', '2020-02-18', freq='D')), 'Date'] = pd.to_datetime('2020-02-18')\n",
        "# Filter the correlation matrix for values above 0.3\nmask = corr >= 0.3\n# Extract the column indices where the mask is True\nindices = np.where(mask)[0]\n# Use the column indices to extract the desired columns from the DataFrame\nresult = df[indices]\n# Print the result\nprint(result)",
        "# Define a function to filter the correlation matrix\ndef filter_corr(corr, threshold=0.3):\n    # Convert the correlation matrix to a Series\n    corr_series = pd.Series(corr)\n    # Filter the correlation matrix based on the threshold\n    above_threshold = corr_series[corr_series > threshold]\n    # Get the column indices of the above_threshold values\n    col_indices = above_threshold.index\n    # Get the corresponding values from the original DataFrame\n    result = df.iloc[:, col_indices]\n    return result\n# Apply the filter_corr function to the correlation matrix\nresult = filter_corr(corr)\n# Print the result\nprint(result)",
        "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\n",
        "\ndf.rename(columns={df.columns[0]: 'Test'}, inplace=True)\n[Instruction]",
        "\nfrequent = df.groupby(by=['bit1', 'bit2', 'bit3', 'bit4', 'bit5']).size()\nfreq_count = pd.DataFrame({'frequent': frequent.index, 'freq_count': frequent})\nresult = pd.merge(df, freq_count, on=['bit1', 'bit2', 'bit3', 'bit4', 'bit5'])\nprint(result)",
        "\nfrequent = df.groupby('bit1', as_index=False).size()\nfreq_count = df.groupby('bit1').frequent.reset_index()\nresult = pd.merge(df, freq_count, on='bit1')\nprint(result)",
        "\nfrequent = df.groupby([\"bit1\", \"bit2\", \"bit3\", \"bit4\", \"bit5\"])[\"bit6\"].count()\nfreq_count = frequent.reset_index().groupby([\"bit1\", \"bit2\", \"bit3\", \"bit4\", \"bit5\"]).agg({\"index\": \"first\"})[\"index\"].reset_index(name=\"freq\")\nresult = pd.merge(df, freq_count, on=[\"bit1\", \"bit2\", \"bit3\", \"bit4\", \"bit5\"])\nprint(result)",
        "\n# Calculate the mean of 'foo' and 'bar' for each group\nres = df.groupby([\"id1\", \"id2\"])[\"foo\", \"bar\"].mean()\n# Create a new column 'bar' with the calculated mean values\naggrFrame = res.reset_index()\naggrFrame[\"bar\"] = res[\"bar\"].mean()\n# Merge the new 'bar' column back into the original dataframe\nresult = pd.merge(df, aggrFrame, on=[\"id1\", \"id2\"])\n",
        "\ngroupedFrame = res.groupby([\"id1\",\"id2\"])\naggrFrame = groupedFrame.aggregate(numpy.mean)\naggrFrame[\"bar\"] = aggrFrame[\"bar\"].fillna(0)\nresult = pd.concat([res,aggrFrame],axis=1)\nprint(result)",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n# Define a list of columns to keep\ncols_to_keep = ['EntityNum', 'foo', 'a_col']\n# Use merge to join the two dataframes on EntityNum\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\n# Drop unwanted columns\nresult = result[cols_to_keep]\n# Print result\nprint(result)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n# Remove unwanted columns from df_b\ndf_b = df_b[['EntityNum', 'b_col']]\n# Merge the two dataframes on EntityNum\nresult = pd.merge(df_a, df_b, on='EntityNum')\n# Drop the original EntityNum column from df_a\nresult = result.drop('EntityNum', axis=1)\n# Print the result\nprint(result)\n"
    ],
    "Numpy": [
        "\ndimensions = a.shape\nprint(dimensions)",
        "x = x[~np.isnan(x)]",
        "\nx[np.isnan(x)] = np.inf\nprint(x)",
        "\nimport numpy as np\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\n# Replace nan values with fill_value\nresult = np.nan_to_num(x, nan=0)\n# Convert back to list of lists\nresult = result.tolist()\nprint(result)\n",
        "\nb = np.zeros((len(a), 4))\nb[np.arange(len(a)), a] = 1\nprint(b)",
        "\nb = np.zeros((len(a)+1, len(max(a))+1))\nb[:-1, a] = 1",
        "\nb = np.zeros((len(a)+1, len(a[0])))\nb[1:, a] = 1\nprint(b)",
        "\nb = np.zeros((len(a), 3))\nb[np.arange(len(a)), np.argsort(a)] = 1\nprint(b)",
        "b = np.zeros((a.shape[0], a.shape[1], a.max() + 1))\nb[np.arange(a.shape[0]), np.arange(a.shape[1]), a] = 1\nprint(b)",
        "\npercentile = np.percentile(a, p)\nresult = np.append(result, percentile)\nprint(result)",
        "B = np.array(A).reshape((ncol, -1))",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nnrow = 3\n# Create a new array with the specified number of rows\nB = np.empty((nrow, len(A)), dtype=int)\n# Copy the values from A into B\nB[:len(A)] = A\nprint(B)\n",
        "B = np.array(A).reshape((len(A), ncol))",
        "\nB = np.array(A).reshape((len(A)//ncol, ncol))\nif len(A)%ncol != 0:\n    B = B[:-1]\nprint(B)",
        "result = np.roll(a, shift)\nprint(result)",
        "result = np.roll(a, shift, axis=0)\nprint(result)",
        "\nresult = np.pad(a, ((0, shift[0]), (0, shift[1])), 'constant', constant_values=0)\nprint(result)\n",
        "\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.permutation(r_old)\n",
        "\nresult = np.argmax(a, axis=0)\nindices = result.ravel()\nprint(indices)",
        "result = np.unravel_index(a.argmin(), a.shape)\nprint(result)",
        "result = np.unravel_index(a.argmax(), a.shape)\nprint(result)",
        "result = np.unravel_index(a.argmax(), a.shape)\nprint(result)",
        "\nresult = np.argmax(a, axis=0)\nreturn result.ravel('C')",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\nprint(result)",
        "z = np.any(isnan(a), axis=0)\ndelete_cols = np.where(z)[0]\na = np.delete(a, delete_cols, axis=1)\nprint(a)",
        "\nmask = np.isnan(a)\na = a[~mask, :]\n",
        "\nresult = np.array(a)\nprint(result)",
        "\nb = np.empty_like(a)\nb[:, permutation] = a\na = b\nprint(a)",
        "\nresult = np.moveaxis(a, permutation, 0)\nprint(result)",
        "\nmin_idx = np.unravel_index(a.argmin(), a.shape)\nresult = (min_idx[0], min_idx[1])\nprint(result)",
        "\nmax_val, max_idx = a.max()\nrow, col = max_idx\nprint(f\"The maximum value is {max_val} and it is located at ({row}, {col})\")",
        "\nmin_idx = np.unravel_index(a.argmin(), a.shape)\nresult = np.ravel_multi_index(min_idx, a.shape)",
        "\nimport numpy as np\ndegree = 90\n# Convert degree to radians\nresult = np.sin(np.deg2rad(degree))\nprint(result)\n",
        "\nimport numpy as np\ndegree = 90\n# Convert degree to radians\nangle_in_radians = np.deg2rad(degree)\n# Compute cosine value\nresult = np.cos(angle_in_radians)\n# Print result\nprint(result)\n",
        "\nif np.sin(number * np.pi / 180) > 0:\n    result = 0\nelse:\n    result = 1\n",
        "\nimport numpy as np\nvalue = 1.0\n# Convert the value to radians\nvalue_in_radians = value * np.pi / 180.0\n# Calculate the arcsine of the value\nresult = np.arcsin(value_in_radians)\n# Convert the result back to degrees\nresult_in_degrees = result * 180.0 / np.pi\n# Print the result\nprint(result_in_degrees)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\n# Pad the array with zeros to the nearest multiple of 1024\npad_size = 1024 - (len(A) % pad_size)\nA = np.pad(A, [(0, pad_size), (0, 0)], 'constant', constant_values=0)\n# Print the result\nprint(A)\n",
        "\n# Calculate the number of elements needed to pad the array to the closest multiple of 1024\nnum_elements = (length // 1024) * 1024\n# Create an array of zeros with the correct number of elements\npadded_array = np.zeros((num_elements,))\n# Concatenate the original array and the padded array\nresult = np.concatenate((A, padded_array))\n",
        "\nresult = a ** power\n",
        "\n    result = a ** power\n    ",
        "\nresult = numerator / denominator\nprint(result)",
        "\nimport numpy as np\ndef f(numerator = 98, denominator = 42):\n    # Convert the input to fractions\n    numerator = numerator // denominator\n    denominator = denominator // denominator\n    \n    # Reduce the fraction\n    while denominator != 1:\n        numerator, denominator = numerator // denominator, denominator // denominator\n    \n    # Return the result as a tuple\n    return (numerator, denominator)\n",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\n# Convert the fractions to integers\nnumerator_int = numerator // denominator\ndenominator_int = denominator // denominator\n# Check if the denominator is zero\nif denominator_int == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator_int, denominator_int)\n# Print the result\nprint(result)\n",
        "\nresult = (a + b + c) / 3\nprint(result)",
        "\nresult = np.maximum(a, b, c)\nprint(result)",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n# Get the indices of the main diagonal\nrow_indices, col_indices = np.diag_indices(a.shape[0])\n# Reverse the order of the indices\nrow_indices = row_indices[::-1]\n# Get the values at the indices\nresult = a[row_indices, col_indices]\nprint(result)\n",
        "\ndiagonal = np.diag_indices(a.shape[0])[::-1]\nresult = a[diagonal]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n# Get the indices of the main diagonal\nmain_diag = np.diag_indices(a.shape[0])[0]\n# Get the indices of the top-right diagonal\ntop_right_diag = np.diag_indices(a.shape[0] - 1)[0]\n# Combine the two diagonals\ndiagonal = np.append(main_diag, top_right_diag)\n# Get the values at the diagonal indices\nresult = a[diagonal]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n# Fill in the missing code here:\ndiagonal = np.diag_indices(a.shape[0])[::-1]\nresult = a[diagonal]\nprint(result)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i, row in enumerate(X):\n    for j, val in enumerate(row):\n        result.append(val)\nprint(result)\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])",
        "\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    # Begin of Missing Code\n    result = []\n    for i, row in enumerate(X):\n        for j, cell in enumerate(row):\n            result.append(cell)\n    # End of Missing Code\n    return result\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])",
        "\nimport numpy as np\nmystr = \"100110\"\n# Split the string into individual digits\ndigits = list(mystr)\n# Convert each digit to an integer\ndigits = [int(digit) for digit in digits]\n# Use np.fromstring to convert the list of integers to a numpy array\nresult = np.fromstring(digits, dtype=int, sep='')\n# Print the result\nprint(result)\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\ncol = 2\nmultiply_number = 5.2\n# Multiply the values in the specified column by the given number\na[:, col] *= multiply_number\n# Calculate the cumulative sum along the specified column\nresult = np.cumsum(a[:, col], axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\n# Multiply the row by the given number\na[row-1] *= multiply_number\n# Calculate the cumulative sum along the row\nresult = np.cumsum(a[row-1])\nprint(result)\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\ndivide_number = 5.2\n# Calculate the division of the row-th row by the divide_number\nrow_divided = a[row] / divide_number\n# Calculate the multiplication of the numbers in that row\nresult = row_divided * a[row]\nprint(result)\n",
        "\nmaximal_set = []\nfor i in range(len(a)):\n    if i not in maximal_set:\n        for j in range(i+1, len(a)):\n            if np.linalg.matrix_rank(np.vstack((a[i], a[j]))) == 1:\n                maximal_set.append(j)\nresult = np.vstack((a[i] for i in maximal_set))\nprint(result)",
        "\nrow_size = a.shape[0]\n    # do something with a[i]\nprint(result)",
        "\nimport numpy as np\nimport scipy.stats\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n# Calculate the mean and standard deviation of each sample\nmean_a = np.mean(a)\nstd_a = np.std(a)\nmean_b = np.mean(b)\nstd_b = np.std(b)\n# Calculate the weighted t-statistic\nt = scipy.stats.ttest_ind(a, b, equal_var=False)\n# Calculate the p-value\np_value = t.pvalue\n# Print the p-value\nprint(p_value)\n",
        "\nn1 = len(a)\nn2 = len(b)\nif n1 != n2:\n    raise ValueError(\"Data arrays must be of equal size\")\nmean_a = np.mean(a)\nstd_dev_a = np.std(a)\nmean_b = np.mean(b)\nstd_dev_b = np.std(b)\nt, p_value = scipy.stats.ttest_ind(a, b, nan_policy='omit')\nprint(p_value)",
        "\nimport numpy as np\nimport scipy.stats\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\n# Calculate the weighted mean and variance of sample 1\nwmean1 = (anobs * amean) / (anobs + bnobs)\nwvar1 = (anobs * avar + bnobs * bvar) / (anobs + bnobs)\n# Calculate the weighted mean and variance of sample 2\nwmean2 = (bnobs * bmean) / (anobs + bnobs)\nwvar2 = (bnobs * bvar + anobs * avar) / (anobs + bnobs)\n# Calculate the weighted t-statistic\nt = (wmean1 - wmean2) / np.sqrt(wvar1 / anobs + wvar2 / bnobs)\n# Calculate the p-value\np_value = scipy.stats.t.sf(t, df=anobs + bnobs - 2)\nprint(p_value)\n",
        "output = np.delete(A, np.in1d(A, B), axis=0)\nprint(output)",
        "\nC = A - B\nD = B - A\noutput = np.concatenate((C, D), axis=0)\n",
        "\nsort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices].reshape((3, 3, 3))\n",
        "\nsort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "sort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]",
        "\n# Sort b according to the sum of a\nsums = a.sum(axis=-1)\nindices = numpy.argsort(sums)\nresult = b[indices, ...]\n",
        "\ndel a[:, 2]\n",
        "\ndel a[2, :]\n",
        "\ndel a[:, 0]\ndel a[:, 2]\n",
        "\nresult = a[:, del_col]\nprint(result)\n",
        "\nimport numpy as np\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\n# Insert the element at the specified position\na = np.insert(a, pos, element)\nprint(a)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\ndef f(a = example_a, pos=2, element = 66):\n    # insert element at position pos\n    a = np.insert(a, pos, element)\n    return a\n",
        "\ninserted_rows = element.tolist()\nfor i in range(len(pos)):\n    a = np.insert(a, pos[i], inserted_rows[i], axis=0)\n",
        "\nresult = np.array(array_of_arrays, copy=True)\nprint(result)",
        "\nresult = np.all(np.array_equal(a[0], a), axis=1)\nprint(result)\n",
        "\nnp.allclose(a, a.T)\n",
        "\ndef f(a = example_a):\n    # Check if all rows are equal\n    result = np.all(a == a[0], axis=1)\n    return result\n",
        "\nfrom scipy.interpolate import RectBivariateSpline\nspline = RectBivariateSpline(x, y, z)\nresult = spline.integral(x, y)\nprint(result)",
        "\ndef f(x = example_x, y = example_y):\n    # Calculate the weights and indices for the Simpson's rule\n    dx = x[1] - x[0]\n    dy = y[1] - y[0]\n    nx = len(x)\n    ny = len(y)\n             (dx * dy)\n    indices = np.array([0, 1, 2, 0, 1, 2], dtype=int)\n    # Use the weights and indices to perform the Simpson's rule integration\n    result = 0.0\n    for i in range(3):\n        result += weights[i] * (f(x[indices[i]], y) + f(x[indices[i + 1]], y) + f(x[indices[i + 2]], y)) / 6.0\n    return result\n",
        "\nresult = np.cumsum(grades) / np.sum(grades)\n",
        "\nimport numpy as np\ngrades = np.array((93.5, 93, 60.8, 94.5, 82, 87.5, 91.5, 99.5, 86, 93.5, 92.5, 78, 76, 69, 94.5, 89.5, 92.8, 78, 65.5, 98, 98.5, 92.3, 95.5, 76, 91, 95, 61))\neval = np.array([88, 87, 62])\n# Define the ecdf function\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n# Apply the ecdf function to the eval array to get corresponding values\nresult = ecdf(grades)\n# Print the result\nprint(result)\n",
        "# Compute ECDF\necdf = np.cumsum(grades / np.sum(grades))\n# Find longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow, high = np.searchsorted(ecdf, [threshold, 1.0], side='left')\nlow = max(low - 1, 0)\nhigh = min(high + 1, len(grades) - 1)\n",
        "\nimport numpy as np\none_ratio = 0.9\nsize = 1000\n# Generate a random array of zeros and ones\nrandomLabel = np.random.randint(2, size=(size,))\n# Count the number of ones in the array\nnum_ones = randomLabel.sum()\n# Calculate the ratio of ones to the total number of elements\nratio = num_ones / size\n# Calculate the number of ones to add to the array to achieve the desired ratio\nnum_to_add = int(size * one_ratio - ratio * size)\n# Add the required number of ones to the array\nrandomLabel[randomLabel == 0] = np.random.randint(2, size=(num_to_add,))\n# Print the final array\nprint(randomLabel)\n",
        "\na_np = a.numpy()\nprint(a_np)",
        "\na_pt = torch.from_numpy(a)\nprint(a_pt)",
        "\na_np = a.numpy()\nprint(a_np)",
        "\na_tf = tf.convert_to_tensor(a)\nprint(a_tf)",
        "result = np.argsort(a)[::-1]",
        "\nresult = np.argsort(a)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\n# Sort the array in decreasing order\nsorted_indices = np.argsort(-a)[:N]\n# Get the indexes of the biggest elements\nresult = sorted_indices\nprint(result)\n",
        "\nresult = result.reshape(4, 4) ",
        "\nresult = np.split(a, 2, axis=0)\n# result = np.split(a, 2, axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n# Extract patches of 2x2 size using sliding window\nresult = []\nfor i in range(a.shape[0] - 2):\n    for j in range(a.shape[1] - 2):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\nprint(result)\n",
        "\n[[[1, 5],\n  [2, 6]],\n [[9, 13],\n  [10, 14]],\n [[3, 7],\n  [4, 8]],\n [[11, 15],\n  [12, 16]]]\n",
        "\nresult = np.split(a, a.shape[0] // patch_size, axis=0)\n# result will be a list of arrays, each with shape (patch_size, patch_size)\n# We need to transpose the result to get the desired output\nresult = [arr.T for arr in result]\n# The final result will be a 3-d array or list\nresult = np.array(result) if len(result) == 1 else result\nprint(result)\n",
        "result = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i][j] = a[i//2][j//2][i%2][j%2]\nprint(result)",
        "\nresult = np.split(a, a.shape[0] // patch_size, axis=0)\n# result will be a list of arrays, each with shape (patch_size, patch_size)\n# We can then flatten the list to get the desired 3-d array or list\nresult = np.vstack(result)\nprint(result)\n",
        "result = a[:, low:high]\nprint(result)",
        "result = a[low:high, :]\nprint(result)",
        "\nresult = a[:, low:high]\nprint(result)",
        "\narr = np.array([list(map(float, x.split())) for x in string.strip('[]').split(',')])\n",
        "\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n# Generate log-uniform random numbers\nresult = np.random.loguniform(low=np.log(min), high=np.log(max), size=n)\n# Print result\nprint(result)\n",
        "\nimport numpy as np\nmin = 0\nmax = 1\nn = 10000\n# Generate random numbers from a uniform distribution\nuniform = np.random.uniform(min, max, size=n)\n# Compute the logarithm of the random numbers\nlog_uniform = np.log(uniform)\n# Normalize the logarithm to get a distribution in the desired range\nnormalized_log_uniform = (log_uniform - np.log(max)) / (np.log(min) - np.log(max))\n# Convert back to the original range\nresult = np.exp(normalized_log_uniform)\nprint(result)\n",
        "\nimport numpy as np\ndef f(min=1, max=np.e, n=10000):\n    # Generate logarithmically spaced values\n    log_range = np.linspace(np.log(min), np.log(max), n)\n    # Convert to uniform values\n    uniform_range = np.exp(log_range)\n    # Sample from the uniform distribution\n    return np.random.choice(uniform_range, size=n)\n",
        "\nB = np.zeros(len(A))\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]",
        "\nB = pd.Series(np.zeros(10))\nfor i in range(10):\n    B[i] = a * A[i] + b * B[i-1] + c * B[i-2]\nprint(B)\n",
        "\nimport numpy as np\n# Begin of Missing Code\ndemod4 = np.empty((0,), dtype=int)\n# End of Missing Code\nprint(result)\n",
        "\ndemod4 = np.empty((3,0))\n",
        "linear_index = index + np.array(dims) * np.arange(dims[0])\nresult = a[linear_index]\nprint(result)",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\n# Begin of Missing Code\nlinear_index = np.sum(index * np.array(dims)[::-1])\n# End of Missing Code\nprint(linear_index)\n",
        "\nvalues = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,1,1,2,2,1])\n# Fill in the missing code here\nresult = np.sum(a[accmap==1], axis=0)\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\n# Compute the maximum value for each group of consecutive indices\ngroup_sizes = np.diff(index)\nmax_values = np.zeros_like(a)\nfor i, group_size in enumerate(group_sizes):\n    max_values[index[i]:index[i]+group_size] = a[index[i]:index[i]+group_size].max()\n# Find the indices of the maximum values\nresult = index[max_values == max_values]\nprint(result)\n",
        "result = np.zeros_like(a)\nresult[accmap] = a[accmap]\nprint(result)",
        "result = a[index == 0]",
        "\ndef elementwise_function(element_1,element_2):\n    return (element_1 + element_2)\nz = np.empty_like(x)\nfor i in range(len(x)):\n    z[i] = np.array([elementwise_function(x[i][j], y[i][j]) for j in range(len(x[i]))])\nprint(z)\n",
        "\n# Convert the list of tuples to a NumPy array\narr = np.array(lista_elegir)\n# Use the `choice` method to select samples with the given probabilities\nresult = np.random.choice(arr, size=samples, p=probabilit)\n# Convert the array back to a list of tuples\nresult = list(result)\n",
        "\nresult = np.pad(a, ((0, new_shape[0] - a.shape[0]), (0, new_shape[1] - a.shape[1]), (0, new_shape[2] - a.shape[2])), mode='constant', constant_values=0)\n",
        "result = x[x >= 0]",
        "\nmask = x != 0\nresult = x[mask]\n",
        "\nbin_data = np.array_split(data, np.ceil(len(data) / bin_size))\nbin_data_mean = []\nfor bin in bin_data:\n    bin_data_mean.append(np.mean(bin))\nprint(bin_data_mean)",
        "\nbin_data_max = np.max(np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)]), axis=0)\n",
        "\nbin_data = np.array([[0,1],[2,3]])\nbin_data_mean = np.mean(data[bin_data],axis=1)\n",
        "\nbin_data = np.histogram(data, bins=bin_size, density=False)[0]\nbin_data_mean = np.mean(bin_data, axis=0)\n",
        "\n# Calculate the bin indices\nbin_indices = np.arange(len(data))[::-1] // bin_size\n# Create a mask for the last bin\nlast_bin_mask = bin_indices[-1] < len(data)\n# Pad the last bin with zeros if necessary\nif last_bin_mask.any():\n\tbin_indices = np.append(bin_indices, [len(data)])\n\tdata = np.append(data, [0], axis=0)\n# Calculate the bin means\nbin_means = np.zeros((len(data) // bin_size, data.shape[1]))\nfor i, bin_indices_i in enumerate(bin_indices):\n\tbin_data = data[bin_indices_i]\n\tbin_means[i] = np.mean(bin_data, axis=0)\n# Remove the last bin if necessary\nif not last_bin_mask.any():\n\tbin_means = bin_means[:-1]\n",
        "\nbin_data = np.array([[range(i, j+1) for j in range(bin_size)] for i in range(len(data)-bin_size+1)])\nbin_data = bin_data[~bin_data[:,0].isna()]\nbin_data_mean = np.mean(data[bin_data[:,0],:], axis=1)\n",
        "\ndef smoothclamp(x):\n    x_squared = x ** 2\n    x_cubed = x ** 3\n    return x_min + (x_max - x_min) / (1 + x_squared + x_cubed)\n",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\nN = 5\n# Begin of Missing Code\ndef smoothclamp(x, N=5):\n    x_range = x_max - x_min\n    step = 1 / (N - 1)\n    y = np.zeros(N)\n    y[0] = x_min\n    y[N - 1] = x_max\n    for i in range(1, N):\n        y[i] = x_min + step * (i - 1)\n    y = y / (y[1] - y[0])\n    y = (x - y[0]) / (y[1] - y[0])\n    return y\n# End of Missing Code\nresult = smoothclamp(x, N=N)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\n# Calculate the circular cross-correlation of a and b using the Fourier transform\nresult = np.fft.fft(a) * np.conj(np.fft.fft(b)) / len(a)\n# Convert the result back to the time domain using the inverse Fourier transform\nresult = np.fft.ifft(result)\n# Take the circular correlation of the first half of b with the fixed a\nresult = result[0:len(b)//2]\n# Pad the result with zeros to match the length of a\nresult = np.pad(result, (len(a) - len(result)), 'constant', constant_values=0)\nprint(result)\n",
        "\narray = np.zeros((4, len(names), len(times)), dtype=np.float32)\nfor i, major in enumerate(names):\n    for j, timestamp in enumerate(times):\n        index = pd.MultiIndex.from_product([[major], [timestamp]], names=['major', 'timestamp'])\n        array[i, :, j] = df.loc[index].values\nresult = array.reshape((4, -1))\n",
        "\narray = np.zeros((len(names), df.shape[1], len(times)), dtype=float)\nfor i, major in enumerate(names):\n    for j, col in enumerate(df.columns):\n        for k, timestamp in enumerate(times):\n            array[i, j, k] = df[major][df.index.get_level_values(0) == major][col].values\nresult = np.array(array)\n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 8\n# Convert each element in the array to a binary representation\nbinary_array = np.unpackbits(np.uint8(a))\n# Reshape the binary array to a matrix of size n x m\nresult = binary_array.reshape((a.size, m))\nprint(result)\n",
        "\ndef convert_to_binary_array(a, m):\n    result = np.zeros((len(a), m))\n    for i, num in enumerate(a):\n        if num >= 0 and num < 2**m:\n            result[i] = np.unpackbits(np.uint8(num))\n        elif num < 0:\n            result[i] = np.unpackbits(np.uint8(num + 2**m))\n        else:\n            result[i] = np.unpackbits(np.uint8(num - 1))\n    return result\n",
        "\nb = np.unpackbits(np.uint8(a))\nresult = np.zeros((m,), dtype=np.uint8)\nfor i in range(m):\n    result[i] = b[i] ^ b[i]\nprint(result)",
        "\ndef third_std_dev(arr):\n    mean = np.mean(arr)\n    std = np.std(arr)\n    third_std = mean + 3 * std\n    return (mean - third_std, mean + third_std)\n# Apply the function to the input array\nresult = third_std_dev(a)\nprint(result)",
        "\ndef second_std_deviation(arr):\n    n = len(arr)\n    mean = np.mean(arr)\n    std = np.std(arr)\n    z_score = (arr - mean) / std\n    return (mean - 2 * std, mean + 2 * std)\nresult = second_std_deviation(a)\nprint(result)",
        "\ndef f(a = example_a):\n    # Calculate mean\n    mu = np.mean(a)\n    \n    # Calculate standard deviation\n    sigma = np.std(a)\n    \n    # Calculate 3rd standard deviation\n    third_sigma = mu + 3 * sigma\n    \n    # Calculate 3rd standard deviation interval\n    interval = (mu - third_sigma, mu + third_sigma)\n    \n    return interval",
        "\nimport numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n# Calculate the mean and standard deviation of the array\nmu = np.mean(a)\nsigma = np.std(a)\n# Calculate the 2nd standard deviation\nsigma2 = sigma * 2\n# Calculate the z-scores for each element in the array\nz_scores = (a - mu) / sigma2\n# Detect outliers of 2nd standard deviation interval from array x\nresult = np.abs(z_scores) > 2\nprint(result)\n",
        "\nimport numpy as np\nDataArray = np.arange(-5.5, 10.5)\npercentile = 50\n# Create a masked array by masking values less than 0\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\n# Use the masked array with the np.percentile() function\nprob = np.percentile(masked_data, percentile)\nprint(prob)\n",
        "\nimport numpy as np\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = 0\nzero_cols = 0\n# Fill in the missing code here:\nmask = a == 0\na[mask] = 0\nprint(a)\n",
        "\nfor row in zero_rows:\n    for col in zero_cols:\n        a[row][col] = 0",
        "\na[1, :] = 0\na[:, 0] = 0\n",
        "\n# Calculate the maximum values along axis=1\nmax_values = np.amax(a, axis=1)\n# Create a boolean mask array with the same shape as a\nmask = max_values == a\n# Check the result\nprint(mask)\n",
        "\nmask = np.min(a, axis=1) == a",
        "\nimport numpy as np\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n# Calculate the number of postal codes in each distance range\ncounts = np.bincount(post, minlength=len(distance))\n# Calculate the mean of each distance range\nmeans = np.mean(distance, axis=0)\n# Calculate the covariance matrix\ncov = np.cov(post, counts)\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(post, means)[0, 1]\nprint(result)\n",
        "\nresult = np.einsum('ij,jk->ik', X, X)",
        "\nX = np.array([[4, 12, 8],\n            [12, 36, 24],\n            [8, 24, 16],\n            [25, 35, 25],\n            [35, 49, 35],\n            [25, 30, 10],\n            [30, 36, 12],\n            [10, 12,  4]])\nfor i in range(Y.shape[0]):\n    X += Y[i].dot(Y[i].T)\n",
        "\nis_contained = np.all(a == number)\nimport numpy as np\na = np.array([9, 2, 7, 0])\nnumber = 0\nis_contained = np.all(a == number)\nprint(is_contained)\nTrue",
        "\nC = np.delete(A, np.in1d(A, B), axis=0)\nprint(C)\n",
        "\nC = np.isin(A, B)\nA = A[~C]\n",
        "\nC = np.zeros_like(A)\nC[A >= B[0]] = B[0]\nC[A <= B[1]] = B[1]\nC[A >= B[0] & A <= B[1]] = B[0]\nC[A >= B[1] & A <= B[2]] = B[2]\nC[A >= B[0] & A > B[1]] = B[0]\nC[A <= B[1] & A > B[2]] = B[1]\nC[A > B[0] & A <= B[1]] = B[1]\nC[A > B[1] & A <= B[2]] = B[2]\nC[A > B[0] & A > B[1]] = B[0]\n",
        "\nresult = np.flip(rankdata(a))\n",
        "\nresult = np.sort(np.unique(a, return_index=True)[1])\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\ndef f(a = example_a):\n    # Reverse the order of the input array\n    a = a[::-1]\n    \n    # Calculate the rank of each element in the input array\n    ranks = rankdata(a)\n    \n    # Convert the ranks to integers\n    result = ranks.astype(int)\n    \n    # Reverse the order of the result array\n    result = result[::-1]\n    \n    return result\n",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, 1, -2],\n                 [ -1,  0, 1],\n                 [ -2,  1,  0]])\n# Fill in the missing code here:\ndists = np.stack((x_dists, y_dists), axis=1)\nprint(dists)\n",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n# Begin of Missing Code\ndists = np.zeros((x_dists.shape[0], x_dists.shape[1], 2))\nfor i in range(x_dists.shape[0]):\n    for j in range(x_dists.shape[1]):\n        dists[i][j][0] = x_dists[i][j]\n        dists[i][j][1] = y_dists[i][j]\n# End of Missing Code\nprint(dists)\n",
        "\nindices = np.array(list(zip(second, third)))\nresult = a[indices]\nprint(result)",
        "\narr = numpy.zeros((20,)*4)\nshape = arr.shape\nfor i in range(len(shape)):\n    if shape[i] != 20:\n        arr = arr.reshape((shape[0],)*i + (20,)*(len(shape)-i))",
        "\nl1 = X.sum(axis=1)\nresult = X / l1.reshape(5,1)\n",
        "\nx = np.array([LA.norm(v,ord=2) for v in X.T])\n",
        "\nresult = np.array([LA.norm(v, ord=np.inf) for v in X.T])\n",
        "\nconditions = [df['a'].str.contains(target),\n               df['a'].str.contains(choices)]\nresult = np.select(conditions, choices, default=np.nan)\n",
        "\ndistances = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        distances[i, j] = distances[j, i] = np.linalg.norm(a[i] - a[j])\nresult = distances\n",
        "\nfor i in range(len(a)):\n    for j in range(i+1, len(a)):\n        result[i, j] = ... # calculate distance between points i and j\n",
        "\nfor i in range(len(a)):\n    for j in range(i+1, len(a)):\n        result[i, j] = ... # calculate distance between i-th and j-th points\n",
        "AVG = np.mean(NA, axis=0)",
        "AVG = np.mean(NA, axis=0, dtype=float)\n",
        "\nNA = np.array(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)",
        "\nnon_zero_indices = np.where(a != 0)[0]\nunique_indices = np.unique(non_zero_indices)\nresult = a[unique_indices]\nprint(result)",
        "\nmask = np.diff(a, axis=0) != 0\nresult = a[mask]\n",
        "\ndf = pd.DataFrame({'lat': lat[:, 0], 'lon': lon[:, 0], 'val': val[:, 0]})\nfor i in range(1, lat.shape[1]):\n    df = pd.concat([df, pd.DataFrame({'lat': lat[:, i], 'lon': lon[:, i], 'val': val[:, i]})], axis=1)\n",
        "\n    df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\n    ",
        "\ndf['max'] = df.groupby(level=0).max()\nprint(df)\n",
        "\ndef rolling_window(a, size, step=1):\n    nrows, ncols = a.shape\n    start = [max(0, i - size[0] // 2) for i in range(nrows)]\n    end = start + size\n    indices = np.indices((nrows, ncols))\n    indices = indices[start[0]:end[0], start[1]:end[1]]\n    return a[indices]\n# Example usage\nresult = rolling_window(a, size)\nprint(result)",
        "\ndef rolling_window(a, size, step=1):\n    nrows, ncols = a.shape\n    start = [max(0, i - size[0] // 2) for i in range(nrows)]\n    end = start + size\n    indices = np.indices((nrows, ncols))\n    indices = indices[start[0]:end[0], start[1]:end[1]]\n    return a[indices]\n# Example usage\nresult = rolling_window(a, size)\nprint(result)",
        "\nimport numpy as np\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\n# Replace nan with 0\na[a == np.inf] = a[a == np.inf].real\nprint(np.mean(a))\n",
        "\nimport numpy as np\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    # Replace nan with 0\n    a[a != a] = 0\n    # Compute the mean\n    result = np.mean(a)\n    # Replace 0 with nan\n    result[result == 0] = np.nan\n    return result\n",
        "\ndim = Z.ndim\nresult = Z[:, :, -1:]\nprint(result)",
        "\ndim = len(a.shape)\nif dim == 0:\n    result = a\nelif dim == 1:\n    result = a[:, -1]\nelif dim == 2:\n    result = a[:, :, -1]\nelif dim == 3:\n    result = a[:, :, :, -1]\nelif dim == 4:\n    result = a[:, :, :, :, -1]\nelif dim == 5:\n    result = a[:, :, :, :, :, -1]\nelif dim == 6:\n    result = a[:, :, :, :, :, :, -1]\nelif dim == 7:\n    result = a[:, :, :, :, :, :, :, -1]\nelif dim == 8:\n    result = a[:, :, :, :, :, :, :, :, -1]\nelse:\n    raise ValueError(\"Invalid array dimension\")",
        "\ndef is_member(c, CNTS):\n    for i in range(len(CNTS)):\n        if np.allclose(c, CNTS[i]):\n            return True\n    return False\nresult = is_member(c, CNTS)\nprint(result)",
        "",
        "\ninterp_func = intp.LinearNDInterpolator(a.ravel(), a.ravel())\nresult = interp_func(x_new, y_new)\nprint(result)",
        "\ndf[name] = df.groupby('D')[['Q']].cumsum()\n",
        "diag = np.diag(i)\ni = diag.dot(U).dot(diag)\nprint(i)",
        "\nb = a.copy()\nb[np.triu_indices_from(b)] = 0\na = b",
        "\nimport numpy as np\nimport pandas as pd\nstart = \"23-FEB-2015 23:09:19.445506\"\nend = \"24-FEB-2015 01:09:22.404973\"\nn = 50\n# Create a datetime index with the desired number of equally spaced elements\ndelta = end - start\ndt_range = pd.date_range(start=start, end=end, freq=\"1S\")\ndt_index = pd.PeriodIndex(dt_range)\ndt_index = dt_index.resample(\"1T\").asfreq()\ndt_index = dt_index.diff() / n\n# Create a pandas Series with the desired number of elements\nseries = pd.Series(index=dt_index)\n# Fill the series with the desired values\nseries.fillna(method=\"ffill\")\n# Convert the series to a numpy array\nresult = series.values\nprint(result)\n",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\n# Find the index of (a, b) in x and y\nidx_x = np.where(x == a)[0][0]\nidx_y = np.where(y == b)[0][0]\n# Check if both indices are valid (i.e., not -1)\nif idx_x != -1 and idx_y != -1:\n    # The index of (a, b) is the minimum of idx_x and idx_y\n    result = min(idx_x, idx_y)\nelse:\n    # If either index is -1, the corresponding element does not exist in the array\n    result = -1\n# Print the result\nprint(result)\n",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\n# Find indices of (a, b) in x and y\nindices_x = np.where(x == a)[0]\nindices_y = np.where(y == b)[0]\n# Check if both indices exist\nif len(indices_x) == 1 and len(indices_y) == 1:\n    # indices_x and indices_y should have the same length now\n    indices = np.append(indices_x, indices_y)\nelse:\n    indices = np.empty(0)\n# Print the indices\nprint(indices)\n",
        "\nimport numpy as np\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\n# Use scipy.optimize.leastsq to minimize the squared error\nfrom scipy.optimize import leastsq\nresult = leastsq(lambda a, b, c: f(x, a, b, c) - y, [a, b, c], args=(x, y))\n# Print the result\nprint(result)\n",
        "# Calculate the matrix of the function and its values at the given points\nA = np.array([[1, x[0] + x[0] * b + x[0] ** 2 * c],\n              [x[0] + x[0] * b + x[0] ** 2 * c, 1, x[0] * b + x[0] ** 2 * c],\n              [x[0] ** 2 * c, x[0] * b + x[0] ** 2 * c, 1, x[0] ** 3 * c],\n              [x[1] + x[1] * b + x[1] ** 2 * c, x[0] * b + x[0] ** 2 * c, x[1] + x[1] * b + x[1] ** 2 * c]])\ny_values = np.array([y[0], y[1], y[2], y[3]])\n# Use the least squares method to solve for the coefficients\nresult = np.linalg.lstsq(A, y_values, rcond=None)[0]\n# Print the result\nprint(result)",
        "temp_arr = np.array([0,1,2,3])\ntemp_df = pd.DataFrame(temp_arr)\nfor i, row in df.iterrows():\n    row = row - temp_arr[i]\ndf = df.apply(lambda x: x-temp_arr)\nprint(df)",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "scaler = MinMaxScaler()\nscaled_a = scaler.fit_transform(a)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\narr = np.array([[1.0,2.0,3.0],[0.1, 5.1, 100.1],[0.01, 20.1, 1000.1]])\n# Begin of Missing Code\nscaler = MinMaxScaler()\nscaled_arr = scaler.fit_transform(arr)\n# End of Missing Code\nprint(scaled_arr)\n",
        "\nscaler = MinMaxScaler()\nscaled_a = scaler.fit_transform(a)\nresult = np.array([[scaled_a[0][0], scaled_a[0][1], scaled_a[0][2]], [scaled_a[1][0], scaled_a[1][1], scaled_a[1][2]]])\nprint(result)",
        "\nmask3 = mask & mask2\narry[mask3] = 30\narry[~mask2] = 35\n",
        "\nmask3 = mask ^ mask2\narry[mask3] = 0\narry[mask3] += 5\narry[~mask2] = 30\n",
        "\neps = np.finfo(tag.dtype).eps\nresult = np.abs(s1 - s2) < eps\nprint(result)",
        "\ndef find_diff_elements(s1, s2):\n    diff = np.where(s1 != s2)[0]\n    nan_diff = np.where(np.isnan(s1) != np.isnan(s2))[0]\n    diff = np.append(diff, nan_diff)\n    return diff\n# Run the function to get the indices of the different elements\ndiff_indices = find_diff_elements(s1, s2)\n# Count the number of truly different elements\nresult = len(diff_indices)\n",
        "\ndef all_equal(arrays):\n    if len(arrays) == 1:\n        return True\n    elif len(arrays) == 2:\n        return np.array_equal(arrays[0], arrays[1])\n    else:\n        return all(np.array_equal(a, b) for a, b in zip(arrays, arrays[1:]))\n",
        "\nfor arr in a:\n    if np.isnan(arr).any():\n        result = False\n        break\nprint(result)",
        "\nimport numpy as np\na = np.ones((41, 13))\nshape = (93, 13)\n# Fill in the missing code here\npadded = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), 'constant', constant_values=0)\nprint(padded)\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\n# Pad the array to match the largest shape\npadded = np.zeros((93, 13))\npadded[:a.shape[0], :a.shape[1]] = a\nprint(padded)\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\n# Pad the array to match the shape of the target array\npadded = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), mode='constant', constant_values=element)\nprint(padded)\n",
        "\nimport numpy as np\ndef f(arr = example_arr, shape=(93,13)):\n    # Pad the array to match the shape\n    pad_width = [(shape[i] - arr.shape[i]) for i in range(len(arr.shape))]\n    padded_arr = np.pad(arr, pad_width=pad_width, mode='constant', constant_values=0)\n    # Reshape the padded array to match the shape\n    result = padded_arr.reshape(shape)\n    return result\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\n# Pad the first dimension of `a` to match the first dimension of `shape`\na = np.pad(a, ((0, shape[0] - a.shape[0]), (0, 0)), 'constant', constant_values=0)\n# Pad the second dimension of `a` to match the second dimension of `shape`\na = np.pad(a, ((0, 0), (shape[1] - a.shape[1], 0)), 'constant', constant_values=0)\n# Print the result\nprint(a)\n",
        "\nb = a.shape[0] // 3\nc = 3\nd = a.reshape(b, c)\nprint(d)",
        "\nresult = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(b.shape[0]):\n    for j in range(b.shape[1]):\n        if b[i][j] == 1:\n            result[i][j] = a[i][b[i][j] - 1][b[i][j] - 1]\n",
        "\nresult = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        result[i][j] = a[i][b[i][j], :]\nprint(result)\n",
        "\nresult = np.zeros((b.shape[0], a.shape[1]))\nfor i in range(b.shape[0]):\n    result[i, b[i]] = a[:, b[i]]\nprint(result)\n",
        "\nresult = np.sum(a[b[:, 0], b[:, 1], :], axis=2)\n",
        "\nresult = np.sum(a[b[:, 0], b[:, 1], :], axis=2)\n",
        "\nx = df['a']\ny = np.where(1 < x <= 4, df['b'], np.nan)\nresult = pd.DataFrame({'b': y})\nprint(result)\n",
        "\nimport numpy as np\nim = np.array([[0,0,0,0,0,0],\n               [0,0,1,1,1,0],\n               [0,1,1,0,1,0],\n               [0,0,0,1,1,0],\n               [0,0,0,0,0,0]])\n# Define a function to remove the peripheral zeros\ndef remove_peripheral_zeros(arr):\n    # Get the dimensions of the array\n    m, n = arr.shape\n    \n    # Create a boolean mask for the peripheral zeros\n    mask = np.zeros((m,n), dtype=bool)\n    for i in range(m):\n        mask[i][0] = arr[i][0] != 0\n        mask[i][-1] = arr[i][-1] != 0\n        for j in range(1,n-1):\n            mask[i][j] = arr[i][j] != arr[i][j-1] != arr[i][j+1]\n    \n    # Use the mask to remove the peripheral zeros\n    result = arr[mask]\n    \n    return result\n# Apply the function to the input array\nresult = remove_peripheral_zeros(im)\n# Print the result\nprint(result)\n",
        "\nmask = np.any(A, axis=0)\nA = A[mask, :]\nmask = np.any(A, axis=1)\nA = A[:, mask]\n",
        "\nimport numpy as np\nim = np.array([[1,1,1,1,1,5],\n               [1,0,0,1,2,0],\n               [2,1,0,0,1,0],\n               [1,0,0,7,1,0],\n               [1,0,0,0,0,0]])\n# Define a function to remove the peripheral non-zeros that fill an entire row/column\ndef remove_peripheral_zeros(arr):\n    # Initialize a copy of the input array\n    arr_copy = arr.copy()\n    \n    # Loop over each row and column\n    for i in range(arr.shape[0]):\n        for j in range(arr.shape[1]):\n            # Check if the current pixel is non-zero and if it is surrounded by zeros\n            if arr_copy[i][j] != 0 and (arr_copy[i][j-1] == 0 and arr_copy[i][j+1] == 0 or arr_copy[i-1][j] == 0 and arr_copy[i+1][j] == 0):\n                # Set the current pixel to zero\n                arr_copy[i][j] = 0\n    \n    # Return the modified array\n    return arr_copy\n# Apply the function to the input image\nresult = remove_peripheral_zeros(im)\n# Print the result\nprint(result)\n",
        "\nimport numpy as np\nim = np.array([[0,0,0,0,0,0],\n               [0,0,5,1,2,0],\n               [0,1,8,0,1,0],\n               [0,0,0,7,1,0],\n               [0,0,0,0,0,0]])\n# Use np.sum to get the sum of each row\n# Use np.where to check if the sum is 0\n# Use np.delete to remove the rows with a sum of 0\nresult = np.delete(im, np.where(np.sum(im, axis=1) == 0), axis=0)\nprint(result)\n"
    ],
    "Matplotlib": [
        "\nplt.scatter(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"x vs y\")\nplt.legend([\"x-y\"])\nplt.show()\n",
        "\n# Set the minor tick parameters\nminorticks_on = True\nminor_locator = plt.AutoLocator(5)\nminor_formatter = plt.FuncFormatter(lambda x, pos: '$' + '{:,.0f}'.format(x) + 'k')\n# Turn on the minor ticks on the y-axis\nplt.yticks(minor_locator, minor=True, labels=minor_formatter)\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# Turn on minor ticks\nplt.tick_params(axis='both', which='minor')\n# Set the minor tick length\nplt.tick_params(axis='both', which='minor', length=4)\n# Set the minor tick color\nplt.tick_params(axis='both', which='minor', color='gray')\n# Set the minor tick width\nplt.tick_params(axis='both', which='minor', width=1)\nplt.show()\n",
        "\nplt.xticks(rotation=90)\nplt.minorticks_on()\nplt.xlim(0, 10)\n",
        "\nfor style, color in zip(['-', '--', '-.'], ['b', 'r', 'g']):\n    plt.plot(x, np.random.randn(len(x)), label=style)\nplt.legend()\nplt.show()\n",
        "\nfor style, color in zip(['-', '--', '-.'], ['b', 'r', 'g']):\n    plt.plot(x, np.random.randn(len(x)), label=style)\nplt.legend()\nplt.show()\n",
        "\nplt.plot(x, y, 'o')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nplt.plot(x, y, marker='D', markeredgewidth=2, markersize=10)\nplt.show()\n",
        "\nplt.ylim(0, 40)\n",
        "\nplt.plot(x)\nx_range = [2, 4]\nplt.axvspan(x_range[0], x_range[1], facecolor='red', edgecolor='red', alpha=0.3)\nplt.show()\n",
        "\nx1, y1 = 0, 0\nx2, y2 = 1, 2\nplt.plot([x1, x2], [y1, y2], '-')\n",
        "\n# draw a line segment from (0,0) to (1,2)\nx1, y1 = 0, 0\nx2, y2 = 1, 2\nplt.plot([x1, x2], [y1, y2], '-', color='black', linewidth=2)\n# plot a scatter plot of (1,2) and (3,4)\nx = np.array([1, 3])\ny = np.array([2, 4])\nplt.scatter(x, y, marker='o', color='red', s=50)\n# set the title and labels for the axes\nplt.title('Line Segment and Scatter Plot')\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\n# display the plot\nplt.show()\n",
        "\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", kind=\"scatter\", hue=\"Gender\", data=df)\n",
        "\nplt.scatter(x, y)\nsns.distplot(y)\nplt.show()\n",
        "\nplt.figure()\nsns.lineplot(x=x, y=y)\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', markeredgewidth=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.legend(loc='best', fontsize=20, frameon=False, labels=['x', 'y', 'z'])\n",
        "\nplt.setp(l, facecolor='r', alpha=0.2)\n",
        "\n# Generate some random data\nx = np.random.randn(10)\ny = np.random.randn(10)\n# Plot the data with open markers and solid black borders\n(l,) = plt.plot(range(10), x, \"o-\", lw=5, markersize=30, markeredgecolor=\"k\")\n# Add a title and labels to the plot\nplt.title(\"Random Data\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Data\")\n# Display the plot\nplt.show()\n",
        "",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H, cmap='gray', origin='lower')\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray', vmin=H.min(), vmax=H.max())\nplt.show()\n",
        "\nplt.xlabel(\"X\")\nplt.gca().set_xlim([-1, 3])\nplt.gca().xaxis.label.set_rotation(90)\n",
        "\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
        "\nplt.title(myTitle, wrap=True)\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Random Data')\nplt.gca().invert_yaxis()\nplt.show()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nplt.scatter(x, y)\nplt.scatter(x, z)\nplt.scatter(y, z)\n",
        "\nplt.scatter(x, y, s=10, edgecolors='black', facecolors='blue')\n",
        "\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\nplt.gca().yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "\ndata = {\n    \"reports\": [4, 24, 31, 2, 3],\n    \"coverage\": [35050800, 54899767, 57890789, 62890798, 70897871],\n}\ndf = pd.DataFrame(data)\nsns.factorplot(y=\"coverage\", x=\"reports\", kind=\"bar\", data=df, label=\"Total\")\n# set the y axis tick labels to integers\nax = plt.gca()\nax.set_yticks(ax.get_yticks().astype(int))\n# format the y axis tick labels\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: \"{:,d}\".format(x)))\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nax = sns.lineplot(x=x, y=y, linestyle='--')\nplt.show()\n",
        "\nax1 = plt.subplot(121)\nax2 = plt.subplot(122)\nax1.plot(x, y1)\nax2.plot(x, y2)\nplt.show()\n",
        "\nax1 = plt.subplot(121)\nax2 = plt.subplot(122)\nplt.plot(x, y1, ax=ax1)\nplt.plot(x, y2, ax=ax2)\nplt.tight_layout()\nplt.show()\n",
        "\nplt.setp(plt.gca(), xticks=[], yticks=[])\n",
        "\nax = sns.lineplot(x=\"x\", y=\"y\", data=df)\nax.set_xticklabels([])\n",
        "\nplt.xticks([3, 4])\nplt.yticks([])\nplt.grid(True, which='major', color='k', linestyle='-')\n",
        "\nplt.yticks([3, 4], [\"3\", \"4\"])\nplt.hlines(y=3, xmin=0, xmax=9, color=\"k\")\nplt.hlines(y=4, xmin=0, xmax=9, color=\"k\")\n",
        "\nplt.yticks([3, 4], [\"3\", \"4\"])\nplt.xticks([1, 2], [\"1\", \"2\"])\nplt.grid(True, which='both', axis='y', linestyle='--')\nplt.grid(True, which='both', axis='x', linestyle='-')\n",
        "\nplt.grid()\n",
        "\nplt.legend(loc='lower right')\n",
        "\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\", rotation=90)\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\", rotation=90)\nplt.show()\nplt.clf()\n",
        "\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n",
        "\n# Move the x-axis of this heatmap to the top of the plot\nax.set_xlabel('')\nax.set_ylabel('')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.xlim([-20, 20])\nplt.ylim([-20, 20])\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.gca().invert_yaxis()\n# move the y axis ticks to the right\nplt.gca().set_yticks([i for i in range(10)])\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.xlabel(\"X\")\nplt.show()\n",
        "\ntips = sns.load_dataset(\"tips\")\n# Make a seaborn joint regression plot (kind='reg') of 'total_bill' and 'tip' in the tips dataframe\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\")\n# Change the line and scatter plot color to green\nsns.set_palette(\"green\")\n# Change the distribution plot color to blue\nplt.set_facecolor(\"blue\")\n# Show the plot\nplt.show()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"green\")\nplt.show()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\")\n",
        "\nax = df.plot(kind=\"bar\", x=\"celltype\", y=[\"s1\", \"s2\"])\nax.set_xticklabels(rotation=90)\nplt.show()\n",
        "\nplt.bar(x=\"celltype\", y=[\"s1\", \"s2\"], rot=45)\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.ylabel(\"Y\", color=\"red\")\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.xaxis.set_color(\"r\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x', fontsize=10)\nplt.ylabel('y', fontsize=10)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.gca().invert_yaxis()\nplt.show()\n",
        "\nplt.vlines(x= [0.22058956, 0.33088437, 2.20589566], ymin=0, ymax=1, color='r', linestyles='dashed')\n",
        "\nplt.xlabel(xlabels[2], ha='center')\nplt.ylabel(ylabels[::-1], va='center')\nplt.xticks(range(4), xlabels, rotation=90, ha='right')\n# Plot of heatmap with data in rand_mat and use xlabels for x-axis labels and ylabels as the y-axis labels\nplt.imshow(rand_mat, aspect='auto', cmap='coolwarm')\n# Make the x-axis tick labels appear on top of the heatmap\nplt.gca().invert_yaxis()\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n",
        "\nplt.subplot(2, 1, 1)\nplt.title('Y')\nplt.plot(y)\nplt.subplot(2, 1, 2)\nplt.title('Y')\nplt.plot(y)\n",
        "\nplt.scatter(df[\"bill_length_mm\"], df[\"bill_depth_mm\"], marker=\"o\", s=30)\nplt.show()\n",
        "\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n# make scatter plot of a over b and annotate each data point with correspond numbers in c\nplt.scatter(b, a, c=c)\nplt.xlabel('b')\nplt.ylabel('a')\nplt.title('Scatter Plot of a over b')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend Title\")\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", bbox_to_anchor=(1, 1), loc=\"upper left\", borderaxespad=0.)\nplt.legend(fontsize=16, frameon=False, facecolor=\"white\", edgecolor=\"black\")\n",
        "\nplt.hist(x, bins=10, density=True)\nplt.show()\nplt.hist(x, bins=10, density=True, histtype='stepfilled')\nplt.show()\n# Make the outline of each bar has a line width of 1.2\nplt.hist(x, bins=10, density=True, histtype='stepfilled', linewidth=1.2)\nplt.show()\n",
        "\nax1 = plt.subplot2grid((2, 1), (0, 0), colspan=3)\nax2 = plt.subplot2grid((2, 1), (1, 0), colspan=1)\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nbins = np.linspace(-1, 1, 100)\n# Plot two histograms of x and y on a single chart with matplotlib\n# Set the transparency of the histograms to be 0.5\nax = plt.gca()\nax.hist(x, bins=bins, alpha=0.5, label='x')\nax.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n",
        "\nplt.figure()\nax = plt.gca()\nbins = np.linspace(x.min(), x.max(), 10)\nplt.hist(x, bins=bins, label='x', histtype='stepfilled', color='blue')\nplt.hist(y, bins=bins, label='y', histtype='stepfilled', color='red')\nplt.legend()\nplt.show()\n",
        "\nx1, y1 = a, b\nx2, y2 = c, d\nplt.plot([x1, x2], [y1, y2], 'r-')\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n",
        "\ncmap1 = plt.get_cmap('jet')\ncmap2 = plt.get_cmap('coolwarm')\nfig, axs = plt.subplots(nrows=2, ncols=1)\nim1 = axs[0].imshow(x, cmap=cmap1)\nim2 = axs[1].imshow(y, cmap=cmap2)\ncbar = fig.add_axes([0.85, 0.15, 0.05, 0.7])\ncbar.set_label('Intensity')\nplt.colorbar(im1, cax=cbar)\nplt.colorbar(im2, cax=cbar)\nplt.show()\n",
        "\nplt.plot(x[:, 0], label=\"a\")\nplt.plot(x[:, 1], label=\"b\")\nplt.legend()\n",
        "\nplt.figure()\nax1 = plt.subplot2grid((2, 1), (0, 0))\nax1.plot(x, y)\nax1.set_title(\"Y over X\")\nax2 = plt.subplot2grid((2, 1), (1, 0))\nax2.plot(z, a)\nax2.set_title(\"Z over A\")\nplt.suptitle(\"Y and Z\")\nplt.show()\n",
        "\npoints = [(3, 5), (5, 10), (10, 150)]\n# plot a line plot for points in points.\n# Make the y-axis log scale\nplt.scatter(points[:, 0], points[:, 1], s=20)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('My Line Plot')\nplt.xscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title('My Plot', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n",
        "\nax.set_xticks(range(1, 11))\nax.set_xticklabels(range(1, 11))\n",
        "\n# Plot line segments according to the positions specified in lines\nfor line in lines:\n    x1, y1 = line[0]\n    x2, y2 = line[1]\n    plt.plot([x1, x2], [y1, y2], color=c[0])\n# Use the colors specified in c to color each line segment\nplt.scatter(c[1][0], c[1][1], c=c[1])\nplt.scatter(c[2][0], c[2][1], c=c[2])\nplt.show()\n",
        "\nplt.loglog(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(np.arange(1, 11))\nplt.yticks(np.arange(1, 11))\nplt.show()\n",
        "\nplt.figure()\nfor col in df.columns:\n    plt.plot(df.index, df[col], label=col)\nplt.legend()\nplt.show()\n",
        "\ndata = [1000, 1000, 5000, 3000, 4000, 16000, 2000]\n# Make a histogram of data and renormalize the data to sum up to 1\nhist, bins = np.histogram(data, bins=range(0, 20000, 1000))\nhist = hist / hist.sum()\n# Format the y tick labels into percentage and set y tick labels as 10%, 20%, etc.\nplt.gca().set_yticks([i / 10.0 for i in range(1, 11)])\nplt.gca().set_yticklabels(['%d%%' % i for i in range(1, 11)])\n# Plot the histogram\nplt.bar(bins[:-1], hist, align='center')\nplt.xlabel('Number of Students')\nplt.ylabel('Percentage of Students')\nplt.show()\n",
        "\nplt.plot(x, y, 'o', alpha=0.5)\n",
        "\nplt.figure()\nax1 = plt.subplot2grid((2, 1), (0, 0), colspan=1)\nax1.plot(x, y)\nax1.set_title(\"y over x\")\nax1.set_ylabel(\"y\")\nax2 = plt.subplot2grid((2, 1), (1, 0), colspan=1)\nax2.plot(z, a)\nax2.set_title(\"a over z\")\nax2.set_ylabel(\"a\")\nfiglegend(axes=[ax1, ax2], labels=[\"y\", \"a\"], loc=\"upper left\")\nplt.show()\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n# Make 2 subplots.\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=False)\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\n# Add titles and labels to the subplots\nax1.set_title(\"Bill Depth vs. Bill Length\")\nax1.set_xlabel(\"Bill Length (mm)\")\nax1.set_ylabel(\"Bill Depth (mm)\")\nax2.set_title(\"Flipper Length vs. Bill Length\")\nax2.set_xlabel(\"Bill Length (mm)\")\nax2.set_ylabel(\"Flipper Length (mm)\")\n# Display the plot\nplt.show()\n",
        "\nax.set_xticklabels([\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\"])\n",
        "\nplt.plot(x, y)\nplt.legend(labels=[\"y\"], loc=\"upper left\", frameon=False, ncol=1, handletextpad=0.1, borderaxespad=0.1)\nplt.ylabel(\"y\")\nplt.xlabel(\"x\")\nplt.title(\"Plot of y over x\")\nplt.savefig(\"plot.png\")\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(y, x)\nplt.xticks(range(0, 10, 2))\n# Add extra ticks [2.1, 3, 7.6] to existing xticks\nnew_ticks = [2.1, 3, 7.6]\nnew_ticks = np.append(new_ticks, plt.xticks())\nplt.xticks(new_ticks)\n",
        "\nplt.xticks(rotation=60, ha='left')\n",
        "\nplt.gca().set_yticklabels(plt.gca().get_yticklabels(), rotation=60)\nplt.gca().invert_yaxis()\n",
        "\nplt.xticks(rotation=90)\nplt.setp(plt.gca().get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\nplt.gca().set_ylim(0, 10)\nplt.gca().invert_yaxis()\nplt.gca().set_xlabel(\"Year\", rotation=90, ha=\"right\", fontsize=12)\nplt.gca().set_ylabel(\"Value\", fontsize=12)\nplt.gca().set_title(\"Transparent X-axis Labels\", fontsize=14)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=90, ha=\"right\", fontsize=10, rotation_mode=\"anchor\", color=\"w\", alpha=0.5)\n",
        "\nplt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\nplt.yticks([i for i in range(10) if i != 0], [str(i) for i in range(10) if i != 0])\nplt.gca().invert_yaxis()\n",
        "\nplt.figure()\nax = plt.gca()\nax.plot(x, y)\nax.set_title(\"Figure\")\nplt.show()\n",
        "\nplt.plot(df.index, df[\"Type A\"], label=\"Type A\")\nplt.plot(df.index, df[\"Type B\"], label=\"Type B\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, marker='v', markerfacecolor='none', hatch='//')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='v', edgecolors='none')\n",
        "\nplt.scatter(x, y, marker='*')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='*', s=100, hatch='^v')\nplt.show()\n",
        "\nxlim = [1, 5]\nylim = [1, 4]\nplt.imshow(data, cmap='viridis', aspect='auto', extent=(xlim, ylim))\nplt.show()\n",
        "\nplt.stem(x, y, orientation='horizontal')\nplt.show()\n",
        "\nplt.bar(d.keys(), d.values(), color=c.get)\nplt.show()\n",
        "\nplt.axvline(x=3, color='k', linestyle='--', label='cutoff')\nplt.legend()\n",
        "\nplt.subplots(polar=True)\nplt.bar(labels, height, width=0.5)\nplt.xticks(rotation=90)\nplt.show()\n",
        "\nplt.pie(data, labels=l, autopct='%1.1f%%', shadow=True, startangle=90,\n        wedgewidth=0.4)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(color='blue', linestyle='dashed')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.yticks([])\nplt.grid(color='gray', linestyle='dashed')\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90, labeldistance=1.1)\nplt.legend(loc='upper left')\nplt.title('Activity Distribution')\nplt.ylabel('Percentage')\nplt.xlabel('Activity')\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90, labeldistance=1.1)\nplt.legend(loc='upper left')\nplt.title('Activity Distribution')\nplt.ylabel('Percentage')\nplt.xlabel('Activity')\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markeredgecolor='black', markeredgewidth=2, alpha=0.5)\n",
        "\nplt.axvline(55, color=\"green\")\n",
        "\nplt.bar(range(len(blue_bar)), blue_bar, color='blue', label='Blue')\nplt.bar(range(len(blue_bar)) + 0.4, orange_bar, color='orange', label='Orange')\nplt.legend()\n",
        "\nax1 = plt.subplot(121)\nax1.plot(x, y, label=\"y over x\")\nax1.legend()\nax2 = plt.subplot(122)\nax2.plot(a, z, label=\"z over a\")\nax2.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, c=y, cmap='spectral')\n",
        "\nplt.plot(x, y)\nplt.xticks(range(10), rotation=90)\nplt.show()\n",
        "\nplt.figure(figsize=(12, 8))\nsns.factorplot(x=\"species\", y=\"bill_length_mm\", data=df, hue=\"sex\", col=\"sex\", size=8)\n# Do not share y axis across subplots\nplt.setp(plt.gca().get_axes(), yticks=[])\n",
        "\n# draw a circle centered at (0.5, 0.5) with radius 0.2\nplt.plot([0.5, 0.5], [0.5, -0.5], 'ro')\n",
        "\nplt.plot(x, y)\nplt.title('$\\\\phi$', fontweight='bold')\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False, borderaxespad=0.1)\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handles=[plt.Line2D([0], [0], color=\"black\", label=\"Line\", lw=3)], loc=\"upper left\")\n# Adjust the length of the legend handle to be 0.3\nplt.legend(handles=[plt.Line2D([0], [0], color=\"black\", label=\"Line\", lw=3)], loc=\"upper left\", bbox_to_anchor=(1.05, 1), frameon=False)\n",
        "\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=2, labels=['Line', 'Flipped'])\n",
        "\nplt.legend()\nplt.scatter(x, y, marker=\"o\", label=\"Scatter\")\n",
        "\ndata = np.random.random((10, 10))\n# plot the 2d matrix data with a colorbar\nplt.imshow(data, cmap='viridis', aspect='auto')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Figure 1\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.gca().set_title(\"Figure 1\", fontweight=\"bold\")\n",
        "\nsns.pairplot(data=df, x=\"x\", y=\"y\", hue=\"id\", legend=False)\n",
        "\nplt.plot(y, x)\nplt.xaxis.invert()\n",
        "\nplt.scatter(x, y)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n",
        "\nplt.scatter(x, y, c='r', edgecolors='k')\nplt.show()\n",
        "\nplt.subplot(2, 2, 1)\nplt.plot(x, y)\nplt.subplot(2, 2, 2)\nplt.plot(x, y)\nplt.subplot(2, 2, 3)\nplt.plot(x, y)\nplt.subplot(2, 2, 4)\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.hist(x, bins=5, range=(0, 10), normed=True, histtype='bar', width=2)\n",
        "\nplt.plot(x, y, 'o')\nplt.fill_between(x, y - error, y + error, alpha=0.2)\n",
        "\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Contour Plot')\n# draw x=0 and y=0 axis in my contour plot with white color\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.plot([-5, 5], [-5, 5], 'w')\n",
        "\nax.errorbar(box_position, box_height, yerr=box_errors, color=c)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n# Plot y over x and z over a in two side-by-side subplots\nfig, axs = plt.subplots(nrows=2, sharex=True)\naxs[0].plot(x, y)\naxs[0].set_title(\"Y\")\naxs[1].plot(z, a)\naxs[1].set_title(\"Z\", rotation=90, va=\"top\")\nplt.tight_layout()\n# Raise the title of the second subplot to be higher than the first one\naxs[1].title.set_position((1.05, 0.5))\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make 4 by 4 subplots with a figure size (5,5)\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(5, 5))\n# plot y over x in each subplot and show axis tick labels\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_ylabel('y')\n        axs[i, j].set_xlabel('x')\n# give enough spacing between subplots so the tick labels don't overlap\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\n# show the plot\nplt.show()\n",
        "\nplt.imshow(d, cmap='gray', aspect='auto')\nplt.axis('off')\nplt.tight_layout()\nplt.figure(figsize=(8, 8))\n",
        "\nplt.style.use('seaborn-whitegrid')\nsns.set(style=\"whitegrid\")\nsns.lmplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, fit_reg=True)\nplt.title(\"Bill Depth vs Bill Length\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Bill Depth (mm)\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(range(10), rotation=90)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(range(10), rotation=90)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks(range(10))\nplt.yticks([])\nplt.show()\n",
        "\nplt.figure(figsize=(12, 8))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\nplt.suptitle(\"Group: Fat and No Fat\", fontsize=16)\nplt.subplots_adjust(top=0.85)\nplt.show()\n",
        "\nplt.figure(figsize=(12, 8))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\", legend=False)\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Exercise Time\")\nplt.show()\n",
        "\nax = df.plot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", kind=\"scatter\", legend=False)\nax[0].set_ylabel(\"\")\nax[1].set_ylabel(\"\")\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.plot(x, y)\nplt.figure(figsize=(5, 5))\nplt.tight_layout()\nplt.savefig(\"plot.png\", dpi=300)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(0, 1))\nplt.frameon(False)\n",
        "\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='a+b')\nplt.legend()\nplt.show()\n",
        "\nplt.figure()\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df)\nplt.legend(title=\"Species\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.show()\n",
        "\nsns.set(style=\"whitegrid\")\nfig, axs = plt.subplots(nrows=3, ncols=1, sharex=True)\nsns.FacetGrid(df, col=\"b\", size=4).map(plt.scatter, \"c\", \"a\").add_legend()\nfor ax in axs.ravel():\n    ax.set_xticks(np.arange(1, 31) + 0.5)\n    ax.set_xticklabels(np.arange(1, 31) + 1)\nplt.show()\n",
        "\nax.view_init(elev=50, azim=100)\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\n",
        "\nx = np.random.random((10, 10))\nfrom matplotlib import gridspec\nnrow = 2\nncol = 2\nfig = plt.figure(figsize=(ncol + 1, nrow + 1))\n# Make a 2x2 subplots with fig and plot x in each subplot as an image\nsubplot_grid = gridspec.GridSpec(nrow, ncol, wspace=0, hspace=0)\nax1 = fig.add_subplot(subplot_grid[:, 0])\nax2 = fig.add_subplot(subplot_grid[:, 1])\nax3 = fig.add_subplot(subplot_grid[0, 1])\nax4 = fig.add_subplot(subplot_grid[1, 1])\n# Remove the space between each subplot and make the subplot adjacent to each other\nplt.subplots_adjust(hspace=0)\n# Remove the axis ticks from each subplot\nfor ax in fig.axes:\n    ax.set_xticks([])\n    ax.set_yticks([])\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\n# Convert the list of integers to a tensor of shape (5, 10)\nindices = tf.constant(labels, dtype=tf.int32)\nvalues = tf.constant([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                      [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n                      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n                      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\nresult = tf.SparseTensor(indices, values, dense_shape=(5, 10))\nprint(result)\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\n# Convert the list of integers to a tensor of shape (5, 10)\nindices = tf.constant(labels, dtype=tf.int32)\nvalues = tf.constant([[1] * 10 for _ in range(5)], dtype=tf.int32)\nresult = tf.SparseTensor(indices, values, dense_shape=(5, 10))\n# Print the result\nprint(result.numpy())\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\n# Convert the list of integers to a tensor\nindices = tf.constant(labels, dtype=tf.int32)\nlabels_one_hot = tf.one_hot(indices, depth=10)\n# Transpose the one-hot tensor to get the desired shape\nresult = tf.transpose(labels_one_hot, perm=[0, 2, 1])\nprint(result)\n",
        "\nimport tensorflow as tf\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    # Convert labels to one-hot tensor\n    num_classes = len(example_labels)\n    labels = tf.one_hot(labels, depth=num_classes)\n    # Extract the row corresponding to the given labels\n    result = tf.gather(labels, example_labels)\n    return result\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\n# Convert labels to one-hot encoding\nnum_classes = 10\none_hot_labels = tf.one_hot(labels, depth=num_classes)\n# Select the one-hot encoding for the desired class\nresult = tf.gather(one_hot_labels, indices=0)\nprint(result)\n",
        "\ndef my_map_func(i):\n  return [i, i+1, i+2]\n# Create a dataset from the input tensor\nds = tf.data.Dataset.from_tensor_slices(input)\n# Apply the map function to the dataset\nelement = ds.map(map_func=my_map_func)\n# Print the result\nprint(result)\n[Instruction]",
        "\nresult = []\nfor i in input:\n    result.append(i)\n    result.append(i+1)\n    result.append(i+2)\nreturn result",
        "\nresult = tf.ones((8,), dtype=tf.int32) * 1\nresult = tf.scatter_nd(indices=tf.range(8), updates=lengths, shape=(4,))\nprint(result)",
        "\nresult = tf.pad(lengths, [[0, 1], [0, 0]], constant_values=0)",
        "\nmask = tf.ones((len(lengths), 8), dtype=tf.float32)\nmask = tf.pad(mask, [[0, 0], [0, 0], [0, (8 - tf.reduce_sum(lengths))]], \"CONSTANT\")\nresult = tf.where(tf.equal(lengths, tf.range(8)), mask, tf.zeros_like(mask))\nprint(result)",
        "\nresult = tf.ones((len(example_lengths), 8)) * 0\nresult[..., :len(example_lengths)] = tf.cast(tf.range(len(example_lengths)), tf.float32)\nreturn result",
        "\nresult = tf.ones((8,), dtype=tf.int32) * 0\nfor i, length in enumerate(lengths):\n    result[i+1:i+1+length] = 1\nprint(result)",
        "\nresult = tf.math.multiply(a, b)\n",
        "\nresult = tf.concat([a,b], axis=0)\nreturn result",
        "\nresult = tf.reshape(a, [50, 100, 512])\nprint(result)",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\n# Add a new dimension to the tensor\na = tf.expand_dims(a, axis=3)\n# Print the result\nprint(result)\n",
        "\nb = tf.expand_dims(a, axis=0)\nresult = tf.reshape(b, [1, 50, 100, 1, 512])\nprint(result)",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100, size=(5, 3)))\n# Sum along the first axis (columns) using tf.reduce_sum\nresult = tf.reduce_sum(A, axis=0)\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100, size=(5, 3)))\n# The equivalent of np.prod(A, axis=1) in TensorFlow is tf.reduce_prod(A, axis=1).\nresult = tf.reduce_prod(A, axis=1)\nprint(result)\n",
        "\nresult = tf.reciprocal(A)\n",
        "\nc = tf.math.sqrt(tf.square(a) + tf.square(b))\nresult = tf.reduce_sum(c, axis=1)\nprint(result)",
        "\nd = tf.square(tf.sub(a, b))\nresult = tf.reduce_sum(d, axis=1)\nprint(result)",
        "\nresult = tf.square(tf.sub(lhs, rhs))\nresult = tf.reduce_sum(result, axis=1)\nreturn result",
        "\nimport tensorflow as tf\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\n# Define a slice operation to extract the desired elements from x\nslice_op = tf.slice(x, [0, y], [-1, 2])\n# Perform the slice operation and assign the result to result\nresult = slice_op\n# Print the result\nprint(result.numpy())\n",
        "\nm = tf.gather(x, row, axis=0)\nm = tf.gather(m, col, axis=1)\nprint(result)",
        "\ndef f(x=example_x,y=example_y,z=example_z):\n    # Get the indices of the selected rows\n    row_indices = tf.gather(tf.range(tf.shape(x)[0]), y)\n    \n    # Get the selected columns\n    col_indices = tf.gather(tf.range(tf.shape(x)[1]), z)\n    \n    # Use the indices to slice the input tensor\n    sliced_x = tf.gather(x, indices=tf.stack([row_indices, col_indices], axis=1))\n    \n    # Return the sliced tensor\n    return sliced_x\n",
        "\nC = tf.einsum('bik,bjk->bij', A, B)\n",
        "\nC = tf.einsum('bns,bnk->bnk', A, B)\n",
        "\n# Convert bytes to strings using TensorFlow 2.x\nresult = []\nfor x in x:\n    result.append(tf.io.decode_raw(x, tf.io.VarLenFeature(tf.string)))\nprint(result)",
        "\nresult = []\nfor x in x:\n    result.append(tf.io.decode_raw(x, tf.io.VarLenFeature(tf.int64)))\nreturn result",
        "\ny = tf.math.reduce_sum(x, axis=-1) / tf.math.reduce_sum(tf.math.not_equal(x, 0), axis=-1)\nprint(y)",
        "\ny = tf.math.reduce_mean(x, axis=-1, keepdims=True)\n# Calculate the variance along the non-padded dimensions\nvariance = tf.math.reduce_mean((x - y) ** 2, axis=-1, keepdims=True)\n# Calculate the square root of the variance\nstd = tf.math.sqrt(variance + tf.keras.backend.epsilon())\n# Apply the calculated std and y to the input tensor\nresult = (x - y) / std + y\n",
        "\nresult = tf.reduce_sum(x, axis=-1) / tf.reduce_sum(tf.cast(x != 0, dtype=tf.float32), axis=-1)\nreturn result",
        "\ntf.compat.v1.enable_eager_execution()\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n",
        "\nresult = tf.argmax(a, axis=1)\nresult = tf.reshape(result, [-1])\nresult = tf.math.reduce_sum(tf.cast(tf.equal(result, y), tf.int32))\nprint(result)",
        "\nmax_indices = tf.argmax(a, axis=1)\nsorted_indices = tf.sort(max_indices)\nsorted_indices = tf.reshape(sorted_indices, [-1])\nresult = tf.gather(a, sorted_indices)\n",
        "\nscores, indices = tf.nn.top_k(scores, k=1)\nresult = tf.gather(indices, tf.argmax(scores, axis=1))\n",
        "result = tf.argmin(a, axis=1)\nprint(result)",
        "\nmodel.save(\"my_model\")\n",
        "\nimport tensorflow as tf\nseed_x = 10\n# Generate a tensor of random integers from 1 to 4, inclusive\nresult = tf.random.uniform([10], minval=1, maxval=4, seed=seed_x)\n# Print the result\nprint(result)\n",
        "\nimport tensorflow as tf\nseed_x = 10\n# Generate a tensor of random integers from a uniform distribution\nresult = tf.random.uniform([114], minval=2, maxval=5, dtype=tf.int32, seed=seed_x)\n# Print the result\nprint(result)\n",
        "\nimport tensorflow as tf\ndef f(seed_x=10):\n    # Set the random seed\n    tf.random.set_seed(seed_x)\n    # Create a tensor of random integers from a uniform distribution\n    result = tf.random.uniform([10], minval=1, maxval=4, dtype=tf.int32)\n    return result",
        "\nimport tensorflow as tf\n# output the version of tensorflow into variable 'result'\nresult = tf.__version__\nprint(result)\n"
    ],
    "Scipy": [
        "\nA = np.polyfit(x, y, 1)\nB = np.polyfit(x, y, 2)\nresult = np.array([A[0], B[0]])\nprint(result)",
        "\nmodel = np.polyfit(x, y, 1)\nA, B = model",
        "\nfrom scipy.optimize import curve_fit\ndef exponential_fit(y, x, p0):\n    A, B, C = p0\n    y_fit = A * np.exp(B * x) + C\n    return curve_fit(exponential_fit, x, y, p0=p0)\n# Call the function with the initial guess p0\nresult = curve_fit(exponential_fit, x, y, p0=p0)\nprint(result)",
        "\ntest_stat = stats.kstest(x, y)\nprint(test_stat)",
        "\ntest_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\nprint(result)\n",
        "\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\n# Define the objective function to be minimized\ndef f(params):\n    a, b, c = params\n    return ((a + b - c) - 2) ** 2 + ((3 * a - b - c) ** 2) + sin(b) + cos(b) + 4\n# Use the scipy.optimize.minimize function to find the minimum\nresult = optimize.minimize(f, initial_guess, method='Nelder-Mead')\nprint(result.x)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)",
        "\np_values = scipy.stats.norm.cdf(z_scores, loc=mu, scale=sigma)",
        "\nz_scores = scipy.stats.norm.ppf(p_values)",
        "\ndist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\n# Calculate the expected value and median of the lognormal distribution\nexpected_value, median = stats.lognorm.stats(total, loc=mu, scale=stddev)\nprint(expected_value, median)\n",
        "\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\nresult = sa.multiply(sb)\nprint(result)\n",
        "\ndef f(sA = example_sA, sB = example_sB):\n    # Multiply the sparse matrices\n    result = sA.multiply(sB)\n    # Convert the result back to a CSR matrix\n    return sparse.csr_matrix(result)\n",
        "\n# Define a function to interpolate the values\ndef interpolate(points, V, request):\n    # Create a grid of points\n    x, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n    # Use scipy.interpolate.griddata to perform the interpolation\n    result = scipy.interpolate.griddata(points, V, (x, y, z), method='linear')\n    # Return the interpolated value\n    return result\n# Call the interpolate function with the given data\nresult = interpolate(points, V, request)\nprint(result)\n",
        "\n# Define a function to interpolate the values\ndef interpolate(points, V, request):\n    # Create a grid of points\n    x, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n    # Use scipy.interpolate.LinearNDInterpolator to interpolate the values\n    interp = scipy.interpolate.LinearNDInterpolator(\n        (x, y, z), V)\n    # Interpolate the values at the request points\n    result = interp(request[:, 0], request[:, 1], request[:, 2])\n    return result\n# Call the interpolate function with the points and V arrays\nresult = interpolate(points, V, request)\n# Print the result\nprint(result)\n",
        "\nrotated_coords = rotate(data_orig, angle, reshape=False)\nx_rot, y_rot = rotated_coords[y0, x0]\nprint(data_rot, (x_rot, y_rot))\n",
        "diag = M.diagonal()\nresult = diag[0]",
        "\nimport scipy.stats as stats\nfrom scipy.stats import kstest\nresult = kstest(times, \"uniform\")\nprint(result)",
        "\nimport scipy.stats\nfrom scipy.stats import kstest\ndef f(times, rate, T):\n    times = np.array(times)\n    uniform_hypothesis = np.linspace(0, T, times.shape[0])\n    ks_result = kstest(times, uniform_hypothesis)\n    return ks_result",
        "\n# Run the Kolmogorov-Smirnov test to check if the times are uniformly distributed\nfrom scipy.stats import kstest\ntimes = poisson_simul(rate, T)\nresult = kstest(times, \"uniform\")\n# Interpret the result\nif result[1] < 0.05:\n    print(\"The times are not uniformly distributed.\")\nelse:\n    print(\"The times are uniformly distributed at the 95% confidence level.\")\n",
        "\nFeature = sparse.vstack( [c1, c2] )\nFeature = csr_matrix(Feature)\nprint(Feature)",
        "\nFeature = sparse.hstack((c1, c2))\nprint(Feature)",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n# Create a new sparse matrix with the concatenated features\nFeature = sparse.vstack((c1, c2))\n# Print the resulting matrix\nprint(Feature)\n",
        "# Compute the distance matrix\ndist = scipy.spatial.distance.cdist(points1, points2)\n# Apply Hungarian algorithm to find optimal assignment\nresult = scipy.optimize.linear_sum_assignment(dist)\n# Map points from set 1 to set 2 using the assignment\nmapped_points = [points2[i] for i in result[1]]\nprint(mapped_points)",
        "\ndef manhattan_distance(p1, p2):\n    return np.sum(np.abs(p1-p2))\ndef assign_points(points1, points2):\n    # Initialize distance matrix\n    D = np.zeros((len(points1), len(points2)))\n    for i, p1 in enumerate(points1):\n        for j, p2 in enumerate(points2):\n            D[i,j] = manhattan_distance(p1, p2)\n    # Find minimum spanning tree using scipy.spatial.Delaunay\n    delaunay = scipy.spatial.Delaunay(points2)\n    tree = delaunay.find_simplex_tree()\n    edges = tree.simplices\n    # Assign points\n    result = []\n    for edge in edges:\n        p1, p2 = edge\n        result.append(p2)\n        result.append(p1)\n    return result\n# Example usage\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\nresult = assign_points(points1, points2)\nprint(result)",
        "\nb.setdiag(0, drop=True)\n",
        "\nlabels, n_regions = ndimage.label(img > threshold)\nresult = n_regions\n",
        "\nlabels, n_regions = ndimage.label(img < threshold)\nresult = n_regions\n",
        "\ndef f(img = example_img):\n    threshold = 0.75\n    # Count the number of regions of cells which value exceeds a given threshold, i.e. 0.75\n    labels = ndimage.label(img > threshold)\n    num_regions = len(ndimage.find_objects(labels))\n    return num_regions",
        "\ndef find_regions(img, threshold):\n    labels = ndimage.label(img > threshold)\n    regions = []\n    for i in range(1, labels.max() + 1):\n        region = []\n        for x, y in ndimage.find_objects(labels == i):\n            region.append((x, y))\n        regions.append(region)\n    return regions\nresult = find_regions(img, threshold)\ndistances = []\nfor region in result:\n    x_min, y_min = region[0]\n    x_max, y_max = region[-1]\n    center = [(x_min + x_max) / 2, (y_min + y_max) / 2]\n    distances.append(np.linalg.norm(center - (0, 0)))\nprint(distances)",
        "\ndef make_symmetric(self):\n    \"\"\"\n    Make the sparse matrix symmetric.\n    \"\"\"\n    for i in range(self.shape[0]):\n        for j in range(i, self.shape[0]):\n            if self[i, j] != self[j, i]:\n                self[j, i] = self[i, j]",
        "\ndef f(sA = example_sA):\n    # make a copy of the matrix\n    sA_copy = sA.copy()\n    \n    # loop over all non-zero elements\n    for i, j, v in sA_copy.nonzero():\n        # check if the element is already present\n        if not sA_copy.has_key((j, i)):\n            # add the element with the same value\n            sA_copy[(j, i)] = v\n    \n    # make the matrix symmetric\n    sA_copy.make_symmetric()\n    \n    # return the modified matrix\n    return sA_copy",
        "\nimport numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\n# Use scipy.ndimage.binary_erosion and scipy.ndimage.binary_dilation to remove small isolated regions\nsquare = scipy.ndimage.binary_erosion(square, iterations=1)\nsquare = scipy.ndimage.binary_dilation(square, iterations=1)\n# Use scipy.ndimage.label to find connected components\nlabels = scipy.ndimage.label(square)\n# Use numpy.sum to count the number of pixels in each component\nnum_pixels = np.sum(labels == 1, axis=0)\n# Remove components with only one pixel\nto_remove = np.where(num_pixels == 1)[0]\nsquare[labels == to_remove] = 0\n# Display the result\nprint(square)\n",
        "\n# Use scipy.ndimage.label to find connected components\nlabels = scipy.ndimage.label(square)\n# Find all components with only one pixel\nsingle_cells = np.where(labels == 1)[0]\n# Get the coordinates of these cells\nsingle_cell_coords = np.array([[x, y] for x, y in zip(*single_cells)])\n# Check if each cell is surrounded by other \"0\"s\nsurrounded = np.zeros_like(square, dtype=bool)\nfor coord in single_cell_coords:\n    x, y = coord\n    surrounded[x - 1:x + 2, y - 1:y + 2] = (square == 0) & (square[x - 1:x + 2, y - 1:y + 2] != 0)\n# Set the values of isolated single cells to 0\nsquare[surrounded] = 0\n# Use scipy.ndimage.binary_fill_holes to fill any remaining holes\nsquare = scipy.ndimage.binary_fill_holes(square)\n# Convert back to uint8 and print the result\nsquare = square.astype(np.uint8)\nprint(square)\n",
        "\nmean = np.mean(col)\nstandard_deviation = np.std(col)",
        "\nmax_val = col.max()\nmin_val = col.min()\nprint(max_val)\nprint(min_val)",
        "\nmedian = np.median(col.data)\nmode = np.mode(col.data)[0][0]\nprint(median)\nprint(mode)",
        "\n# Define a function to generate the Fourier series coefficients\ndef fourier_series(x, degree):\n    # Initialize the coefficients to zero\n    a = np.zeros(degree + 1)\n    \n    # Loop over the harmonics and calculate the coefficients\n    for n in range(1, degree + 1):\n        a[n] = (1/np.pi) * np.cos(n * np.pi / tau * x)\n    \n    # Return the coefficients\n    return a\n# Call the function with the desired degree of the Fourier series\npopt, pcov = curve_fit(fourier_series, z, Ua, p0=[0] * (degree + 1))\n",
        "\ndistances, indices = scipy.spatial.distance.cdist(example_array, example_array, 'euclidean')\nresult = np.zeros((len(example_array), len(example_array)), dtype=int)\nresult[indices[:, 1], indices[:, 0]] = distances\nprint(result)\n",
        "\nfrom scipy.spatial.distance import cdist\ndistances = cdist(example_array, example_array, 'manhattan')\nresult = np.zeros((len(example_array), len(example_array)), dtype=int)\nfor i in range(len(example_array)):\n    for j in range(len(example_array)):\n        result[i][j] = distances[i][j].min()\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.spatial.distance\ndef f(example_array):\n    # Calculate Euclidean distances\n    dist = np.zeros((example_array.shape[0], example_array.shape[0]))\n    for i in range(example_array.shape[0]):\n        for j in range(i + 1, example_array.shape[0]):\n            dist[i, j] = dist[j, i] = scipy.spatial.distance.euclidean(example_array[i], example_array[j])\n    # Find minimum distances\n    min_dist = np.zeros_like(dist)\n    min_dist[dist != 0] = np.min(dist[dist != 0], axis=1)\n    # Convert back to original coordinates\n    from_to = np.zeros((example_array.shape[0], 2))\n    from_to[:, 0] = np.arange(example_array.shape[0])\n    from_to[:, 1] = np.arange(example_array.shape[0]) + 1\n    from_to[min_dist != 0, 1] = min_dist[min_dist != 0, 0] // example_array.shape[1]\n    from_to[min_dist != 0, 0] -= 1\n    return from_to\n",
        "\nfrom scipy import interpolate\nimport numpy as np\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85.36, 93.24],\n              [61.48, 69.31, 75.36, 81.87, 89.35],\n              [57.61, 65.75, 71.7, 79.1, 86.13],\n              [55.12, 63.34, 69.32, 77.29, 83.88],\n              [54.58, 62.54, 68.7, 76.72, 82.92],\n              [56.58, 63.87, 70.3, 77.69, 83.53],\n              [61.67, 67.79, 74.41, 80.43, 85.86],\n              [70.08, 74.62, 80.93, 85.06, 89.84]])\nx_val = np.linspace(-1, 1, 100)\n# Extrapolate the data using B-spline interpolation\ntck = interpolate.splrep(x[:, -1], y[:, -1], k = 2, s = 4)\nresult = interpolate.splev(x_val, tck, der = 0)\nprint(result)",
        "\n# Import the necessary libraries\nimport numpy as np\nimport scipy.stats as ss\n# Create the four datasets\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\nx3 = [34.0, 35.0, 39.0, 40.0, 43.0, 43.0, 44.0, 45.0]\nx4 = [34.0, 34.8, 34.8, 35.4, 37.2, 37.8, 41.2, 42.8]\n# Create a list to store the results\nresults = []\n# Loop through each dataset and perform the Anderson-Darling test\nfor data in [x1, x2, x3, x4]:\n    statistic, critical_values, significance_level = ss.anderson(data)\n    results.append([statistic, critical_values, significance_level])\n# Print the results\nprint(results)\n",
        "\ny1 = np.array(x1)\ny2 = np.array(x2)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndef tau1(x):\n    y = np.array(df['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\nprint(df)\n",
        "\ndef is_csr_matrix_only_zeroes(my_csr_matrix):\n    return(my_csr_matrix.nnz == 0)\n",
        "\ndef is_lil_matrix_only_zeroes(my_lil_matrix):\n    return(len(my_lil_matrix.nonzero()[0]) == my_lil_matrix.shape[0]*my_lil_matrix.shape[1])\n",
        "\nblock_diags = []\nfor i in range(len(a)):\n    block_diags.append(block_diag(a[i]))\nresult = block_diags\n",
        "\np_value = stats.ranksums(pre_course_scores, during_course_scores)[1]\nprint(p_value)",
        "\nimport numpy as np\nfrom scipy.stats import ranksums\ndef f(pre_course_scores, during_course_scores):\n    # Calculate the ranksums result\n    ranksums_result = ranksums(pre_course_scores, during_course_scores)\n    \n    # Extract the p-value from the result\n    p_value = ranksums_result.pvalue\n    \n    return p_value\n",
        "\nmu = np.mean(a)\nsigma = np.std(a)\nn = len(a)\nkurtosis_result = (mu**4 - 3*mu**2*sigma**2 + 2*sigma**4)/(n**4 - 3*n**2*sigma**2 + sigma**4)\nprint(kurtosis_result)",
        "\nmu = np.mean(a)\nsigma = np.std(a)\nkurtosis_result = scipy.stats.kurtosis(a, fisher=True) - 3 * (mu**4) / (sigma**6)\nprint(kurtosis_result)",
        "\ninterpolated = scipy.interpolate.interp2d(t, s, z, kind='cubic')\nresult = interpolated(t, s)\nprint(result)",
        "\ndef f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    # interpolate the function values at the given points\n    result = scipy.interpolate.griddata(s, t, z, method='cubic')\n    return result",
        "\ndef count_points_in_cells(vor, extra_points):\n    cell_indices = vor.regions\n    point_indices = []\n    for point in extra_points:\n        x, y = point\n        x_min, y_min = vor.vertices[0][0], vor.vertices[0][1]\n        x_max, y_max = vor.vertices[-1][0], vor.vertices[-1][1]\n        if x_min <= x <= x_max and y_min <= y <= y_max:\n            cell_index = vor.point_region(x, y)\n            point_indices.append(cell_indices.index(cell_index))\n    return point_indices\n# Example usage\ncell_indices = count_points_in_cells(vor, extraPoints)\nprint(cell_indices)\n",
        "\ndef count_points_in_cells(vor, extra_points):\n    cell_indices = vor.regions\n    point_indices = []\n    for point in extra_points:\n        x, y = point\n        cell_index = vor.point_region(x, y)\n        point_indices.append(cell_indices.index(cell_index))\n    return point_indices\n# Example usage\ncell_indices = vor.regions\npoint_indices = count_points_in_cells(vor, extraPoints)\nprint(point_indices)",
        "\nsparse_matrix = sparse.csr_matrix((data, (row, col)), shape=(len(vectors), max_vector_size))\nprint(sparse_matrix)",
        "\nimport numpy as np\nimport scipy.ndimage\na= np.zeros((5, 5))\na[1:4, 1:4] = np.arange(3*3).reshape((3, 3))\n# Shift the filter one cell to the right\nfilter_shifted = np.roll(filter, 1, axis=0)\n# Apply the shifted filter to the array\nb = nd.median_filter(a, filter_shifted)\nprint(b)\n",
        "\nvalue = M.getrow(row).toarray()[0][column]\nprint(value)",
        "\nresult = M.getrow(row).toarray()[0]\nresult.append(M.getrow(row)[0][column])\nprint(result)",
        "\nf = scipy.interpolate.interp1d(x, array[:, 0, 0], kind='cubic')\nnew_array[:, 0, 0] = f(x_new)\n",
        "\n# Calculate the probability density function (PDF) at each point in the grid\nP_grid = np.zeros((len(x_grid),))\nfor i in range(len(x_grid)):\n    P_grid[i] = scipy.integrate.quad(NDfx, -dev, x_grid[i])[0]\n# Calculate the cumulative probability distribution function (CDF)\nP_cdf = np.cumsum(P_grid)\n# Find the index of the first point where the CDF is greater than or equal to the probability up to position 'x'\nidx = np.searchsorted(P_cdf, prob)\n# Calculate the position of interest where the CDF is greater than or equal to the probability up to position 'x'\nposition_of_interest = x_grid[idx-1]\n",
        "\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n",
        "\ndctmtx_scipy = sf.dctn(N, norm='ortho')\nresult = dctmtx_scipy @ np.diag(np.sqrt(np.arange(1, N+1)))\nprint(result)",
        "\ndiags(matrix, [-1,0,1], (5, 5)).toarray()\n",
        "\n# Calculate the binomial coefficients using the formula\ndef binomial_coeff(n, k):\n    return choose(n, k)\n# Initialize the 2D array to store the binomial distribution probability matrix\nM = np.zeros((N+1, N+1))\n# Loop over the rows and columns of the matrix and fill in the values using NumPy's broadcasting feature\nfor i in range(N+1):\n    for j in range(i+1):\n        M[i,j] = binomial_coeff(i, j) * p**j * (1-p)**(i-j)\n",
        "\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n# Fill in the missing code here:\nresult = df.apply(lambda x: (x-x.mean())/x.std(),axis=1)\nprint(result)\n",
        "\nz_scores = stats.zscore(df)\nresult = pd.DataFrame(z_scores,index=df.index,columns=df.columns)\nprint(result)",
        "\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n# Calculate z-scores for each column\nzscores = stats.zscore(df)\n# Merge z-scores with original dataframe\nresult = pd.concat([df, zscores], axis=1)\n# Print result\nprint(result)\n",
        "\nimport pandas as pd\nimport io\nimport numpy as np\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n# Calculate z-scores for each column\nzscores = np.array([stats.zscore(df[col],axis=0) for col in df.columns])\n# Round z-scores to 3 decimal places\nzscores = np.round(zscores,3)\n# Merge z-scores with original dataframe\nresult = pd.concat([df,zscores],axis=1)\n# Print result\nprint(result)\n",
        "alpha = sp.optimize.linesearch(test_func,test_grad,starting_point,direction)\nprint(alpha)",
        "\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\n# Create a 2D array of distances from the center to each point\nmid = np.array([[3, 3], [3, 4], [4, 3]])\nresult = distance.cdist(y, x, mid)\nprint(result)\n",
        "\nresult = distance.cdist(y, x, mid) # <-- ",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # Create a 2D array of the same shape as the first two dimensions of the input image\n    y, x = np.ogrid[0:shape[0], 0:shape[1]]\n    mid = np.array([[shape[0]//2, shape[1]//2], [shape[0]//2, 0], [0, shape[1]//2], [0, 0]])\n    # Use scipy.spatial.distance.cdist to compute the Euclidean distances from the center point to every point in the image\n    result = distance.cdist(scipy.dstack((y, x)), mid)\n    return result\n",
        "\nzoom_factor = shape[0] / x.shape[0], shape[1] / x.shape[1]\nresult = scipy.ndimage.zoom(x, zoom_factor, order=1)\nprint(result)",
        "\n# Define objective function\ndef objective_function(params):\n    x = params['x'].value\n    a = np.array([ [ 0, 0, 1, 1, 1 ],\n                   [ 1, 0, 1, 0, 1 ],\n                   [ 0, 1, 0, 1, 0 ] ])\n    return np.sum((y - a.dot(x ** 2)) ** 2)\n# Use scipy.optimize.minimize to find the minimum\nout = scipy.optimize.minimize(objective_function, x0, method='Nelder-Mead', options={'maxiter': 1000})\n# Print the result\nprint(out)\n",
        "\nfit_params = Parameters()\nfit_params.add('x', value=x0, min=x_lower_bounds)\nout = minimize(residual, fit_params, args=(a, y), method='L-BFGS-B')\nprint(out)",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 + np.sin(t)\ndef solve_ivp_with_time_varying_input(fun, t_span, y0):\n    sol = scipy.integrate.solve_ivp(fun, t_span, y0)\n    return sol\n# Call the new function with the time-varying input\nsol = solve_ivp_with_time_varying_input(dN1_dt, time_span, [N0])\nresult = sol.y\nprint(result)",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 + t - np.sin(t) if 0 < t < 2 * np.pi else 2 * np.pi\ndef solve_ivp_with_time_varying_input(fun, t_span, y0, input_data):\n    t, y = scipy.integrate.solve_ivp(fun, t_span, y0, args=(input_data,))\n    return y\n# Define the time-varying input data\ninput_data = np.sin(np.linspace(0, 2 * np.pi, num=len(t_span)))\n# Call the function to solve the IVP with the time-varying input\nresult = solve_ivp_with_time_varying_input(dN1_dt, time_span, [N0], input_data)\n# Print the result\nprint(result)",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 - np.cos(t)\n# Define the ODE function with the time-varying input\ndef dN1_dt_with_input(t, N1):\n    return -100 * N1 - np.cos(t) + cos(t)\n# Update the solution with the new ODE function\nsol = scipy.integrate.solve_ivp(dN1_dt_with_input, t_span, y0=[N0,])\n# Get the result from the updated solution\nresult = sol.y\nprint(result)",
        "\ndef const(x):    \n    y=x[t]\n    return y\ncons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sa.tocsr() + sb.tocsr()",
        "\nresult = sa.multiply(sb)\nprint(result)",
        "\n# create an empty list to store the results\nresults = []\n# loop through each value of c\nfor n in range(len(c)):\n    # create a new integration interval\n    x = np.linspace(low, high, 100)\n    \n    # define the integrand function\n    def integrand(x):\n        return 2*x*c[n]\n    \n    # use scipy.integrate.quad to integrate the function over the interval\n    result, error = scipy.integrate.quad(integrand, low, high)\n    \n    # append the result to the list of results\n    results.append(result)\n# convert the list of results to a numpy array\nresults = np.array(results)\n",
        "\ndef f(c=5, low=0, high=1):\n    # calculate the integral\n    result = scipy.integrate.quad(lambda x: 2*x*c, low, high)[0]\n    return result\n",
        "\nimport numpy as np\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'dok', random_state = 42)\nx = 99\n# Fill in the missing code here\nmask = V.getnonzero()\nV[mask] += x\nprint(V)\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\n# Create a sparse matrix with the same non-zero elements as `V` and add it to `V`\nindices = V.nonzero()\nvalues = x * V.data[indices]\ndata = V.indices\nshape = V.shape\nnew_data = sparse.coo_matrix((values, indices), shape=shape).tocsr()\nC = sparse.coo_matrix((1.0,), shape=shape)\nC[indices] = new_data\nC = C + V\nprint(C)\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\ny = 99\n# Create a sparse matrix with non-zero values only\nmask = V.getnnz() != 0\nV = V[mask]\nx = x[mask]\n# Add the scalar values to the non-zero values of the sparse matrix\nV += x\n# Create a new sparse matrix with the updated values\nB = sparse.coo_matrix((x, (V.row, V.col)), shape = V.shape)\nprint(B)\n",
        "\nfor col in range(sa.shape[1]):\n    column = sa[:,col].data\n    list = [x**2 for x in column]\n    len = math.sqrt(sum(list))\n    sa.data[col] = sa.data[col] / len\n",
        "\nfor col in range(sa.shape[1]):\n    column = sa.getcol(col).toarray()\n    norm = math.sqrt(sum(column**2))\n    sa.setcol(col, column/norm)\n",
        "\nimport scipy\nimport numpy as np\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n# Convert the matrix to a binary matrix using scipy.sparse.csr_matrix\nb = scipy.sparse.csr_matrix(a)\n# Set all non-zero elements to 1 and all zero elements to 0\nb.setdiag(0)\nb = b.astype(int)\n# Print the resulting binary matrix\nprint(b)\n",
        "\nimport scipy\nimport numpy as np\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n# Convert the matrix to a binary matrix using scipy.sparse.csr_matrix\nb = scipy.sparse.csr_matrix(a)\n# Set all non-zero elements to 1 and all zero elements to 0\nb.setdiag(0)\nb = b.astype(int)\n# Print the resulting binary matrix\nprint(b)\n",
        "# Extract the distance matrix from the linkage object\ndist = linkage.distance\n# Calculate the squared distance between each sample and the centroids\nsquared_dist = np.sum((data - centroids) ** 2, axis=1)\n# Find the index of the minimum squared distance for each sample\nresult = np.argmin(squared_dist, axis=1)\nprint(result)",
        "# Extract the cluster labels\nlabels = scipy.cluster.hierarchy.cut_tree(linkage, n_clusters=5)\n# Get the indices of the closest point to each cluster\nresult = []\nfor i in range(5):\n    idx = scipy.spatial.KDTree(centroids[labels == i]).query(data, k=1)[0][0]\n    result.append(data[idx])\nprint(result)",
        "\ndef get_k_closest_elements(centroids, data, k):\n    dists = scipy.spatial.distance.cdist(centroids, data)\n    idx = np.argsort(dists)[:,-k:][:,None]\n    return idx",
        "\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\nresult = fsolve(eqn, (a, xdata, bdata))\nprint(result)",
        "\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\nresult = []\nfor i in range(len(xdata)):\n    b_guess = (eqn(xdata[i], a, b_guess) for b_guess in adata[i])\n    result.append(b_guess)\nresult = np.array(result)\n",
        "\n# Calculate the cumulative distribution function (cdf) of the sample data\ncdf = np.cumsum(bekkers(sample_data, estimated_a, estimated_m, estimated_d))\n# Calculate the cumulative distribution function (cdf) of the fitted distribution\nfitted_cdf = np.cumsum(bekkers(range_start, estimated_a, estimated_m, estimated_d))\n# Calculate the Kolmogorov-Smirnov statistic (D) and p-value\nresult = stats.kstest(sample_data, fitted_cdf)[1]\nprint(result)\n",
        "\nimport scipy.stats as ss\ndef kstest(sample_data, fitted_data, params):\n    mu, sigma = params\n    n = len(sample_data)\n    x = np.linspace(range_start, range_end, n)\n    p = bekkers(x, *params)\n    ks_result = ss.kstest(sample_data, fitted_data, args=(mu, sigma))\n    reject = ks_result.pvalue < 0.05\n    return reject\n# Call the kstest function\nreject = kstest(sample_data, fitted_data, (mu, sigma))\n# Print the result\nprint(reject)",
        "\nimport pandas as pd\nimport io\nfrom scipy import integrate\nstring = '''\nTime                      A\n2017-12-18-19:54:40   -50187.0\n2017-12-18-19:54:45   -60890.5\n2017-12-18-19:54:50   -28258.5\n2017-12-18-19:54:55    -8151.0\n2017-12-18-19:55:00    -9108.5\n2017-12-18-19:55:05   -12047.0\n2017-12-18-19:55:10   -19418.0\n2017-12-18-19:55:15   -50686.0\n2017-12-18-19:55:20   -57159.0\n2017-12-18-19:55:25   -42847.0\n'''\ndf = pd.read_csv(io.StringIO(string), sep = '\\s+')\n# Rolling integral over time\nrolling_integral = df.groupby(pd.Grouper(freq='25S')).apply(lambda x: x.sum())\nrolling_integral = rolling_integral.reset_index(name='Time')\nrolling_integral['A_rolling_integral'] = rolling_integral['A'].diff() + rolling_integral['A_rolling_integral']\n# Integrate over function of one of the columns\nintegral_df = pd.concat([df, rolling_integral], axis=1)\nintegral_df = integral_df.set_index('Time')\nintegral_df = integral_df.groupby(pd.Grouper(freq='25S')).apply(integrate.trapz)\nprint(integral_df)\n",
        "\ngrid = scipy.interpolate.griddata(x, y, eval)\nresult = grid[0,0]\nprint(result)",
        "\nimport scipy.optimize as sciopt\nimport numpy as np\nimport pandas as pd\na=pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\n# Calculate the multinomial probabilities\np = np.array([a[i].sum() for i in range(len(a))]) / a.sum()\n# Use the scipy.optimize.minimize function to find the best parameters for the multinomial distribution\nresult = sciopt.minimize(lambda x: -np.sum((p * x) ** 2), x0=[1e-5 for i in range(len(a))], method='Nelder-Mead')\n# Extract the best weights\nweights = result.x\n# Print the weights\nprint(weights)",
        "\npopt = sciopt.minimize(e, p0=pmin, bounds=[(pmax[0]-pmax[1])/2, (pmax[0]+pmax[1])/2], method='Nelder-Mead')\nprint(popt)\n",
        "\nresult = np.where((arr[1:] <= arr[:-1]) & (arr[:-1] <= arr[0:-2]))[0]",
        "\ndef find_relative_extrema(arr, n=2):\n    rows, cols = arr.shape\n    indices = []\n    for i in range(rows):\n        for j in range(cols):\n            if arr[i][j] <= arr[i][j - 1] and arr[i][j] <= arr[i][j + 1]:\n                indices.append([i, j])\n    return indices\n",
        "\nimport pandas as pd\nimport numpy as np\ndef remove_outliers(df, cols):\n    \"\"\"\n    Function to remove outliers from a pandas dataframe based on numeric columns\n    \n    Args:\n        df (pandas.DataFrame): The dataframe to remove outliers from\n        cols (list): A list of columns to evaluate for outliers\n    \n    Returns:\n        pandas.DataFrame: The dataframe with outliers removed\n    \"\"\"\n    for col in cols:\n        z_scores = stats.zscore(df[col])\n        mask = np.abs(z_scores) < 3\n        df = df[mask]\n    \n    return df\n# Apply function to dataframe\ndf = remove_outliers(df, ['NUM1', 'NUM2'])\nprint(df)"
    ],
    "Sklearn": [
        "\n# Convert the Bunch object to a Pandas DataFrame\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n# Print the resulting DataFrame\nprint(data1)\n",
        "\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndf = pd.DataFrame.from_dict(data.data, orient='index')\nprint(df)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_boston\nimport pandas as pd\ndata = load_boston()\n# Convert the Bunch object to a dictionary\ndata_dict = data.to_dict()\n# Create a Pandas DataFrame from the dictionary\ndata1 = pd.DataFrame.from_dict(data_dict)\nprint(data1)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndef solve(data):\n    # Convert the Bunch object to a dictionary\n    data_dict = data.to_dict()\n    \n    # Create a Pandas DataFrame from the dictionary\n    dataframe = pd.DataFrame.from_dict(data_dict)\n    \n    return dataframe\ndata1 = solve(data)\nprint(data1)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\n# Convert the list column to a MultiIndex column\ndf = df.assign(**{f\"{col}\": pd.MultiIndex.from_product([[col], ['present'] * len(set(df[col]))]) for col in df.columns if isinstance(df[col], list)})\n# Fill the MultiIndex column with 1s and 0s\ndf = df.fillna(0).astype(int)\n# Convert the MultiIndex column to a separate column for each unique element\ndf = df.melt(id_vars=['Col1', 'Col2', 'Col3'], value_name='value', var_name='element')\n# Drop the original list column\ndf = df.drop(columns=['Col1', 'Col2', 'Col3'])\nprint(df_out)\n",
        "\ndf_out = pd.get_dummies(df[\"Col3\"], drop_first=True)\n",
        "# One-hot encode the last column\ndf_out = pd.get_dummies(df['Col4'], drop_first=True)\n# Print the resulting dataframe\nprint(df_out)",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\n# Split the last column into multiple columns based on the unique elements\ndf_out = pd.DataFrame()\nfor col_name, col in df.iteritems():\n    if col_name == 'Col3':\n        unique_elements = list(set(col))\n        for element in unique_elements:\n            df_out[element] = df[col_name].str.contains(element)\nprint(df_out)\n",
        "\ndef one_hot_encode_column(df, column_index):\n    # Get the unique values in the column\n    unique_values = df[column_index].unique()\n    \n    # Create a new column for each unique value\n    for value in unique_values:\n        df[f\"one_hot_{value}\"] = np.zeros(len(df))\n        df[f\"one_hot_{value}\"] = df[f\"one_hot_{value}\"] + (df[column_index] == value).astype(int)\n    \n    return df\n# Apply the one_hot_encode_column function to the last column of df\ndf_out = one_hot_encode_column(df, 3)\n",
        "probabilities = svmmodel.decision_function(x_test)\nproba = 1 / (1 + np.exp(-probabilities))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm\nX, y, x_predict = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_predict) == np.ndarray\nmodel = svm.LinearSVC()\n# Convert decision scores to probabilities using the logistic function\nproba = np.exp(model.decision_function(x_predict)) / (1 + np.exp(model.decision_function(x_predict)))\nprint(proba)\n",
        "\ndf = pd.concat([df_origin, transform_output], axis=1)\nprint(df)",
        "\n# Convert the sparse matrix to a Pandas DataFrame\nsparse_df = pd.DataFrame(transform_output, columns=sparse_columns)\n# Merge the sparse DataFrame with the original DataFrame\nmerged_df = pd.concat([df_origin, sparse_df], axis=1)\n# Print the merged DataFrame\nprint(merged_df)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ndef solve(df, transform_output):\n    # Convert the sparse matrix back to a Pandas DataFrame\n    result = pd.DataFrame(transform_output.toarray(), columns=df.columns)\n    # Merge the transformed data back into the original DataFrame\n    df = pd.concat([df, result], axis=1)\n    return df\ndf = solve(df_origin, transform_output)\nprint(df)\n",
        "\nsteps = clf.named_steps()\nstep_to_delete = 'reduce_dim'\nif step_to_delete in steps:\n    del steps[step_to_delete]\n",
        "\nsteps = clf.named_steps()\ndel steps['reduce_poly']\nclf = Pipeline(estimators)\nclf.set_params(**steps)\nprint(len(clf.steps))\n[Instruction]",
        "\nsteps = clf.named_steps()\ndel steps[1]\nclf.set_params(**{step[0]: estimators[1][1].get_params() for step in steps})\nprint(clf.named_steps)\n[Instruction]\n",
        "\nnew_estimator = [('new_step', YourNewEstimator())]\nsteps = clf.named_steps()\nsteps.insert(0, new_estimator)\nclf = Pipeline(steps)\nprint(len(clf.steps))\n",
        "\nsteps = clf.named_steps()\nnew_steps = []\nfor step in steps:\n    if step[0] != 'reduce_poly':\n        new_steps.append(step)\nnew_steps.append(('insert_step', 'new_estimator'))\nclf = Pipeline(new_steps)\n",
        "\nsteps = clf.named_steps()\nnew_steps = [(name, step) if name != 'svdm' else (name, 't1919810', PCA()) for name, step in steps]\nclf = Pipeline(new_steps)\nprint(clf.named_steps)\n",
        "\n# Add early stopping parameters to the GridSearchCV object\ngridsearch.set_params(**fit_params)\n# Perform the grid search with early stopping\ngridsearch.fit(trainX, trainY)\n# Get the best model from the grid search\nbest_model = gridsearch.best_estimator_\n# Use the best model to make predictions on the test set\ntest_predictions = best_model.predict(testX)\n# Calculate the mean absolute error (MAE) of the predictions on the test set\nmae = np.mean(np.abs(test_predictions - testY))\n# Print the MAE of the predictions\nprint(\"MAE:\", mae)\n",
        "\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n# Load data\ngridsearch, testX, testY, trainX, trainY = load_data()\n# Define the parameters for the GridSearchCV\nparam_grid = {'max_depth': [3, 6, 9],\n              'eta': [0.1, 0.3, 0.5],\n              'silent': [True, False]}\n# Define the early stopping parameters\nfit_params = {'early_stopping_rounds': 42,\n              'eval_metric': 'mae',\n              'eval_set': [[testX, testY]]}\n# Use GridSearchCV with the specified parameters\ngridsearch_with_early_stopping = GridSearchCV(xgb.XGBRegressor(), param_grid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=3, iid=False, fit_params=fit_params)\n# Fit the GridSearchCV model with early stopping\ngridsearch_with_early_stopping.fit(trainX, trainY)\n# Get the best hyperparameters\nbest_params = gridsearch_with_early_stopping.best_params_\n# Print the best score and the predicted values\nb = gridsearch_with_early_stopping.score(trainX, trainY)\nc = gridsearch_with_early_stopping.predict(trainX)\nprint(b)\nprint(c)\n",
        "\nprobabilities = logreg.predict_proba(X_test)\nproba = probabilities[:, 1]",
        "\n# Calculate the probabilities for each sample\nproba = logreg.predict_proba(X_test)\n# Save the probabilities into a list\nprobabilities = []\nfor i in range(len(y_test)):\n    probabilities.append(proba[i][1])\n# Convert the list into an array\nprobabilities = np.array(probabilities)\n",
        "\ninversed_t = scaler.inverse_transform(scaled)\ncheck_inversed = np.allclose(inversed_t, data['t'])\nif check_inversed:\n    print(\"inversed t is close to real t\")\nelse:\n    print(\"inversed t is not close to real t\")",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\nscores = cross_val_score(model, X, y, cv=5)\nmean_score = scores.mean()\ndf = pd.DataFrame({'Model Name': [model_name], 'Mean Score': [mean_score]})\nprint(df)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n# Get the list of model names\nmodel_names = [model.__class__.__name__ for model in models]\n# Get the name of the first model\nmodel_name = model_names[0]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import LinearSVC\nmodel = LinearSVC()\nmodel_name = model.__class__.__name__\nscores = cross_val_score(model, X, y, cv=5)\nmean_score = scores.mean()\ndf = pd.DataFrame({'Model': [model_name], 'Mean Score': [mean_score]})\nprint(df)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data)\n",
        "\n# Get the intermediate result of tf_idf\ntf_idf = pipe.named_steps[\"tf_idf\"]\ntf_idf_out = tf_idf.transform(data.test)\n# Print the intermediate result of tf_idf\nprint(tf_idf_out)\n",
        "\nselect_out = pipe.get_params()['select'].fit_transform(data, target)\n",
        "\ngrid = GridSearchCV(bc, param_grid, cv=5)\ngrid.fit(X_train, y_train)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nX, y, X_test = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(X_test) == np.ndarray\n# Check the number of features in the X data\nnum_features = X.shape[1]\n# Check the number of labels in the y data\nnum_labels = y.shape[0]\n# Check that the number of features and labels match\nif num_features != num_labels:\n    raise ValueError(\"Number of labels={} does not match number of samples={}\".format(num_labels, X.shape[0]))\n# Check that the X_test data has the same number of features as the X data\nif X_test.shape[1] != num_features:\n    raise ValueError(\"X_test data has only one feature and I assume one of my sets of values is in the wrong format but its not too clear to me from the documentation.\")\n# Convert the X_test data to a numpy array\nX_test = np.array(X_test)\n# Fit the RandomForestRegressor model\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\n# Make predictions using the test data\npredict = regressor.predict(X_test)\n# Print the predictions\nprint(predict)\n",
        "\ny = y.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X_train, y_train)\npredict = regressor.predict(X_test)\nprint(predict)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Define the preprocessing function\ndef preprocess(s):\n    return s.upper()\n# Create the TF-IDF vectorizer\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n# Fit the vectorizer to the training data\ntfidf.fit(X_train)\n# Transform the test data\nX_test = tfidf.transform(X_test)\n# Print the preprocessor\nprint(tfidf.preprocessor)\n",
        "\n# Define a custom preprocessor function\ndef prePro(text):\n    return text.lower()\n# Set the preprocessor parameter to the custom function\nvectorizer = TfidfVectorizer(preprocessor=prePro)\n# Print the preprocessor function\nprint(vectorizer.preprocessor)\n",
        "# Scale the data using preprocessing.scale\nscaler = preprocessing.StandardScaler()\ndf_out = scaler.fit_transform(data)\nprint(df_out)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n# Convert the DataFrame to a NumPy array\nX = data.values\n# Apply the preprocessing.scale function to the NumPy array\nscaler = preprocessing.StandardScaler()\nscaled_X = scaler.fit_transform(X)\n# Create a new DataFrame from the scaled NumPy array\ndf_out = pd.DataFrame(scaled_X, index=data.index, columns=data.columns)\nprint(df_out)\n",
        "\ncoef = grid.best_params_\n",
        "\n# Get the fitted model from the GridSearchCV object\nmodel = grid.best_estimator_\n# Get the coefficients from the fitted model\ncoef = model.coef_\n# Print the coefficients\nprint(coef)\n",
        "\nselected_features = model.get_support(indices=True)\ncolumn_names = X.columns[selected_features]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = model.get_support(indices=True)\n",
        "\nselected_columns = model.get_support(indices=True)\ncolumn_names = X.columns[selected_columns]\nprint(column_names)",
        "\nselected_features = model.get_support(indices=True)\ncolumn_names = X.columns[selected_features]\n",
        "\ndef get_closest_50_samples(p, X):\n    cluster_centers = km.cluster_centers_\n    distances = np.sqrt(np.sum((X - cluster_centers[p]) ** 2, axis=1))\n    indices = np.argpartition(distances, -50)[-50:]\n    return X[indices, :], distances[indices]",
        "\ncenters = km.fit_predict(X)\np = 2\nclosest_50_samples = np.argsort(np.linalg.norm(X - centers[p], axis=1))[:50]",
        "\ncenters = km.cluster_centers_\n# Calculate the distance between each sample and each center\ndistances = np.sqrt(np.sum((X - centers) ** 2, axis=1))\n# Find the indices of the k closest centers to each sample\nindices = np.argpartition(distances, -100)[-100:]\n# Get the corresponding samples\nclosest_100_samples = X[indices]\n",
        "\ndef get_samples(p, X, km):\n    centroids = km.cluster_centers_\n    distances = np.array([np.linalg.norm(x - centroids[i]) for i, x in enumerate(X)])\n    idx = np.argsort(distances)[::-1][:50]\n    return X[idx][p]",
        "\nX_train = pd.get_dummies(X_train, columns=[0])\nX_train = pd.concat([X_train, y_train], axis=1)\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n",
        "\n# convert categorical variables to numerical using get_dummies\ncat_cols = ['col1', 'col2']\nX_train = pd.get_dummies(X_train, columns=cat_cols)\n# merge the converted categorical variables back with the original dataframe\nX_train = pd.merge(X_train, X_train, on=['col1', 'col2'])\n",
        "# Create a Gaussian kernel\nkernel = sklearn.preprocessing.KernelTransformer(kernel='gaussian', gamma=0.1)\n# Fit the kernel to the training data\nkernel.fit(X)\n# Transform the training data using the kernel\nX_transformed = kernel.transform(X)\n# Create an SVM regression model using the transformed data\nsvm_regressor = sklearn.svm.SVR(gamma='auto')\nsvm_regressor.fit(X_transformed, y)\n# Predict the target values using the transformed data\npredict = svm_regressor.predict(X_transformed)\n",
        "# Define the SVM model\nsvm = sklearn.svm.SVC(kernel='rbf', gamma=0.1)\n# Fit the model to the training data\nsvm.fit(X_train, y_train)\n# Make predictions on the test data\npredict = svm.predict(X_test)\n# Print the results\nprint(f'The predicted values are: {predict}')",
        "# Create polynomial kernel\nkernel = sklearn.preprocessing.PolynomialFeatures(degree=2)\n# Fit polynomial kernel to training data\nX_poly = kernel.fit_transform(X)\n# Use polynomial kernel with SVM\nsvm = sklearn.svm.SVC(kernel='poly')\nsvm.fit(X_poly, y)\n# Predict using polynomial kernel and SVM\npredict = svm.predict(X_poly)\n",
        "# Define the polynomial kernel\nkernel = sklearn.kernel.Polynomial(degree=2)\n# Fit the SVM regression model\nmodel = sklearn.svm.SVR(kernel=kernel)\nmodel.fit(X_train, y_train)\n# Make predictions on the test set\npredict = model.predict(X_test)",
        "\n# Calculate the term frequency-inverse document frequency (tf-idf) vector for each query\nquery_tfidf = tfidf.transform(queries)\n# Calculate the cosine similarity between each query and each document\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents))\n# Calculate the cosine similarity between each query and each document\ncosine_similarities_of_queries = np.array(cosine_similarities_of_queries)\n# Sort the cosine similarities in descending order\nsorted_cosine_similarities = np.sort(cosine_similarities_of_queries)[::-1]\n# Print the top 3 most similar documents for each query\nfor i in range(len(queries)):\n    print(f\"Query {i+1}:\")\n    print(f\"Most similar document: {documents[sorted_cosine_similarities[i][0]]}\")\n    print(f\"Second most similar document: {documents[sorted_cosine_similarities[i][1]]}\")\n    print(f\"Third most similar document: {documents[sorted_cosine_similarities[i][2]]}\")\n",
        "\n# Calculate the term frequency-inverse document frequency (tf-idf) vector for each query\nquery_tfidf = tfidf.transform(queries)\n# Calculate the cosine similarity between each query and each document\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents))\n# Convert the cosine similarities into a pandas DataFrame\ncosine_similarities_df = pd.DataFrame({'query': queries, 'document': documents, 'cosine_similarity': cosine_similarities_of_queries})\n# Sort the DataFrame by cosine similarity\ncosine_similarities_df = cosine_similarities_df.sort_values('cosine_similarity', ascending=False)\n# Print the top 5 most similar queries for each document\nfor index, row in cosine_similarities_df.iterrows():\n    print(f'{row[\"query\"][0]} is the {index+1} most similar query to document {row[\"document\"][0]} with a cosine similarity of {row[\"cosine_similarity\"][0]:.4f}')\n",
        "\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    # Calculate the term frequency - inverse document frequency (TF-IDF) matrix for the queries\n    query_tfidf = tfidf.transform(queries)\n    # Calculate the cosine similarity between the TF-IDF matrix of the queries and the TF-IDF matrix of the documents\n    cosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T)\n    return cosine_similarities_of_queries\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\n# Load the data\nfeatures = load_data()\n# Convert the features to a 2D array\nnew_features = np.array(features)\nnew_features = new_features.reshape((len(features), -1))\nprint(new_features)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\n# Load the data\nf = pd.DataFrame(f)\n# Convert the data to a 2D array\nnew_f = np.array(f)\nnew_f = new_f.T\nprint(new_f)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\n# Load the data\nfeatures = load_data()\n# Convert the features to a 2D array\nnew_features = np.zeros((len(features), len(features[0])))\nfor i, row in enumerate(features):\n    for j, feature in enumerate(row):\n        new_features[i][j] = features[i].index(feature)\n# Print the new features\nprint(new_features)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # Convert each sample to a 2D array\n    new_features = np.zeros((len(features), len(features[0])))\n    for i, sample in enumerate(features):\n        for j, feature in enumerate(sample):\n            new_features[i][j] = 1 if feature == '1' else 0\n    # Create a DataFrame with the new features\n    new_features = pd.DataFrame(new_features, columns=features[0])\n    return new_features\nnew_features = solve(features)\nprint(new_features)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\n# Convert each sample to a 2D array\nnew_features = np.array([list(feature) for feature in features])\n# Transpose the array to get the desired format\nnew_features = new_features.T\nprint(new_features)\n",
        "\ndata_matrix = np.array(data_matrix)\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2, linkage='ward').fit_predict(data_matrix)\nprint(cluster_labels)",
        "\n# Define the linkage method to use for agglomerative clustering\nlinkage = sklearn.cluster.AgglomerativeClustering.linkage\n# Initialize the AgglomerativeClustering object with the desired number of clusters\ncluster = sklearn.cluster.AgglomerativeClustering(n_clusters=2, linkage=linkage)\n# Fit the AgglomerativeClustering object to the distance matrix\ncluster.fit(data_matrix)\n# Get the cluster labels for each row in the data matrix\ncluster_labels = cluster.labels_\n",
        "\nsim = np.array(simM)\ndissim = 1 - sim\nlinkage = 'ward'\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2, linkage=linkage).fit_predict(sim)\nprint(cluster_labels)",
        "\ndef compute_distance_matrix(data_matrix):\n    \"\"\"\n    Compute the distance matrix from the given data matrix.\n    \"\"\"\n    distance_matrix = np.zeros((len(data_matrix), len(data_matrix)))\n    for i in range(len(data_matrix)):\n        for j in range(i + 1, len(data_matrix)):\n            distance_matrix[i][j] = distance_matrix[j][i] = np.linalg.norm(data_matrix[i] - data_matrix[j])\n    return distance_matrix\ndef perform_hierarchical_clustering(distance_matrix, n_clusters):\n    \"\"\"\n    Perform hierarchical clustering using the scipy.cluster.hierarchy module.\n    \"\"\"\n    linkage = scipy.cluster.hierarchy.linkage(distance_matrix, method='ward')\n    dendrogram = scipy.cluster.hierarchy.dendrogram(linkage, labels=None, orientation='left')\n    cluster_labels = scipy.cluster.hierarchy.fcluster(linkage, n_clusters, criterion='maxclust')\n    return cluster_labels\ncluster_labels = perform_hierarchical_clustering(compute_distance_matrix(data_matrix), 2)\n",
        "\ncluster_labels = scipy.cluster.hierarchy.linkage(data_matrix, method='ward')\nprint(cluster_labels)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\nsimM = load_data()\n# Perform hierarchical clustering using scipy.cluster.hierarchy\nlinkage = 'ward'  # choose your linkage method\nn_clusters = 2  # expected number of clusters\ncluster_labels = scipy.cluster.hierarchy.linkage(simM, method=linkage, n_clusters=n_clusters)\nprint(cluster_labels)\n",
        "\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\nprint(centered_scaled_data)",
        "\nscaler = sklearn.preprocessing.StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\nprint(centered_scaled_data)",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Perform Box-Cox transformation using sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nscaler = StandardScaler()\ntransformer = PowerTransformer(method='box-cox')\ntransformed_data = scaler.fit_transform(data)\ntransformed_data = transformer.fit_transform(transformed_data)\n# Save the transformed data\nbox_cox_data = pd.DataFrame(transformed_data)\nbox_cox_data.to_csv('box_cox_data.csv', index=False)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Box-Cox transformation using sklearn\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nbox_cox_data = scaler.fit_transform(data)\n# Print the transformed data\nprint(box_cox_data)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Perform Yeo-Johnson transformation\nfrom sklearn.preprocessing import PowerTransformer\ntransformer = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = transformer.fit_transform(data)\nprint(yeo_johnson_data)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\n# Perform Yeo-Johnson transformation\ntransformer = sklearn.preprocessing.YeoJohnson()\nyeo_johnson_data = transformer.fit_transform(data)\nprint(yeo_johnson_data)\n",
        "\n# Define a custom tokenizer that preserves punctuation marks\ndef custom_tokenizer(text):\n    from nltk.tokenize import RegexpTokenizer\n    tokenizer = RegexpTokenizer(r'\\w+|\\W+')\n    return tokenizer.tokenize(text)\n# Use the custom tokenizer in CountVectorizer\nvectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n# Apply the vectorizer to the text data\ntransformed_text = vectorizer.fit_transform(text)\n",
        "# Split dataset into training and testing sets\ntrain_index, test_index = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n# Split training and testing sets into X and y\nx_train = dataset.iloc[train_index, :-1]\ny_train = dataset.iloc[train_index, -1]\nx_test = dataset.iloc[test_index, :-1]\ny_test = dataset.iloc[test_index, -1]",
        "\nimport numpy as np\nimport pandas as pd\ndata = load_data()\n# Split the data into training and testing sets\ntrain_size = int(0.8 * len(data))\nx_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n# Print the training and testing sets\nprint(\"Training set:\")\nprint(x_train)\nprint(y_train)\nprint(\"Testing set:\")\nprint(x_test)\nprint(y_test)\n",
        "# Split dataset into training and testing sets\ntrain_index, test_index = train_test_split(range(len(dataset)), test_size=0.3, random_state=42)\n# Split training and testing sets into X and y\nx_train = dataset.iloc[train_index, :-1]\ny_train = dataset.iloc[train_index, -1]\nx_test = dataset.iloc[test_index, :-1]\ny_test = dataset.iloc[test_index, -1]",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\ndef split_dataset(dataset, test_size=0.2):\n    X = dataset.iloc[:, :-1]  # Get all columns except the last one\n    y = dataset.iloc[:, -1]  # Get the last column\n    # Split the dataset into training and testing sets\n    train_size = 1 - test_size\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    return X_train, y_train, X_test, y_test\nX_train, y_train, X_test, y_test = split_dataset(dataset)\nprint(X_train)\nprint(y_train)\nprint(X_test)\nprint(y_test)\n",
        "\nfrom sklearn.cluster import KMeans\ndf = load_data()\n# Reshape the data to a 2D array with mse values as columns and row indices as rows\nX = df['mse'].values.reshape(-1, 1)\n# Use KMeans to cluster the data with 2 clusters\nkmeans = KMeans(n_clusters=2).fit(X)\n# Get the cluster labels\nlabels = kmeans.predict(X)\n# Print the cluster labels\nprint(labels)\n",
        "\n# Reshape the data to have a 2D array of mse values\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\n# Reshape X to have shape (n_samples, 2)\nX = X.reshape(len(f1), 2)\n# Use KMeans to cluster the data\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n",
        "\nfeature_selector = LinearSVC(penalty='l1', random_state=42)\nfeature_selector.fit(X, y)\nselected_feature_indices = feature_selector.get_support(indices=True)\nselected_feature_names = vectorizer.get_feature_names()[selected_feature_indices]\n",
        "\nlinear_svc = LinearSVC(penalty='l1', C=1.0)\nlinear_svc.fit(X, y)\nselected_feature_names = vectorizer.get_feature_names()\nfeature_selector = sklearn.feature_selection.SelectKBest(f_classif, k=10)\nfeature_selector.fit(X, y)\nselected_feature_indices = feature_selector.get_support(indices=True)\nselected_features = np.array(vectorizer.get_feature_names())[selected_feature_indices]\nprint(selected_feature_names)",
        "\nfeature_indices = [i for i, v in enumerate(vectorizer.get_feature_names()) if v in selected_feature_names]\nreturn feature_indices\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'},sort=True)\nX = vectorizer.fit_transform(corpus)\nprint(feature_names)\nprint(X)\n",
        "\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nX = X.toarray()\nX = np.sort(X,axis=0)\n",
        "\nX = vectorizer.fit_transform(corpus)\nX = X.toarray()\nX = np.transpose(X)\n",
        "\nfor col in df1.columns:\n    df2 = df1[~df1[col].isna()]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    series = np.concatenate((SGR_trips, m), axis = 0)\n",
        "\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    series = np.concatenate((SGR_trips, m), axis = 0)",
        "\nencoder = LabelEncoder()\ntransformed_df['Sex'] = encoder.fit_transform(df['Sex'])\n",
        "\nencoder = LabelEncoder()\ndf['Sex'] = encoder.fit_transform(df['Sex'])\n",
        "\nencoder = LabelEncoder()\nencoder.fit(df['Sex'])\ndf['Sex'] = encoder.transform(df['Sex'])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import linear_model\nimport statsmodels.api as sm\nX_train, y_train, X_test, y_test = load_data()\nElasticNet = linear_model.ElasticNet()\nElasticNet.fit(X_train, y_train)\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\nprint(training_set_score)\nprint(test_set_score)\n",
        "scaler = MinMaxScaler()\nscaled_array = scaler.fit_transform(np_array)",
        "scaler = MinMaxScaler()\nscaled_array = scaler.fit_transform(np_array)",
        "\nnew_a = scaler.fit_transform(np_array)\nreturn new_a",
        "\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\nclf.predict(b)\n",
        "\n# Replace this line with your code to convert the strings in the second column of X to floats\n# Fit the classifier to the new data\nclf.fit(new_X, ['2', '3'])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['asdf', '1'], ['asdf', '0']]\nclf = DecisionTreeClassifier()\n# Convert the strings in the first column of X to floats\nnew_X = np.array(X)\nnew_X[:, 0] = new_X[:, 0].astype(float)\nclf.fit(new_X, ['2', '3'])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['dsa', '2'], ['sato', '3']]\nclf = DecisionTreeClassifier()\n# Convert the string values in the first column to float\nnew_X = np.array(X)\nnew_X[:,0] = new_X[:,0].astype(float)\n",
        "\n# reshape the data\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n# reshape the data\nX = X.values\ny = y.values\n# reshape the data\nX = X.reshape(len(y),1)\n# reshape the data\ny = y.reshape(len(y))\n",
        "\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\nlogReg = LogisticRegression()\nlogReg.fit(X,y)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n# Split the data into train and test sets\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n# Sort the train and test sets by date in ascending order\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\n# Check if the train set is newer than the test set\ntrain_min_date = train_dataframe['date'].min()\ntest_min_date = test_dataframe['date'].min()\nif train_min_date <= test_min_date:\n    print(\"Good results:\")\n    print(f\"1) train set = {train_dataframe.head(20)['date']}\")\n    print(f\"test set = {test_dataframe.head(20)['date']}\")\nelse:\n    print(\"The train and test sets are not in the correct order. Please check your code.\")\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n# Sort the dataframe by date\nsorted_df = features_dataframe.sort_values(by='date')\n# Calculate the number of days to use for the train and test sets\ntrain_size = 0.8\nnum_days_train = int(len(sorted_df) * train_size)\nnum_days_test = len(sorted_df) - num_days_train\n# Split the dataframe into train and test sets\ntrain_dataframe = sorted_df.iloc[0:num_days_train]\ntest_dataframe = sorted_df.iloc[num_days_train:len(sorted_df)]\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort(by='date')\ntest_dataframe = test_dataframe.sort(by='date')\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = cross_validation.train_test_split(features_dataframe, train_size=train_size)\nsorted_dates = train_dataframe[\"date\"].sort_values()\nlatest_train_date = sorted_dates[-1]\nsplit_index = train_dataframe[\"date\"].searchsorted(latest_train_date)\ntrain_dataframe = train_dataframe.iloc[:split_index]\ntest_dataframe = train_dataframe.iloc[split_index:]\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n",
        "",
        "\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].scaler.fit_transform(myData[cols])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nwords = load_data()\n# Replace \"your_code_here\" with your code\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform(words)\nfeature_names = count.get_feature_names()\nprint(feature_names)\n",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names_out())",
        "\n# Define a function to get the score and full results of the GridSearchCV\ndef get_score_and_full_results(grid_search):\n    # Get the score of the best model\n    best_score = grid_search.best_score_\n    best_params = grid_search.best_params_\n    # Get the full results of the GridSearchCV\n    full_results = grid_search.cv_results_\n    # Create a pandas DataFrame with the score and full results\n    df = pd.DataFrame({'Score': [best_score], 'Params': [best_params], 'Full Results': [full_results]})\n    # Return the DataFrame\n    return df\n# Get the fitted GridSearchCV object\nGridSearch_fitted = load_data()\n# Call the function to get the score and full results\nfull_results = get_score_and_full_results(GridSearch_fitted)\n# Print the DataFrame\nprint(full_results)\n",
        "\n# Define a function to get the score and mean_fit_time for each model in the GridSearchCV\ndef get_scores_and_fit_times(grid_search):\n    scores = grid_search.cv_results_['mean_test_score']\n    fit_times = grid_search.cv_results_['mean_fit_time']\n    return scores, fit_times\n# Get the scores and fit times for all 12 models in the GridSearchCV\nscores, fit_times = get_scores_and_fit_times(GridSearch_fitted)\n# Create a pandas DataFrame with the scores and fit times\nfull_results = pd.DataFrame({'Score': scores, 'Fit Time (s)': fit_times})\n# Sort the DataFrame by mean_fit_time\nfull_results = full_results.sort_values(by='Fit Time (s)', ascending=False)\n# Print the sorted DataFrame\nprint(full_results)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport joblib\n# Load the clean data\nX_clean = pd.read_csv('data/clean_data.csv')\n# Train the IsolationForest model\nmodel = IsolationForest(contamination=0.1, random_state=42)\nmodel.fit(X_clean)\n# Save the model to a file\njoblib.dump(model, 'sklearn_model')\n# Load the saved model\nloaded_model = joblib.load('sklearn_model')\n",
        "\ntfidf_matrix = tfidf.fit_transform(df['description'])\nsimilarity_matrix = 1 - np.array(cosine_similarity(tfidf_matrix))\n"
    ],
    "Pytorch": [
        "\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        # Forward pass\n        outputs = model(batch)\n        # Compute loss\n        loss = criterion(outputs, targets)\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # Check learning rate every 10 epochs\n        if epoch % 10 == 0:\n            lr = optimizer.param_groups[0]['lr']\n            print(f'Learning rate: {lr}')\n            # Change learning rate\n            if lr > 0.001:\n                lr = lr / 10\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005)\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        # Forward pass\n        outputs = model(batch['inputs'])\n        loss = criterion(outputs, batch['targets'])\n        # Backward and step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # Check learning rate every 10 epochs\n        if epoch % 10 == 0:\n            lr = optimizer.param_groups[0]['lr']\n            print('Epoch {}: Learning rate = {}'.format(epoch, lr))\n            # Change learning rate if necessary\n            if lr > 0.0005:\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = new_lr\n",
        "\nembedding_weights = word2vec.wv.vectors\ntorch.nn.Embedding.from_numpy(embedding_weights).float()\n",
        "\ndef get_embedded_input(input_Tensor):\n    # Load the pre-trained word2vec embedding weights\n    embedding_weights = word2vec.wv.vectors\n    # Convert the input Tensor to a numpy array\n    input_array = input_Tensor.numpy()\n    # Reshape the input array to match the shape of the embedding weights\n    input_array = input_array.reshape(-1, 1)\n    # Apply the embedding weights to the input array to get the embedded input\n    embedded_input = torch.matmul(input_array, embedding_weights)\n    return embedded_input",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\n# Convert the tensor to a numpy array\ny = x.detach().numpy()\n# Create a Pandas DataFrame from the numpy array\npx = pd.DataFrame(y)\nprint(px)\n",
        "\npd.DataFrame(x.numpy(), index=None, columns=None)",
        "C = torch.ByteTensor(B[:, A_log].numpy())\n",
        "\nC = torch.ByteTensor(A_logical.shape[0], B.shape[1])\nC.zero_()\nC[A_logical] = B\n",
        "C = torch.ByteTensor(B[:, A_log].numpy())",
        "C = torch.ByteTensor(B[:, A_log].numpy())\n",
        "C = torch.ByteTensor(B.shape[0], B.shape[1] - A_log.sum())\nC.zero_()\nC[A_log] = 1\nC = C.view(torch.LongTensor)\n",
        "\nC = torch.ByteTensor([[0, 0, 1]]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log]\nprint(C)\n",
        "C = torch.index_select(B, 0, idx)\n# 1 3\n# 4 6",
        "\nx_tensor = torch.from_numpy(x_array.astype(np.float32))",
        "\nx_tensor = torch.tensor(x)\n",
        "\nt = torch.from_numpy(a.astype(np.float32))\nreturn t",
        "\nmask = torch.zeros((len(lens), max(lens)), dtype=torch.long)\nfor i, len_ in enumerate(lens):\n    mask[i, :len_] = torch.ones((len(lens[i]),), dtype=torch.long)\nprint(mask)",
        "\nmask = torch.zeros((len(lens), max(lens)), dtype=torch.long)\nfor i, len_ in enumerate(lens):\n    mask[i, :len_] = torch.ones((len(lens[i]),), dtype=torch.long)\nprint(mask)",
        "\nmask = torch.zeros((len(lens), max(lens)), dtype=torch.long)\nfor i, len_ in enumerate(lens):\n    mask[i, :len_] = torch.ones((len(lens[i])), dtype=torch.long)\nprint(mask)",
        "\nmask = torch.zeros((len(lens), max(lens)), dtype=torch.long)\nfor i, len_ in enumerate(lens):\n    mask[i, :len(len_)] = torch.ones((len(len_),), dtype=torch.long) * (i < len(len_))\nreturn mask",
        "\ndiag_ele = Tensor_2D.diagonal(dim1=-2, dim2=-1)\ndiag_ele = diag_ele.unsqueeze(dim=-1)\nTensor_3D = torch.matmul(Tensor_2D, diag_ele)\nprint(Tensor_3D)",
        "\ndiag_ele = torch.diag(diag_ele)\nresult = torch.matmul(index_in_batch, diag_ele)\n",
        "\nc = torch.cat((a, b), dim=0)\nab = torch.stack((a, b), dim=0)",
        "\nc = torch.cat((a, b), 0)\nab = torch.stack((a, b), 0)",
        "\nc = torch.cat((a, b), dim=0)\nab = torch.stack((a, b), dim=0)\n[Explanation]",
        "\nmask = torch.zeros_like(a)\nmask[:, :lengths, :] = 1\na[ : , lengths : , : ] = a[ : , lengths : , :] * mask\nprint(a)",
        "\nmask = torch.zeros_like(a)\nmask[:, :lengths, :] = 1\na[ : , lengths : , : ] = 2333 * mask\nprint(a)",
        "\nmask = torch.zeros_like(a)\nmask[:, :lengths, :] = 1\na = a * mask\nprint(a)",
        "\nmask = torch.zeros_like(a)\nmask[:, :lengths, :] = 1\na[ : , : lengths , : ] = 2333 * mask\nprint(a)",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)",
        "\nnew_tensors = torch.stack(list)\n",
        "\ntt = torch.tensor(list_of_tensors)\nreturn tt",
        "\n# Convert list of tensors to tensor of tensors using torch.stack()\ntensor_of_tensors = torch.stack(list_of_tensors)",
        "result = t[idx]",
        "result = t[idx]",
        "result = t[idx]",
        "result = torch.gather(x, 1, ids)",
        "result = torch.gather(x, 1, ids)",
        "result = torch.gather(x, 1, ids)",
        "\ny = torch.argmax(softmax_output, dim=1)\nprint(y)",
        "\ny = torch.argmax(softmax_output, dim=1)\nprint(y)",
        "\ny = torch.argmin(softmax_output, dim=1)\nprint(y)",
        "\ny = torch.argmax(softmax_output, dim=1)\nreturn y",
        "\ny = torch.argmin(softmax_output, dim=1)\nreturn y",
        "\n# calculate the log softmax of the output\nlog_p = F.log_softmax(output, dim=1)\n# reshape the log_p to (batch_size, num_classes, height, width)\nlog_p = log_p.permute(0, 2, 3, 1).contiguous()\n# create one-hot labels\nlabels = torch.zeros_like(log_p)\nlabels.scatter_(1, labels_flattened.unsqueeze(1), 1)\n# calculate the cross entropy loss\nloss = F.nll_loss(log_p, labels, reduction='none')\nloss = loss.mean(dim=1)\n# calculate the final loss\nloss = loss.sum()\n# backpropagation\nloss.backward()\n# update the weights\noptimizer.step()\n",
        "\ncnt_equal = (A == B).sum()\nprint(cnt_equal)",
        "\ncnt_equal = np.sum(A == B)\nprint(cnt_equal)",
        "\ncnt_not_equal = np.sum(np.logical_not(A == B))\nprint(cnt_not_equal)",
        "\ncnt_equal = np.sum(np.equal(A, B))\nprint(cnt_equal)",
        "\ncnt_equal = np.sum(np.equal(A[-x:], B[-x:]))\nprint(cnt_equal)",
        "\ncnt_not_equal = np.sum(np.logical_not(np.equal(A[-x:], B[-x:])))\nprint(cnt_not_equal)",
        "\nchunk_size = 10\nnum_chunks = a.shape[3] // chunk_size + 1\nfor i in range(num_chunks):\n    start = i * chunk_size\n    end = start + chunk_size if i * chunk_size < a.shape[3] else a.shape[3]\n    tensor_31[i] = a[:, :, start:end, :, :]\n",
        "\nchunk_size = a.shape[2] // 31\nfor i in range(31):\n    start = i * chunk_size\n    end = start + chunk_size\n    tensor_i = a[:, :, start:end, :, :]\n    tensors_31.append(tensor_i)\n",
        "\nmask = mask.float()\nmask = mask.unsqueeze(0)\nmask = mask.expand(clean_input_spectrogram.size())\noutput = output.float()\noutput = output.unsqueeze(0)\noutput = output.expand(clean_input_spectrogram.size())\noutput = output * mask\noutput = output.numpy()\n",
        "\nmask = mask.float()\nmask = mask.unsqueeze(0)\noutput = output.float()\noutput = output.unsqueeze(0)\nclean_input_spectrogram = clean_input_spectrogram.float()\nclean_input_spectrogram = clean_input_spectrogram.unsqueeze(0)\nmask = mask.to(device)\nclean_input_spectrogram = clean_input_spectrogram.to(device)\noutput = output.to(device)\noutput = output * (1 - mask) + clean_input_spectrogram * mask\nprint(output)",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nmask_x = sign_x * (x >= 0)\nmask_y = sign_y * (y >= 0)\nsigned_min = torch.where(mask_x, x, y)\nprint(signed_min)",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.mul(sign_x, max) + torch.mul(sign_y, -1 * max)\nprint(signed_max)",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nresult = sign_x * min + sign_y * (-min)\nreturn result",
        "\nconfidence_score = torch.nn.functional.softmax(output, dim=1)\nconfidence_score = confidence_score.reshape(-1)\nconfidence_score = confidence_score.detach().numpy()\n",
        "\nresult = torch.cat((a, b), dim=1)\noverlap = torch.zeros_like(result)\noverlap[:, -1] = (a[:, -1] + b[:, 0]) / 2\nresult = torch.cat((result, overlap), dim=1)\n",
        "\nresult = torch.cat((a, b), dim=1)\nresult = result.mean(dim=1)\n",
        "\nnew = torch.zeros_like(t)\nnew[:, :-1, :] = t\nresult = torch.cat((t, new), dim=1)\n",
        "\nnew = torch.zeros_like(t)\nnew[:, 1:, :] = t\nresult = torch.cat([t, new], dim=1)\n",
        "new = torch.tensor([[-1] * 4 for _ in range(t.shape[0])])\nr = torch.cat([t, new], dim=1)\n",
        "\nresult = torch.bmm(data, W).squeeze()\n"
    ]
}