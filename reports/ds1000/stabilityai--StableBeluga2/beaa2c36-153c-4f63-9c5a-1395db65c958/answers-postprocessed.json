{
    "Pandas": [
        "df = df.iloc[List]",
        "\ndf_shuffled = df.iloc[List]\nresult = df_shuffled.Type.tolist()\n",
        "\nresult = df.copy()\nfor col in ['Qu1', 'Qu3']:\n    counts = df[col].value_counts()\n    counts_2 = counts[counts >= 2]\n    others = counts_2.index.difference(['apple', 'egg'])\n    result[col] = result[col].replace(['apple', 'egg'], 'other')\n",
        "\nresult = df.copy()\nfor col in ['Qu1', 'Qu2', 'Qu3']:\n    counts = df[col].value_counts()\n    counts_3 = counts[counts >= 3]\n    if counts_3.empty:\n        continue\n    else:\n        others = 'other'\n        result[col] = result[col].replace(counts_3.index, others)\n",
        "\nresult = example_df.copy()\nfor col in ['Qu1', 'Qu3']:\n    counts = example_df[col].value_counts()\n    counts_2 = counts[counts >= 2]\n    counts_2_index = counts_2.index\n    result[col] = result[col].replace(counts_2_index, 'other')\n",
        "\n# [Missing Code]\n",
        "\nresult = df.copy()\n# Qu1 column\nresult.loc[~df.Qu1.isin(['apple']), 'Qu1'] = 'other'\n# Qu2 column\nresult.loc[df.Qu2.isin(['apple']), 'Qu2'] = 'apple'\n# Qu3 column\nresult.loc[df.Qu3.isin(['apple']), 'Qu3'] = 'apple'\n",
        "\nresult = df.drop_duplicates(subset='url', keep=lambda x: x['keep_if_dup'] == 'Yes')\n",
        "\nresult = df.drop_duplicates(subset='url', keep=lambda x: x['drop_if_dup'] == 'No')\n",
        "\nresult = df.drop_duplicates(subset='url', keep=lambda x: x['keep_if_dup'] == 'Yes')\n",
        "\nresult = {}\nfor index, row in df.iterrows():\n    name = row['name']\n    v1 = row['v1']\n    v2 = row['v2']\n    v3 = row['v3']\n    if name not in result:\n        result[name] = {}\n    if v1 not in result[name]:\n        result[name][v1] = {}\n    result[name][v1][v2] = v3\n",
        "df['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\nresult = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(tz='UTC').dt.tz_convert(tz='US/Eastern').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\ndf['job'] = df['message'].apply(lambda x: x.split(': ')[1].split(',')[0].replace(' ', ''))\ndf['money'] = df['message'].apply(lambda x: x.split(': ')[1].split(',')[1].replace(' ', ''))\ndf['wife'] = df['message'].apply(lambda x: x.split(': ')[1].split(',')[2].replace(' ', ''))\ndf['group'] = df['message'].apply(lambda x: x.split(': ')[1].split(',')[3].replace(' ', ''))\ndf['kids'] = df['message'].apply(lambda x: x.split(': ')[1].split(',')[4].replace(' ', ''))\n",
        "for product in products:\n    df.loc[df['product'] == product, 'score'] *= 10",
        "for product in products:\n    df.loc[df['product'] == product, 'score'] *= 10",
        "for product_list in products:\n    for product in product_list:\n        result.loc[result['product'] == product, 'score'] *= 10",
        "\ndf.loc[df['product'].isin(products), 'score'] = df['score'].min()\n",
        "\nresult['category'] = df.apply(lambda row: ''.join(sorted(list(row.index[row.values == 1]))), axis=1)\n",
        "\nresult['category'] = df.apply(lambda row: 'A' if row['A'] == 0 else 'B' if row['B'] == 0 else 'C' if row['C'] == 0 else 'D', axis=1)\n",
        "\nresult['category'] = df.apply(lambda row: [col for col, val in zip(df.columns, row) if val == 1], axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\nresult = df.loc[df['Date'].between('2017-08-17', '2018-01-31')]\n",
        "\ndf.iloc[0, 0] = df.iloc[-1, 0]\ndf.iloc[-1, 0] = df.iloc[0, 0]\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf.loc['1980-01-01', '#1'] = df['#1'].iloc[-1]\n",
        "\ndf.iloc[0, 0] = df.iloc[-1, 0]\ndf.iloc[-1, 0] = df.iloc[0, 0]\ndf.iloc[0, 1] = df.iloc[-1, 1]\ndf.iloc[-1, 1] = df.iloc[0, 1]\n",
        "\ndf = df.iloc[[4, 3, 2, 1, 0]]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = [f\"X{col}X\" for col in df.columns]\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", \"val3\": \"mean\"})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", \"val3\": \"sum\"})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"mean\"})\n",
        "result = df.loc[row_list, column_list].mean()",
        "result = df.loc[row_list, column_list].sum()",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "result = df.value_counts()",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor column in df.columns:\n    result += \"---- \" + column + \" ---\\n\"\n    result += str(df[column].value_counts()) + \"\\n\"\n    result += \"Name: \" + column + \", dtype: int64\\n\"\n",
        "df.iloc[[0, 1]] = df.iloc[[0, 1]].apply(lambda x: x.str.strip())",
        "df.iloc[[0, 1]] = df.iloc[[0, 1]].apply(lambda x: x.str.strip())",
        "\nresult = df.apply(lambda x: x.dropna().values.tolist() + x.isnull().values.tolist(), 1)\n",
        "\nresult = df.apply(lambda x: x.fillna(method='ffill').fillna(method='bfill'), axis=1)\n",
        "\nresult = df.apply(lambda x: (x[x.isnull()].values.tolist() + x[x.notnull()].values.tolist()), 0)\n",
        "\nsmall_values = df.loc[df['value'] < thresh]\nsum_small_values = small_values['value'].sum()\nresult = df.copy()\nresult.loc['X'] = [sum_small_values]\nresult.drop(small_values.index, inplace=True)\n",
        "\nresult = df.loc[df['value'] < thresh]\naggregated_rows = df.loc[df['value'] >= thresh].groupby(level=0).mean()\ndf = df.drop(df.index[df['value'] >= thresh])\ndf.loc['X'] = aggregated_rows.values[0]\n",
        "\nresult = df.loc[df.index.isin(range(section_left, section_right))].mean()\n",
        "\nresult = df.join(df.apply(lambda x: 1 / x, axis=0))\n",
        "\nresult = df.join(df.apply(lambda x: np.exp(x), axis=0))\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    inv_col = f\"inv_{col}\"\n    result[inv_col] = 1 / df[col]\n",
        "\nresult = df.apply(lambda x: 1 / (1 + np.exp(-x)), axis=0)\n",
        "\nresult = df.idxmax()\nresult = result.loc[result.index <= df.idxmin().index]\n",
        "\nresult = df.idxmax()\n",
        "\nresult = df.set_index('dt').reindex(pd.date_range(df['dt'].min(), df['dt'].max())).reset_index()\nresult['val'] = result['val'].fillna(0)\n",
        "\nresult = df.set_index('dt').reindex(pd.date_range(df['dt'].min(), df['dt'].max())).reset_index()\nresult['val'] = result['val'].fillna(0)\n",
        "\nresult = df.set_index('dt').reindex(pd.date_range(df['dt'].min(), df['dt'].max())).reset_index()\nresult['val'] = 233\nresult.loc[result['val'] == 233, 'val'] = 0\nresult.loc[result['user'] == 'a', 'val'] = df.loc[df['user'] == 'a', 'val']\nresult.loc[result['user'] == 'b', 'val'] = df.loc[df['user'] == 'b', 'val']\n",
        "\nresult = df.set_index('dt').resample('D').max()\nresult['val'] = result['val'].fillna(method='ffill')\nresult = result.reset_index()\n",
        "\nresult = df.set_index('dt').resample('D').max()\nresult['val'] = result['val'].fillna(method='ffill')\nresult = result.reset_index()\n",
        "\nresult = df.groupby('name').apply(lambda x: x.assign(name=x.name.map(str))).reset_index(drop=True)\n",
        "\nresult = df.copy()\nresult['a'] = result['a'].apply(lambda x: 1 if x == 3 else 2 if x == 4 else 3 if x == 5 else x)\n",
        "\nresult = df.copy()\nresult['name'] = result['name'].apply(lambda x: 1 if x == 'Aaron' else 2 if x == 'Brave' else 3)\n",
        "\nresult = df.assign(ID=df.groupby(['name', 'a']).cumcount())\nresult = result.drop(['name', 'a'], axis=1)\n",
        "\ndf = df.set_index('user')\ndf = df.stack().reset_index()\ndf.columns = ['user', 'date', 'value', 'someBool']\ndf['date'] = df['date'].astype(str)\nresult = df.pivot_table(index='user', columns='date', values='value', fill_value=0)\nresult['someBool'] = df['someBool']\nresult = result.reset_index()\n",
        "\nresult = df.pivot_table(index='user', columns='01/12/15', values=['02/12/15', 'someBool'])\n",
        "\ndf = df.set_index('user')\ndf = df.stack().reset_index()\ndf.columns = ['user', 'date', 'value', 'someBool']\ndf['date'] = df['date'].astype(str)\ndf = df.sort_values(['user', 'date'])\n",
        "result = df.loc[:, columns][df.c > 0.5]",
        "result = df.loc[df['c'] > 0.45, columns]",
        "result = df.loc[:, columns]\nreturn result",
        "\nresult = df.loc[df['c'] > 0.5, columns]\n",
        "result = df.loc[df['c'] > 0.5, columns]",
        "\nfilter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(days=i)))\ndf = df[~df.index.isin(filter_dates)]\n",
        "\nfilter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(days=i)))\ndf = df[~df.index.isin(filter_dates)]\n",
        "\nfilter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(days=i)))\ndf = df[~df.index.isin(filter_dates)]\n",
        "\nresult = df.resample('3M').mean()\n",
        "\nresult = df.groupby(df.index // 3).agg({'col1': 'count'})\n",
        "\nresult = df.groupby(df.index // 4).agg({'col1': 'sum'})\n",
        "\nresult = df.iloc[-3::-3].mean()\n",
        "\nresult = df.rolling(3, min_periods=1).sum()\nresult = result.append(df.rolling(2, min_periods=1).mean())\n",
        "\nresult = df.rolling(3, min_periods=1).sum()\nresult.iloc[-2:] = df.rolling(2, min_periods=1).mean()\n",
        "\ndf['A'] = df['A'].fillna(method='ffill')\n",
        "\ndf.fillna(method='ffill', inplace=True)\n",
        "\ndf['A'] = df['A'].fillna(method='pad')\n",
        "df['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\ndf['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)",
        "df['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\ndf['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n",
        "df['number'] = df.duration.replace(r'\\D+', '', regex=True, inplace=True)\ndf['time'] = df.duration.replace(r'\\d+', '', regex=True, inplace=True)",
        "result = np.where((df1[columns_check_list] != df2[columns_check_list]).any(axis=1))",
        "result = [df1[column] == df2[column] for column in columns_check_list]",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\n    dates = df.index.get_level_values(1).to_list()\n    dates_parsed = [datetime.strptime(date, '%m/%d/%Y') for date in dates]\n    df['date'] = dates_parsed\n    df = df.set_index(['date'])\n    df = df.drop(['date'], axis=1)\n    df = df.to_numpy()\n    ",
        "df.index = df.index.set_levels(pd.to_datetime(df.index.get_level_values(0)), level=0)\ndf = df.swaplevel(axis=0)",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\n",
        "\ndf = df.melt(id_vars=['Country', 'Variable'], var_name='year', value_name='value')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\n",
        "\nresult = df.loc[(df['Value_B'].abs() < 1) & (df['Value_C'].abs() < 1) & (df['Value_D'].abs() < 1)]\n",
        "\nresult = df.loc[df.filter(like='Value').abs() > 1]\n",
        "\ndf = df.filter(items=['Value_B', 'Value_C', 'Value_D'], func=lambda x: abs(x) > 1)\ndf.columns = df.columns.str.replace('Value_', '')\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['C'] = df['C'].str.replace('&AMP;', '&')\n",
        "\ndf['A'] = df['A'].str.replace('&LT;', '<')\ndf['C'] = df['C'].str.replace('&LT;', '<')\n",
        "\n    for col in df.columns:\n        df[col] = df[col].str.replace('&AMP;', '&')\n    ",
        "\ndf.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, inplace=True)\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['A'] = df['A'].str.replace('1 & 1', '1 & 1 = 2')\ndf['A'] = df['A'].str.replace('1 & 0', '1 & 0 = 0')\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        first_name, last_name = name.split(' ', 1)\n        return first_name, last_name\n    else:\n        return name, None\ndf['first_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['1_name'] = df['name'].apply(lambda x: x.split()[0])\ndf['2_name'] = df['name'].apply(lambda x: x.split()[1])\ndf = df.replace(np.nan, None)\ndf = df.dropna()\ndf = df.fillna('')\n",
        "\ndef split_name(name):\n    parts = name.split()\n    if len(parts) == 1:\n        return None\n    elif len(parts) == 2:\n        return parts\n    else:\n        return [parts[0], parts[1], parts[-1]]\nresult = df.apply(lambda row: split_name(row['name']), axis=1)\n",
        "\nresult = df2.merge(df1, on='Timestamp', how='left')\n",
        "\nresult = df1.merge(df2, on='Timestamp', how='left')\n",
        "\ndf['state'] = np.where((df['col2'] <= 50) & (df['col3'] <= 50), df['col1'], df['col1'].max(axis=1))\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if (row['col2'] <= 50 and row['col3'] <= 50) else (row['col1'] + row['col2'] + row['col3']), axis=1)\n",
        "\nerrors = []\nfor index, row in df.iterrows():\n    if not row[\"Field1\"].is_integer():\n        errors.append(row[\"Field1\"])\nresult = errors\n",
        "\nresult = []\nfor index, row in df.iterrows():\n    if row[\"Field1\"].isdigit():\n        result.append(int(row[\"Field1\"]))\n",
        "\n    result = []\n    for index, row in df.iterrows():\n        if not row[\"Field1\"].isdigit():\n            result.append(row[\"Field1\"])\n    return result\n",
        "\ndf['val1'] = df['val1'] / df['val1'].sum() * 100\ndf['val2'] = df['val2'] / df['val2'].sum() * 100\ndf['val3'] = df['val3'] / df['val3'].sum() * 100\ndf['val4'] = df['val4'] / df['val4'].sum() * 100\n",
        "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\ndf = df.drop(test, errors='ignore')\n",
        "\nresult = df.loc[[x for x in test if x not in test[:i] for i in range(len(test))]]\n",
        "\nfrom scipy.spatial import distance\ndef get_nearest_neighbour(df, time_index):\n    distances = distance.cdist(df.iloc[time_index, 1:4].values, df.iloc[time_index, 1:4].values, 'euclidean')\n    nearest_neighbours = distances.argmin(axis=1)\n    return nearest_neighbours\ndef get_euclidean_distance(df, time_index):\n    distances = distance.cdist(df.iloc[time_index, 1:4].values, df.iloc[time_index, 1:4].values, 'euclidean')\n    euclidean_distances = distances.min(axis=1)\n    return euclidean_distances\nfor time in df['time'].unique():\n    nearest_neighbours = get_nearest_neighbour(df, df['time'] == time)\n    euclidean_distances = get_euclidean_distance(df, df['time'] == time)\n    df.loc[df['time'] == time, 'nearest_neighbour'] = nearest_neighbours\n    df.loc[df['time'] == time, 'euclidean_distance'] = euclidean_distances\nresult = df\n",
        "\nfrom scipy.spatial import distance\ndef farmost_neighbour(df, time_index):\n    distances = distance.cdist(df.loc[time_index, ['x', 'y']], df.loc[time_index, ['x', 'y']])\n    farmost_neighbours = []\n    for i in range(len(distances)):\n        farmost_neighbour_index = np.argmax(distances[i])\n        if farmost_neighbour_index != i:\n            farmost_neighbours.append(df.loc[time_index, 'car'][farmost_neighbour_index])\n        else:\n            farmost_neighbours.append(np.nan)\n    return farmost_neighbours\nresult = df.copy()\nfor time_index in range(len(df['time'])):\n    result.loc[time_index, 'farmost_neighbour'] = farmost_neighbour(df, time_index)\n    result.loc[time_index, 'euclidean_distance'] = distances[time_index, :]\n",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda x: \",\".join(filter(str.strip, x)), axis=1)",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda x: \"-\".join(filter(str.strip, x)), axis=1)",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda x: \"-\".join(filter(str.strip, x)), axis=1)",
        "cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda x: \"-\".join(filter(str.strip, x)), axis=1)",
        "\nsample_df = df.sample(frac=0.2, random_state=0)\nsample_df['Quantity'] = 0\nresult = df.append(sample_df)\n",
        "\ndf_sample = df.sample(frac=0.2, random_state=0)\ndf_sample['ProductId'] = 0\nresult = df.append(df_sample)\n",
        "\nfor user in df['UserId'].unique():\n    user_df = df[df['UserId'] == user]\n    sampled_df = user_df.sample(frac=0.2, random_state=0)\n    sampled_df['Quantity'] = 0\n    df.loc[user_df.index, 'Quantity'] = user_df['Quantity']\n",
        "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate",
        "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.index\n    result = duplicate\n    ",
        "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.map(lambda x: df.index[duplicate_bool.index[x]])\nduplicate",
        "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nprint(result)",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\nresult = df.loc[df['count'].eq(result)]\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\nresult = df[df['count'].eq(result)].reset_index(drop=True)\n",
        "\nresult = df.groupby(['Sp', 'Mt']).min()['count']\n",
        "\nresult = df.groupby(['Sp', 'Value'])['count'].max()\n",
        "\ndf.query(\"Category in @filter_list\")\n",
        "\ndf.query(\"Category not in @filter_list\")\n",
        "\nvalue_vars = []\nfor i in range(len(df.columns)):\n    for j in range(i+1, len(df.columns)):\n        for k in range(j+1, len(df.columns)):\n            value_vars.append(('A', df.columns[i], df.columns[k]))\n",
        "value_vars = [('A', 'B', 'C'), ('A', 'B', 'D'), ('A', 'E', 'F'), ('A', 'E', 'G'), ('A', 'E', 'H'), ('A', 'E', 'I'), ('A', 'E', 'J')]\nresult = df.melt(id_vars=['A'], value_vars=value_vars)",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].transform('cumsum')\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].apply(lambda x: 0 if x < 0 else x)\n",
        "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum(skipna=False))\n",
        "\nresult = df.groupby('r')['v'].apply(lambda x: x.sum(skipna=False))\n",
        "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum(skipna=False))\n",
        "\nresult = []\nfor i in range(len(df.columns)):\n    for j in range(i, len(df.columns)):\n        if i == j:\n            result.append(f'{df.columns[i]} {df.columns[j]} one-to-one')\n        elif i < j:\n            result.append(f'{df.columns[i]} {df.columns[j]} one-to-many')\n        else:\n            result.append(f'{df.columns[i]} {df.columns[j]} many-to-one')\nfor i in range(len(df.columns)):\n    for j in range(i, len(df.columns)):\n        if i == j:\n            result.append(f'{df.columns[j]} {df.columns[i]} one-to-one')\n        elif i < j:\n            result.append(f'{df.columns[j]} {df.columns[i]} many-to-one')\n        else:\n            result.append(f'{df.columns[j]} {df.columns[i]} many-to-many')\n",
        "\nresult = []\nfor i in range(len(df.columns)):\n    for j in range(i, len(df.columns)):\n        if i == j:\n            result.append(f'{df.columns[i]} {df.columns[j]} one-2-one')\n        elif i < j:\n            result.append(f'{df.columns[i]} {df.columns[j]} one-2-many')\n        else:\n            result.append(f'{df.columns[i]} {df.columns[j]} many-2-one')\nfor i in range(len(df.columns)):\n    for j in range(i, len(df.columns)):\n        if i == j:\n            result.append(f'{df.columns[j]} {df.columns[i]} one-2-one')\n        elif i < j:\n            result.append(f'{df.columns[j]} {df.columns[i]} many-2-one')\n        else:\n            result.append(f'{df.columns[j]} {df.columns[i]} many-2-many')\n",
        "\nresult = df.apply(lambda x: x.map(str).apply(type_relationship, axis=1))\n",
        "\nresult = df.apply(lambda x: x.map(lambda y: str(type(y))).apply(lambda x: x.str.replace('<', '').str.replace('>', '').str.replace('int', '').str.replace('float', '').str.replace('object', '').str.replace(' ', '')), axis=1)\n",
        "\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\nresult = df.loc[uniq_indx]\n",
        "df.astype(str).str.replace(',','')",
        "\nresult = df.groupby(df['SibSp'].gt(0) | df['Parch'].gt(0)).mean()['Survived']\n",
        "\nresult = df.groupby([\"Survived\", \"Parch\"])[\"SibSp\"].mean()\n",
        "\nresult = df.groupby([\"SibSp\", \"Parch\"])[\"Survived\"].mean()\n",
        "\nresult = df.groupby('cokey').sort_values('A')\n",
        "\nresult = df.groupby('cokey').sort_values('A')\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(l)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(l)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(l)\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "stdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('a').b.apply(stdMeann))",
        "result = df.groupby('b')['a'].agg(['mean', 'std'])",
        "\ndf['softmax'] = df.groupby('a')['b'].apply(lambda x: x / x.sum()).apply(lambda x: x.apply(lambda y: np.exp(y)))\ndf['min-max'] = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n",
        "\nresult = df.drop(columns=['C'])\n",
        "\nresult = df.loc[df.sum(axis=1) != 0]\n",
        "\nresult = df.loc[:, df.max() == 2]\n",
        "\nresult = df.copy()\nresult.loc[result == 2] = 0\n",
        "s.sort_values(ascending=True, inplace=True)",
        "s.sort_values(ascending=True, inplace=True)",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "result = df[df['A'].apply(lambda x: isinstance(x, str))]",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\nresult = df.loc[df['count'].eq(result)]\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\nresult = df[df['count'].eq(result)].reset_index(drop=True)\n",
        "\nresult = df.groupby(['Sp', 'Mt']).min()['count']\n",
        "\nresult = df.groupby(['Sp', 'Value'])['count'].max()\n",
        "\ndf['Date'] = df['Member'].map(dict)\n",
        "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(17/8/1926)\n",
        "\nresult = df.copy()\nresult['Date'] = result['Member'].map(dict)\nresult['Date'] = result['Date'].fillna(result['Member'])\n",
        "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17-Aug-1926')\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\ndf1['Count_m'] = df1.month.map(df1.groupby('month').size())\ndf1['Count_y'] = df1.year.map(df1.groupby('year').size())\nresult = df1.join(df.drop('Count_d', axis=1))",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').size()\ndf1 = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).agg({'count': ['size', 'count']})\nresult = df.merge(df1, on=['Date', 'Val'], how='left')\nresult.columns = ['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_Val']\nprint(result)",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').size()\ndf1 = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).agg({'count': ['size', 'sum']})\nresult = df.merge(df1, on=['Date', 'Val'], how='left')\nresult.columns = ['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_w', 'Count_Val']\nprint(result)",
        "\nresult1 = df.eq(0).groupby(['Date']).sum()\nresult2 = df.ne(0).groupby(['Date']).sum()\n",
        "\nresult1 = df.groupby(['Date']).apply(lambda x: (x['B'] % 2 == 0).sum() + (x['C'] % 2 == 0).sum())\nresult2 = df.groupby(['Date']).apply(lambda x: (x['B'] % 2 != 0).sum() + (x['C'] % 2 != 0).sum())\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\nresult = df.set_index('var1').apply(lambda x: x.str.split(',')).explode('var2').reset_index()\n",
        "\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2')\n",
        "\nresult = df.assign(var2=df['var2'].str.split('-')).explode('var2')\n",
        "def count_special_char(string):\n    special_char = 0\n    for char in string:\n        if char.isalpha():\n            continue\n        else:\n            special_char += 1\n    return special_char\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n",
        "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\ndf[\"new\"] = df.apply(count_special_char, axis=1)\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\n",
        "\ndf['fips'] = df['row'].str[:3]\ndf['row'] = df['row'].str[3:]\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['medi'] = df['row'].str[5:10]\ndf['row'] = df['row'].str[10:]\n",
        "\ndf['2001'] = df['2001'].fillna(0)\ndf['2002'] = df['2002'].fillna(0)\ndf['2003'] = df['2003'].fillna(0)\ndf['2004'] = df['2004'].fillna(0)\ndf['2005'] = df['2005'].fillna(0)\ndf['2006'] = df['2006'].fillna(0)\ndf['cum_avg'] = df.iloc[:, 1:].apply(lambda x: x.cumsum() / (x.notnull().cumsum() + 1), axis=1)\nresult = df\n",
        "\ndf['2001'] = df['2001'].fillna(0)\ndf['2002'] = df['2002'].fillna(0)\ndf['2003'] = df['2003'].fillna(0)\ndf['2004'] = df['2004'].fillna(0)\ndf['2005'] = df['2005'].fillna(0)\ndf['2006'] = df['2006'].fillna(0)\nfor i in range(len(df.columns) - 1, -1, -1):\n    df.iloc[:, i] = df.iloc[:, i].cumsum()\nfor i in range(len(df.columns) - 1, -1, -1):\n    df.iloc[:, i] = df.iloc[:, i].div(df.iloc[:, i].ne(0).cumsum())\n",
        "\n    result = df.copy()\n    for i in range(1, len(df.columns)):\n        result.iloc[:, i] = result.iloc[:, i-1].add(df.iloc[:, i]).div(2)\n        result.iloc[:, i] = result.iloc[:, i].replace(0, np.nan)\n        result.iloc[:, i] = result.iloc[:, i].fillna(method='ffill').fillna(method='bfill')\n        result.iloc[:, i] = result.iloc[:, i].replace(np.nan, 0)\n        result.iloc[:, i] = result.iloc[:, i].div(i)\n ",
        "\nfor i in range(len(df.columns) - 1, -1, -1):\n    df[f'{df.columns[i]}_cum_avg'] = df[df.columns[i]].cumsum() / (df[df.columns[i]].notnull().cumsum() + 1)\n",
        "\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0)\n",
        "\ndf['label'] = 1\nfor i in range(1, len(df)):\n    diff = df['Close'][i] - df['Close'][i-1]\n    if diff > 0:\n        df['label'][i] = 1\n    elif diff < 0:\n        df['label'][i] = -1\n    else:\n        df['label'][i] = 0\n",
        "\ndf['label'] = (df['Close'].diff() > 0).astype(int) - 1\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'one').sum())\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'two').sum())\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'].str.endswith('e')].count()['key2'])\n",
        "max_result = df.index.max()\nmin_result = df.index.min()",
        "\nmode_result = df.index[df.value.mode()[0]]\nmedian_result = df.index[df.value.median()]\n",
        "df = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]",
        "df = df[~(df['closing_price'].between(99, 101))]",
        "\nresult = df.groupby(\"item\", as_index=False)[\"diff\"].min().merge(df, on=\"item\", how=\"left\")\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1, expand=True)[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', n=1, expand=True)[1]\n",
        "\n    result = df['SOURCE_NAME'].str.split('_').str[-1]\n    ",
        "\nfirst_half_index = int(len(df) / 2)\ndf.iloc[:first_half_index, :].fillna(0, inplace=True)\ndf.iloc[first_half_index:, :].fillna(1, inplace=True)\n",
        "\ndf['Column_x'] = df['Column_x'].fillna(method='ffill')\ndf.loc[df['Column_x'].isna(), 'Column_x'] = 0\ndf.loc[df['Column_x'].isna(), 'Column_x'] = 0.5\ndf.loc[df['Column_x'].isna(), 'Column_x'] = 1\n",
        "\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace= True)\n",
        "\nresult = pd.DataFrame([list(zip(a.values.flatten(), b.values.flatten()))], columns=['one', 'two'])\n",
        "\nresult = pd.DataFrame([[a.iloc[i, 0], b.iloc[i, 0], c.iloc[i, 0]], [a.iloc[i, 1], b.iloc[i, 1], c.iloc[i, 1]]], columns=['one', 'two'])\n",
        "result = pd.DataFrame([list(zip(a.itertuples(index=False), b.itertuples(index=False)))], columns=['one', 'two'])",
        "groups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.username.count()",
        "groups = df.groupby(['username', pd.cut(df.views, bins)]).username.count()\nresult = groups.unstack()",
        "groups = df.groupby(pd.cut(df.views, bins))\nresult = groups.username.value_counts()",
        "df['text'] = df['text'].apply(lambda x: ', '.join(x))\nresult = df.groupby('text').agg({'text': ', '.join})",
        "df['text'] = df['text'].apply(lambda x: '-'.join(x))\nresult = df.groupby('text').agg({'text': 'first'})",
        "df['text'] = df['text'].apply(lambda x: ', '.join(sorted(x)))",
        "df.agg(' '.join, axis=1)",
        "df['text'] = df['text'].astype(str).apply(lambda x: ''.join(sorted(x)))\nresult = df['text'].str.cat(sep='-')",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['city'] = result['city'].fillna(method='ffill')\nresult['district'] = result['district'].fillna(method='ffill')\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = result['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\nresult = result.sort_values(['id', 'date'], ascending=[True, True])\n",
        "\nresult = pd.concat([df1, df2], axis=0).sort_values(['id', 'date'])\n",
        "\nresult = C.merge(D, how='left', on='A')\nresult.loc[result['B_y'].notna(), 'B'] = result['B_y']\nresult = result.drop(columns=['B_y'])\n",
        "\nresult = C.merge(D, how='left', on='A')\n",
        "\nresult = C.merge(D, how='left', on='A', suffixes=('_x', '_y'))\nresult['dulplicated'] = result['A_x'] == result['A_y']\nresult = result.drop(['A_y', 'B_y'], axis=1)\n",
        "result = df.groupby('user').agg(lambda x: x.tolist()).apply(lambda x: sorted(x, key=lambda y: (y[0], y[1])))",
        "result = df.groupby('user').agg(lambda x: x.tolist()).apply(sorted, axis=1)",
        "result = df.groupby('user').agg(lambda x: x.tolist()).apply(lambda x: sorted(x, key=lambda y: (y[1], y[0])))",
        "\ndf_concatenated = series.to_frame()\nresult = df_concatenated\n",
        "\ndf_concatenated = series.to_frame()\nresult = df_concatenated\n",
        "result = [col for col in df.columns if s in col and col != s]",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = df.rename(columns=lambda x: f'spike{df.columns.get_loc(x)}' if s in x else x)\n",
        "\nresult = df['codes'].apply(pd.Series).add_prefix('code_').to_frame()\n",
        "\nresult = df['codes'].apply(pd.Series).add_prefix('code_').to_frame()\n",
        "\nresult = df['codes'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('code_1').to_frame()\nresult['code_2'] = result['code_1'].shift(-1).fillna(np.nan)\nresult['code_3'] = result['code_1'].shift(-2).fillna(np.nan)\nprint(result)\n",
        "\nresult = [item for sublist in df['col1'] for item in sublist]\n",
        "\nresult = ''.join(str(x) for x in df['col1'].values.flatten())\n",
        "\nresult = ','.join(str(x) for x in df['col1'].values.flatten())\n",
        "\ndf = df.set_index('Time')\ndf = df.resample('2T').mean()\ndf = df.reset_index()\n",
        "\ndf['Time'] = df['Time'].dt.floor('3min')\ndf = df.groupby('Time').sum()\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\ndf['RANK'] = df.groupby('ID')['TIME'].transform(lambda x: x.rank(ascending=False))\ndf['TIME'] = df['TIME'].apply(lambda x: x.strftime('%d-%b-%Y %a %H:%M:%S'))\n",
        "\nresult = df[filt.index.get_level_values('a').isin(filt.index)]\n",
        "\nresult = df[filt.index.isin(df.index)]\n",
        "\nresult = df.columns[df.iloc[0] != df.iloc[8]]\n",
        "\nresult = df.columns[df.iloc[0].isin(df.iloc[8])]\n",
        "\nresult = []\nfor i in range(len(df.columns)):\n    if not equalp(df.iloc[0, i], df.iloc[8, i]):\n        result.append(df.columns[i])\n",
        "\nresult = []\nfor i in range(len(df.columns)):\n    if df.iloc[0, i] != df.iloc[8, i]:\n        if math.isnan(df.iloc[0, i]) and math.isnan(df.iloc[8, i]):\n            continue\n        result.append((df.iloc[0, i], df.iloc[8, i]))\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n",
        "\nresult = df.T.reset_index(drop=True).T\n",
        "\nresult = df.T.reset_index(drop=True).T\n",
        "\ndf['dogs'] = df['dogs'].round(2)\n",
        "\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if not isinstance(x, float) else x)\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2) if not isinstance(x, float) else x)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "df['Avg'] = df[list_of_my_columns].mean(axis=1)",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf.sort_index(level=['time'], ascending=True, inplace=True)\n",
        "\ndf.sort_index(level=['treatment', 'dose'], ascending=[True, True], inplace=True)\n",
        "\ndf = df.loc[~(df.index.isin(['2020-02-17', '2020-02-18']))]\n",
        "\ndf = df.loc[~((df.index.date == '2020-02-17') | (df.index.date == '2020-02-18'))]\n",
        "\nresult = corr.loc[:, corr.ge(0.3)]\n",
        "\nresult = corr.loc[:, corr.gt(0.3)]\n",
        "\ndf.columns[-1] = 'Test'\n",
        "\ndf.iloc[:, 0] = 'Test'\n",
        "\nresult['frequent'] = result.apply(lambda row: row.idxmax(), axis=1)\nresult['freq_count'] = result.apply(lambda row: (row == row.idxmax()).sum(), axis=1)\n",
        "\nfrom collections import Counter\ndef find_frequent_values(row):\n    counts = Counter(row)\n    frequent, freq_count = counts.most_common(1)[0]\n    return frequent, freq_count\nresult = df.copy()\nresult['frequent'] = result.apply(lambda row: find_frequent_values(row)[0], axis=1)\nresult['freq_count'] = result.apply(lambda row: find_frequent_values(row)[1], axis=1)\n",
        "\nfrom collections import Counter\ndef find_frequent_values(row):\n    counts = Counter(row)\n    frequent_values = [value for value, count in counts.items() if count == max(counts.values())]\n    return frequent_values\nresult = df.apply(lambda row: find_frequent_values(row), axis=1)\n",
        "result = df.groupby([\"id1\", \"id2\"])[[\"foo\", \"bar\"]].mean()",
        "\nresult = df.groupby([\"id1\", \"id2\"])[[\"foo\", \"bar\"]].mean()\n",
        "\nresult = df_a.merge(df_b, on='EntityNum', how='left')\nresult = result.drop('b_col', axis=1)\n",
        "result = df_a.merge(df_b, on='EntityNum', how='left')\nresult = result.drop('a_col', axis=1)\nprint(result)"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "result = [list(filter(lambda x: not np.isnan(x), i)) for i in x]\n",
        "b = np.eye(a.max()+1)[a]",
        "b = np.eye(a.max()+1)[a]",
        "b = np.eye(a.max() + 1)[a.argsort()]",
        "\nb = np.zeros((len(a), len(a)), dtype=int)\nfor i in range(len(a)):\n    b[i, np.searchsorted(a, a[i])] = 1\n",
        "\nb = np.zeros((a.max()+1, a.size), dtype=np.int8)\nfor i in range(a.size):\n    b[a[i], i] = 1\n",
        "np.percentile(a, p)",
        "B = np.reshape(A, (-1, ncol))",
        "B = np.reshape(A, (nrow, -1))",
        "B = np.reshape(A, (-1, ncol))",
        "B = A.reshape(-1, ncol)",
        "\nresult = np.roll(a, shift)\n",
        "\nresult = np.roll(a, shift, axis=0)\n",
        "result = a[:, np.roll(np.arange(a.shape[1]), shift, axis=0)]",
        "\nr_old = r_new = np.random.RandomState(42).randint(3, size=(100, 2000)) - 1\n",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "z = any(isnan(a), axis=0)\ndelete(a, z, axis = 1)",
        "\na = np.delete(a, np.where(np.isnan(a).any(axis=1)), axis=0)\n",
        "\nresult = np.array(a)\n",
        "a[:, permutation]",
        "result = np.array(a)[permutation]",
        "\nresult = a.argmin()\n",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nresult = a.argmin(axis=None)\n",
        "\nresult = np.sin(np.rad2deg(degree))\n",
        "\nresult = np.cos(np.rad2deg(degree))\n",
        "\nif np.sin(number * np.pi / 180) > np.sin(number * (180 / np.pi)):\n    result = 1\nelse:\n    result = 0\n",
        "result = np.arcsin(value) * 180 / np.pi",
        "\nresult = np.pad(A, (0, (length - A.size) % length), 'constant', constant_values=0)\n",
        "\nresult = np.pad(A, (0, (length - A.size) % length), 'constant', constant_values=0)\n",
        "a = np.power(a, power)",
        "result = np.power(a, power)",
        "result = np.divmod(numerator, denominator)",
        "result = np.divide(numerator, denominator)\nreturn result",
        "\nresult = np.divmod(numerator, denominator)\n",
        "result = (a + b + c) / 3",
        "result = np.maximum(a, b)\nresult = np.maximum(result, c)",
        "\nresult = a[::-1, ::-1]\n",
        "\nresult = a[::-1, ::-1]\n",
        "\nresult = np.diag_indices(5, offset=1)\n",
        "\nresult = np.diag_indices(a.shape[0], k=-1)\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n",
        "result = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\n",
        "\nresult = []\nfor i in range(r):\n    for j in range(c):\n        result.append(X[i, j])\n",
        "mystr = \"100110\"\nresult = np.array([int(digit) for digit in mystr])\nprint(result)",
        "\nmultiplied_column = a[:, col] * multiply_number\nresult = np.cumsum(multiplied_column)\n",
        "\nmultiplied_row = a[row] * multiply_number\ncumulative_sum = np.cumsum(multiplied_row)\nresult = cumulative_sum\n",
        "\na[row, :] = a[row, :] / divide_number\nresult = np.prod(a[row, :])\n",
        "\nresult = a[:, np.any(a, axis=0)]\n",
        "result = a.shape[1]",
        "\nn1, n2 = len(a), len(b)\nmean_a, std_a = np.mean(a), np.std(a)\nmean_b, std_b = np.mean(b), np.std(b)\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n",
        "\na_mean = np.mean(a)\na_std = np.std(a)\nb_mean = np.mean(b)\nb_std = np.std(b)\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(np.array([amean, bmean]), np.array([avar, bvar]), equal_var=False, nobs=[anobs, bnobs])\n",
        "\noutput = [x for x in A if x not in B]\n",
        "\noutput = np.concatenate((A[~np.isin(A, B)], B[~np.isin(B, A)]))\n",
        "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]",
        "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]",
        "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]",
        "\nresult = b[np.argsort(a.sum(axis=1))]\n",
        "a[:, 2] = 0",
        "a = np.delete(a, 2, 0)",
        "a[:, [1, 3]] = np.delete(a, [0, 2], axis=1)",
        "\nresult = a[:, ~np.isin(np.arange(4), del_col)]\n",
        "a.insert(pos, element)",
        "a = np.insert(a, pos, element, axis=0)",
        "a = np.insert(a, pos, element)\n",
        "a = np.insert(a, pos, element, axis=0)",
        "\nresult = [np.copy(arr) for arr in array_of_arrays]\n",
        "\nresult = np.all(np.equal(a[0], a))\n",
        "\nresult = np.all(np.diff(a, axis=1) == 0)\n",
        "\nresult = np.all(np.equal(a[0], a))\n",
        "\nresult = 0\nfor i in range(1, len(x)-1):\n    for j in range(1, len(y)-1):\n        result += (4/3)*x[i]*y[j]*((np.cos(x[i]))**4 + (np.sin(y[j]))**2)\nresult += (1/3)*(x[-1]*y[-1]*((np.cos(x[-1]))**4 + (np.sin(y[-1]))**2) + x[0]*y[0]*((np.cos(x[0]))**4 + (np.sin(y[0]))**2))\n",
        "\n    result = (cosx)**4 + (siny)**2\n    ",
        "\nresult = ecdf(grades)\n",
        "\nresult = ecdf(grades)(eval)\n",
        "\nlow = grades.searchsorted(threshold)\nhigh = grades.searchsorted(threshold, side='right')\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "a_np = a.numpy()",
        "a_pt = torch.from_numpy(a)",
        "a_np = np.array(a)",
        "a_tf = tf.convert_to_tensor(a)",
        "result = a.argsort()[::-1]",
        "result = [i for i, v in enumerate(a) if v == sorted(a)[i]]",
        "\nresult = np.argsort(-a)[:N]\n",
        "\nresult = np.power(A, n)\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size])\n",
        "\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i, j] = a[i // 2, j // 2][i % 2 * 3 + j % 2]\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size])\n",
        "result = a[:, low:high]",
        "result = a[low:high+1]",
        "result = a[:, low:high]",
        "a = np.array(eval(string))",
        "\nbase = np.e\nresult = np.logspace(np.log10(min), np.log10(max), num=n, base=base)\n",
        "\nresult = np.random.uniform(np.exp(min), np.exp(max), size=n).log()\n",
        "\n    result = np.random.uniform(min, max, n)\n    result = np.log(result)\n    ",
        "B = pd.Series(np.zeros(len(A)))\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\nB[0] = a * A[0]",
        "B = A.copy()\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]",
        "result = np.array([])\n",
        "\nresult = np.zeros((3,0))\n",
        "linear_index = np.ravel_multi_index(index, dims) - 1",
        "\nresult = np.ravel_multi_index(index, dims)\n",
        "values = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])",
        "\nresult = np.bincount(accmap, a)\n",
        "\nresult = np.max(a[index], axis=0)\n",
        "\nresult = np.bincount(accmap, a)\n",
        "\nresult = np.minimum.reduceat(a, index)\n",
        "z = elementwise_function(x, y)",
        "np.random.choice(lista_elegir, samples, p=probabilit)",
        "\nresult = a[low_index:high_index+1, low_index:high_index+1]\n",
        "\nresult = x[x >= 0]\n",
        "result = x[np.iscomplex(x)]",
        "\nbin_data = np.array_split(data, len(data) // bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size)\nbin_data_mean = [np.mean(bin, axis=1) for bin in bin_data]\n",
        "\nbin_data = []\nfor i in range(len(data) - bin_size + 1):\n    bin_data.append(data[i:i+bin_size])\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size, axis=1)\nbin_data_mean = [np.mean(bin, axis=1) for bin in bin_data]\n",
        "\nbin_data = []\nfor i in range(0, data.shape[0], bin_size):\n    bin_data.append(data[i:i+bin_size])\nbin_data_mean = [np.mean(bin, axis=0) for bin in bin_data]\n",
        "\ndef smoothclamp(x):\n    return 3 * (x - x_min) * (x - x_min) * (x - x_max) * (x - x_max) * (x - x_max) * (x - x_max)\n",
        "\ndef smoothclamp(x, N):\n    x_min = 0\n    x_max = 1\n    t = (x - x_min) / (x_max - x_min)\n    return np.poly1d(np.polynomial.legendre.legendre(N))(t) * (x_max - x_min) + x_min\n",
        "\nfrom scipy.signal import correlate2d\nresult = correlate2d(a, b, boundary='periodic')\n",
        "result = df.unstack('major').values",
        "result = df.unstack('major').values",
        "\nresult = np.array([np.unpackbits(np.uint8(num)) for num in a])\n",
        "\nresult = np.array([np.unpackbits(np.uint8(num)) for num in a])\n",
        "\nresult = np.array([np.unpackbits(np.uint8(num)) for num in a])\n",
        "\nmean = a.mean()\nstd = a.std()\nresult = (mean - 3 * std, mean + 3 * std)\n",
        "\nmean = a.mean()\nstd = a.std()\nresult = (mean - 2 * std, mean + 2 * std)\n",
        "\nmean = a.mean()\nstd = a.std()\nresult = (mean - 3 * std, mean + 3 * std)\n",
        "\nmean = a.mean()\nstd = a.std()\ntwo_std = 2 * std\nresult = np.logical_or(a < (mean - two_std), a > (mean + two_std))\n",
        "masked_data = ma.masked_where(DataArray < 0, DataArray)\nprob = np.percentile(masked_data, percentile)",
        "a[zero_rows, :] = 0\na[:, zero_cols] = 0",
        "\nfor row in zero_rows:\n    a[row] = 0\nfor col in zero_cols:\n    a[:, col] = 0\n",
        "a[:, 0] = 0\na[1, :] = 0",
        "\nmask = np.zeros(a.shape, dtype=bool)\nmask[np.arange(a.shape[0]), np.argmax(a, axis=1)] = True\n",
        "mask = (a == a.min(axis=1)).astype(np.bool)\n",
        "\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nresult = np.stack([xi.dot(xi.T) for xi in X.T], axis=2)\n",
        "\nX = np.array([[81, 4, 25, 25],\n              [63, 12, 35, 30],\n              [63, 8, 25, 10],\n              [49, 36, 49, 36],\n              [49, 24, 35, 12],\n              [25, 16, 25, 4]])\n",
        "is_contained = number in a",
        "C = A[~np.isin(A, B)]",
        "C = A[np.isin(A, B)]",
        "\nC = A[(A >= B[0]) & (A <= B[1])]\nC = np.append(C, A[(A >= B[1]) & (A <= B[2])])\n",
        "\nresult = rankdata(a).astype(int)\nresult = result[::-1]\n",
        "\nresult = rankdata(a).astype(int)\nresult = result[::-1]\n",
        "\nresult = rankdata(a).astype(int)\nresult = result[::-1]\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\nresult = a[:, second, third]\n",
        "arr = np.zeros((20,10,10,2))",
        "l1 = LA.norm(X, ord=1, axis=1)\nresult = X / l1[:, np.newaxis]",
        "result = X / LA.norm(X, ord=2, axis=1)[:, np.newaxis]\nprint(result)",
        "result = X / np.expand_dims(x, axis=1)\nprint(result)",
        "conditions = [df['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)",
        "\nfrom scipy.spatial.distance import pdist\nresult = pdist(a, 'euclidean')\n",
        "\ndistances = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        distances[i,j] = np.linalg.norm(a[i] - a[j])\n        distances[j,i] = distances[i,j]\n",
        "\ndistances = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        distances[i,j] = np.linalg.norm(a[i] - a[j])\n        distances[j,i] = distances[i,j]\n",
        "AVG = np.mean(NA, axis=0)\n",
        "AVG = np.mean(NA, axis=0)\n",
        "A = [float(i) for i in A]\n",
        "\nresult = np.unique(a[~np.all(a == 0, axis=0)])\n",
        "\nresult = np.unique(a, axis=0)\n",
        "\ndf = pd.DataFrame(np.dstack((lat, lon, val)), columns=['lat', 'lon', 'val'])\n",
        "\ndf = pd.DataFrame(np.dstack((lat, lon, val)),\n                  columns=['lat', 'lon', 'val'])\n",
        "\ndf = pd.DataFrame(np.column_stack((lat, lon, val)), columns=['lat', 'lon', 'val'])\ndf['maximum'] = df.max(axis=1)\n",
        "\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        result.append(a[i:i+size[0], j:j+size[1]])\n",
        "\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        window = a[i:i+size[0], j:j+size[1]]\n        if window.size == size:\n            result.append(window)\n",
        "\nresult = np.mean(a)\n",
        "\nresult = a.mean()\n",
        "\nresult = Z[:, :, -1:]\n",
        "\nresult = a[-1:, :]\n",
        "\nresult = np.any(np.array_equal(c, CNTS))\n",
        "\nresult = any(np.array_equal(c, CNTS_item) for CNTS_item in CNTS)\n",
        "\nf = intp.interp2d(a.ravel(), x_new, y_new)\nresult = f(x_new, y_new)\n",
        "df[name] = df.groupby('D')['Q'].transform(lambda x: x.cumsum())",
        "\ni = np.diag(i.ravel())\n",
        "a[np.triu_indices(a.shape[0], k=1)] = 0",
        "\nresult = np.linspace(dateutil.parser.parse(start), dateutil.parser.parse(end), n)\n",
        "result = np.where(x == a)[0][0]\nif y[result] == b:\n    print(result)\nelse:\n    print(-1)",
        "result = []\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result.append(i)\nprint(result)",
        "\nresult = np.polyfit(x, y, 2)\n",
        "\nfrom scipy.optimize import least_squares\ndef f(x, a, b, c):\n    return a + b * x + c * x ** 2\nresult = least_squares(f, p0=[0, 0, 0], x=x, y=y, loss='soft_l1', ftol=1e-6, max_nfev=10000)\n",
        "temp_arr = [0, 1, 2, 3]\ndf.apply(lambda x: x - temp_arr[a], axis=1)",
        "\nresult = np.einsum('ijk,lm->ijlm', A, B)\n",
        "\nresult = MinMaxScaler().fit_transform(a)\n",
        "\nresult = MinMaxScaler().fit_transform(arr)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n",
        "\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n",
        "\nmask = arr < n1\nmask2 = arr >= n2\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n",
        "\nresult = np.nonzero(s1 != s2)[0].shape[0]\n",
        "\nresult = np.nonzero(s1 != s2)[0].shape[0]\n",
        "\nresult = all(np.array_equal(a[0], b) for b in a[1:])\n",
        "\nresult = all(np.isnan(arr).any() for arr in a)\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.pad(a, ((shape[0]-a.shape[0], 0), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n",
        "result = np.zeros(shape)\nresult[:arr.shape[0], :arr.shape[1]] = arr\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\na.reshape(int(a.shape[0]/3), 3)\n",
        "\nresult = a[:,:,b]\n",
        "\nresult = a[:,:,b]\n",
        "\nresult = a[np.arange(a.shape[0])[:, None], b]\n",
        "\nresult = np.sum(a[:,:,b], axis=2)\n",
        "\nresult = np.sum(a[:,:,b[:,:,0]], axis=2)\n",
        "\nresult = df.loc[df['a'].between(1, 4), 'b']\n",
        "\nresult = im[1:4, 1:5]\n",
        "\nresult = A[2:5, 2:5]\n",
        "\nresult = im[~np.all(im == 0, axis=0)]\n",
        "\nresult = im[1:-1, 1:-1]\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label=\"x-y\")\nplt.legend()\n",
        "\nplt.gca().yaxis.set_minor_locator(plt.LinearLocator(10))\n",
        "\nplt.minorticks_on()\n",
        "\nplt.minorticks_on()\n",
        "\nstyles = ['-', '--', '-.', ':']\ny_values = [np.random.randint(0, 10) for _ in range(len(styles))]\nfor i, style in enumerate(styles):\n    plt.plot(x, y_values[i], style)\nplt.show()\n",
        "\nstyles = ['-', '--', '-.', ':']\ny_values = [np.random.randint(0, 10) for _ in range(len(styles))]\nfor i, style in enumerate(styles):\n    plt.plot(x, y_values[i], style)\nplt.show()\n",
        "\nplt.plot(x, y, marker='d', markersize=2, linestyle='-', linewidth=1)\n",
        "\nplt.plot(x, y, marker='d', markersize=10, linewidth=3)\n",
        "\nax.set_ylim(0, 40)\n",
        "\nplt.plot(x, color='red', where=(x >= 2) & (x <= 4))\n",
        "\nx = np.linspace(0, 1, 100)\ny = 2 * x\nplt.plot(x, y)\nplt.show()\n",
        "\nx = np.linspace(0, 1, num=100)\ny = 2 * x\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Segment from (0,0) to (1,2)')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nsns.regplot(x=\"Height (cm)\", y=\"Weight (kg)\", data=df, ax=ax, color=\"blue\")\nsns.regplot(x=\"Height (cm)\", y=\"Weight (kg)\", data=df, ax=ax, color=\"red\", linestyle=\"dashed\")\nsns.regplot(x=\"Height (cm)\", y=\"Weight (kg)\", data=df, ax=ax, color=\"green\", linestyle=\"dotted\")\nsns.regplot(x=\"Height (cm)\", y=\"Weight (kg)\", data=df, ax=ax, color=\"black\", linestyle=\"dashdot\")\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"Weight (kg)\")\nax.set_title(\"Relation between Height and Weight by Gender\")\nfor i, gender in enumerate(_genders):\n    ax.plot(df.loc[df[\"Gender\"] == gender, \"Height (cm)\"], df.loc[df[\"Gender\"] == gender, \"Weight (kg)\"], color=i+1, linestyle=i+1, marker=\"o\", markersize=5)\n",
        "\nfig, ax = plt.subplots()\nsns.scatterplot(x, y, ax=ax)\n",
        "\nfig, ax = plt.subplots()\nsns.lineplot(x, y, ax=ax)\n",
        "\nplt.plot(x, y, marker='+', markersize=7, linewidth=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.legend(title=\"xyz\", fontsize=20)\n",
        "\nl.set_facecolor((1, 1, 1, 0.2))\n",
        "\nl.set_markeredgecolor(\"k\")\n",
        "\nl.set_color('r')\nl.set_markerfacecolor('r')\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(np.arange(0, 2 * np.pi, 2))\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H, cmap='viridis')\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.xlabel(\"X\")\nplt.xlim(x[0], x[-1])\n",
        "\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
        "\nmyTitle = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\ntitle_list = myTitle.split(\"-\")\nplt.suptitle(\" \".join(title_list))\n",
        "\nplt.gca().invert_yaxis()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, color='blue', alpha=0.5)\nax.plot(y, color='red', alpha=0.5)\nax.plot(z, color='green', alpha=0.5)\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolor='black', facecolor='blue')\n",
        "\nplt.xticks(x, x)\nplt.yticks(np.arange(0, 2.1, 1))\n",
        "\nplt.ticklabel_format(style='plain', axis='y')\n",
        "\nax = sns.lineplot(x=x, y=y, dashes=True)\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(10, 6))\nax1.plot(x, y1, label='sin(x)')\nax1.set_ylabel('sin(x)')\nax1.grid()\nax2.plot(x, y2, label='cos(x)')\nax2.set_ylabel('cos(x)')\nax2.grid()\nplt.suptitle('sin(x) and cos(x) plots')\nplt.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(10, 5))\nax1.plot(x, y1)\nax1.set_title('sin(x)')\nax2.plot(x, y2)\nax2.set_title('cos(x)')\nfor ax in (ax1, ax2):\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\nplt.show()\n",
        "\nplt.gca().set_xlabel('')\n",
        "\nplt.xticks([])\n",
        "\nplt.xticks([3, 4])\nplt.grid(True, which='major', axis='x', linestyle='-', linewidth='0.5', color='k')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y', linestyle='--')\n",
        "\nplt.yticks([3, 4])\nplt.grid(True, axis='y', linestyle='-', which='major', color='k', alpha=0.5)\nplt.xticks([1, 2])\nplt.grid(True, axis='x', linestyle='-', which='major', color='k', alpha=0.5)\n",
        "\nplt.grid(True)\n",
        "\nplt.legend(loc='lower right')\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\nplt.show()\nplt.clf()\n",
        "\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n",
        "\nax.set_xlabel(\"\", labelpad=40)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.tick_params(axis='x', labelbottom=True, labeltop=False, labelright=False, labelleft=False, bottom=False, top=False, right=False, left=False, labelbottom=True, pad=20)\n",
        "\nplt.plot(x, y)\nplt.yticks([])\n",
        "\nplt.plot(x, y)\nplt.yticks(np.arange(10), rotation=90)\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.yticks(np.arange(10))\nplt.tick_params(axis='y', labelleft=True, labelright=True)\n",
        "\nfig, ax = plt.subplots()\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips, ax=ax, kind=\"reg\", color=\"green\")\nax.scatter(tips[\"total_bill\"], tips[\"tip\"], color=\"green\")\nsns.distplot(tips[\"total_bill\"], ax=ax, color=\"blue\")\n",
        "\nfig, ax = plt.subplots()\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips, ax=ax, kind=\"reg\", color=\"green\")\nax.set_xlabel(\"Total Bill\")\nax.set_ylabel(\"Tip\")\n# Change the color of the histograms to blue\nsns.distplot(tips[\"total_bill\"], color=\"blue\", ax=ax)\nsns.distplot(tips[\"tip\"], color=\"blue\", ax=ax)\n",
        "\ng = sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\")\n",
        "\nfig, ax = plt.subplots()\ndf.plot(kind='bar', x='celltype', y=['s1', 's2'], ax=ax)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\nfig, ax = plt.subplots()\ndf.plot(kind='bar', x='celltype', y=['s1', 's2'], ax=ax, rot=45)\nax.set_xlabel('celltype')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xticks(x, color='red')\nplt.tick_params(axis='x', labelcolor='red')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.axhline(y=0, color='r', linestyle='-')\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='both', which='major', labelsize=10)\nplt.xticks(rotation=90)\n",
        "\nplt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], 'k', linewidth=2)\nplt.plot([0.22058956, 0.33088437, 2.20589566], [1, 1, 1], 'k', linewidth=2)\n",
        "\nplt.imshow(rand_mat, cmap='viridis')\nplt.xticks(range(4), xlabels, rotation=45, ha='right')\nplt.yticks(range(4), ylabels[::-1])\nplt.ylabel('Inverted Y-axis labels')\nplt.xlabel('X-axis labels on top')\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n# copy the code of the above plot and edit it to have legend for all three curves in the two subplots\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(10, 4))\nax1.plot(x, y)\nax1.set_title(\"Y\")\nax2.plot(x, y)\nax2.set_title(\"Y\")\n",
        "\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n                markersize=30, edgecolor=\"k\")\n",
        "\nfig, ax = plt.subplots()\nax.scatter(a, b)\nfor i in range(len(a)):\n    ax.annotate(str(c[i]), (a[i], b[i]))\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend Title\")\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", prop={'size': 15, 'weight': 'bold'})\n",
        "\nplt.hist(x, edgecolor='k', linewidth=1.2)\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(10, 10))\nax1.set_position([0.125, 0.5, 0.775, 0.4])\nax2.set_position([0.125, 0.1, 0.775, 0.4])\n",
        "\nfig, ax = plt.subplots()\nax.hist(x, bins=bins, alpha=0.5, label='x')\nax.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nplt.hist(x, bins=10, label='x', alpha=0.5, color='blue', density=True)\nplt.hist(y, bins=10, label='y', alpha=0.5, color='red', density=True)\nplt.legend()\nplt.show()\n",
        "\n# Calculate the slope and intercept of the line\nslope = (d - b) / (c - a)\nintercept = b - slope * a\n# Set the xlim and ylim to be between 0 and 5\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n# Plot the line\nplt.plot([a, c], [b, d], color='red')\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n# create two colormaps with x and y\ncmap1 = plt.cm.get_cmap('viridis')\ncmap2 = plt.cm.get_cmap('inferno')\n# plot x and y in separate subplots\nim1 = ax1.imshow(x, cmap=cmap1)\nim2 = ax2.imshow(y, cmap=cmap2)\n# create a single colorbar for both subplots\ncbar = fig.colorbar(im1, ax=ax1, ax2=ax2)\n",
        "\nfig, ax = plt.subplots()\nfor i in range(x.shape[1]):\n    ax.plot(x[:, i], label=f\"a\")\nax.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 6))\nax1.plot(x, y)\nax2.plot(a, z)\nplt.suptitle(\"Y and Z\")\n",
        "\nfig, ax = plt.subplots()\nax.plot(points, np.log10(points))\nax.set_xlabel('x')\nax.set_ylabel('log(y)')\nax.set_title('Logarithmic Plot')\nax.set_yscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Plot of y over x\", fontsize=20)\nplt.xlabel(\"x-axis\", fontsize=18)\nplt.ylabel(\"y-axis\", fontsize=16)\nplt.show()\n",
        "\nax.plot(x, y)\nax.set_xticks(range(1, 11))\nax.set_yticks(range(1, 11))\nax.set_xticklabels(range(1, 11))\nax.set_yticklabels(range(1, 11))\n",
        "\nfor i in range(len(lines)):\n    x1, y1 = lines[i][0]\n    x2, y2 = lines[i][1]\n    color = c[i]\n    plt.plot([x1, x2], [y1, y2], color=color)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.yscale('log')\nplt.xscale('log')\nplt.gca().set_yscale('log')\nplt.gca().set_xscale('log')\nplt.gca().set_xlim(0, 1000)\nplt.gca().set_ylim(0, 1000)\nplt.gca().set_xticks([1, 10, 100, 1000])\nplt.gca().set_yticks([1, 10, 100, 1000])\nplt.gca().set_xticklabels(['1', '10', '100', '1000'])\nplt.gca().set_yticklabels(['1', '10', '100', '1000'])\nplt.show()\n",
        "\nfig, axes = plt.subplots(nrows=4, ncols=1, figsize=(12, 12))\nfor i in range(4):\n    df.iloc[:, i].plot(ax=axes[i], marker='o')\nplt.show()\n",
        "\n# Create a histogram of the data\nplt.hist(data, bins=np.arange(0, 20000, 1000))\n# Renormalize the data to sum up to 1\ndata_norm = data / sum(data)\n# Set y tick labels as percentage\nplt.yticks(np.arange(0, 1.1, 0.1), np.arange(0, 11, 1))\n# Format the y tick labels into percentage\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:.0%}'))\n",
        "\nplt.plot(x, y, marker='o', markersize=10, markerfacecolor='blue', markeredgecolor='blue', markeredgewidth=1, markeralpha=0.5)\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\nax1.plot(x, y)\nax2.plot(z, a)\nax1.set_title('y')\nax2.set_title('a')\nfiglegend(ax1, ['y'], loc='upper right')\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=False, figsize=(12, 8))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\n",
        "\nax.set_xticklabels(['second'] + [str(i) for i in range(3, 10)])\n",
        "\nplt.plot(x, y, label='\u03bb')\nplt.legend()\n",
        "\nplt.xticks(np.append(range(0, 10, 2), [2.1, 3, 7.6]))\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.tick_params(axis='y', labelrotation=-60)\nplt.tick_params(axis='x', which='both', top=True)\n",
        "\nplt.tick_params(axis='x', labelbottom=True, labeltop=False, labelleft=False, labelright=False, bottom=True, top=False, left=False, right=False, labelcolor='w', alpha=0.5)\n",
        "\nplt.subplots_adjust(left=0, bottom=0)\n",
        "\nplt.gca().yaxis.set_major_locator(plt.NullLocator())\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\naxes[0].plot(x, y)\naxes[1].plot(x, y)\nfig.suptitle(\"Figure\")\n",
        "\nplt.plot(df.index, df.values, label=\"Values\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, marker='v', hatch='/', hatch_density=1)\n",
        "\nplt.scatter(x, y, edgecolor='k', edgewidth=0.5, marker='|', hatch='/')\n",
        "\nplt.scatter(x, y, marker='*', hatch='/')\n",
        "\nplt.scatter(x, y, s=100, marker='*\\\\\\\\')\n",
        "\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(1, 5)\nplt.ylim(1, 4)\nplt.imshow(data)\n",
        "\nplt.stem(x, y, 'r-', use_line_collection=True, linewidth=2, markerfmt='')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Stem Plot of y over x')\nplt.grid(True)\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.bar(d.keys(), d.values(), color=list(c.values()))\nax.set_xticklabels(d.keys())\nplt.show()\n",
        "\nplt.plot([3, 3], [-1, 1], color='black', linestyle='-', linewidth=2, label='cutoff')\nplt.legend()\n",
        "\nfig, ax = plt.subplots(subplot_kw=dict(polar=True))\nax.bar(labels, height)\n",
        "\nfig, ax = plt.subplots()\nax.pie(data, labels=l, wedgeprops={'width': 0.4})\nplt.show()\n",
        "\nplt.plot(x, y, color='blue', linestyle='dashed')\nplt.grid(True, color='blue', linestyle='dashed')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='dashed', color='gray')\nplt.grid(which='major', linestyle='', alpha=0)\n",
        "\nfig, ax = plt.subplots()\nax.pie(sizes, labels=labels, colors=colors, autopct=lambda pct: f\"{pct:.1f}%\",\n        explode=(0.05, 0.05, 0.05, 0.05), shadow=True, startangle=90)\nax.axis('equal')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.pie(sizes, labels=labels, colors=colors, autopct=lambda pct: f\"{pct:.1f}%\",\n        explode=(0.05, 0.05, 0.05, 0.05), shadow=True, startangle=90)\nax.axis('equal')\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markersize=10, markeredgecolor='k', markerfacecolor='none', markeredgewidth=1)\n",
        "\nplt.axvline(x=55, color=\"green\")\n",
        "\nfig, ax = plt.subplots()\nax.bar(0, blue_bar, color='blue', width=0.5, align='center', label='Blue')\nax.bar(1, orange_bar, color='orange', width=0.5, align='center', label='Orange')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Blue', 'Orange'])\nax.set_ylabel('Height')\nax.set_title('Bar Plot')\nplt.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=False, sharey=False, figsize=(10, 6))\nax1.plot(x, y, label='y over x')\nax2.plot(a, z, label='z over a')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax2.set_xlabel('a')\nax2.set_ylabel('z')\nax1.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, cmap=\"Spectral\", norm=matplotlib.colors.Normalize(vmin=0, vmax=1))\n",
        "\nplt.plot(x, y)\nplt.gca().set_xticks(np.arange(0, 10, 1))\n",
        "\ng = sns.FacetGrid(df, col=\"species\", hue=\"sex\", height=4, aspect=1.5, palette=\"Set1\")\ng.map(plt.bar, \"bill_length_mm\", \"sex\", color=\"black\")\ng.set_titles(\"{col_name}\")\n",
        "\nplt.plot([0.5, 0.5], [0.5, 0.5], 'ro')\nplt.xlim(-0.1, 1.1)\nplt.ylim(-0.1, 1.1)\nplt.axis('off')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\phi$')\nplt.suptitle(r'$\\phi$', fontweight='bold')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=2, mode=\"expand\", borderaxespad=0.1)\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\n",
        "\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\nplt.legend()\nplt.plot(x, y, 'o', label=\"Markers\")\n",
        "\nplt.imshow(data, interpolation='nearest', cmap='viridis')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Figure 1\")\nplt.suptitle(\"Figure 1\", fontweight=\"bold\")\n",
        "\nsns.pairplot(df, x_vars=[\"x\"], y_vars=[\"y\"], hue=\"id\", legend=False)\n",
        "\nplt.plot(x, y)\nplt.gca().invert_xaxis()\n",
        "\nplt.scatter(x, y)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n",
        "\nplt.scatter(x, y, color='red', edgecolor='black')\n",
        "\nfig, axes = plt.subplots(2, 2, figsize=(15, 15))\nfor i in range(2):\n    for j in range(2):\n        axes[i, j].plot(x, y)\n",
        "\nplt.hist(x, bins=np.linspace(0, 10, 5), width=2)\n",
        "\nplt.plot(x, y, color='blue')\nplt.fill_between(x, y-error, y+error, alpha=0.2, color='blue')\nplt.show()\n",
        "\nplt.plot([0, 0], [-5, 5], color='white')\nplt.plot([-5, 5], [0, 0], color='white')\n",
        "\nfor i in range(len(box_position)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], color=c[i], fmt='none')\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(10, 6))\nax1.plot(x, y)\nax2.plot(a, z)\nax1.set_title('Y')\nax2.set_title('Z', loc='left', va='top')\n",
        "\nfig, axes = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        axes[i, j].plot(x, y)\n        axes[i, j].set_xticks(x)\n        axes[i, j].set_yticks(y)\n        axes[i, j].set_xticklabels(x)\n        axes[i, j].set_yticklabels(y)\n        axes[i, j].set_xlabel('')\n        axes[i, j].set_ylabel('')\nfig.subplots_adjust(hspace=0.5)\nplt.show()\n",
        "\nplt.matshow(d, cmap='viridis', aspect='auto', origin='lower', size=(8, 8))\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.axis('off')\nthe_table = ax.table(cellText=df.values,\n                     colLabels=df.columns,\n                     loc='center',\n                     bbox=[0, 0, 1, 1])\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='both', which='both', labeltop=True, labelbottom=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='both', which='both', bottom=True, top=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='x', labelbottom=True, bottom=False, labeltop=False, top=False)\n",
        "\nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=axs[0])\naxs[0].set_title(\"Group: Fat\")\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=axs[1])\naxs[1].set_title(\"Group: No Fat\")\n",
        "\nfig, ax = plt.subplots()\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, ax=ax)\nax.set_xlabel(\"Exercise Time\")\nax.set_ylabel(\"Pulse\")\n",
        "\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 8))\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=ax[0])\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=ax[1])\nfor ax_ in ax:\n    ax_.set_ylabel(\"\")\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.plot(x, y, figsize=(5, 5), dpi=300)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\n",
        "\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(12, 8))\nax1.plot(t, a, label='sin(t)')\nax1.set_title('sin(t)')\nax1.set_ylabel('Amplitude')\nax1.grid()\nax2.plot(t, b, label='cos(t)')\nax2.set_title('cos(t)')\nax2.set_ylabel('Amplitude')\nax2.grid()\nax3.plot(t, c, label='sin(t) + cos(t)')\nax3.set_title('sin(t) + cos(t)')\nax3.set_ylabel('Amplitude')\nax3.grid()\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nsns.stripplot(data=df, x=\"sex\", y=\"bill_length_mm\", hue=\"species\", ax=ax)\nax.legend_.remove()\n",
        "\ng = sns.FacetGrid(df, row=\"b\", height=4, aspect=1.5)\ng.map(sns.pointplot, \"a\", \"c\", ci=None, join=False, markers=[\"o\", \"x\", \"+\"], color=[\"r\", \"b\", \"g\"])\ng.set_xticklabels(g.get_xticks()[::2], rotation=0)\n",
        "\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(x, y, z)\nax.view_init(100, 50)\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tick_params(labelbottom=False, labelleft=False)\n",
        "\ngs = gridspec.GridSpec(nrow, ncol)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x[i, j])\n        ax.set_xticks([])\n        ax.set_yticks([])\nplt.subplots_adjust(hspace=0, wspace=0)\n"
    ],
    "Tensorflow": [
        "x.assign(1)",
        "x.assign(114514)",
        "result = tf.one_hot(labels, depth=10)",
        "result = tf.one_hot(labels, depth=10)",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(tf.convert_to_tensor(labels), depth=10)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "def my_map_func(i):\n  return [i, i+1, i+2]",
        "result = []\nfor i in input:\n  result.extend([i, i+1, i+2])\nreturn result",
        "lengths = [4, 3, 5, 2]\nresult = tf.zeros((4, 8), dtype=tf.int8)\nfor i in range(len(lengths)):\n    result[i, lengths[i]:] = 1\nprint(result)",
        "lengths = [4, 3, 5, 2]\nresult = tf.zeros([8, 8])\nfor i, length in enumerate(lengths):\n    result[i, length:] = 1\nprint(result)",
        "\nresult = tf.zeros([8, 8])\nfor i, length in enumerate(lengths):\n    result[i, length:] = 1\n",
        "\nresult = tf.zeros([8, 8])\nfor i, length in enumerate(lengths):\n    result[i, :length] = 1\n",
        "\nresult = tf.zeros(8)\nfor i, length in enumerate(lengths):\n    result[:length] = 1\n",
        "\nresult = tf.stack(tf.meshgrid(a, b), axis=-1)\n",
        "result = tf.stack(tf.meshgrid(a, b), axis=-1)\n",
        "a = tf.reshape(a, (50, 100, 512))",
        "a = tf.reshape(a, (50, 100, 1, 512))",
        "a = tf.reshape(a, (1, 50, 100, 1, 512))",
        "result = tf.reduce_sum(A, axis=1)",
        "result = tf.reduce_prod(A, axis=1)",
        "result = tf.math.reciprocal(A)",
        "\nresult = tf.reduce_sum(tf.square(tf.sub(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.sub(a, b)), axis=0)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.sub(A, B)), axis=1)\n",
        "m = tf.gather_nd(x, indices=tf.stack([y, z], axis=1))\nresult = m",
        "m = tf.gather_nd(x, indices=[[row, col]])\nresult = m.numpy()",
        "result = tf.gather_nd(x, indices=tf.stack([y, z], axis=1))",
        "\nresult = tf.matmul(A, B, transpose_b=True)\n",
        "\nresult = tf.matmul(A, B, transpose_b=True)\n",
        "\nresult = [tf.io.decode_raw(x_element, \"utf-8\") for x_element in x]\n",
        "result = [x.decode('utf-8') for x in example_x]\n",
        "\nresult = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "\nresult = tf.math.reduce_variance(x, axis=-1)\n",
        "\nresult = tf.reduce_mean(x, axis=-1)\n",
        "import tensorflow as tf\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\nprint(result)",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "result = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmin(a, axis=1)\n",
        "model.save('export/1')",
        "result = tf.random.uniform(shape=10, minval=1, maxval=4, seed=seed_x, dtype=tf.int32)",
        "\nresult = tf.random.uniform(shape=[114], minval=2, maxval=5, seed=seed_x)\n",
        "\nresult = tf.random.uniform(shape=[10], minval=1, maxval=4, seed=seed_x)\n",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nresult = scipy.optimize.curve_fit(lambda x, A, B: A*np.log(x) + B, x, y)\n",
        "\nresult = scipy.optimize.curve_fit(lambda x, A, B: A + B * np.log(x), x, y)\n",
        "\nresult = scipy.optimize.curve_fit(lambda p, x: p[0]*np.exp(p[1]*x) + p[2], x, y, p0)\n",
        "test_stat = stats.kstest(x, y)",
        "test_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha",
        "def f(x):\n    a, b, c = x\n    return ((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4\nresult = optimize.minimize(f, initial_guess)",
        "\np_values = scipy.stats.norm.sf(z_scores)\n",
        "\np_values = scipy.stats.norm.sf(z_scores, loc=mu, scale=sigma)\n",
        "\nz_scores = scipy.stats.norm.ppf(p_values)\n",
        "dist = stats.lognorm(mu, sigma=stddev)\nresult = dist.cdf(x)",
        "from scipy.stats import lognorm\nexpected_value = lognorm.mean(total, mu, stddev)\nmedian = lognorm.median(total, mu, stddev)",
        "result = sa * sb",
        "result = sA * sB",
        "\nresult = scipy.interpolate.LinearNDInterpolator(points[:, :3], V)(request)\n",
        "\nresult = scipy.interpolate.LinearNDInterpolator(points[:, :3], V)(request)\n",
        "\ndata_rot = rotate(data_orig, angle)\nxrot, yrot = data_rot[y0, x0]\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, \"uniform\")\n",
        "result = stats.kstest(times, \"uniform\", args = (0, T))\n",
        "\nresult = stats.kstest(times, \"uniform\", 95)\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "Feature = sparse.hstack([c1, c2])",
        "\nfrom scipy.optimize import linear_sum_assignment\ncost_matrix = scipy.spatial.distance.cdist(points1, points2, 'euclidean')\nresult = linear_sum_assignment(cost_matrix)\n",
        "\nfrom scipy.optimize import linear_sum_assignment\ncost_matrix = scipy.spatial.distance.cdist(points1, points2, 'cityblock')\nresult = linear_sum_assignment(cost_matrix)\n",
        "\nb.setdiag(0)\n",
        "\nresult = len(np.where(img > threshold))\n",
        "\nresult = len(np.where(img < threshold, True, False))\n",
        "\nresult = len(np.where(img > threshold))\n",
        "\nregions = ndimage.label(img > threshold)\ncenters = []\nfor region in range(1, np.max(regions) + 1):\n    center_of_mass = ndimage.center_of_mass(img, labels=regions, index=region)\n    centers.append(center_of_mass)\ndistances = []\nfor center in centers:\n    distance = np.linalg.norm(center - (0, 0))\n    distances.append(distance)\nresult = distances\n",
        "\nM.make_symmetric()\n",
        "\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA += example_sA\n    sA.eliminate_zeros()\n    sA.sort_indices()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()\n    sA.transpose()\n    sA.data = np.abs(sA.data)\n    sA.eliminate_zeros()",
        "\nsquare = scipy.ndimage.morphology.binary_erosion(square)\nsquare = scipy.ndimage.morphology.binary_dilation(square)\n",
        "\n# [Missing Code]\nsquare = scipy.ndimage.morphology.binary_erosion(square, structure=np.ones((3, 3)))\nsquare = scipy.ndimage.morphology.binary_dilation(square, structure=np.ones((3, 3)))\n",
        "\nmean = np.mean(col.data)\nstandard_deviation = np.std(col.data)\n",
        "Max = col.max()\nMin = col.min()",
        "\nMedian = np.median(col.toarray())\nMode = np.mode(col.toarray())[0][0]\n",
        "def fourier_series(x, a, degree):\n    result = 0\n    for i in range(1, degree+1):\n        result += a[i-1] * np.cos(i * np.pi / tau * x)\n    return result\npopt, pcov = curve_fit(fourier_series, z, Ua, p0=np.ones(degree), maxfev=10000)",
        "\nfrom_id = example_array.flatten()\nto_id = example_array.flatten()\ndistances = scipy.spatial.distance.cdist(from_id, to_id)\nresult = np.column_stack((from_id, to_id, distances))\n",
        "\nfrom_id = example_array[:, 0]\nto_id = example_array[:, 1]\ndistance = scipy.spatial.distance.cdist(from_id, to_id, 'manhattan')\nresult = np.column_stack((from_id, to_id, distance))\n",
        "\nfrom scipy.spatial.distance import cdist\nresult = cdist(example_arr, example_arr, metric='euclidean')\n",
        "tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\ny_int = interpolate.splev(x_val, tck, der = 0)\nresult = y_int",
        "\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x1, x2, x3, x4)\n",
        "\nresult = ss.anderson_ksamp(x1, x2)\n",
        "def tau1(x, y):\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndef rolling_kendalltau(df, window_size, columns):\n    for i in range(len(columns)):\n        for j in range(i+1, len(columns)):\n            col_name = '{}_{}'.format(columns[i], columns[j])\n            df[col_name] = pd.rolling_apply(df, window_size, lambda x: tau1(x[columns[i]], x[columns[j]]))\n    return df\nrolling_kendalltau(df, 3, ['B', 'C'])",
        "\nresult = sa.data.size == 0\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = block_diag(*a)\n",
        "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue",
        "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\n",
        "\nkurtosis_result = (a.var() - a.mean()) / (a.std() ** 4)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=True)\n",
        "\nresult = scipy.interpolate.interp2d(x, y, z)\nresult = result(s, t)\n",
        "\n    result = scipy.interpolate.interp2d(x, y, z)\n    ",
        "\nresult = np.zeros(len(extraPoints), dtype=int)\nfor i, extraPoint in enumerate(extraPoints):\n    region = vor.point_region[vor.nearest_point[extraPoint]]\n    result[i] = region\n",
        "\nresult = []\nfor extraPoint in extraPoints:\n    region = vor.point_region[vor.nearest_point[extraPoint]]\n    result.append(region)\n",
        "\nresult = sparse.hstack(vectors).toarray()\n",
        "\norigin = 1\nb = nd.median_filter(a, 3, origin=origin)\n",
        "\nresult = M.getrow(row)[column]\n",
        "\nresult = [M.getrow(row[i])[column[i]] for i in range(len(row))]\n",
        "\nnew_array = scipy.interpolate.interp1d(x, array, kind='linear')(x_new)\n",
        "\ndev = abs((x-u)/o2)\nP_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\nP_outer = 1 - P_inner\nP = P_inner + P_outer/2\nprob = P\n",
        "\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    prob = P_inner + P_outer/2\n    return prob\n",
        "\ndct_matrix = sf.dctn(np.ones(N), norm='ortho')\nresult = dct_matrix\n",
        "\nresult = sparse.diags(matrix, [-1, 0, 1], (5, 5)).toarray()\n",
        "M = scipy.stats.binom.pmf(np.arange(N+1), N, p)",
        "\nresult = df.apply(lambda x: stats.zscore(x.values.tolist()), axis=1)\n",
        "\nresult = df.apply(lambda x: stats.zscore(x))\n",
        "\nresult = df.join(df.apply(lambda x: stats.zscore(x), axis=0))\n",
        "\nresult = df.join(df.apply(lambda x: stats.zscore(x, ddof=1), axis=0))\nresult = result.round(3)\n",
        "alpha = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)",
        "mid = np.array([3, 3])\nresult = distance.cdist(np.dstack((y, x)), mid)",
        "mid = np.array([[3, 3]])\nresult = distance.cdist(np.dstack((y, x)), mid)",
        "mid = np.array([shape[0] / 2, shape[1] / 2])\nresult = distance.cdist(np.dstack((y, x)), mid)",
        "result = scipy.ndimage.zoom(x, (shape[0] / x.shape[0], shape[1] / x.shape[1]), order=1)",
        "\nres = scipy.optimize.minimize(lambda x: (a.dot(x ** 2) - y) ** 2, x0)\nout = res.x\n",
        "\nres = scipy.optimize.minimize(\n    fun=lambda x: (a.dot(x ** 2) - y) ** 2,\n    x0=x0,\n    method='L-BFGS-B',\n    bounds=((x_lower_bounds, None),),\n)\nout = res.x\n",
        "def dN1_dt_time_varying(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = solve_ivp(fun=dN1_dt_time_varying, t_span=time_span, y0=[N0,])",
        "def dN1_dt_simple(t, N1):\n    return -100 * N1 + t - np.sin(t) if t < 2 * np.pi else 2 * np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)",
        "def dN1_dt_sinusoid(t, N1):\n    return -100 * N1 - cos(t)\nsol = solve_ivp(fun=dN1_dt_sinusoid, t_span=time_span, y0=[N0,])",
        "for t in range(4):\n    def const(x):\n        y = x[t]\n        return y - 0.000001\n    cons.append({'type': 'ineq', 'fun': const})",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\nresult = sparse.hstack([sa, sb], format='csr')\n",
        "for n in range(len(c)):\n    # equation\n    eqn = 2*x*c[n]\n    # integrate\n    result, error = scipy.integrate.quad(lambda x: eqn, low, high)\n    print(result)",
        "\n    eqn = 2*x*c\n    result, error = scipy.integrate.quad(lambda x: eqn, low, high)\n    ",
        "\nA = V + x\n",
        "\nA = V.copy()\nA.data += x\n",
        "\nA = V.copy()\nA.data += x\nB = A + y\n",
        "\n sa[:, Col] = (1/Len) * Column\n ",
        "\n sa[:, Col] = (1/Len) * Column\n ",
        "\na = scipy.sparse.csr_matrix(a)\nbinary_a = a.astype(np.bool)\n",
        "a = scipy.sparse.csr_matrix(a)\na = a.astype(np.int8)\nprint(a)",
        "\nfrom scipy.cluster.hierarchy import cut_tree, linkage\n# Compute the linkage matrix\nlinkage_matrix = linkage(data, method='single')\n# Cut the linkage matrix to obtain the clusters\ncluster_labels = cut_tree(linkage_matrix)\n# Initialize the result list\nresult = []\n# Find the closest element to each cluster centroid\nfor i in range(len(centroids)):\n    # Find the index of the closest element to the centroid\n    closest_index = np.argmin(scipy.spatial.distance.cdist(data, centroids[i, :], 'euclidean'))\n    result.append(closest_index)\n",
        "\nresult = []\nfor i in range(len(centroids)):\n    dists = scipy.spatial.distance.cdist(data, centroids[i])\n    min_idx = np.argmin(dists)\n    result.append(data[min_idx])\n",
        "\nresult = []\nfor i in range(len(centroids)):\n    dists = scipy.spatial.distance.cdist(data, centroids[i])\n    closest_k = np.argsort(dists)[:k]\n    result.append(closest_k)\n",
        "\nresult = fsolve(lambda a: eqn(xdata, a, bdata), 0.5)\n",
        "\nresult = []\nfor x, a in zip(xdata, adata):\n    b_root = fsolve(lambda b: eqn(x, a, b), 0.5)\n    result.append([b_root, a])\n    b_root = fsolve(lambda b: eqn(x, a, b), -0.5)\n    result.append([b_root, a])\n",
        "\n# [Missing Code]\n# Create a continuous distribution function\ndef bekkers_cont(x, a, m, d):\n    return integrate.quad(lambda y: bekkers(y, a, m, d), range_start, x)[0]\n# Perform KS test\nresult = stats.kstest(sample_data, 'uniform', args=(range_start, range_end))\n",
        "\n# [Missing Code]\n# Create a continuous distribution function\ndef bekkers_cont(x, a, m, d):\n    return integrate.quad(lambda y: bekkers(y, a, m, d), range_start, x)[0]\n# Perform KS test\nks_result, p_value = stats.kstest(sample_data, 'uniform', args=(range_start, range_end))\n# Check if p-value is less than 0.05\nresult = p_value < 0.05\n",
        "\ndf['Time'] = pd.to_datetime(df['Time'])\nintegral_df = df.rolling('25S', on='Time').apply(integrate.trapz, args=(df['Time'],))\n",
        "\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\n",
        "\nweights = np.array([0.001, 0.1, 0.2, 0.12, 0.2])\n",
        "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))",
        "\nresult = []\nfor i in range(len(arr)):\n    if i-n < 0:\n        continue\n    if arr[i] <= arr[i-n:i+1].max():\n        result.append(i)\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if arr[i, j] <= arr[i, max(j-n, 0):min(j+n+1, arr.shape[1])].min():\n            result.append([i, j])\n",
        "\nnumerical_columns = ['NUM1', 'NUM2', 'NUM3']\ndf = df[(np.abs(stats.zscore(df[numerical_columns])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "data1 = pd.DataFrame(data.data, columns=data.feature_names)",
        "\ndf_out = pd.get_dummies(df['Col3'], prefix=['Col3'], prefix_sep='_')\n",
        "\ndf_out = pd.get_dummies(df['Col3']).astype(int)\n",
        "\ndf_out = pd.get_dummies(df['Col4'], prefix='', prefix_sep='')\n",
        "\ndf_out = pd.get_dummies(df['Col3'], prefix=['Col3'], prefix_sep='_')\n",
        "\ndf_out = pd.get_dummies(df['Col3']).reindex(columns=df.columns[:-1], index=df.index)\n",
        "\nproba = 1 / (1 + np.exp(-predicted_test_scores))\n",
        "proba = model.predict_proba(x_predict)",
        "\ndf = pd.concat([df_origin, transform_output.toarray()], axis=1)\n",
        "\ndf = pd.concat([df_origin, pd.DataFrame(transform_output.todense())], axis=1)\n",
        "\nresult = pd.concat([df_origin, transform_output.toarray()], axis=1)\n",
        "\nsteps = clf.named_steps()\ndel steps[1]\nclf.steps = steps\n",
        "\nsteps = clf.named_steps()\ndel steps[1]\nclf.steps = steps\n",
        "\nsteps = clf.named_steps()\ndel steps[1]\nclf.steps = steps\n",
        "\nsteps = clf.named_steps()\nsteps.insert(1, ('poly', PolynomialFeatures()))\nclf = Pipeline(steps)\n",
        "\nsteps = clf.named_steps()\nsteps.insert(1, ('new_step', PolynomialFeatures()))\nclf = Pipeline(steps)\n",
        "steps = clf.named_steps()\nsteps.insert(1, ('t1919810', PCA()))\nclf.steps = steps",
        "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [2, 3, 4],\n    'learning_rate': [0.1, 0.2, 0.3],\n    'min_child_weight': [1, 2, 3],\n    'gamma': [0, 0.5, 1],\n    'subsample': [0.5, 0.7, 0.9],\n    'colsample_bytree': [0.5, 0.7, 0.9],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [0, 0.5, 1],\n    'scale_pos_weight': [1, 2, 3]\n}\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]})\ngridsearch.fit(trainX, trainY)\n",
        "\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3), n_jobs=n_jobs, iid=iid, fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]})\n",
        "\nproba = []\nfor train, test in cv:\n    logreg.fit(X[train], y[train])\n    proba.append(logreg.predict_proba(X[test])[:, 1])\n",
        "\nproba = logreg.fit(X, y).predict_proba(X)\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\nmodel_name = model.__class__.__name__\n",
        "model_name = model.__class__.__name__",
        "\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "select_out = pipe.named_steps['select'].fit_transform(data, target)",
        "\nclf = GridSearchCV(estimator=bc, param_grid=param_grid, scoring='accuracy', cv=5)\nclf.fit(X_train, y_train)\n",
        "\nX = X.reshape(-1, 1)\n",
        "\nX = np.array(X).reshape(-1, 1)\n",
        "\npreprocessor=preprocess\n",
        "\ntfidf = TfidfVectorizer(preprocessor=prePro)\n",
        "\ndf_out = preprocessing.scale(data)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n",
        "\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "column_names = X.columns[model.get_support()]\n",
        "column_names = X.columns[model.get_support()]",
        "\ncolumn_names = X.columns[model.get_support()]\n",
        "column_names = X.columns[model.get_support()]\n",
        "\nclosest_50_samples = km.cluster_centers_[p]\nclosest_50_samples = X[km.labels_ == p, :]\nclosest_50_samples = closest_50_samples.iloc[:50, :]\n",
        "\nclosest_50_samples = km.cluster_centers_[p]\nclosest_50_samples = X[km.labels_ == p]\n",
        "\nclosest_100_samples = []\nfor i in range(p, p+100):\n    closest_samples = km.cluster_centers_[i]\n    closest_100_samples.extend(X[km.labels_ == i])\n",
        "\n    centers = km.cluster_centers_\n    distances = np.linalg.norm(X - centers[:, p], axis=1)\n    samples = X[np.argsort(distances)[:50]]\n    ",
        "X_train[0] = np.array(X_train[0])",
        "X_train = pd.get_dummies(X_train, columns=[0])",
        "\nfrom sklearn.svm import SVR\nregressor = SVR(kernel='rbf')\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nregressor = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='auto')\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nsvr = SVR(kernel='poly')\nsvr.fit(X_poly, y)\npredict = svr.predict(X_poly)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\n# fit, then predict X\nregressor = SVR(kernel='poly', degree=2)\nX_poly = PolynomialFeatures(degree=2).fit_transform(X)\nregressor.fit(X_poly, y)\npredict = regressor.predict(X_poly)\n",
        "\nqueries_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = queries_tfidf.dot(tfidf.idf_)\n",
        "\nqueries_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = queries_tfidf.dot(tfidf.idf_)\n",
        "\nqueries_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = queries_tfidf.dot(tfidf.idf_)\n",
        "\nnew_features = np.array(features)\n",
        "\nnew_f = np.array(f)\n",
        "\nnew_features = np.array(features)\n",
        "\nnew_features = np.array(features)\n",
        "\nnew_features = np.array(features)\n",
        "\nfrom sklearn.cluster import AgglomerativeClustering\n# Convert the data matrix to a distance matrix\ndist_matrix = 1 - data_matrix\n# Perform hierarchical clustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = cluster.fit_predict(dist_matrix)\n",
        "\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2).fit_predict(data_matrix)\n",
        "\nfrom sklearn.cluster import AgglomerativeClustering\ncluster_labels = AgglomerativeClustering(n_clusters=2).fit_predict(simM)\n",
        "\nlinkage_matrix = scipy.cluster.hierarchy.linkage(data_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maxclust')\n",
        "\nlinkage_matrix = scipy.cluster.hierarchy.linkage(data_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maxclust')\n",
        "\nfrom scipy.cluster.hierarchy import linkage, fcluster\nlink_matrix = linkage(simM, method='complete')\ncluster_labels = fcluster(link_matrix, 2, criterion='maxclust')\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "from sklearn.preprocessing import PowerTransformer\nbox_cox_data = PowerTransformer(method='box-cox', standardize=True).fit_transform(data)",
        "\nfrom sklearn.preprocessing import PowerTransformer\nbox_cox_data = PowerTransformer(method='box-cox', standardize=True).fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\nyeo_johnson_data = PowerTransformer(method='yeo-johnson', standardize=True).fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\nyeo_johnson_data = PowerTransformer(method='yeo-johnson', standardize=True).fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(strip_punctuation=False)\ntransformed_text = vectorizer.fit_transform(text)\n",
        "\nfrom sklearn.model_selection import train_test_split\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\ndata = data.sample(frac=1, random_state=42)\ntrain_size = int(data.shape[0] * 0.8)\nx_train = data.iloc[:, :-1].values[:train_size]\ny_train = data.iloc[:, -1].values[:train_size]\nx_test = data.iloc[:, :-1].values[train_size:]\ny_test = data.iloc[:, -1].values[train_size:]\n",
        "\nfrom sklearn.model_selection import train_test_split\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nX = X.reshape(-1, 2)\n",
        "\nX = X.reshape(X.shape[0], 2)\n",
        "\nclf = LinearSVC(penalty='l1')\nclf.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[clf.coef_.nonzero()[1]]\n",
        "\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]\n",
        "\nclf = LinearSVC(penalty='l1')\nclf.fit(X, y)\nselected_feature_names = vectorizer.get_feature_names()[clf.coef_.nonzero()[1]]\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=['Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'])\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()",
        "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=['Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'])\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()",
        "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()",
        "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript', '.Net', 'TypeScript', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()",
        "for col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y).coef_[0]\n    series = np.concatenate((series, slope), axis=0)",
        "for col in df1.columns:\n    if col.startswith('A') or col.startswith('B'):\n        df2 = df1[~np.isnan(df1[col])]\n        df3 = df2[[col, 'Time']]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        series = np.concatenate((series, m), axis=0)",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "\n le = LabelEncoder()\n df['Sex'] = le.fit_transform(df['Sex'])\n ",
        "\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n",
        "\nscaler = MinMaxScaler()\nnew_a = scaler.fit_transform(np_array)\n",
        "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict(b)",
        "\nnew_X = np.array(X).astype(np.float)\n",
        "\nnew_X = np.array(X)\nnew_X[:, 1] = new_X[:, 1].astype(float)\n",
        "\nnew_X = np.array(X).astype(np.float)\n",
        "X = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]",
        "X = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n# Sort the dataframes by date in ascending order\ntrain_dataframe = train_dataframe.sort_values(by=['date'])\ntest_dataframe = test_dataframe.sort_values(by=['date'])\n# Select the newer dates for the test set\ntest_dataframe = test_dataframe.loc[test_dataframe['date'] > train_dataframe['date'].max()]\n",
        "\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, stratify=features_dataframe['date'])\n",
        "\n train_size = 0.2\n train_dataframe, test_dataframe = cross_validation.train_test_split(features_dataframe, train_size=train_size)\n",
        "cols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].apply(scaler.fit_transform)",
        "cols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].apply(lambda x: scaler.fit_transform(x))",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "count = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)",
        "\nfull_results = GridSearch_fitted.cv_results_\ndf = pd.DataFrame(full_results)\n",
        "\nfull_results = GridSearch_fitted.cv_results_\nfull_results = pd.DataFrame(full_results)\nfull_results.sort_values(by=['mean_fit_time'], ascending=False, inplace=True)\n",
        "\n fitted_model.save('sklearn_model')\n ",
        "\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = 1 - sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix)\n"
    ],
    "Pytorch": [
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\nembedded_input = torch.tensor(word2vec.wv.vectors)\n",
        "embedded_input = torch.tensor(word2vec[input_Tensor])\n",
        "\npx = px.applymap(lambda x: x.item())\n",
        "x = torch.rand(4,4)\npx = pd.DataFrame(x)\npx = px.applymap(lambda x: x.item())\nprint(px)",
        "\npx = pd.DataFrame(x.numpy())\n",
        "C = B[:, A_log.nonzero()]",
        "C = B[:, A_logical]",
        "C = B[:, A_log.nonzero()]",
        "C = B[:, A_log]",
        "\nC = B[:, A_log.nonzero()]\n",
        "\nC = B[:, A_log.nonzero()]\n",
        "C = torch.index_select(B, 1, idx)",
        "\nx_tensor = torch.from_numpy(x_array)\n",
        "x_tensor = torch.from_numpy(x)",
        "\nt = torch.from_numpy(np.array(a))\n",
        "mask = torch.zeros(len(lens), max(lens)).type(torch.LongTensor)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1",
        "\nmask = torch.zeros(len(lens), max(lens)).type(torch.LongTensor)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "mask = torch.zeros(len(lens), max(lens))\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1",
        "mask = torch.zeros(len(lens), max(lens)).type(torch.LongTensor)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "Tensor_3D = torch.diag(Tensor_2D)\n",
        "\nresult = torch.diag(Tensor_2D)\n",
        "\nab = torch.cat((a, b), 0)\n",
        "\nab = torch.cat((a, b), dim=0)\n",
        "\nab = torch.cat((a, b), 0)\n",
        "a[:, lengths:, :] = 0",
        "a[:, lengths:, :] = 2333",
        "a[:, :lengths, :] = 0",
        "a[:, :lengths, :] = 2333",
        "tensor_of_tensors = torch.stack(list_of_tensors)\n",
        "new_tensors = torch.stack(list)\n",
        "tt = torch.stack(list_of_tensors)\n",
        "tensor_of_tensors = torch.stack(list_of_tensors)\n",
        "result = t[idx]",
        "result = t[idx]",
        "\nresult = t[idx]\n",
        "result = x.gather(1,ids)[:,0]",
        "result = x.gather(1,ids)",
        "result = torch.gather(x, 2, ids.float())[:, 1]",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmin(softmax_output, dim=1)\n",
        "y = torch.argmax(softmax_output, dim=1)\n",
        "y = torch.argmin(softmax_output, dim=1)\n",
        "log_p = F.log_softmax(input, dim=1)\nlog_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)  # make class dimension last dimension\nlog_p = log_p[target.view(n, w, z, 1).repeat(0, 0, 0, c) == 1]  # this looks wrong -> Should rather be a one-hot vector\nlog_p = log_p.view(-1, c)\nmask = target == 1\ntarget = target[mask]\nloss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False)\nif size_average:\n    loss /= mask.data.sum()",
        "\ncnt_equal = (A == B).sum()\n",
        "\ncnt_equal = 0\nfor i in range(11):\n    if A[i] == B[i]:\n        cnt_equal += 1\n",
        "\ncnt_not_equal = (A != B).sum()\n",
        "\ncnt_equal = (A == B).sum()\n",
        "\ncnt_equal = (A[-x:] == B[-x:]).sum()\n",
        "\ncnt_not_equal = 0\nfor i in range(x):\n    if A[-i-1] != B[-i-1]:\n        cnt_not_equal += 1\n",
        "a_split = torch.chunk(a, chunk_dim, dim=3)",
        "a_split = torch.chunk(a, chunk_dim, dim=2)",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = sign_x * min + sign_y * (min - x) * (min - y) / (torch.abs(x) - torch.abs(y))",
        "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = sign_x * max * (x >= 0) + sign_y * max * (y >= 0)",
        "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = sign_x * min + sign_y * (min - x) * (min - y)",
        "\noutput = MyNet(input)\nconfidence_score = torch.softmax(output, dim=1)\n",
        "result = torch.cat((a[:, :2], (a[:, 2] + b[:, 2]) / 2, b[:, 2:]), dim=1)",
        "\nresult = torch.cat((a[:, :2], torch.Tensor([(a[:, 2].float() + b[:, 0].float()) / 2]).reshape(-1, 1), b[:, 1:]), dim=1)\n",
        "new = torch.tensor([[0., 0., 0., 0.],\n                     [0., 1., 2., 0.],\n                     [0., 3., 4., 0.],\n                     [0., 5., 6., 0.],\n                     [0., 7., 8., 0.],\n                     [0., 0., 0., 0.]])\nr = torch.stack([t, new])",
        "new = torch.tensor([[0., 0., 0., 0.], [0., 1., 2., 0.], [0., 3., 4., 0.], [0., 0., 0., 0.]])\nr = torch.stack([t, new])",
        "new = torch.tensor([[-1, -1, -1, -1],\n                     [-1, 1, 2, -1],\n                     [-1, 3, 4, -1],\n                     [-1, 5, 6, -1],\n                     [-1, 7, 8, -1],\n                     [-1, -1, -1, -1]])\nr = torch.stack([t, new])",
        "result = torch.bmm(data, W).squeeze()"
    ]
}