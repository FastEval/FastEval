{
    "Pandas": [
        "\nresult = df.iloc[List].reset_index(drop=True)\n",
        "\ndf = df.iloc[List].reset_index(drop=True)\nresult = sum(df['Type'] != df['Type'].shift())\n",
        "\nfor col in df.columns:\n    counts = df[col].value_counts()\n    values_to_keep = counts[counts >= 2].index\n    df[col] = df[col].apply(lambda x: x if x in values_to_keep else 'other')\nresult = df\n",
        "\nfor col in df.columns:\n    counts = df[col].value_counts()\n    df[col] = df[col].apply(lambda x: x if counts[x] >= 3 else 'other')\n",
        "\n    for col in df.columns:\n        counts = df[col].value_counts()\n        values_to_keep = counts[counts >= 2].index\n        df[col] = df[col].apply(lambda x: x if x in values_to_keep else 'other')\n    result = df\n    ",
        "\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if df['Qu1'].value_counts()[x] < 3 else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: x if df['Qu2'].value_counts()[x] < 2 else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if df['Qu3'].value_counts()[x] < 2 else x)\nresult = df\n",
        "\nvalue_counts_qu1 = pd.value_counts(df.Qu1)\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x not in value_counts_qu1[value_counts_qu1 >= 3].index.tolist() and x != 'apple' else x)\nvalue_counts_qu2 = pd.value_counts(df.Qu2)\ndf['Qu2'] = df['Qu2'].apply(lambda x: x if x in value_counts_qu2[value_counts_qu2 >= 2].index.tolist() else 'other')\nvalue_counts_qu3 = pd.value_counts(df.Qu3)\ndf['Qu3'] = df['Qu3'].apply(lambda x: x if x in value_counts_qu3[value_counts_qu3 >= 2].index.tolist() else 'other')\n",
        "\ndf.loc[df['keep_if_dup'] == 'No', 'url'] = None\nresult = df.drop_duplicates(subset='url', keep='first')\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = pd.concat([result, df[df['drop_if_dup'] == 'No']])\n",
        "\ndf = df.sort_values(by=['url', 'keep_if_dup'], ascending=[True, False])\ndf = df.drop_duplicates(subset='url', keep='last')\nresult = df.sort_index()\n",
        "\nresult = {}\nfor index, row in df.iterrows():\n    current_dict = result\n    for col in df.columns[:-1]:\n        if row[col] not in current_dict:\n            current_dict[row[col]] = {}\n        current_dict = current_dict[row[col]]\n    current_dict[df.columns[-1]] = row[df.columns[-1]]\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\nresult = example_df['datetime'].dt.tz_localize(None)\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None).dt.strftime('%d-%b-%Y %H:%M:%S').sort_values()\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\ndf = df.sort_values(by='datetime')\n",
        "\ndf['message'] = df['message'].apply(lambda x: x.strip('[]'))\ndf['message'] = df['message'].apply(lambda x: x.split(', '))\ndf['message'] = df['message'].apply(lambda x: dict(map(str.strip, i.split(':')) for i in x))\nresult = pd.concat([df.drop('message', axis=1), df['message'].apply(pd.Series)], axis=1)\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score']\ndf.loc[~df['product'].isin(products), 'score'] = df.loc[~df['product'].isin(products), 'score'] * 10\n",
        "\ndf.loc[df['product'].isin(products[0] + products[1]), 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = (df.loc[df['product'].isin(products), 'score'] - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min())\n",
        "\nresult = df.apply(lambda x: ''.join(x.index[x==1]), axis=1)\nresult.name = 'category'\ndf = pd.concat([df, result], axis=1)\n",
        "\nresult = df.apply(lambda x: x.index[x==0][0], axis=1)\n",
        "\ndf['category'] = df.apply(lambda x: list(x.index[x==1]), axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\ndf = df[(df['Date'] >= List[0]) & (df['Date'] <= List[1])]\ndf['Month'] = df['Date'].dt.month_name().str.slice(stop=3)\ndf['Year'] = df['Date'].dt.year\ndf['Day'] = df['Date'].dt.day_name().str.slice(stop=3)\ndf['Date'] = df['Month'] + '-' + df['Day'] + '-' + df['Year'].astype(str)\n",
        "\ndf['#1'] = df['#1'].shift(periods=-1)\ndf.iloc[-1, 0] = df.iloc[0, 0]\n",
        "\ndf = pd.concat([df.tail(1), df.head(-1)], axis=0)\n",
        "\ndf['#1'] = df['#1'].shift(periods=-1)\ndf['#1'][0] = df['#1'].iloc[-1]\ndf['#2'] = df['#2'].shift(periods=1)\ndf['#2'][df.index[-1]] = df['#2'][df.index[0]]\n",
        "\ndf['#1'] = df['#1'].shift(-1)\ndf.iloc[-1, 0] = df.iloc[0, 0]\ndf = df[['#1', '#2']]\nmin_r2 = float('inf')\nfor i in range(len(df)):\n    r2 = df.corr().iloc[0, 1] ** 2\n    if r2 < min_r2:\n        min_r2 = r2\n        result = df.copy()\n    df = df.shift(-1)\n",
        "\ndf.columns = [col + 'X' for col in df.columns]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = [col + 'X' if not col.endswith('X') else 'X' + col for col in df.columns]\n",
        "\nvalue_cols = [col for col in df.columns if col.startswith('val')]\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"mean\" for col in value_cols}})\n",
        "\nvalue_cols = [col for col in df.columns if col.startswith('val')]\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"sum\" for col in value_cols}})\n",
        "\nval_cols = [col for col in df.columns if col.startswith('val')]\nagg_dict = {\"group_color\": \"first\"}\nfor col in val_cols:\n    if col.endswith('2'):\n        agg_dict[col] = 'mean'\n    else:\n        agg_dict[col] = 'sum'\nresult = df.groupby('group').agg(agg_dict)\n",
        "\nresult = df.loc[row_list, column_list].mean()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.loc[row_list, column_list].sum()\nresult.drop(result.idxmax(), inplace=True)\n",
        "\nresult = pd.Series({col: df[col].value_counts() for col in df.columns})\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor col in df.columns:\n    result += \"---- \" + col + \" ---\\n\"\n    result += str(df[col].value_counts()) + \"\\n\"\n",
        "\nresult = df.copy()\nresult.columns = result.iloc[0]\nresult = result.drop(result.index[0])\nresult = result.reset_index(drop=True)\n",
        "\nresult = df.copy()\nresult.columns = result.iloc[0]\nresult = result.drop(result.index[0])\nresult = result.reset_index(drop=True)\n",
        "\ndf = df.apply(lambda x: pd.Series(x.dropna().values))\nresult = df.reindex(columns=df.columns[::-1]).ffill().reindex(columns=df.columns)\n",
        "\ndf = df.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
        "\nresult = df.apply(lambda x: pd.Series(x.dropna().values.tolist() + x[x.isnull()].values.tolist()), axis=1)\n",
        "\ndf.loc[df['value'] < thresh, 'lab'] = 'X'\ndf = df.groupby('lab').sum()\n",
        "\ndf.loc[df['value'] < thresh, 'lab'] = 'X'\ndf = df.groupby('lab').mean()\n",
        "\nmask = (df['value'] < section_left) | (df['value'] > section_right)\ndf_filtered = df[mask]\ndf_mean = pd.DataFrame({'lab':['X'], 'value':[df[~mask]['value'].mean()]})\nresult = pd.concat([df_filtered, df_mean])\n",
        "\ninv_cols = df.columns.map(lambda x: f\"inv_{x}\")\ninv_df = pd.DataFrame(1/df.values, columns=inv_cols)\nresult = pd.concat([df, inv_df], axis=1)\n",
        "\nexp_df = np.exp(df)\nexp_df.columns = [\"exp_\" + col for col in df.columns]\nresult = pd.concat([df, exp_df], axis=1)\n",
        "\ninv_cols = df.columns[:-1].tolist()\ninv_df = df[inv_cols].replace(0, 1).apply(lambda x: 1/x)\ninv_df.columns = [\"inv_\" + col for col in inv_cols]\nresult = pd.concat([df, inv_df], axis=1)\n",
        "\nsigmoid = lambda x: 1/(1+np.exp(-x))\nresult = df.assign(**{'sigmoid_'+col: sigmoid(df[col]) for col in df})\n",
        "\nmin_idx = df.idxmin()\nresult = df.apply(lambda x: x[:min_idx[x.name]].idxmax())\n",
        "\nmask = df.apply(lambda x: x == x.max()).cumsum() <= df.apply(lambda x: x == x.min()).cumsum()\nresult = df.where(mask).idxmax()\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\nusers = df['user'].unique()\nresult = pd.DataFrame({'dt': date_range})\nfor user in users:\n    user_df = df[df['user'] == user]\n    user_result = pd.merge(result, user_df, on='dt', how='left')\n    user_result['user'].fillna(user, inplace=True)\n    user_result['val'].fillna(0, inplace=True)\n    result = pd.concat([result, user_result])\nresult.reset_index(drop=True, inplace=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\nresult = pd.DataFrame({'dt': date_range})\nresult['user'] = df['user'].unique()[0]\nresult = pd.merge(result, df, on=['dt', 'user'], how='left').fillna(0)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\nresult = pd.DataFrame({'dt': date_range})\nresult['user'] = df['user'].unique()[0]\nresult['val'] = 233\nresult = pd.concat([df, result], sort=False).reset_index(drop=True)\nresult = result.sort_values(['user', 'dt']).reset_index(drop=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nusers = df['user'].unique()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date), 'user': users.repeat(max_date-min_date+1)})\nresult = result.merge(df, on=['dt', 'user'], how='left')\nresult['val'] = result.groupby('user')['val'].fillna(method='ffill')\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nusers = df['user'].unique()\nresult = pd.DataFrame(columns=['dt', 'user', 'val'])\nfor user in users:\n    user_df = df[df['user'] == user].copy()\n    user_df.set_index('dt', inplace=True)\n    user_df = user_df.reindex(pd.date_range(min_date, max_date), method='ffill')\n    user_df.reset_index(inplace=True)\n    user_df['dt'] = user_df['dt'].dt.strftime('%d-%b-%Y')\n    result = pd.concat([result, user_df])\n",
        "\ndf['name'] = pd.factorize(df['name'])[0] + 1\nresult = df\n",
        "\ndf['a'] = pd.factorize(df['name'])[0] + 1\nresult = df\n",
        "\n    df['name'] = pd.factorize(df['name'])[0] + 1\n    result = df\n    ",
        "\ndf['ID'] = df.groupby(['name', 'a']).ngroup() + 1\nresult = df[['ID', 'b', 'c']]\n",
        "\nresult = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\n",
        "\nresult = pd.melt(df, id_vars=['user', '01/12/15'], var_name='others', value_name='value')\n",
        "\ndf = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.dropna()\n",
        "\nresult = df.loc[df['c'] > 0.5, columns].values\n",
        "\nresult = df.loc[df['c'] > 0.45, columns].values\n",
        "\n    result = df.loc[df['c'] > 0.5, columns].values\n    ",
        "\n    result = df.loc[df['c'] > 0.5, columns].sum(axis=1)\n    ",
        "\n    result = df.loc[df['c'] > 0.5, columns]\n    ",
        "\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\ndf = df.sort_values(by='date')\ndf = df.reset_index(drop=True)\nfiltered_dates = [df['date'][0]]\nfor i in range(1, len(df)):\n    if (df['date'][i] - filtered_dates[-1]).days > X:\n        filtered_dates.append(df['date'][i])\nresult = df[df['date'].isin(filtered_dates)]\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(by='date')\ndf['diff'] = df['date'].diff()\ndf = df[df['diff'] > timedelta(weeks=X)]\nresult = df.drop(columns=['diff'])\n",
        "\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\ndf = df.sort_values(by='date')\ndf['date_diff'] = df['date'].diff()\ndf['date_diff'] = df['date_diff'].fillna(pd.Timedelta(seconds=0))\ndf['date_diff'] = df['date_diff'].apply(lambda x: x.days)\ndf = df[df['date_diff'] >= X*7]\ndf['date'] = df['date'].dt.strftime('%d-%b-%Y')\nresult = df[['ID', 'date', 'close']]\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 3).sum()\n",
        "\nresult = df.groupby(df.index // 4).sum()\n",
        "\nresult = df.iloc[::-1].rolling(3).mean().iloc[2::3][::-1]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\nresult = pd.DataFrame()\nfor i in range(0, len(df), 5):\n    if i+3 <= len(df):\n        result = result.append(pd.DataFrame({'col1': [df[i:i+3].sum().values[0]]}))\n    else:\n        result = result.append(pd.DataFrame({'col1': [df[i:].mean().values[0]]}))\nprint(result)\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 3):\n    if i+3 <= len(df):\n        result = pd.concat([result, pd.DataFrame({'col1': [df.iloc[i:i+3]['col1'].sum()]})])\n    else:\n        result = pd.concat([result, pd.DataFrame({'col1': [df.iloc[i:]['col1'].mean()]})])\n    if i+2 <= len(df):\n        result = pd.concat([result, pd.DataFrame({'col1': [df.iloc[i:i+2]['col1'].mean()]})])\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, method='ffill')\n",
        "\ndf['A'] = df['A'].replace(0, pd.np.nan)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].fillna(method='bfill')\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ntime_dict = {'year': 365, 'month': 30, 'week': 7, 'day': 1}\ndf['time_days'] = df['time'].map(time_dict)\n",
        "\ndf['time number'] = df['duration'].str.extract(r'(\\d+)')\ndf['time'] = df['duration'].str.extract(r'(\\D+)')\ndf['time_day'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\n    df['number'] = df.duration.str.extract('(\\d+)')\n    df['time'] = df.duration.str.extract('(\\D+)')\n    time_dict = {'year': 365, 'month': 30, 'week': 7, 'day': 1}\n    df['time_days'] = df['time'].map(time_dict)\n    ",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ndf['time_day'] = df.time.replace({'year': 365, 'month': 30, 'week': 7, 'day': 1}) * df['number'].astype(int)\n",
        "\nresult = np.where(np.logical_or.reduce([df1[column] != df2[column] for column in columns_check_list]))\n",
        "\ncheck = np.where(np.all([df1[column] == df2[column] for column in columns_check_list]))\nresult = check[0].tolist()\n",
        "\ndf.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n",
        "\ndf.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n",
        "\n    df.reset_index(inplace=True)\n    df['date'] = pd.to_datetime(df['date'])\n    output = df.values\n    ",
        "\n    df.index = pd.to_datetime(df.index.get_level_values(0)) # parse date index\n    df.index = df.index.swaplevel() # swap the two levels\n    ",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\ndf = df.pivot_table(index=['Variable', 'Country', 'year'], columns='Variable', values='value').reset_index()\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(by=['Country', 'Variable', 'year'], ascending=[True, True, False])\ndf = df.pivot(index=['Variable', 'Country', 'year'], columns='Variable', values='value').reset_index()\ndf = df[['Variable', 'Country', 'year', 'var1', 'var2']]\n",
        "\nresult = df[(abs(df.filter(like='Value')) < 1).all(axis=1)]\n",
        "\nresult = df.loc[(abs(df.filter(like='Value')) > 1).any(axis=1)]\n",
        "\ncols = [col for col in df.columns if col.startswith('Value')]\ndf = df[abs(df[cols]) > 1].dropna()\ndf.columns = [col.replace('Value_', '') for col in df.columns]\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT', '<', regex=True)\n",
        "\n    df = df.replace('&AMP;', '&', regex=True)\n    result = df\n    ",
        "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\nimport pandas as pd\nimport re\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\ndf['first_name'] = df['name'].apply(lambda x: x.split()[0] if len(x.split()) == 2 else x)\ndf['last_name'] = df['name'].apply(lambda x: x.split()[1] if len(x.split()) == 2 else None)\ndf = df.drop(columns=['name'])\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport re\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].str.split(' ').str[1]\ndf['1_name'] = df['1_name'].str.split(' ').str[0]\ndf = df[['1_name', '2_name']]\nresult = df\nprint(result)\n",
        "\ndef split_name(name):\n    if name.count(' ') > 1:\n        name_parts = name.split(' ')\n        first_name = name_parts[0]\n        middle_name = ' '.join(name_parts[1:-1])\n        last_name = name_parts[-1]\n        return pd.Series([first_name, middle_name, last_name])\n    else:\n        return pd.Series([name, None, None])\ndf[['first name', 'middle_name', 'last_name']] = df['name'].apply(split_name)\ndf.drop('name', axis=1, inplace=True)\n",
        "\nresult = pd.merge_asof(df2, df1, on='Timestamp', direction='backward')\nresult = result.drop_duplicates(subset='Timestamp', keep='last')\nresult = result[['Timestamp', 'stuff', 'data']]\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp', direction='nearest')\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n",
        "\nerror_list = []\nfor index, row in df.iterrows():\n    if not isinstance(row[\"Field1\"], int):\n        error_list.append(row[\"Field1\"])\n",
        "\ninteger_list = []\nfor index, row in df.iterrows():\n    if isinstance(row['Field1'], int):\n        continue\n    elif str(row['Field1']).isnumeric():\n        integer_list.append(int(row['Field1']))\n",
        "\n    result = []\n    for index, row in df.iterrows():\n        if not isinstance(row[\"Field1\"], int):\n            result.append(row[\"Field1\"])\n    ",
        "\nresult = df.set_index('cat')\nresult = result.div(result.sum(axis=1), axis=0)\n",
        "\ndf.set_index('cat', inplace=True)\nresult = df.div(df.sum(axis=0), axis=1)\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\n",
        "\n    result = df.loc[test].drop_duplicates()\n    ",
        "\nimport pandas as pd\nfrom scipy.spatial.distance import cdist\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Begin of Missing Code\ndef get_nearest_neighbour(df):\n    dist_matrix = cdist(df[['x', 'y']], df[['x', 'y']])\n    np.fill_diagonal(dist_matrix, np.inf)\n    nearest_neighbour = dist_matrix.argmin(axis=1)\n    euclidean_distance = dist_matrix.min(axis=1)\n    return pd.DataFrame({'nearest_neighbour': df.iloc[nearest_neighbour]['car'].values, 'euclidean_distance': euclidean_distance})\nresult = df.groupby('time').apply(get_nearest_neighbour).reset_index()\nresult = pd.concat([result.drop(['level_1'], axis=1), result['level_1'].apply(pd.Series)], axis=1).drop(['nearest_neighbour'], axis=1)\n# End of Missing Code\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Begin of Missing Code\ndef get_farmost_neighbour(df):\n    distances = np.sqrt((df['x'] - df['x'].values[:, np.newaxis])**2 + (df['y'] - df['y'].values[:, np.newaxis])**2)\n    np.fill_diagonal(distances, np.inf)\n    farmost_neighbour = distances.argmin(axis=1)\n    euclidean_distance = distances.min(axis=1)\n    return pd.DataFrame({'farmost_neighbour': farmost_neighbour, 'euclidean_distance': euclidean_distance})\nresult = df.groupby('time').apply(get_farmost_neighbour).reset_index()\nresult = result[['time', 'car', 'farmost_neighbour', 'euclidean_distance']]\n# End of Missing Code\nprint(result)\n",
        "\ncols = [\"keywords_0\", \"keywords_1\", \"keywords_2\", \"keywords_3\"]\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \",\".join(x.dropna()), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna().astype(str)), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna().astype(str)), axis=1)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna().tolist()), axis=1)\n",
        "\nrows_to_change = df.sample(frac=0.2, random_state=0)\nrows_to_change['Quantity'] = 0\nresult = pd.concat([df.drop(rows_to_change.index), rows_to_change])\n",
        "\nn = int(len(df)*0.2)\nrandom_sample = df.sample(n=n, random_state=0)\nrandom_sample['ProductId'] = 0\nresult = pd.concat([df.drop(random_sample.index), random_sample])\n",
        "\nresult = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\nresult['Quantity'] = 0\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate_bool[duplicate_bool == True].index\nresult = duplicate[['col1', 'col2', 'index_original']]\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nlast_duplicate_index = df.loc[duplicate_bool == True, 'col1'].index.tolist()\nresult = pd.concat([duplicate.reset_index(drop=True), pd.DataFrame({'index_original': last_duplicate_index})], axis=1)\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    index_original = df[duplicate_bool].index.tolist()\n    duplicate['index_original'] = index_original\n    result = duplicate\n    ",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate_bool[duplicate_bool == True].index\nresult = duplicate\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.tolist()\nresult = duplicate\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\nprint(result)\n",
        "\nresult = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\n",
        "\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmin()]\n",
        "\nresult = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n",
        "\nresult = df.query(\"Category in @filter_list\")\n",
        "\nresult = df.query(\"Category not in @filter_list\")\n",
        "\nvalue_vars = [tuple(df.columns.levels[i][j] for i in range(df.columns.nlevels)) for j in range(df.columns.nunique())]\nresult = pd.melt(df, value_vars=value_vars)\n",
        "\nid_vars = df.columns.levels[0]\nvalue_vars = [tuple(df.columns.levels[i+1]) for i in range(len(df.columns.levels)-1)]\nresult = pd.melt(df, id_vars=id_vars, value_vars=value_vars)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf.loc[df['cumsum'] < 0, 'cumsum'] = 0\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('r')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('l')['v'].sum()\nresult[result.isna()] = np.nan\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].nunique() == df[col2].nunique() == len(df):\n                result.append(f\"{col1} {col2} one-to-one\")\n            elif df[col1].nunique() == len(df) and df[col2].nunique() < len(df):\n                result.append(f\"{col1} {col2} one-to-many\")\n            elif df[col2].nunique() == len(df) and df[col1].nunique() < len(df):\n                result.append(f\"{col1} {col2} many-to-one\")\n            else:\n                result.append(f\"{col1} {col2} many-to-many\")\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].nunique() == df[col2].nunique() == len(df):\n                result.append(f\"{col1} {col2} one-2-one\")\n            elif df[col1].nunique() == len(df) and df[col2].nunique() < len(df):\n                result.append(f\"{col1} {col2} one-2-many\")\n            elif df[col1].nunique() < len(df) and df[col2].nunique() == len(df):\n                result.append(f\"{col1} {col2} many-2-one\")\n            else:\n                result.append(f\"{col1} {col2} many-2-many\")\n",
        "\nresult = df.corr(method='pearson').applymap(lambda x: 'many-to-many' if x > 0.8 else ('one-to-one' if x == 1 else ('many-to-one' if x > 0.0 else None)))\nresult = result.where(pd.notnull(result), 'one-to-many')\n",
        "\nresult = df.corr(method='pearson').applymap(lambda x: 'many-2-many' if x > 0.7 else ('one-2-one' if x == 1 else ('many-2-one' if x > 0.0 else 'NaN')))\n",
        "\n# drop duplicates based on firstname, lastname, email and keep the one with bank account\ndf = df.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', ignore_index=True)\n# drop rows with no bank account\ndf = df.dropna(subset=['bank'], axis=0, how='all', thresh=None, subset=None, inplace=False)\nresult = df\n",
        "\nresult = pd.to_numeric(s.astype(str).str.replace(',',''), errors='coerce')\n",
        "\nresult = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0))['Survived'].mean().rename({True: 'Has Family', False: 'No Family'})\n",
        "\nresult = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0))['SibSp'].mean().rename({True: 'Has Family', False: 'No Family'})\n",
        "\nresult = df.groupby((df['SibSp'] == 1) & (df['Parch'] == 1)).mean().rename({True: 'Has Family', False: 'No Family'}).iloc[:, 0]\nresult['New Family'] = df.groupby((df['SibSp'] == 0) & (df['Parch'] == 1)).mean().iloc[:, 0]\nresult['Old Family'] = df.groupby((df['SibSp'] == 1) & (df['Parch'] == 0)).mean().iloc[:, 0]\nresult = result.reindex(['Has Family', 'New Family', 'No Family', 'Old Family'])\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\ndf = df.stack(0).unstack(0).swaplevel(0,1,axis=1).sort_index(axis=1)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\ndf = df.stack(level=[0,1]).unstack(level=[1,2])\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\ndf = df.stack(level=[0,1])\ndf.index = pd.MultiIndex.from_tuples(df.index, names=['Caps', 'Middle', 'Lower'])\ndf = df.unstack(level=[0,1])\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\nresult = df.groupby('a')['b'].agg([np.mean, np.std])\n",
        "\nresult = df.groupby('b')['a'].agg([np.mean, np.std])\n",
        "\ndf['softmax'] = df.groupby('a')['b'].apply(lambda x: np.exp(x) / np.sum(np.exp(x)))\ndf['min-max'] = df.groupby('a')['b'].apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
        "\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf = df.loc[(df != 0).any(axis=1)]\nresult = df\n",
        "\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf = df.loc[(df != 0).any(axis=1)]\nresult = df.reset_index(drop=True)\n",
        "\nresult = df.loc[(df.max(axis=1) <= 1), ['A', 'D']]\n",
        "\ndf[df.max(axis=1) > 1] = 0\nresult = df[df.max(axis=0) <= 1]\n",
        "\nresult = s.sort_values(ascending=True)\n",
        "\ndf = pd.DataFrame({'index': s.index, 1: s.values})\ndf = df.sort_values(by=[1, 'index'], ascending=[True, True])\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\n",
        "\nresult = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\n",
        "\nresult = df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmin()]\n",
        "\nresult = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Date'])\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\n",
        "\n    df['Date'] = df['Member'].map(dict).fillna(df['Date'])\n    ",
        "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%d-%b-%Y').fillna('17-Aug-1926')\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Val'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year'))['Val'].transform('count')\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Date'].transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year)['Date'].transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val'])['Val'].transform('count')\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Val'].transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year')])['Val'].transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.weekday.rename('weekday')])['Val'].transform('count')\ndf['Count_Val'] = df.groupby(['Val'])['Date'].transform('count')\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: (x == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x != 0).sum())\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: (x % 2 == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x % 2 == 1).sum())\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\nresult = result.reset_index(drop=True)\n",
        "\nimport dask.dataframe as dd\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=2)\nresult = df.assign(var2=df.var2.str.split(',')).compute().explode('var2')\n",
        "\nimport dask.dataframe as dd\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=2)\nresult = df.assign(var2=df.var2.str.split('-')).compute().explode('var2')\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha() and not string[i].isspace():\n            special_char += 1\n    return special_char\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"]=df[\"str\"].apply(count_special_char)\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', expand=True)\n",
        "\ndf[['fips', 'medi', 'row']] = df['row'].str.split(' ', n=2, expand=True)\n",
        "\ndf = df.set_index('Name')\ndf = df.apply(lambda x: x[x!=0].expanding().mean(), axis=1).fillna(0)\ndf = df.reset_index()\n",
        "\ndf = df.set_index('Name')\ndf = df.iloc[:, ::-1]\ndf = df.cumsum(axis=1)\ndf = df.iloc[:, ::-1]\ndf = df.replace(0, pd.np.nan)\ndf = df.apply(lambda x: x.ffill().bfill())\ndf = df.fillna(0)\ndf = df.div(df.count(axis=1), axis=0)\ndf = df.round(2)\n",
        "\n    result = df.copy()\n    for i in range(len(df)):\n        row = df.iloc[i, 1:]\n        row = row[row != 0]\n        cum_avg = row.expanding().mean()\n        result.iloc[i, 1:] = cum_avg\n    ",
        "\ndf = df.set_index('Name')\ndf = df.iloc[:, ::-1]\ndf = df.cumsum(axis=1)\ndf = df.iloc[:, ::-1]\ndf = df.replace(0, pd.np.nan)\ndf = df.apply(lambda x: x.dropna().reset_index(drop=True).fillna(method='ffill'), axis=1)\ndf = df.fillna(0)\ndf = df.iloc[:, ::-1]\ndf = df.cumsum(axis=1)\ndf = df.iloc[:, ::-1]\ndf = df.div(df.count(axis=1), axis=0)\n",
        "\ndf['Label'] = 0\ndf.loc[0, 'Label'] = 1\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf.loc[0, 'label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().fillna(0)\ndf['label'] = df['label'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['label'][0] = 1\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors='coerce')\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time']\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['Duration'] = df['Duration'].dt.total_seconds()\n",
        "\nresult = df[df['key2'] == 'one'].groupby(['key1']).size().reset_index(name='count').fillna(0)\n",
        "\nresult = df[df['key2'] == 'two'].groupby(['key1']).size().reset_index(name='count')\n",
        "\nresult = df[df['key2'].str.endswith('e')].groupby('key1').size().reset_index(name='count')\nresult['count'] = result['count'].apply(lambda x: x if x > 0 else 0)\nresult = result[['key1', 'count']]\n",
        "\nmax_result = max(df.index)\nmin_result = min(df.index)\n",
        "\nmode_result = df.index.mode()[0]\nmedian_result = df.index.median()\n",
        "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n",
        "\ndf1 = df.loc[df.groupby(\"item\")[\"diff\"].idxmin()]\nresult = df1.sort_values(\"item\")[[\"item\", \"diff\", \"otherstuff\"]]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.rsplit('_', 1)[0] if '_' in x else x)\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n",
        "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.rsplit('_', 1)[0] if '_' in x else x)\n    result = df['SOURCE_NAME']\n    ",
        "\nnum_nan = df['Column_x'].isna().sum()\nnum_fill_0 = num_nan // 2\nnum_fill_1 = num_nan - num_fill_0\ndf.loc[df['Column_x'].isna()[:num_fill_0], 'Column_x'] = 0\ndf.loc[df['Column_x'].isna()[-num_fill_1:], 'Column_x'] = 1\n",
        "\nn = len(df['Column_x'])\nfirst_third = int(n * 0.3)\nsecond_third = int(n * 0.6)\ndf['Column_x'].fillna(value={i: 0 for i in range(first_third)})\ndf['Column_x'].fillna(value={i: 0.5 for i in range(first_third, second_third)})\ndf['Column_x'].fillna(value={i: 1 for i in range(second_third, n)})\n",
        "\nnum_zeros = df['Column_x'].isna().sum() // 2\nnum_ones = df['Column_x'].isna().sum() - num_zeros\ndf['Column_x'].fillna(0, limit=num_zeros, inplace=True)\ndf['Column_x'].fillna(1, limit=num_ones, inplace=True)\n",
        "\nresult = pd.concat([a, b], axis=1)\nresult = pd.DataFrame(result.values.reshape(-1, len(a.columns)), columns=['one', 'two'])\nresult = pd.DataFrame(result.apply(tuple, axis=1), columns=['one', 'two'])\n",
        "\ndfs = [a, b, c]\nresult = pd.concat([df.stack() for df in dfs], axis=1)\nresult.columns = [f\"{col}_{i}\" for i, df in enumerate(dfs) for col in df.columns]\nresult = result.groupby(level=0, axis=1).apply(lambda x: tuple(x.values.tolist())).reset_index(drop=True)\n",
        "\ndfs = [a, b]\nmax_len = max([len(df) for df in dfs])\nresult = pd.DataFrame(index=range(max_len), columns=[col for df in dfs for col in df.columns])\nfor df in dfs:\n    result.loc[:len(df)-1, df.columns] = df.values\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\nresult = pd.crosstab(df['username'], pd.cut(df['views'], bins))\n",
        "\nresult = pd.DataFrame({'text': [', '.join(df['text'].tolist())]})\n",
        "\nresult = pd.DataFrame({'text': ['-'.join(df['text'].tolist())]})\n",
        "\nresult = pd.DataFrame({'text': [', '.join(df['text'].tolist()[::-1])]})\n",
        "\nresult = pd.Series(df['text'].str.cat(sep=', '))\n",
        "\nresult = pd.Series('-'.join(df['text'][::-1]))\n",
        "\nresult = pd.concat([df1.set_index('id'), df2.set_index('id')], axis=0, sort=True).reset_index()\nresult = result.groupby('id').apply(lambda x: x.ffill().bfill()).reset_index(drop=True)\nresult[['city', 'district']] = result.groupby('id')[['city', 'district']].apply(lambda x: x.ffill().bfill())\nresult = result[['id', 'city', 'district', 'date', 'value']]\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date'])\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\nresult = result.fillna(method='ffill')\nresult = result.drop_duplicates(subset=['id', 'city', 'district', 'date'], keep='first')\n",
        "\nresult = pd.concat([df1[['id', 'city', 'district']], df2[['id', 'date', 'value']]], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult = result.fillna(method='ffill')\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_x'].update(result['B_y'])\nresult = result.drop(columns=['B_y'])\nresult = result.rename(columns={'B_x': 'B'})\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B_x'].fillna(result['B_y'], inplace=True)\nresult.drop(columns=['B_y'], inplace=True)\nresult.rename(columns={'B_x': 'B'}, inplace=True)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['B_x'] = result['B_y'].fillna(result['B_x'])\nresult.drop('B_y', axis=1, inplace=True)\nresult['dulplicated'] = result.duplicated(subset=['A'], keep=False)\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist()).rename('transactions')\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['time', 'amount']].values.tolist())\n",
        "\nresult = df.groupby('user').apply(lambda x: x[['amount', 'time']].values.tolist()[::-1])\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index)\n",
        "\ndf = pd.DataFrame(series.values.tolist(), index=series.index, columns=['0', '1', '2', '3'])\ndf.reset_index(inplace=True)\ndf.rename(columns={'index': 'name'}, inplace=True)\n",
        "\nresult = [col for col in df.columns if s in col and not col == s]\n",
        "\nresult = df.loc[:, df.columns.str.contains(s) & ~df.columns.str.match(s)]\n",
        "\nresult = [col for col in df.columns if s in col]\nresult = {col: f\"{s}{i+1}\" for i, col in enumerate(result)}\ndf = df.rename(columns=result)\n",
        "\nresult = pd.DataFrame(df['codes'].values.tolist()).add_prefix('code_')\n",
        "\nresult = pd.DataFrame(df['codes'].values.tolist()).add_prefix('code_')\n",
        "\nresult = pd.DataFrame(df['codes'].values.tolist()).add_prefix('code_')\n",
        "\nresult = []\nfor lst in df['col1']:\n    result += lst\n",
        "\nresult = df['col1'].apply(lambda x: ','.join(map(str, reversed(x)))).str.cat(sep=',')\n",
        "\nresult = ','.join([','.join(map(str, lst)) for lst in df['col1']])\n",
        "\nresult = df.set_index('Time').resample('2T').mean().interpolate()\nresult = result.reset_index()\nresult['Time'] = result['Time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\ndf = df.set_index('Time')\ndf = df.resample('3T').sum()\ndf = df.reset_index()\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n",
        "\nresult = df[filt[df.index.get_level_values('a')].values]\n",
        "\nresult = df[filt[df.index.get_level_values('a')].values]\n",
        "\nresult = df.loc[[0, 8]].isnull().eq(df.loc[[0, 8]].isnull().all()).all().loc[0]\n",
        "\nresult = df.loc[[0, 8]].T.apply(lambda x: x[0] == x[1] or (pd.isna(x[0]) and pd.isna(x[1]))).loc[lambda x: x].index\n",
        "\nresult = df.loc[[0, 8]].isna().all().reset_index()\nresult = result[result[0]].iloc[:, 0].tolist()\n",
        "\nresult = []\nfor col in df.columns:\n    if df.loc[0, col] != df.loc[8, col]:\n        result.append((df.loc[0, col], df.loc[8, col]))\n",
        "\nts = df.set_index('Date')['Value']\n",
        "\nresult = pd.DataFrame(df.values.reshape(1, -1), columns=[f\"{col}_{i}\" for i in range(1, len(df.columns)+1) for col in df.columns])\n",
        "\nresult = pd.DataFrame(df.values.reshape(1,-1), columns=[f\"{col}_{i}\" for i in range(df.shape[0]) for col in df.columns])\n",
        "\ndf['dogs'] = df['dogs'].round(2).fillna(pd.NA)\n",
        "\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2).astype('Float64')\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\n",
        "\nresult = df.reset_index().sort_values(['time','index']).set_index(['treatment','dose','time'])['VIM']\n",
        "\nresult = df.sort_values(['VIM', 'time'])\n",
        "\ndf = df[~df.index.isin(['2020-02-17', '2020-02-18'])]\n",
        "\ndf = df[~df.index.isin(['2020-02-17', '2020-02-18'])]\nresult = df.index.strftime('%d-%b-%Y %A')\n",
        "\nresult = corr[corr > 0.3].stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult = result[result['Pearson Correlation Coefficient'].notnull()]\nresult = result[result['Col1'] != result['Col2']]\nresult = result.set_index(['Col1', 'Col2'])\n",
        "\nresult = corr[corr > 0.3].stack().reset_index()\nresult = result[result['level_0'] != result['level_1']]\nresult = result[[0, 'level_1']]\nresult.set_index([result['level_0'], 'level_1'], inplace=True)\nresult = result[0]\n",
        "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\n",
        "\ndf.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nresult = df\n",
        "\ndf['frequent'] = df.mode(axis=1)[0]\ndf['freq_count'] = df.apply(lambda x: x.value_counts().max(), axis=1)\n",
        "\nfrequent = df.mode(axis=1)\nfreq_count = df.apply(lambda x: x.value_counts().max(), axis=1)\ndf['frequent'] = frequent\ndf['freq_count'] = freq_count\n",
        "\nfrequent = []\nfreq_count = []\nfor index, row in df.iterrows():\n    counts = row.value_counts()\n    max_count = max(counts)\n    freq = counts[counts == max_count].index.tolist()\n    frequent.append(freq)\n    freq_count.append(max_count)\ndf['frequent'] = frequent\ndf['freq_count'] = freq_count\n",
        "\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\nresult = df.groupby([\"id1\",\"id2\"]).mean()\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce').fillna(0)\nresult = df.groupby([\"id1\",\"id2\"]).agg({'foo': np.mean, 'bar': np.mean})\nprint(result)\n",
        "\nresult = pd.merge(df_a[['EntityNum','foo']], df_b[['EntityNum','a_col']], on='EntityNum')\n",
        "\nresult = pd.merge(df_a, df_b[['EntityNum','b_col']], on='EntityNum')\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nresult = []\nfor row in x:\n    new_row = []\n    for val in row:\n        if not np.isnan(val):\n            new_row.append(val)\n    result.append(new_row)\n",
        "\nb = np.zeros((a.size, a.max()+1))\nb[np.arange(a.size), a] = 1\n",
        "\nb = np.zeros((a.size, a.max()+1))\nb[np.arange(a.size), a] = 1\n",
        "\nb = np.zeros((a.size, a.max() - a.min() + 1), dtype=int)\nb[np.arange(a.size), a-a.min()] = 1\n",
        "\nunique_a = np.unique(a)\nn_unique_a = len(unique_a)\nb = np.zeros((len(a), n_unique_a))\nfor i, val in enumerate(unique_a):\n    b[:, i] = (a == val).astype(int)\n",
        "\nb = np.zeros((a.size, a.max()+1))\nb[np.arange(a.size), a.ravel()] = 1\nb = b.reshape(*a.shape, -1)\n",
        "\nresult = np.percentile(a, p)\n",
        "\nB = A.reshape(-1, ncol)\n",
        "\nB = np.reshape(A, (nrow, -1))\n",
        "\nB = A[:len(A)//ncol*ncol].reshape(-1, ncol)\n",
        "\nnrow = -(-len(A) // ncol) # calculate number of rows needed\nA = A[-nrow*ncol:] # discard elements at the beginning if necessary\nB = A.reshape(nrow, ncol)[::-1] # reshape and reverse the array\n",
        "\nresult = np.roll(a, shift)\nif shift > 0:\n    result[:shift] = np.nan\nelse:\n    result[shift:] = np.nan\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.zeros_like(a)\nfor i, s in enumerate(shift):\n    if s > 0:\n        result[i, s:] = a[i, :-s]\n    elif s < 0:\n        result[i, :s] = a[i, -s:]\n    else:\n        result[i] = a[i]\n",
        "\nimport numpy as np\nnp.random.seed(42)\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nnp.random.seed(42)\nr_new = np.random.randint(3, size=(100, 2000)) - 1\nprint(r_old, r_new)\n",
        "\nresult = np.argmax(a.ravel())\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape, order='F')\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    result = np.unravel_index(np.argmax(a), a.shape)\n    ",
        "\nmax_val = np.max(a)\na[a == max_val] = -np.inf\nsecond_max_val = np.max(a)\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\na = a[:, ~np.any(np.isnan(a), axis=0)]\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.argwhere(a == np.min(a))\n",
        "\nresult = np.sin(np.deg2rad(degree))\n",
        "\nresult = np.cos(np.deg2rad(degree))\n",
        "\nsine_degree = np.sin(np.deg2rad(number))\nsine_radian = np.sin(number)\nif sine_degree > sine_radian:\n    result = 0\nelse:\n    result = 1\n",
        "\nresult = np.arcsin(value) * 180 / np.pi\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant')\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant')\n",
        "\na_power = np.power(a, power)\n",
        "\n    result = np.power(a, power)\n    ",
        "\nresult = Fraction(numerator, denominator)\n",
        "\n    from fractions import Fraction\n    result = Fraction(numerator, denominator)\n    ",
        "\nresult = Fraction(numerator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, b, c)\n",
        "\nresult = np.diag(np.fliplr(a))\n",
        "\nresult = np.diag(np.fliplr(a), k=1)\n",
        "\nresult = np.diag(np.fliplr(a))\n",
        "\ndiagonal = np.arange(a.shape[1]-1, -1, -1)\nresult = a[diagonal, diagonal[::-1]]\n",
        "\nresult = []\nfor row in X:\n    for element in row:\n        result.append(element)\n",
        "\nresult = X.flatten(order='C').tolist()\n",
        "\n    for row in X:\n        for element in row:\n            result.append(element)\n    ",
        "\nresult = []\nfor x in np.nditer(X, order='F'):\n    result.append(x)\n",
        "\nresult = np.array([int(i) for i in mystr])\n",
        "\nresult = np.cumsum(a[:, col] * multiply_number)\n",
        "\nresult = np.cumsum(a[row] * multiply_number)\n",
        "\nresult = np.prod(a[row]/divide_number)\n",
        "\nresult = np.linalg.matrix_rank(a, tol=None)\n",
        "\nresult = a.shape[1]\n",
        "\nt, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\nt, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\ndf = anobs + bnobs - 2\ns_p = np.sqrt(((anobs - 1) * avar + (bnobs - 1) * bvar) / df)\nt = (amean - bmean) / (s_p * np.sqrt(1 / anobs + 1 / bnobs))\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t), df))\n",
        "\noutput = np.array([x for x in A if not any(np.array_equal(x, y) for y in B)])\n",
        "\nset_A = set(map(tuple, A))\nset_B = set(map(tuple, B))\ndiff = np.array(list(set_A.symmetric_difference(set_B)))\noutput = np.array([list(x) for x in diff])\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = np.take_along_axis(b, sort_indices, axis=0)\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = np.take_along_axis(b, sort_indices, axis=0)\n",
        "\nsort_indices = np.argsort(a, axis=0)[::-1,:,:]\nc = b[sort_indices]\n",
        "\nsums = np.sum(a, axis=(1,2))\nsorted_indices = np.argsort(sums)\nresult = b[sorted_indices]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0,2], axis=1)\n",
        "\nresult = np.delete(a, del_col, axis=1)\nresult = result[:, 1:2]\n",
        "\na = np.insert(a, pos, element)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\n    a = np.insert(a, pos, element)\n    ",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\nresult = np.array([np.copy(arr) for arr in array_of_arrays])\n",
        "\nresult = np.all(a == a[0,:], axis=0)\n",
        "\nresult = np.all(a == a[:,0][:,None], axis=0)\n",
        "\n    result = np.all(a[1:] == a[:-1], axis=0)\n    ",
        "\nresult, _ = dblquad(integrand, y[0], y[-1], lambda x: x[0], lambda x: x[-1])\n",
        "\nimport numpy as np\nfrom scipy.integrate import simps\nexample_x = np.linspace(0, 1, 20)\nexample_y = np.linspace(0, 1, 30)\ndef f(x = example_x, y = example_y):\n    X, Y = np.meshgrid(x, y)\n    Z = np.cos(X)**4 + np.sin(Y)**2\n    result = simps(simps(Z, y), x)\n    return result\n",
        "\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\nresult = ecdf(grades)(grades)\n",
        "\nresult = stats.cumfreq(grades, numbins=len(grades))\nresult = result.cumcount / len(grades)\n",
        "\necdf = interp1d(np.sort(grades), np.linspace(0, 1, len(grades)), kind='previous')\nlow = grades.min()\nhigh = grades.max()\nwhile ecdf(high) >= threshold:\n    high -= 0.1\nwhile ecdf(low) < threshold:\n    low += 0.1\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\nsess = tf.compat.v1.Session()\na_np = sess.run(a)\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[::-1][:N]\n",
        "\nresult = np.linalg.matrix_power(A, n)\n",
        "\nresult = np.empty((a.shape[0]//2 * a.shape[1]//2, 2, 2), dtype=a.dtype)\nfor i in range(a.shape[0]//2):\n    for j in range(a.shape[1]//2):\n        result[i*a.shape[1]//2+j] = a[2*i:2*i+2, 2*j:2*j+2]\n",
        "\nresult = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch.tolist())\n",
        "\nresult = np.zeros((a.shape[0]//2, a.shape[1]//2, 2, 2))\nfor i in range(result.shape[0]):\n    for j in range(result.shape[1]):\n        result[i,j] = a[i*2:i*2+2, j*2:j*2+2]\n",
        "\nresult = np.array([a[i:i+patch_size, j:j+patch_size] for i in range(0, a.shape[0]-patch_size+1, patch_size) for j in range(0, a.shape[1]-patch_size+1, patch_size)])\n",
        "\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i][j] = a[i//2, j//2, i%2, j%2]\n",
        "\nresult = np.array([a[i:i+patch_size, j:j+patch_size] for i in range(0, a.shape[0]-patch_size+1, patch_size) for j in range(0, a.shape[1]-patch_size+1, patch_size)])\n",
        "\nresult = a[:, low:high+1]\n",
        "\nresult = a[low:high+1,:]\n",
        "\nresult = a[:, max(low, 0):min(high, a.shape[1])]\n",
        "\na = np.array(eval(string))\n",
        "\nresult = np.exp(np.random.uniform(low=np.log(min), high=np.log(max), size=n))\n",
        "\nresult = np.exp(np.random.uniform(low=np.log(min), high=np.log(max), size=n))\n",
        "\n    result = np.exp(np.random.uniform(low=np.log(min), high=np.log(max), size=n))\n    ",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a*A[0]\nB[1] = a*A[1]+b*B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nresult = np.ravel_multi_index(index, dims, order='F') + 1\n",
        "\nresult = np.ravel_multi_index(index, dims, order='C')\n",
        "\nvalues = np.zeros((2,), dtype=[('a', 'i4'), ('b', 'f4'), ('c', 'f4')])\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\nresult = np.zeros(np.max(index)+1)\nfor i in range(len(a)):\n    if a[i] > result[index[i]]:\n        result[index[i]] = a[i]\n",
        "\nresult = np.zeros(np.max(accmap)+1)\nfor i in range(len(a)):\n    result[accmap[i]] += a[i]\n",
        "\nresult = np.zeros(np.max(index)+1)\nfor i in range(len(index)):\n    if index[i] >= 0:\n        if result[index[i]] == 0:\n            result[index[i]] = a[i]\n        else:\n            result[index[i]] = min(result[index[i]], a[i])\n",
        "\nz = np.vectorize(elementwise_function)(x, y)\n",
        "\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\nresult = np.zeros((high_index-low_index, high_index-low_index))\nresult[max(0, -low_index):min(high_index, a.shape[0]), max(0, -low_index):min(high_index, a.shape[1])] = a[max(0, low_index):min(high_index, a.shape[0]), max(0, low_index):min(high_index, a.shape[1])]\n",
        "\nresult = x[x >= 0]\n",
        "\nresult = x[np.iscomplex(x)]\n",
        "\nnum_bins = len(data) // bin_size\ndata = data[:num_bins*bin_size]\nbin_data = data.reshape((num_bins, bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_max = np.max(bin_data, axis=1)\n",
        "\nn_bins = data.shape[1] // bin_size\nbin_data = data[:, :n_bins*bin_size].reshape(data.shape[0], n_bins, bin_size)\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nn = len(data)\nnum_bins = n // bin_size\nbin_data = np.zeros((num_bins, bin_size))\nfor i in range(num_bins):\n    bin_data[i] = data[n - (i+1)*bin_size : n - i*bin_size]\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nn = data.shape[1]\nnum_bins = n // bin_size\nif n % bin_size != 0:\n    num_bins -= 1\nbin_data = np.array([data[:, i*bin_size:(i+1)*bin_size] for i in range(num_bins-1, -1, -1)])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nn_rows, n_cols = data.shape\nn_bins = n_cols // bin_size\nbin_data = np.zeros((n_rows, n_bins, bin_size))\nfor i in range(n_bins):\n    bin_data[:, i, :] = data[:, -bin_size*(i+1):-bin_size*i]\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\ndef clamp(x, x_min, x_max):\n    if x < x_min:\n        return smoothclamp(0)\n    elif x > x_max:\n        return smoothclamp(1)\n    else:\n        return smoothclamp((x-x_min)/(x_max-x_min))\n",
        "\ndef smoothstep(x, N):\n    return np.power(x, N) * np.power((1 - x), N) * (N * x + 1 - N * (1 - x))\ndef smoothclamp(x, N):\n    return x_min + (x_max - x_min) * smoothstep((x - x_min) / (x_max - x_min), N)\n",
        "\nresult = correlate(a, np.roll(b[::-1], 1), mode='valid')\n",
        "\nresult = df.values.reshape(4, 15, 5)\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\nresult = np.zeros((len(a), m), dtype=int)\nfor i in range(len(a)):\n    binary = bin(a[i])[2:].zfill(m)\n    result[i] = [int(x) for x in binary]\n",
        "\nresult = np.zeros((len(a), m), dtype=int)\nfor i, num in enumerate(a):\n    if num < 0:\n        num = 2**32 + num\n    binary = np.binary_repr(num, width=m)\n    result[i] = [int(digit) for digit in binary]\n",
        "\nresult = np.zeros((len(a), m), dtype=int)\nfor i in range(len(a)):\n    binary = np.binary_repr(a[i], width=m)\n    result[i] = [int(x) for x in binary]\nresult = np.bitwise_xor.reduce(result)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 3*std, mean + 3*std)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 2*std, mean + 2*std)\n",
        "\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3*std, mean + 3*std)\n    ",
        "\nmean = np.mean(a)\nstd = np.std(a)\nsecond_std = 2 * std\nupper_bound = mean + second_std\nlower_bound = mean - second_std\nresult = np.logical_or(a > upper_bound, a < lower_bound)\n",
        "\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\nprob = np.ma.percentile(masked_data, percentile)\n",
        "\na[zero_rows,:] = 0\na[:,zero_cols] = 0\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[1,:] = 0\na[:,0] = 0\n",
        "\nmask = (a == np.amax(a, axis=1)[:, None])\n",
        "\nmin_values = np.min(a, axis=1)\nmask = (a == min_values[:, None])\n",
        "\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nresult = np.einsum('ij,ik->jik', X, X)\n",
        "\nX = np.zeros((Y.shape[1], Y.shape[0]))\nfor i in range(Y.shape[0]):\n    X[:, i] = np.linalg.cholesky(Y[i])\n",
        "\nis_contained = np.isin(a, number)\n",
        "\nmask = np.isin(A, B, invert=True)\nC = A[mask]\n",
        "\nC = np.intersect1d(A, B, assume_unique=True)\n",
        "\nC = np.extract(np.logical_or.reduce([np.logical_and(A>=B[i], A<B[i+1]) for i in range(len(B)-1)]), A)\n",
        "\nresult = len(a) + 1 - rankdata(a).astype(int)\n",
        "\nunique_a = np.unique(a)[::-1]\nresult = np.zeros(len(a), dtype=int)\nfor i, val in enumerate(unique_a):\n    idx = np.where(a == val)[0]\n    result[idx] = i + 1\n",
        "\n    result = len(a) + 1 - rankdata(a).astype(int)\n    ",
        "\ndists = np.stack((x_dists, y_dists), axis=-1)\n",
        "\ndists = np.stack((x_dists, y_dists), axis=2)\n",
        "\nresult = a[:, second, third]\n",
        "\narr = np.zeros((20,10,10,2))\n",
        "\nl1 = LA.norm(X, ord=1, axis=1)\nresult = X / l1.reshape(-1, 1)\n",
        "\nresult = X / LA.norm(X, axis=1, keepdims=True)\n",
        "\nresult = X / np.max(np.abs(X), axis=1, keepdims=True)\n",
        "\nconditions = df['a'].astype(str).str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\n",
        "\nresult = cdist(a, a)\n",
        "\nfrom scipy.spatial.distance import cdist\nresult = cdist(a, a)\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\nresult = squareform(pdist(a))\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=np.float32)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=np.float)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nA = [float(i) if i != 'np.inf' else np.inf for i in A]\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\n",
        "\nresult = []\nprev = None\nfor i in a:\n    if i != 0 and i != prev:\n        result.append(i)\n        prev = i\n",
        "\nresult = []\nprev = None\nfor i in a:\n    if i != 0 and i != prev:\n        result.append(i)\n        prev = i\nresult = np.array(result).reshape(-1, 1)\n",
        "\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n",
        "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ",
        "\ndf = pd.DataFrame({'lat': lat.flatten(),\n                   'lon': lon.flatten(),\n                   'val': val.flatten()})\ndf['maximum'] = df[['lat', 'lon', 'val']].max(axis=1)\n",
        "\nheight, width = a.shape\nh, w = size\nshape = (height - h + 1, width - w + 1, h, w)\nstrides = a.strides + a.strides\nresult = as_strided(a, shape=shape, strides=strides)\n",
        "\nheight, width = a.shape\nh, w = size\nshape = (height - h + 1, width - w + 1, h, w)\nstrides = a.strides + a.strides\nresult = as_strided(a, shape=shape, strides=strides)\n",
        "\nresult = np.nanmean(a)\n",
        "\n    real_mean = np.mean(a.real)\n    imag_mean = np.mean(a.imag)\n    result = real_mean + imag_mean * 1j\n    ",
        "\nresult = Z[..., -1:]\n",
        "\nresult = a[np.index_exp[-1:] + (slice(None),) * (a.ndim - 1)]\n",
        "\nresult = any(np.array_equal(c, x) for x in CNTS)\n",
        "\nresult = False\nfor cnt in CNTS:\n    if np.array_equal(c, cnt):\n        result = True\n        break\nprint(result)\n",
        "\nf = intp.interp2d(np.arange(0, 4, 1), np.arange(0, 4, 1), a, kind='linear')\nresult = f(x_new, y_new)\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n",
        "\ni = np.diag(i)\n",
        "\na[~np.eye(a.shape[0], dtype=bool)] = 0\n",
        "\nstart_dt = datetime.strptime(start, '%d-%b-%Y %H:%M:%S.%f')\nend_dt = datetime.strptime(end, '%d-%b-%Y %H:%M:%S.%f')\ndelta = (end_dt - start_dt) / (n-1)\nresult = pd.date_range(start=start_dt, end=end_dt, periods=n)\nresult = [str(x) for x in result]\n",
        "\nresult = np.where((x == a) & (y == b))[0]\nif len(result) > 0:\n    result = result[0]\nelse:\n    result = -1\n",
        "\nresult = np.where((x == a) & (y == b))[0]\n",
        "\nA = np.vstack([x**2, x, np.ones(len(x))]).T\nresult = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "\nresult = np.polyfit(x, y, degree)\n",
        "\ndf = df.apply(lambda x: x - a)\n",
        "\nresult = np.matmul(A, B.T)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(-1,1)).reshape(a.shape)\n",
        "\nscaler = MinMaxScaler()\nresult = []\nfor row in arr:\n    X_max = np.max(row)\n    X_min = np.min(row)\n    row_rescaled = [(X - X_min)/(X_max - X_min) for X in row]\n    result.append(row_rescaled)\nresult = np.array(result)\n",
        "\nscaler = MinMaxScaler()\nresult = np.array([scaler.fit_transform(matrix) for matrix in a])\n",
        "\narr_temp = arr.copy()\nmask = arr_temp < -10\nmask2 = arr_temp >= 15\narr[mask] = 0\narr[mask2] = 30\narr[~(mask | mask2)] += 5\n",
        "\nfor i in range(arr.shape[0]):\n    mask1 = arr[i] < n1[i]\n    mask2 = arr[i] >= n2[i]\n    mask3 = ~(mask1 | mask2)\n    arr[i][mask1] = 0\n    arr[i][mask2] = 30\n    arr[i][mask3] += 5\n",
        "\nresult = np.count_nonzero(np.isclose(s1, s2))\n",
        "\ns1[np.isnan(s1)] = 0\ns2[np.isnan(s2)] = 0\nresult = np.count_nonzero(np.abs(s1 - s2) > np.finfo(float).eps)\n",
        "\nresult = all(np.array_equal(a[i], a[i+1]) for i in range(len(a)-1))\n",
        "\nresult = all(np.isnan(x).all() for x in a)\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\n    result = np.zeros(shape)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n    ",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\na = a.reshape(int(a.shape[0]/3), 3)\n",
        "\nresult = np.choose(b, a.transpose(2, 0, 1)).T\n",
        "\nresult = a[:,:,1] * b\n",
        "\nresult = a[np.arange(a.shape[0])[:,None], np.arange(a.shape[1]), b]\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:,None,None], np.arange(a.shape[1])[None,:,None], b], axis=2).sum()\n",
        "\nresult = np.sum(a[np.arange(a.shape[0])[:,None,None], np.arange(a.shape[1])[None,:,None], b] )\n",
        "\nresult = np.where((df['a'] > 1) & (df['a'] <= 4), df['b'], np.nan)\n",
        "\nresult = im[(im != 0).any(axis=1)][:, (im != 0).any(axis=0)]\n",
        "\nnonzero_rows, nonzero_cols = np.nonzero(A)\nresult = A[min(nonzero_rows):max(nonzero_rows)+1, min(nonzero_cols):max(nonzero_cols)+1]\n",
        "\nresult = im.copy()\nwhile np.sum(result[0]) == 0:\n    result = result[1:]\nwhile np.sum(result[-1]) == 0:\n    result = result[:-1]\nwhile np.sum(result[:,0]) == 0:\n    result = result[:,1:]\nwhile np.sum(result[:,-1]) == 0:\n    result = result[:,:-1]\n",
        "\nresult = im[~np.all(im == 0, axis=1)]\nresult = result[:, ~np.all(result == 0, axis=0)]\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n",
        "\nplt.minorticks_on()\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\nplt.minorticks_on()\n",
        "\nplt.minorticks_on()\nplt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\nstyles = ['-', '--', '-.', ':']\nfor i, style in enumerate(styles):\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style, label=f'Line {i+1}')\nplt.legend()\nplt.show()\n",
        "\nstyles = ['-', '--', '-.', ':']\nfor i, style in enumerate(styles):\n    y = np.random.rand(10)\n    plt.plot(x, y, linestyle=style, label=f'Line {i+1}')\nplt.legend()\nplt.show()\n",
        "\nplt.plot(x, y, '-D', linewidth=1, markersize=4)\n",
        "\nplt.plot(x, y, '-D', linewidth=2, markersize=10)\n",
        "\nax.set_ylim(0, 40)\n",
        "\nplt.axvspan(2, 4, facecolor='r', alpha=0.5)\n",
        "\nx = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.plot([0,1],[0,2])\nplt.show()\n",
        "\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df)\nplt.show()\n",
        "\nsns.set_style(\"whitegrid\")\nplt.plot(x, y)\nplt.show()\n",
        "\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', markersize=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20)\n",
        "\nl.set_markerfacecolor((0, 0, 1, 0.2))\n",
        "\nl.set_markeredgecolor(\"black\")\n",
        "\nl.set_color('red')\nl.set_markerfacecolor('red')\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# put a x axis ticklabels at 0, 2, 4...\nplt.xticks(np.arange(0, 2*np.pi+0.1, 2*np.pi/5), ['0', '2', '4', '6', '8'])\nplt.show()\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\nsns.distplot(x, label=\"a\", color=\"0.25\")\nsns.distplot(y, label=\"b\", color=\"0.25\")\n# add legends\nplt.legend()\n",
        "\nplt.imshow(H, cmap='viridis')\nplt.colorbar()\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.xlabel(\"X\", ha=\"right\", x=1.0)\n",
        "\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nplt.plot(x, y)\nmyTitle = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\n# fit a very long title myTitle into multiple lines\nplt.title('\\n'.join(myTitle.split(' - ')))\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.invert_yaxis()\nplt.show()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nplt.plot(x, color='blue')\nplt.plot(y, color='orange', alpha=0.7)\nplt.plot(z, color='green', alpha=0.5)\n",
        "\nplt.scatter(x, y, edgecolors='black', facecolors='blue')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\nax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "\nplt.ticklabel_format(style='plain', axis='y')\n",
        "\nax.axhline(y=1, linestyle='--', color='red')\n",
        "\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nax1.plot(x, y1)\nax1.set_ylabel('sin(x)')\nax2.plot(x, y2)\nax2.set_ylabel('cos(x)')\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y1)\nax2.plot(x, y2)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\nax2.spines['left'].set_visible(False)\n",
        "\nplt.xlabel(\"\")\n",
        "\nplt.xticks([], [])\n",
        "\nplt.xticks(np.arange(10))\nplt.gca().xaxis.grid(True)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--')\nplt.gca().xaxis.grid(True, which='major', linestyle='-')\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='red', alpha=0.5, linewidth=2)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='blue', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='green', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='orange', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='purple', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='black', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='brown', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='gray', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='pink', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='yellow', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='cyan', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='magenta', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='olive', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='navy', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='teal', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='maroon', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='coral', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='indigo', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='gold', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='darkgreen', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='sienna', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='slategray', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='peru', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='darkorange', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='darkviolet', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='darkblue', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='darkturquoise', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='darkred', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='major', linestyle='-', color='darkslategray', alpha=0.5, linewidth=1)\nplt.gca().xaxis.grid(True, which='minor', linestyle='--', color='darkkhaki', alpha=0.5, linewidth=1)\nplt.gca().x",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y', yticks=[3, 4])\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y', ydata=[3, 4])\nplt.xticks([1, 2])\nplt.grid(axis='x', xdata=[1, 2])\n",
        "\n",
        "\nplt.legend(loc=\"lower right\")\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.tight_layout()  # Adjust subplot padding\nplt.show()\nplt.clf()\n# Copy the previous plot but adjust the subplot padding to have enough space to display axis labels\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.tight_layout()  # Adjust subplot padding\nplt.show()\n",
        "\n# Added label parameter to the plot function and called legend function to show the legend\n",
        "\nax.xaxis.tick_top()\nax.set_xticklabels(column_labels, minor=False)\nax.set_yticklabels(row_labels, minor=False)\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", labelpad=20)\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.yaxis.tick_right()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.tick_params(axis='y', labelleft=True, labelright=False, left=True, right=False)\nplt.gca().yaxis.set_label_position(\"right\")\nplt.show()\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='g')\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='blue', line_kws={'color': 'green'})\nplt.show()\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\nplt.show()\n",
        "\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xticks(rotation=0)\nplt.xlabel(\"celltype\")\nplt.legend()\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nax.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nax.set_xlabel(\"celltype\")\nax.set_ylabel(\"value\")\nax.set_xticklabels(df[\"celltype\"], rotation=45)\nax.legend()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.tick_params(axis=\"x\", colors=\"red\")\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.axhline(y=0, color='r')\n",
        "\nplt.plot(x, y)\nplt.xticks(fontsize=10, rotation=90)\n",
        "\nplt.axvline(x=0.22058956, color='r')\nplt.axvline(x=0.33088437, color='r')\nplt.axvline(x=2.20589566, color='r')\n",
        "\nfig, ax = plt.subplots()\nim = ax.imshow(rand_mat)\nax.set_xticks(numpy.arange(len(xlabels)))\nax.set_yticks(numpy.arange(len(ylabels)))\nax.set_xticklabels(xlabels)\nax.set_yticklabels(ylabels[::-1])\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\nfor i in range(len(ylabels)):\n    for j in range(len(xlabels)):\n        text = ax.text(j, i, round(rand_mat[i, j], 2), ha=\"center\", va=\"center\", color=\"w\")\nax.set_title(\"Heatmap\")\nfig.tight_layout()\nplt.show()\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\n# add legend for all three curves in the two subplots\nlines, labels = ax.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax2.legend(lines + lines2, labels + labels2, loc=0)\nplt.show()\nplt.clf()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title('Y')\naxs[1].plot(x, y)\naxs[1].set_title('Y')\nplt.show()\n",
        "\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, s=30)\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.scatter(b, a)\nfor i, txt in enumerate(c):\n    ax.annotate(txt, (b[i], a[i]))\nplt.show()\n",
        "\nplt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", title_fontweight=\"bold\")\n",
        "\nplt.hist(x, edgecolor='black', linewidth=1.2)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\nax1.plot(x, y)\nax2.plot(x, y)\nax1.set_aspect('equal')\nax2.set_aspect('equal')\nax1.set_xlim(0, 9)\nax2.set_xlim(0, 9)\nax1.set_ylim(0, 9)\nax2.set_ylim(0, 9)\nax1.set_title('First subplot')\nax2.set_title('Second subplot')\nax1.set_xlabel('X label')\nax2.set_xlabel('X label')\nax1.set_ylabel('Y label')\nax2.set_ylabel('Y label')\nax1.set_xticks(np.arange(0, 10, 2))\nax2.set_xticks(np.arange(0, 10, 2))\nax1.set_yticks(np.arange(0, 10, 2))\nax2.set_yticks(np.arange(0, 10, 2))\nax1.grid(True)\nax2.grid(True)\nplt.tight_layout()\nplt.show()\n",
        "\nplt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n",
        "\nplt.hist([x, y], bins=10, alpha=0.5, label=['x', 'y'])\nplt.legend(loc='upper right')\nplt.show()\n",
        "\nx = [0, 5]\ny = [(d-b)/(c-a)*(i-a)+b for i in x]\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nim1 = ax1.imshow(x, cmap='viridis')\nim2 = ax2.imshow(y, cmap='plasma')\nfig.colorbar(im1, ax=[ax1, ax2])\nplt.show()\n",
        "\nplt.plot(x[:,0], label='a')\nplt.plot(x[:,1], label='b')\nplt.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title('Y')\nax2.plot(a, z)\nax2.set_title('Z')\nfig.suptitle('Y and Z')\nplt.show()\n",
        "\nx = [p[0] for p in points]\ny = [p[1] for p in points]\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Plot of y over x\", fontsize=20)\nplt.xlabel(\"x\", fontsize=18)\nplt.ylabel(\"y\", fontsize=16)\nplt.show()\n",
        "\nax.plot(x, y)\nax.set_xticks(x)\nax.set_xticklabels([str(i+1) for i in x])\n",
        "\nfor i in range(len(lines)):\n    plt.plot([lines[i][0][0], lines[i][1][0]], [lines[i][0][1], lines[i][1][1]], color=c[i])\nplt.show()\n",
        "\nplt.loglog(x, y)\nplt.xticks([1, 10, 100, 1000])\nplt.yticks([1, 10, 100, 1000])\n",
        "\nfig, ax = plt.subplots()\ndf.plot(ax=ax, style='.-')\nax.legend(loc='upper left')\nplt.show()\n",
        "\nplt.hist(data, weights=np.ones(len(data)) / len(data))\nplt.gca().yaxis.set_major_formatter(plt.PercentFormatter(1))\nplt.yticks(np.arange(0, 1.1, 0.1))\n",
        "\nplt.plot(x, y, marker='o', alpha=0.5, linestyle='-')\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y, label='y')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax2.plot(z, a, label='a')\nax2.set_xlabel('z')\nax2.set_ylabel('a')\nfig.legend(loc='center', bbox_to_anchor=(0.5, 0.1), ncol=2)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\nax1.set(title=\"Bill Depth vs Bill Length\", xlabel=\"Bill Length (mm)\", ylabel=\"Bill Depth (mm)\")\nax2.set(title=\"Flipper Length vs Bill Length\", xlabel=\"Bill Length (mm)\", ylabel=\"Flipper Length (mm)\")\nplt.show()\n",
        "\nplt.xticks([2], ['second'])\n",
        "\nplt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n",
        "\nextra_ticks = [2.1, 3, 7.6]\nplt.xticks(list(plt.xticks()[0]) + extra_ticks)\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.xticks(rotation=-60, va='top')\n",
        "\nplt.xticks(alpha=0.5)\n",
        "\nplt.margins(x=0, y=0.1)\n",
        "\nplt.margins(x=0.05, y=-0.1)\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\nfig.suptitle('Figure')\naxs[0].plot(x, y)\naxs[0].set_title('Subplot 1')\naxs[1].plot(x, y)\naxs[1].set_title('Subplot 2')\n",
        "\ndf.plot(kind=\"line\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n",
        "\nplt.scatter(x, y, marker='|', s=100, linewidths=2, edgecolors='black', hatch='|', alpha=0.8)\n",
        "\nplt.scatter(x, y, edgecolors='none', marker='|', hatch='|')\n",
        "\nplt.scatter(x, y, marker='*', edgecolors='black', facecolors='none', linewidths=1.5, hatch='*')\nplt.show()\n",
        "\nplt.scatter(x, y, s=100, marker='*', hatch='|')\n",
        "\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.imshow(data, cmap='hot', extent=[1, 5, 1, 4])\nplt.colorbar()\nplt.show()\n",
        "\nplt.stem(x, y, orientation='horizontal')\nplt.show()\n",
        "\nplt.bar(d.keys(), d.values(), color=[c[key] for key in d.keys()])\nplt.show()\n",
        "\nplt.axvline(x=3, color='r', linestyle='--', label='cutoff')\nplt.legend()\nplt.show()\n",
        "\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, polar=True)\ntheta = [0, 1]\nbars = ax.bar(theta, height, width=0.4)\nax.set_xticklabels(labels)\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\"equal\"))\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.4), startangle=-40)\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1) / 2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(l[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                horizontalalignment=horizontalalignment, **kw)\nax.set_title(\"Donut plot\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(True, linestyle='--', color='blue')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.minorticks_on()\nax.grid(which='minor', linestyle='--', color='gray', alpha=0.4)\nax.grid(which='major', visible=False)\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, textprops={'weight': 'bold'})\nplt.axis('equal')\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, textprops={'weight': 'bold'})\nplt.axis('equal')\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markerfacecolor='none', markeredgecolor='blue', markersize=10, linewidth=2)\n",
        "\nplt.axvline(x=55, color='green')\n",
        "\nbar_width = 0.35\nx = np.arange(len(blue_bar))\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - bar_width/2, blue_bar, bar_width, label='Blue')\nrects2 = ax.bar(x + bar_width/2, orange_bar, bar_width, label='Orange')\nax.set_xticks(x)\nax.set_xticklabels(['A', 'B', 'C'])\nax.legend()\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\na = np.arange(10)\n# Make two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n# Plot y over x in the first subplot and plot z over a in the second subplot\nax1.plot(x, y, label='y over x')\nax2.plot(a, z, label='z over a')\n# Label each line chart and put them into a single legend on the first subplot\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('y over x')\nax1.legend()\nax2.set_xlabel('a')\nax2.set_ylabel('z')\nax2.set_title('z over a')\nax2.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, c=y, cmap='Spectral')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(0, 10, 1))\nplt.show()\n",
        "\ng = sns.factorplot(x=\"sex\", y=\"bill_length_mm\", col=\"species\", data=df, kind=\"bar\", sharey=False)\n",
        "\nfig, ax = plt.subplots()\ncircle = plt.Circle((0.5, 0.5), 0.2, color='r')\nax.add_artist(circle)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r\"$\\mathbf{\\phi}$\", fontsize=16)\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(markerscale=0.1)\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\nplt.show()\n",
        "\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)\n",
        "\nplt.legend()\nplt.plot([3, 7], [3, 7], marker=\"o\", linestyle=\"\", color=\"red\")\n",
        "\nplt.imshow(data, cmap='viridis')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r\"$\\bf{Figure}\\ 1$: Plot of y over x\")\n",
        "\nsns.pairplot(df, x_vars=[\"x\"], y_vars=[\"y\"], hue=\"id\")\nplt.legend().remove()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n",
        "\nplt.scatter(x, y)\nplt.axis('scaled')\nplt.axis('off')\n",
        "\nplt.scatter(x, y, c='red', edgecolors='black')\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax in axs.flatten():\n    ax.plot(x, y)\n",
        "\nplt.hist(x, bins=5, range=(0,10), width=2)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.fill_between(x, y-error, y+error, alpha=0.5)\nplt.show()\n",
        "\nplt.axhline(y=0, color='white')\nplt.axvline(x=0, color='white')\n",
        "\nax.errorbar(box_position, box_height, yerr=box_errors, fmt=\"none\", ecolor=c)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title('Y')\nax2.plot(a, z)\nax2.set_title('Z')\nax2.title.set(y=1.05)\nplt.show()\n",
        "\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].tick_params(axis='both', which='both', labelsize=6)\n        axs[i, j].set_xlim([0, 9])\n        axs[i, j].set_ylim([0, 9])\n        plt.subplots_adjust(wspace=0.5, hspace=0.5)\nplt.show()\n",
        "\nplt.figure(figsize=(8,8))\nplt.matshow(d)\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(10, 5))\nax.axis('off')\ntable = ax.table(cellText=df.values, colLabels=df.columns, loc='center')\ntable.auto_set_font_size(False)\ntable.set_fontsize(14)\ntable.scale(1, 2)\nplt.subplots_adjust(left=0.2, bottom=0.2)\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\nax.xaxis.set_ticks_position('both')\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.xaxis.tick_top()\nax.xaxis.set_ticks_position('both')\n",
        "\nplt.plot(x, y)\nplt.xticks(x)\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True)\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.set_titles(\"Group: {col_name}\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.set_xlabels(\"Exercise Time\")\ng.set_ylabels(\"Pulse\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.set_axis_labels(\"\", \"\")\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.figure(figsize=(5,5), dpi=300)\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\n",
        "\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=True)\nplt.legend().remove()\nplt.show()\n",
        "\ng = sns.FacetGrid(df, row=\"b\", height=1.5, aspect=4)\ng.map(sns.pointplot, \"a\", \"c\", order=np.arange(1, 31), color=\"black\")\ng.set(xticks=np.arange(1, 31, 2), xticklabels=np.arange(1, 31, 2))\nplt.show()\n",
        "\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tick_params(axis='both', which='both', labelbottom=False, labelleft=False)\n",
        "\ngs = gridspec.GridSpec(nrow, ncol, wspace=0.0, hspace=0.0)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\ndef my_map_func(i):\n  return [i, i+1, i+2]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\nresult = list(ds.as_numpy_iterator())\n",
        "\n    ds = tf.data.Dataset.from_tensor_slices(input)\n    ds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(tf.compat.v1.py_func(my_map_func, [x], [tf.int64])))\n    result = []\n    iterator = ds.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    with tf.compat.v1.Session() as sess:\n        for i in range(len(input)*3):\n            result.append(sess.run(next_element))\n    result = [item for sublist in result for item in sublist]\n    ",
        "\nmax_len = 8\nmask = tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.int32)\n",
        "\nmax_len = 8\nmask = tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32)\nresult = 1 - mask\n",
        "\nmax_len = 8\nmask = tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32)\n",
        "\nimport tensorflow as tf\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    max_length = tf.reduce_max(lengths)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n    padded_mask = tf.pad(mask, [[0,0],[0,8-max_length]])\n    return padded_mask\n",
        "\nmax_len = tf.reduce_max(lengths)\nmask = tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32)\npadded_mask = tf.pad(mask, [[0, 0], [0, 8 - max_len]], constant_values=1)\nresult = padded_mask[:, :8]\n",
        "\nresult = tf.transpose([tf.tile(a, [tf.shape(b)[0]]), tf.repeat(b, tf.shape(a)[0])])\n",
        "\n    result = tf.transpose([tf.tile(a, [tf.shape(b)[0]]), tf.repeat(b, tf.shape(a)[0])])\n    ",
        "\nresult = tf.squeeze(a, axis=2)\n",
        "\nresult = tf.expand_dims(a, axis=2)\n",
        "\nresult = tf.reshape(a, [1, 50, 100, 1, 512])\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n",
        "\n    result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n    ",
        "\nindices = tf.transpose([y, z])\nresult = tf.gather_nd(x, indices)\n",
        "\nindices = tf.transpose([row, col])\nresult = tf.gather_nd(x, indices)\n",
        "\n    m = tf.gather_nd(x,tf.transpose([y,z]))\n    ",
        "\nresult = tf.einsum('ijk, ljk->ilj', A, B)\n",
        "\nresult = tf.matmul(A, B, transpose_b=True)\n",
        "\nresult = [s.decode('utf-8') for s in x]\n",
        "\n    result = [s.decode('utf-8') for s in x]\n    ",
        "\nmask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\nsums = tf.reduce_sum(x, axis=-2)\ncounts = tf.reduce_sum(mask, axis=-2)\nresult = tf.math.divide_no_nan(sums, counts)\n",
        "\nmask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\nmean = tf.reduce_sum(x, axis=-2) / tf.reduce_sum(mask, axis=-2)\nvariance = tf.reduce_sum(tf.square(x - mean) * mask, axis=-2) / tf.reduce_sum(mask, axis=-2)\nresult = tf.math.sqrt(variance)\n",
        "\n    mask = tf.cast(tf.math.not_equal(x, 0), tf.float32)\n    sum_mask = tf.reduce_sum(mask, axis=-2)\n    sum_mask = tf.where(tf.equal(sum_mask, 0), tf.ones_like(sum_mask), sum_mask)\n    result = tf.reduce_sum(x, axis=-2) / sum_mask\n    ",
        "\nimport tensorflow as tf\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B)).numpy()\nprint(result)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    result = tf.argmax(a, axis=1)\n    ",
        "\nresult = tf.math.argmin(a, axis=0)\n",
        "#Save the model in \"export/1\"\nmodel.save(\"export/1\", save_format='tf')",
        "\nresult = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n",
        "\nresult = tf.random.uniform(shape=[114], minval=2, maxval=6, dtype=tf.int32)\n",
        "\n    result = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n    ",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\nresult = np.polyfit(np.log(x), y, 1)\n",
        "\ndef func(x, A, B, C):\n    return A * np.exp(B * x) + C\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0=p0)\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\ntest_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n",
        "\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\ndef f(x):\n    a, b, c = x\n    return ((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n",
        "\np_values = scipy.stats.norm.sf(abs(z_scores))\n",
        "\np_values = scipy.stats.norm(mu, sigma).cdf(z_scores)\n",
        "\nz_scores = scipy.stats.norm.ppf(p_values)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nresult = sa.multiply(sb.T)\n",
        "\n    result = sA.multiply(sparse.csr_matrix(np.tile(sB, (sA.shape[0], 1))))\n    ",
        "\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n",
        "\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n",
        "\ncenter = np.array([data_orig.shape[1]/2, data_orig.shape[0]/2])\nrot_mat = np.array([[np.cos(np.deg2rad(angle)), -np.sin(np.deg2rad(angle))], [np.sin(np.deg2rad(angle)), np.cos(np.deg2rad(angle))]])\noffset = center - np.dot(rot_mat, center)\nxrot, yrot = np.dot(rot_mat, np.array([x0, y0])) + offset\ndata_rot = rotate(data_orig, angle)\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, lambda x: x/T, N=len(times))\n",
        "\n    result = stats.kstest(times, lambda x: x/T, args=(rate, T)).pvalue\n    ",
        "\nresult = stats.kstest(times, 'uniform')\nresult = result.pvalue >= 0.05\n",
        "\nfrom scipy.sparse import hstack\nFeature = hstack([c1, c2])\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2)\ndef objective(x, dist_matrix):\n    return np.sum(dist_matrix[x,np.arange(N)])\nx0 = np.arange(N)\nresult = scipy.optimize.minimize(objective, x0, args=(dist_matrix,))\n",
        "\ndist_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)\nresult = col_ind\n",
        "\nb.setdiag(0)\nb.eliminate_zeros()\n",
        "\nlabeled_array, num_features = ndimage.label(img > threshold)\nresult = num_features\n",
        "\nlabeled_array, num_features = ndimage.label(img < threshold)\nresult = num_features\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nexample_img /= img.max()\ndef f(img = example_img):\n    threshold = 0.75\n    labeled, num_regions = ndimage.label(img > threshold)\n    result = num_regions\n    return result\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n# Begin of Missing Code\nlabeled, num_features = ndimage.label(img > threshold)\ncenters = ndimage.center_of_mass(img, labeled, range(1, num_features+1))\nresult = [np.sqrt(center[0]**2 + center[1]**2) for center in centers]\n# End of Missing Code\nprint(result)\n",
        "\nM = M + M.transpose()\n",
        "\nsA = sA + sA.T - sA.multiply(sA.T)\n",
        "\nstructure = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\nlabeled_array, num_features = scipy.ndimage.label(square)\nfor i in range(1, num_features+1):\n    feature_mask = labeled_array == i\n    if np.sum(scipy.ndimage.binary_dilation(feature_mask, structure=structure) & ~feature_mask) == 0:\n        square[feature_mask] = 0\n",
        "\nstructure = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\nlabeled_array, num_features = scipy.ndimage.label(square)\nfor i in range(1, num_features+1):\n    feature_mask = labeled_array == i\n    if np.sum(np.logical_and(feature_mask, structure)) == np.sum(feature_mask):\n        square[feature_mask] = 0\n",
        "\nmean = col.mean()\nstandard_deviation = col.std()\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nMedian = np.median(col.data)\nMode = np.bincount(col.data).argmax()\n",
        "\ndef fourier(x, *a):\n    result = 0\n    for i in range(1, degree+1):\n        result += a[i-1] * np.cos(i * np.pi / tau * x)\n    return result\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n",
        "\nn = np.max(example_array) + 1\ncoords = np.array(list(zip(*np.where(example_array > 0))))\ndists = scipy.spatial.distance.cdist(coords, coords)\nresult = np.zeros((n, n))\nfor i in range(n):\n    for j in range(i+1, n):\n        result[i, j] = np.min(dists[np.where(example_array == i), np.where(example_array == j)])\n        result[j, i] = result[i, j]\n",
        "\nn_regions = np.max(example_array) + 1\ncoords = np.array(list(zip(*np.where(example_array > 0))))\ndistances = scipy.spatial.distance.cdist(coords, coords, metric='cityblock')\nresult = np.zeros((n_regions, n_regions, 3))\nfor i in range(n_regions):\n    for j in range(i+1, n_regions):\n        mask_i = example_array == i\n        mask_j = example_array == j\n        min_dist = np.min(distances[mask_i][:, mask_j])\n        result[i, j] = [i, j, min_dist]\n        result[j, i] = [j, i, min_dist]\n",
        "\n    # Get the indices of all unique IDs in the array\n    unique_ids = np.unique(example_array)\n    # Create an empty N*N array to store the pairwise distances\n    result = np.zeros((len(unique_ids), len(unique_ids), 3))\n    # Loop through all unique ID pairs and calculate the pairwise distances\n    for i in range(len(unique_ids)):\n        for j in range(i+1, len(unique_ids)):\n            # Get the indices of all cells with the current unique IDs\n            idx_i = np.argwhere(example_array == unique_ids[i])\n            idx_j = np.argwhere(example_array == unique_ids[j])\n            # Calculate the pairwise distances between the cells with the current unique IDs\n            dist = scipy.spatial.distance.cdist(idx_i, idx_j, 'euclidean')\n            # Get the minimum distance between the cells with the current unique IDs\n            min_dist = np.min(dist)\n            # Store the pairwise distances in the result array\n            result[i,j] = [unique_ids[i], unique_ids[j], min_dist]\n            result[j,i] = [unique_ids[j], unique_ids[i], min_dist]\n    ",
        "\nresult = []\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k=2, s=4)\n    y_int = interpolate.splev(x_val, tck, der=0)\n    result.append(y_int)\nresult = np.array(result)\n",
        "\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\n",
        "\nresult = ss.anderson_ksamp([x1, x2])\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\ndef tau1(x, y):\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndef kendall_tau(df, window):\n    df_corr = df.rolling(window=window)\n    df_corr = df_corr.corr(tau1)\n    df_corr = df_corr.iloc[::2, 1::]\n    df_corr.columns = [''.join(col).strip() for col in df_corr.columns.values]\n    return df_corr\ndf_corr = kendall_tau(df, 3)\ndf = pd.concat([df, df_corr], axis=1)\nprint(df)\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = block_diag(*a)\n",
        "\n_, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n",
        "\n    _, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n    ",
        "\nn = len(a)\nmean = np.mean(a)\nstd = np.std(a, ddof=0)\nkurtosis_result = np.sum((a-mean)**4)/(n*std**4)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=True, bias=False)\n",
        "\nf = scipy.interpolate.interp2d(x.ravel(), y.ravel(), z.ravel(), kind='cubic')\nresult = f(s, t).diagonal()\n",
        "\n    interp_func = scipy.interpolate.interp2d(x.ravel(), y.ravel(), z.ravel(), kind='cubic')\n    result = interp_func(s, t).ravel()\n    ",
        "\nresult = np.zeros(len(vor.regions))\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    if region_index != -1:\n        result[region_index] += 1\n",
        "\nresult = np.zeros(len(vor.regions))\nfor i, point in enumerate(extraPoints):\n    region_index = vor.point_region[i]\n    if region_index != -1:\n        result[region_index] += 1\n",
        "\npadded_vectors = []\nfor vector in vectors:\n    padded_vector = np.pad(vector, (0, max_vector_size - len(vector)), mode='constant')\n    padded_vectors.append(padded_vector)\nresult = sparse.csr_matrix(padded_vectors)\n",
        "\nkernel = np.ones((3, 3))\nkernel[1, 0] = 0\nkernel[1, 2] = 0\nb = nd.median_filter(a, 3, origin=1, footprint=kernel)\n",
        "\nresult = M[row, column]\n",
        "\nresult = []\nfor r, c in zip(row, column):\n    result.append(M[r, c])\nresult = np.array(result)\n",
        "\nf = scipy.interpolate.interp1d(x, array, axis=0, kind='linear')\nnew_array = f(x_new)\n",
        "\nprob, _ = scipy.integrate.quad(NDfx, -np.inf, x, args=(u, o2))\n",
        "\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    prob = P\n    ",
        "\nresult = np.zeros((N,N))\nfor i in range(N):\n    for j in range(N):\n        result[i,j] = np.sqrt(2/N) * np.cos((np.pi/N)*(i+0.5)*j)\nresult[0,:] = result[0,:] / np.sqrt(2)\n",
        "\nresult = sparse.diags([matrix[1][1:], matrix[0], matrix[2][:-1]], [-1,0,1]).toarray()\n",
        "\nx = np.arange(N+1)\ny = np.arange(N+1)\nxx, yy = np.meshgrid(x, y)\nresult = scipy.stats.binom.pmf(xx, yy, p)\n",
        "\nresult = df.apply(stats.zscore, axis=1)\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\nzscore_df = df.apply(stats.zscore, axis=1, result_type='broadcast')\nresult = pd.concat([df, zscore_df.rename(columns=lambda x: x+'_zscore')], axis=1)\nresult = result.stack().to_frame().T.rename(index={0: 'data', 1: 'zscore'})\n",
        "\nzscore_df = df.apply(stats.zscore)\nresult = pd.concat([df, zscore_df], keys=['data', 'zscore'], axis=0)\nresult = result.round(3)\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)[0]\n",
        "\nmid = np.array([(shape[0]-1)/2, (shape[1]-1)/2]).reshape(1, 1, 2)\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), mid).reshape(shape)\n",
        "\nmid = np.array([(shape[0]-1)/2, (shape[1]-1)/2]).reshape(1, 1, 2)\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), mid, metric='cityblock')\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    rows, cols = shape\n    mid = np.array([rows/2, cols/2]).reshape(1, 1, 2)\n    y, x = np.indices(shape)\n    result = distance.cdist(np.dstack((y, x)), mid).reshape(rows, cols)\n    return result\n",
        "\nzoom_factor = min(np.array(shape)/np.array(x.shape))\nresult = scipy.ndimage.zoom(x, zoom_factor, order=1)\n",
        "\nfit_func = lambda x: np.sum(a * x**2, axis=1) - y\nout = scipy.optimize.minimize(fit_func, x0)\n",
        "\nfit_params = scipy.optimize.Bounds(x_lower_bounds, np.inf)\nout = scipy.optimize.minimize(lambda x: np.dot(a, x**2), x0, method='L-BFGS-B', bounds=fit_params)\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\ndef dN1_dt(t, N1):\n    return [-100 * N1[0] + np.sin(t)]\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\ndef dN1_dt(t, N1):\n    return -100 * N1 + t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\ndef dN1_dt(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nfor t in range (4):\n    def const(x, t=t):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sparse.vstack([sa, sb])\n",
        "\nresult = sparse.hstack([sa, sb])\n",
        "\ndef integrand(x, c):\n    return 2*c*x\nresult, error = scipy.integrate.quad(integrand, low, high, args=(c,))\n",
        "\n    eqn = lambda x: 2*c*x\n    result,error = scipy.integrate.quad(eqn, low, high)\n    ",
        "\nnon_zero_indices = V.nonzero()\nfor i, j in zip(non_zero_indices[0], non_zero_indices[1]):\n    V[i, j] += x\n",
        "\nV.data += x\n",
        "\nV.data += x\nV.data[V.data != 0] += y\n",
        "\n#iterate through columns\nfor Col in range(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    #get the column length\n    Len = math.sqrt(sum(List))\n    #normalize the column\n    sa[:,Col] = (1/Len)*Column\n",
        "\n#iterate through columns\nfor Col in range(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    #get the column length\n    Len = math.sqrt(sum(List))\n    #normalize the column\n    sa[:,Col] = (1/Len)*Column\n",
        "\na[a > 0] = 1\n",
        "\na[a > 0] = 1\n",
        "\nresult = []\nfor i in range(len(centroids)):\n    cluster_points = data[np.where(np.argmax(scipy.spatial.distance.cdist(data, centroids), axis=1) == i)]\n    distances = scipy.spatial.distance.cdist(cluster_points, [centroids[i]])\n    closest_index = np.argmin(distances)\n    closest_point_index = np.where(np.argmax(scipy.spatial.distance.cdist(data, centroids), axis=1) == i)[0][closest_index]\n    result.append(closest_point_index)\n",
        "\nresult = []\nfor i in range(centroids.shape[0]):\n    cluster_points = data[np.where(clusters == i)[0]]\n    centroid = centroids[i]\n    closest_point = cluster_points[scipy.spatial.distance.cdist([centroid], cluster_points).argmin()]\n    result.append(closest_point)\nresult = np.array(result)\n",
        "\nresult = []\nfor i in range(centroids.shape[0]):\n    cluster_data = data[np.where(labels == i)[0]]\n    distances = scipy.spatial.distance.cdist(cluster_data, [centroids[i]])\n    k_closest_indices = np.argsort(distances.flatten())[:k]\n    result.append(np.where(labels == i)[0][k_closest_indices])\n",
        "\nresult = []\nfor i in range(len(xdata)):\n    def eqn_to_solve(a):\n        return eqn(xdata[i], a, bdata[i])\n    sol = fsolve(eqn_to_solve, x0=0.5)\n    result.append(sol)\nresult = np.array(result)\n",
        "\ndef solve_for_b(x, a):\n    def eqn_b(b):\n        return eqn(x, a, b)\n    b1 = fsolve(eqn_b, x0=0.5)[0]\n    b2 = fsolve(eqn_b, x0=1.5)[0]\n    return min(b1, b2), max(b1, b2)\nresult = np.array([solve_for_b(x, a) for x, a in zip(xdata, adata)])\n",
        "\ncdf = lambda x: integrate.quad(bekkers, -np.inf, x, args=(estimated_a, estimated_m, estimated_d))[0]\nresult = stats.kstest(sample_data, cdf)\n",
        "\ndef cdf(x):\n    return integrate.quad(bekkers, -np.inf, x, args=(estimated_a, estimated_m, estimated_d))[0]\nks_statistic, p_value = stats.kstest(sample_data, cdf)\nresult = p_value >= 0.05\n",
        "\nintegral_df = df.set_index('Time').rolling('25s').apply(lambda x: integrate.trapz(x.values, x.index))\n",
        "\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\n",
        "\ndef multinomial_neg_log_likelihood(weights, data):\n    n = np.sum(data)\n    log_likelihood = np.sum(data * np.log(weights)) - n * np.log(np.sum(weights))\n    return -log_likelihood\nweights = sciopt.minimize(multinomial_neg_log_likelihood, np.ones(len(a.A1))/len(a.A1), args=(a.A1,), method='Nelder-Mead').x\n",
        "\nresult = sciopt.fmin_l_bfgs_b(e, [1, 1], bounds=[(pmin[0], pmax[0]), (pmin[1], pmax[1])], args=(x, y))[0]\n",
        "\nresult = signal.argrelextrema(arr, np.less_equal, axis=0, order=n)[0]\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(n, arr.shape[1]-n):\n        if arr[i,j] <= np.max(arr[i,j-n:j+n+1]) and arr[i,j] <= np.max(arr[i,j-n:j+n+1]):\n            result.append([i,j])\n",
        "\nnum_cols = df.select_dtypes(include=np.number).columns\ndf = df[(np.abs(stats.zscore(df[num_cols])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n    df = pd.DataFrame(data.data, columns=data.feature_names)\n    df['target'] = data.target\n    ",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.concat([df, pd.DataFrame(mlb.fit_transform(df['Col3']), columns=mlb.classes_, index=df.index)], axis=1).drop('Col3', axis=1)\n",
        "\nmlb = MultiLabelBinarizer()\ndf = df.join(pd.DataFrame(mlb.fit_transform(df.pop('Col3')),\n                          columns=mlb.classes_,\n                          index=df.index))\ndf_out = df.replace({np.nan: 0, 1: '1', 0: '0'})\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = pd.concat([df.drop(df.columns[-1], axis=1), pd.DataFrame(mlb.fit_transform(df[df.columns[-1]]), columns=mlb.classes_, index=df.index)], axis=1)\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = df.join(pd.DataFrame(mlb.fit_transform(df.iloc[:, -1]), columns=mlb.classes_, index=df.index))\n",
        "\nmlb = MultiLabelBinarizer()\ndf_out = df.join(pd.DataFrame(mlb.fit_transform(df.iloc[:, -1]), columns=mlb.classes_, index=df.index))\ndf_out.iloc[:, -len(mlb.classes_):] = np.where(df_out.iloc[:, -len(mlb.classes_):] == 0, 1, 0)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.svm as suppmach\nfrom sklearn.calibration import CalibratedClassifierCV\nX, y, x_test = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_test) == np.ndarray\n# Fit model:\nsvmmodel = suppmach.LinearSVC()\n# Obtain decision scores\npredicted_test_scores = svmmodel.decision_function(x_test)\n# Convert decision scores to probability estimates using logistic function\nproba = 1 / (1 + np.exp(-predicted_test_scores))\nprint(proba)\n",
        "\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\ntransform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\ndf = pd.concat([df_origin, transform_output], axis=1)\n",
        "\ndf_transformed = pd.DataFrame.sparse.from_spmatrix(transform_output)\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\n    transformed_df = pd.DataFrame.sparse.from_spmatrix(transform_output)\n    result = pd.concat([df, transformed_df], axis=1)\n    ",
        "\n# Delete the second step (PolynomialFeatures)\ndel clf.named_steps['poly']\n# Insert a new step (StandardScaler) as the first step\nfrom sklearn.preprocessing import StandardScaler\nclf.named_steps['scaler'] = StandardScaler()\n# Note: modifying the steps dictionary directly will not affect the clf object\n# We need to modify the named_steps dictionary instead\n",
        "\ndel clf.named_steps['reduce_poly']\n",
        "\ndel clf.named_steps['pOly']\n",
        "\n# Insert a new step after the first step\nclf.steps.insert(1, ('new_step', SomeEstimator()))\n# Delete the second step\ndel clf.steps[2]\n",
        "\nnew_step = ('new_step', SVC())\nclf.steps.insert(1, new_step)\n",
        "\nsteps = clf.named_steps\nsteps.insert(2, ('t1919810', PCA()))\nclf.steps = steps\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\nmodel = xgb.XGBRegressor()\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\ngridsearch.fit(trainX, trainY, **fit_params)\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\ngridsearch = GridSearchCV(xgb.XGBRegressor(), paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\ngridsearch.fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train, test in cv:\n    logreg.fit(X[train], y[train])\n    proba.append(logreg.predict_proba(X[test]))\nproba = np.concatenate(proba)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test)[:, 1])\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nmodel_name = type(model).__name__\n",
        "\nmodel_name = type(model).__name__\n",
        "\nmodel_name = type(model).__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nclf = GridSearchCV(bc, param_grid=param_grid)\nclf.fit(X_train, y_train)\n",
        "\nX = X.reshape(-1, 1)\n",
        "\nX = X.reshape(-1,1)\n",
        "\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef prePro(text):\n    return text.lower()\ntfidf = TfidfVectorizer(preprocessor=prePro)\nprint(tfidf.preprocessor)\n",
        "\nscaler = preprocessing.StandardScaler()\ndf_out = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
        "\nscaler = preprocessing.StandardScaler()\ndf_out = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nselected_features = model.get_support()\ncolumn_names = X.columns[selected_features]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nselected_features = model.get_support()\ncolumn_names = X.columns[selected_features]\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n# get selected feature indices\nselected_feature_indices = model.get_support(indices=True)\n# get column names of selected features\ncolumn_names = X.columns[selected_feature_indices]\nprint(column_names)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nselected_features = model.get_support()\ncolumn_names = list(X.columns[selected_features])\n",
        "\nkm.fit(X)\nclosest_50_samples = X[np.argsort(np.linalg.norm(X - km.cluster_centers_[p], axis=1))[:50]]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\ndistances = km.transform(X)\nclosest_50_samples = X[np.argsort(distances[:, p])[:50]]\nprint(closest_50_samples)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\n# Get the p^th cluster center\ncenter = km.cluster_centers_[p]\n# Calculate the distance between each sample and the center\ndistances = np.linalg.norm(X - center, axis=1)\n# Get the indices of the 100 closest samples\nclosest_indices = np.argsort(distances)[:100]\n# Get the actual samples\nclosest_100_samples = X[closest_indices]\nprint(closest_100_samples)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\ndef get_samples(p, X, km):\n    km.fit(X)\n    distances = km.transform(X)\n    closest_cluster = np.argmin(distances, axis=1)\n    samples = []\n    for i in range(km.n_clusters):\n        cluster_samples = X[closest_cluster == i]\n        center = km.cluster_centers_[i]\n        distances = np.linalg.norm(cluster_samples - center, axis=1)\n        closest_samples = cluster_samples[np.argsort(distances)[:50]]\n        samples.append(closest_samples)\n    return np.concatenate(samples)\nclosest_50_samples = get_samples(p, X, km)\nprint(closest_50_samples)\n",
        "\nX_train = pd.get_dummies(X_train, columns=[0])\n",
        "\nX_train = pd.get_dummies(X_train, columns=[0])\n",
        "\nclf = SVR(kernel='rbf')\nclf.fit(X, y)\npredict = clf.predict(X)\n",
        "\nregressor = SVR(kernel='rbf')\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nregressor = SVR(kernel='poly', degree=2)\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nregressor = SVR(kernel='poly', degree=2)\nregressor.fit(X, y)\npredict = regressor.predict(X)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.T).toarray()\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = cosine_similarity(query_tfidf, tfidf)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = cosine_similarity(query_tfidf, tfidf)\n    return cosine_similarities_of_queries\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n",
        "\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\nnew_features = pd.DataFrame(new_features, columns=mlb.classes_)\n",
        "\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nnew_f = pd.DataFrame(mlb.fit_transform(f), columns=mlb.classes_)\n",
        "\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\nnew_features = pd.DataFrame(new_features, columns=mlb.classes_)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfeatures = load_data()\ndef solve(features):\n    mlb = MultiLabelBinarizer()\n    new_features = mlb.fit_transform(features)\n    return new_features\nnew_features = solve(features)\nprint(new_features)\n",
        "\nmlb = MultiLabelBinarizer()\nnew_features = pd.DataFrame(mlb.fit_transform(features), columns=mlb.classes_)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='complete')\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\ncluster = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='complete')\ncluster_labels = cluster.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='complete')\ncluster_labels = model.fit_predict(simM)\n",
        "\ndistance_matrix = 1 - np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maxclust')\n",
        "\ndistance_matrix = 1 - np.array(data_matrix)\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='complete')\ncluster_labels = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\nZ = scipy.cluster.hierarchy.linkage(simM, method='complete')\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, 2, criterion='maxclust')\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b|[\\?\\!\\\"]+|\\'+')\ntransformed_text = vectorizer.fit_transform(text)\n",
        "\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = pd.read_csv('example.csv', header=None, sep=',')\ndef solve(data):\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nf1 = df['mse'].values\nX = f1.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\nf1 = f1.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(f1)\nlabels = kmeans.predict(f1)\ncentroids = kmeans.cluster_centers_\n",
        "\nlsvc = LinearSVC(penalty='l1', dual=False)\nlsvc.fit(X, y)\ncoef = lsvc.coef_\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.nonzero(coef)[1]]\n",
        "\nlsvc = LinearSVC(penalty='l1', dual=False)\nlsvc.fit(X, y)\nmodel = SelectFromModel(lsvc, prefit=True)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[model.get_support()]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ncorpus, y = load_data()\nassert type(corpus) == list\nassert type(y) == list\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\ndef solve(corpus, y, vectorizer, X):\n    lsvc = LinearSVC(penalty='l1', dual=False).fit(X, y)\n    coef = lsvc.coef_\n    coef = np.squeeze(coef)\n    nonzero_coef_indices = np.nonzero(coef)[0]\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[nonzero_coef_indices]\n    return selected_feature_names\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = [word for word in vectorizer.vocabulary_.keys() if word in vectorizer.get_feature_names()]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = [word for _, word in sorted(zip(vectorizer.vocabulary_.values(), vectorizer.vocabulary_.keys()))]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()[:, [feature_names.index(word) for word in vectorizer.vocabulary]]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = [word for word in vectorizer.vocabulary_.keys() if word in vectorizer.get_feature_names()]\nX = X[:, [vectorizer.vocabulary_[word] for word in feature_names]]\n",
        "\nslopes = np.array([]) # initialize empty array to store slopes\nfor col in df1.columns[1:]: # iterate over all columns except the first one\n    df2 = df1[~np.isnan(df1[col])] # remove NaN values for the current column\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0) # append the slope to the array\n",
        "\nslopes = np.array([])\nfor col in df1.columns:\n    if col != 'Time':\n        df2 = df1[~np.isnan(df1[col])]\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        slopes = np.concatenate((slopes, m), axis = 0)\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ntransformed_df = df\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ntransformed_df = df\n",
        "\n    le = LabelEncoder()\n    transformed_df = df.copy()\n    transformed_df['Sex'] = le.fit_transform(transformed_df['Sex'])\n    ",
        "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1,1)).reshape(np_array.shape)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1)).reshape(np_array.shape)\n",
        "\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a.reshape(-1,1)).reshape(a.shape)\n    ",
        "\npredict = clf.predict(b)\n",
        "\nnew_X = np.array(X)\nfor i in range(len(X)):\n    for j in range(len(X[i])):\n        if isinstance(X[i][j], str):\n            new_X[i][j] = ord(X[i][j])\n",
        "\nle = LabelEncoder()\nnew_X = np.array(X)\nfor i in range(new_X.shape[1]):\n    new_X[:, i] = le.fit_transform(new_X[:, i])\n",
        "\nle = LabelEncoder()\nnew_X = np.array(X)\nfor i in range(new_X.shape[1]):\n    new_X[:, i] = le.fit_transform(new_X[:, i])\n",
        "\n# Splitting the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Creating the logistic regression model\nlogReg = LogisticRegression()\n# Fitting the model on the training data\nlogReg.fit(X_train, y_train)\n",
        "\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\nsplit_index = int(len(features_dataframe)*0.2)\ntrain_dataframe = features_dataframe[:split_index]\ntest_dataframe = features_dataframe[split_index:]\n",
        "\nsplit_index = int(len(features_dataframe)*0.8)\ntrain_dataframe = features_dataframe.iloc[split_index:]\ntest_dataframe = features_dataframe.iloc[:split_index]\n",
        "\ntrain_size = 0.2\nnum_rows = features_dataframe.shape[0]\nsplit_index = int(num_rows * (1 - train_size))\ntrain_dataframe = features_dataframe.iloc[:split_index].sort_values(\"date\")\ntest_dataframe = features_dataframe.iloc[split_index:].sort_values(\"date\")\n",
        "df[cols[0] + '_scale'], df[cols[1] + '_scale'] = zip(*df.groupby('Month')[cols].apply(lambda x: pd.Series(scaler.fit_transform(x))))",
        "\ncols = myData.columns[2:4]\nmyData[['new_' + col for col in cols]] = myData.groupby('Month')[cols].apply(lambda x: pd.DataFrame(scaler.fit_transform(x), columns=['new_'+col for col in cols]))\n",
        "\ncount = CountVectorizer(lowercase = False, token_pattern=r'\\b\\w+\\b|\\S+')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\nresults = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = results.sort_values(by='rank_test_score')\n",
        "\nresults = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = results.sort_values(by='mean_fit_time')\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nimport joblib\n# Fit the model to your clean data\nmodel = IsolationForest()\nmodel.fit(clean_data)\n# Save the model in the file named \"sklearn_model\"\njoblib.dump(model, \"sklearn_model\")\n",
        "\nprint(cosine_similarity_matrix)\n"
    ],
    "Pytorch": [
        "\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\nweights = torch.FloatTensor(word2vec.wv.vectors)\nembedding_layer = torch.nn.Embedding.from_pretrained(weights)\nembedded_input = embedding_layer(input_Tensor)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\ninput_Tensor = load_data()\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\ndef get_embedded_input(input_Tensor):\n    weights = torch.FloatTensor(word2vec.wv.vectors)\n    embedding = torch.nn.Embedding.from_pretrained(weights)\n    embedded_input = embedding(input_Tensor)\n    return embedded_input\nembedded_input = get_embedded_input(input_Tensor)\nprint(embedded_input)\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nA_log = A_log.bool() # convert to boolean tensor\nC = B[:, A_log] # slice using logical index\n",
        "\nA_logical = A_logical.nonzero().squeeze()\nC = B[:, A_logical]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\nC = B[:, A_log.nonzero().squeeze()]\n",
        "\n    C = B[:, A_log.nonzero().squeeze()]\n    ",
        "\nA_log = A_log.nonzero().squeeze()\nC = B[:, A_log]\n",
        "\nC = B.index_select(1, idx)\n",
        "\nx_list = x_array.tolist()\nx_tensor = torch.tensor(x_list)\n",
        "\nx_tensor = torch.tensor(np.stack(x_array), dtype=torch.float)\n",
        "\ndef Convert(a):\n    t = torch.from_numpy(np.stack(a))\n    return t\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\nmask = get_mask(lens)\nprint(mask)\n",
        "\ndiag_ele = Tensor_2D.shape[1]\nbatch_size = Tensor_2D.shape[0]\nTensor_3D = torch.zeros(batch_size, diag_ele, diag_ele)\nfor i in range(batch_size):\n    Tensor_3D[i] = torch.diag(Tensor_2D[i])\n",
        "\n    batch_size, diag_size = t.shape\n    result = torch.zeros(batch_size, diag_size, diag_size)\n    for i in range(batch_size):\n        result[i] = torch.diag(t[i])\n    ",
        "\nif a.shape[0] == 1:\n    ab = torch.cat((a, b.unsqueeze(0)), dim=0)\nelse:\n    ab = torch.cat((a, b.repeat(a.shape[0], 1)), dim=0)\n",
        "\nif a.shape[0] > b.shape[0]:\n    pad = torch.zeros((a.shape[0]-b.shape[0], b.shape[1]))\n    b = torch.cat((b, pad), dim=0)\nelif b.shape[0] > a.shape[0]:\n    pad = torch.zeros((b.shape[0]-a.shape[0], a.shape[1]))\n    a = torch.cat((a, pad), dim=0)\nab = torch.stack((a,b), 0)\n",
        "\ndef solve(a, b):\n    ab = torch.cat((a.unsqueeze(0), b.unsqueeze(0)), dim=0)\n    return ab\n",
        "\nfor i in range(len(lengths)):\n    a[i, int(lengths[i]):, :] = 0\n",
        "\nfor i in range(a.shape[0]):\n    a[i, lengths[i]:, :] = 2333\n",
        "\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 0\n",
        "\nfor i in range(10):\n    a[i, lengths[i]:, :] = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n    ",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[np.arange(len(t)), idx]\n",
        "\nresult = t[:, idx == 1].squeeze()\n",
        "\nresult = t[np.arange(len(t)), idx]\n",
        "\nresult = torch.gather(x, 1, ids.repeat(1,1,x.shape[-1])).squeeze(1)\n",
        "\nresult = torch.gather(x, 1, ids.repeat(1, x.shape[1], 1)).squeeze(1)\n",
        "\nresult = torch.gather(x, 1, torch.unsqueeze(ids, 2))\nresult = torch.squeeze(result, 1)\n",
        "\n_, y = torch.max(torch.Tensor(softmax_output), 1)\ny = y.view(-1, 1)\n",
        "\n_, y = torch.max(torch.tensor(softmax_output), 1)\ny = y.reshape(-1, 1)\n",
        "\n_, y = torch.max(torch.Tensor(softmax_output), 1)\ny = y.view(-1, 1)\n",
        "\n    _, y = torch.max(softmax_output, dim=1)\n    y = y.view(-1, 1)\n    ",
        "\n    _, y = torch.max(softmax_output, dim=1)\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn.functional as F\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, w, z), target: (n, w, z)\n    n, c, w, z = input.size()\n    # log_p: (n, c, w, z)\n    log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*w*z, c)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)  # make class dimension last dimension\n    target = target.view(n*w*z, -1)\n    # target: (n*w*z,)\n    mask = target >= 0\n    target = target[mask]\n    log_p = log_p[mask.squeeze()]\n    loss = F.nll_loss(log_p, target, weight=weight, size_average=size_average)\n    return loss\nimages, labels = load_data()\nloss = cross_entropy2d(images, labels)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = np.count_nonzero(A != B)\n",
        "\n    cnt_equal = np.sum(A == B)\n    ",
        "\nx = A.shape[0] // 2\ncnt_equal = np.sum(A[-x:] == B[-x:])\n",
        "\nx = A.shape[0] // 2\ncnt_not_equal = np.sum(A[x:] != B[x:])\n",
        "\nnum_splits = a.shape[3] - chunk_dim + 1\na_split = torch.split(a, chunk_dim, dim=3)\ntensors_31 = []\nfor i in range(num_splits):\n    tensors_31.append(a_split[0][:,:,:,(i):(i+chunk_dim),:])\n",
        "\na_split = torch.split(a, chunk_dim, dim=2)\ntensors_31 = [a_split[i:i+31] for i in range(31)]\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_vals = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = min_vals * torch.where(torch.abs(x) < torch.abs(y), sign_x, sign_y)\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_vals = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = max_vals * torch.where(torch.abs(x) >= torch.abs(y), sign_x, sign_y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nx, y = load_data()\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_vals, indices = torch.min(torch.stack((torch.abs(x), torch.abs(y))), dim=0)\n    signed_min = min_vals * torch.where(indices == 0, sign_x, sign_y)\n    return signed_min\nsigned_min = solve(x, y)\nprint(signed_min)\n",
        "\noutput = MyNet(input)\nconfidence_score = torch.nn.functional.softmax(output, dim=1)[0][classes.item()].item()\n",
        "\noverlap = (a[:, -1] + b[:, 0]) / 2\nresult = np.concatenate((a[:, :-1], overlap.reshape(-1, 1), b[:, 1:]), axis=1)\nresult = torch.Tensor(result)\n",
        "\n    overlap = (a[:, -1] + b[:, 0]) / 2\n    result = torch.cat((a[:, :-1], overlap.unsqueeze(1), b[:, 1:]), dim=1)\n    ",
        "\nzeros = torch.zeros((1, 6, 4))\nzeros[:, 1:5, 1:3] = t\nresult = zeros\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(4).reshape(1,2,2).float()\nprint(t)\nnew=torch.zeros(1,1,4)\nprint(new)\nresult = torch.cat([new,t,new], dim=1)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(8).reshape(1,4,2).float()\nprint(t)\nnew=torch.tensor([[-1, -1, -1, -1]])\nprint(new)\nnew = new.repeat(t.shape[0], 1)\nnew = new.unsqueeze(1)\nr = torch.cat([new, t, new], dim=1)\nprint(r)\n",
        "\nW = W.unsqueeze(0).unsqueeze(0)\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\n"
    ]
}