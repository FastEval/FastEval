{
    "Pandas": [
        "\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\nresult = df.iloc[List]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\ndf['Qu1'] = df['Qu1'].replace(value_counts(df['Qu1']) >= 2, 'other')\ndf['Qu2'] = df['Qu2'].replace(value_counts(df['Qu2']) >= 2, 'other')\ndf['Qu3'] = df['Qu3'].replace(value_counts(df['Qu3']) >= 2, 'other')\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n# Count the number of occurrences of each value in each column\ncounts = {col: pd.value_counts(df[col]) for col in df.columns}\n# Check if the count of each value is greater than or equal to 3\nvalues_to_keep = {col: val for col, val in counts.items() if val['count'] >= 3}\n# Replace the values in the columns with the values to keep\nresult = df.replace(values_to_keep, 'other')\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\ndef f(df=example_df):\n    value_counts = pd.value_counts(df.Qu1)\n    if value_counts[df.Qu1 >= 2].sum() == len(df.Qu1):\n        df.Qu1 = df.Qu1.replace({'apple': 'other', 'egg': 'other'}, regex=True)\n    else:\n        df.Qu1 = df.Qu1.replace({'apple': 'other', 'egg': 'other'}, regex=True)\n        df.Qu1 = df.Qu1.replace({'banana': 'other', 'potato': 'other'}, regex=True)\n    return df\nresult = f()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n# Change values in column Qu1\ndf.loc[df['Qu1'] == 'apple', 'Qu1'] = 'other'\ndf.loc[df['Qu1'] == 'potato', 'Qu1'] = 'other'\ndf.loc[df['Qu1'] == 'cheese', 'Qu1'] = 'other'\ndf.loc[df['Qu1'] == 'banana', 'Qu1'] = 'other'\ndf.loc[df['Qu1'] == 'egg', 'Qu1'] = 'other'\n# Change values in column Qu2\ndf.loc[df['Qu2'] == 'banana', 'Qu2'] = 'other'\ndf.loc[df['Qu2'] == 'apple', 'Qu2'] = 'other'\ndf.loc[df['Qu2'] == 'sausage', 'Qu2'] = 'other'\n# Change values in column Qu3\ndf.loc[df['Qu3'] == 'apple', 'Qu3'] = 'other'\ndf.loc[df['Qu3'] == 'potato', 'Qu3'] = 'other'\ndf.loc[df['Qu3'] == 'banana', 'Qu3'] = 'other'\ndf.loc[df['Qu3'] == 'cheese', 'Qu3'] = 'other'\ndf.loc[df['Qu3'] == 'egg', 'Qu3'] = 'other'\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n# Replace values in Qu1 with 'other' if count is less than 3\ndf.loc[df['Qu1'] != 'apple', 'Qu1'] = 'other'\n# Replace values in Qu2 with 'other' if count is less than 2\ndf.loc[df['Qu2'] != 'banana', 'Qu2'] = 'other'\n# Replace values in Qu3 with 'other' if count is less than 2\ndf.loc[df['Qu3'] != 'apple', 'Qu3'] = 'other'\nprint(df)\n",
        "\ndf = df.drop_duplicates(subset='url', keep='first')\n",
        "\ndf = df.drop_duplicates(subset='url', keep='first')\n",
        "\ndf = df.drop_duplicates(subset='url', keep='last')\n",
        "\nresult = {}\nfor name, row in df.iterrows():\n    for col, value in row.iteritems():\n        if col not in result:\n            result[col] = {}\n        if name not in result[col]:\n            result[col][name] = value\nprint(result)\n",
        "\ndf['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    return df\n    ",
        "\ndf['datetime'] = pd.to_datetime(df['datetime'], utc=True)\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\ndf['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\ndf['message'] = df['message'].apply(lambda x: x.split(', '))\ndf = df.explode('message')\n",
        "\nresult = df.loc[df['product'].isin(products), 'score'] * 10\n",
        "\nresult = df.loc[df['product'].isin(products), 'score'] * 10\n",
        "\nresult = df.loc[df['product'].isin(products), 'score'] * 10\n",
        "\n# Min-Max Normalize scores corresponding to products 1069104 and 1069105\nresult = df.loc[df['product'].isin(products), 'score'] / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min())\n",
        "\n# [Missing Code]\n",
        "\n# Convert binary columns to categorical\ndf = pd.get_dummies(df, columns=['A', 'B', 'C', 'D'])\n# Rename categories\ndf.columns = ['A', 'B', 'C', 'D', 'category']\n",
        "\n# Convert binary columns to categorical\ndf['category'] = pd.Categorical(df.astype(str) + '0', categories=['A', 'B', 'C', 'D'])\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'])\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'])\n",
        "\n# Extract the dates between 2017-08-17 and 2018-01-31\ndates_between = pd.to_datetime(List) >= pd.to_datetime('2017-08-17') & pd.to_datetime(List) <= pd.to_datetime('2018-01-31')\n# Extract the month name and year and day in a simple way\nresult['Date'] = pd.to_datetime(result['Date']).dt.to_period('M').strftime('%B %Y %a')\nresult = result.loc[dates_between]\n",
        "\ndf = pd.DataFrame({'#1': [72.4399, 11.6985, 43.6431, 54.9089, 63.1225],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n",
        "\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\nresult = df.shift(1, axis=0)\nprint(result)\n",
        "\nresult = df.shift(1, axis=0)\nresult.iloc[0, 1] = df.iloc[4, 1]\nresult.iloc[1, 0] = df.iloc[0, 1]\nresult.iloc[2, 1] = df.iloc[1, 1]\nresult.iloc[3, 0] = df.iloc[2, 1]\nresult.iloc[4, 1] = df.iloc[3, 1]\nresult.iloc[5, 0] = df.iloc[4, 1]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'HeaderA': [476], 'HeaderB': [4365], 'HeaderC': [457]})\ndf.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX'}, inplace=True)\nprint(df)\n",
        "\ndf.rename(columns={col: 'X' + col for col in df.columns})\n",
        "\ndf.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX', 'HeaderX': 'XHeaderX'}, inplace=True)\n",
        "\ndf.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", \"val3\": \"mean\"})\n",
        "\ndf.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", \"val3\": \"sum\"})\n",
        "\ndf.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val32\": \"mean\", \"val42\": \"sum\"})\n",
        "\nresult = df.iloc[row_list, column_list].mean(axis=0)\n",
        "\nresult = df.sum(axis=1, level=row_list, select_dtypes=['int64'])\n",
        "\nresult = df.sum(axis=1, level=row_list, select_dtypes=['int64'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nvalue_counts = df.value_counts()\nresult = pd.Series(value_counts.index.tolist(), index=df.columns)\nprint(result)\n",
        "\n# Count the number of null values in each column\nnull_counts = df.isnull().sum()\n# Print the result\nprint(null_counts)\n",
        "\n# [Missing Code]\n",
        "\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1.883444,0.564835,0.144654]})\ndf = pd.concat([df.iloc[[0]], df.iloc[[1]]], axis=1)\nprint(df)\n",
        "\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1.883444,0.564835,0.144654]})\ndf = pd.concat([df.iloc[[0]], df.iloc[[1]]], axis=1)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\ndf.fillna(method='ffill', inplace=True)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[3,1,2],[1,2,np.nan],[2,np.nan,np.nan]],columns=['0','1','2'])\ndf.fillna(method='ffill', inplace=True)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\ndf.fillna(df.iloc[0], inplace=True)\nprint(df)\n",
        "\n# select the rows whose value is smaller than the threshold\nsmall_rows = df.loc[df['value'] < thresh]\n# sum the values of the selected rows\nresult = small_rows.sum()\n# print the result\nprint(result)\n",
        "\n# Group the rows by lab and calculate the average of value for each group\nresult = df.groupby('lab')['value'].agg(lambda x: x.mean())\n# Replace the rows whose value is greater than the threshold with the average of the substituted rows\nresult = result.loc[result['value'] > thresh, 'value'] = result.loc[result['value'] > thresh, 'value'].mean()\n# Print the result\nprint(result)\n",
        "\ndf.loc[df['lab'].isin([section_left, section_right])] = (df.loc[df['lab'].isin([section_left, section_right])] / 2).mean()\n",
        "\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n",
        "\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \": [e^1, e^2, e^3], \"exp_B \": [e^4, e^5, e^6]})\n",
        "\ninv_A = 1 / df[\"A\"]\ninv_B = 1 / df[\"B\"]\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 0], \"inv_A\": inv_A, \"inv_B\": inv_B})\n",
        "\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nfor col in df.columns:\n    df[col + \"_sigmoid\"] = df[col].apply(sigmoid)\n",
        "\nresult = df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'})\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')\nresult = result.merge(df.idxmin().to_frame().reset_index().rename(columns={'index': 'min_idx'}), on='max_idx')\nresult = result.merge(df.idxmax().to_frame().reset_index().rename(columns={'index': 'max_idx'}), on='min_idx')",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndf['dt'] = pd.date_range(min_date, max_date)\ndf['val'] = 0\nprint(df)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndf['dt'] = pd.date_range(min_date, max_date)\ndf['val'] = 0\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n# Fill in the missing code here\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.to_datetime(df['dt'])",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n# Fill in missing dates\ndf['dt'] = df['dt'].fillna(pd.to_datetime('2016-01-01'))\n# Fill in missing values\ndf['val'] = df['val'].fillna(df['val'].max())\nprint(df)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndf['dt'] = pd.date_range(min_date, max_date)\ndf['user'] = df['user'].fillna(df['user'].mode())\ndf['val'] = df['val'].fillna(df['val'].mode())\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\nresult = pd.DataFrame({'name': [1, 1, 1, 2, 2, 2],\n                     'a': [3, 3, 3, 4, 3, 5],\n                     'b': [5, 6, 6, 6, 6, 1],\n                     'c': [7, 9, 10, 0, 1, 4]})\nresult = result.assign(name=result.index)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\nresult = df.assign(a=lambda x: x.a.map(lambda y: y + 1))\nprint(result)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\ndef f(df=example_df):\n    unique_id = 0\n    result = example_df.assign(id=lambda x: unique_id)\n    unique_id += 1\n    return result\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\nresult = pd.DataFrame({'ID': [1, 1, 1, 2, 3, 3],\n                      'b': [5, 6, 6, 6, 6, 6],\n                      'c': [7, 9, 10, 0, 1, 4]})\nresult = pd.merge(df, result, on='name')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\ndf = df.pivot_table(index='user', columns='date', values='value', aggfunc='sum')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\ndf = pd.pivot_table(df, index='user', columns='01/12/15', values='someBool')\ndf = pd.pivot_table(df, index='user', columns='02/12/15', values='someBool')\ndf = pd.pivot_table(df, index='user', columns='someBool', values='02/12/15')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, None],\n                   '02/12/15': [None, -100, 200],\n                   'someBool': [True, False, True]})\ndf = df.pivot_table(index='user', columns='date', values='value', aggfunc='first')\nresult = df\nprint(result)\n",
        "\ndf.loc[df['c'] > 0.5, columns]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(np.random.rand(4,5), columns=list('abcde'))\nselected_rows = df.loc[df['c'] > 0.45, ['a', 'b', 'e']]\nprint(selected_rows)\n",
        "\nimport pandas as pd\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in ['a', 'd']]\n    result = df[df.c > 0.5][locs]\n    return result\n",
        "\nimport pandas as pd\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in ['a', 'd']]\n    result = df[df.c > 0.5][locs]\n    return result\n",
        "\ndf = df[df['c'] > 0.5]\nresult = df[['b', 'e']]\nreturn result\n",
        "",
        "",
        "",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = pd.DataFrame({'col1':[2, 0.5, 1, 0.5, 0]})\nfor i in range(0, len(df), 3):\n    result.loc[i] = (df.iloc[i:i+3].sum() / 3)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\nresult = pd.DataFrame({'col1':[2, 3]})\nfor i in range(0, len(df), 3):\n    result = pd.concat([result, pd.DataFrame({'col1':[i+1, i+2, i+3]})])\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = pd.DataFrame({'col1':[1.5, 1.333]})\nfor i in range(0, len(df), 3):\n    result = pd.concat([result, pd.DataFrame({'col1':[df.iloc[i:i+3, 'col1'].mean()]})])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\ndef bin_data(df):\n    bins = [3, 2]\n    result = pd.DataFrame()\n    for i in range(len(df)):\n        if i % 3 == 0:\n            result = result.append(df.iloc[i:i+3])\n        elif i % 2 == 0:\n            result = result.append(df.iloc[i:i+2])\n    return result\nresult = bin_data(df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\nresult = []\nfor i in range(0, len(df), 3):\n    sum_row = df.iloc[i:i+3].sum()\n    avg_row = df.iloc[i+1:i+3].mean()\n    result.append(sum_row)\n    result.append(avg_row)\nprint(result)\n",
        "\ndf.ffill(inplace=True)\n",
        "\ndf.fillna(method='ffill', inplace=True)\n",
        "\ndf.ffill(inplace=True)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\ndf['number'] = df.duration.replace(r'\\d+', r'', regex=True)\ndf['time'] = df.duration.replace(r'\\w+', r'', regex=True)\nresult = df\nprint(result)\n",
        "\ndf['numer'] = df.duration.replace(r'\\d.*', r'\\d', regex=True, inplace=True)\ndf['time'] = df.duration.replace(r'\\.w.+', r'\\w.+', regex=True, inplace=True)\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n",
        "\ndf['numer'] = df.duration.replace(r'\\d+', r'', regex=True, inplace=True)\ndf['time'] = df.duration.replace(r'\\w+', r'', regex=True, inplace=True)\n",
        "\ndf['time'] = df['duration'].str.extract(r'\\d+')\ndf['number'] = df['duration'].str.extract(r'\\d+')\ndf['time_day'] = df['time'].map(lambda x: int(x) * 365 if x.endswith('year') else int(x) * 30 if x.endswith('month') else int(x) * 7 if x.endswith('week') else int(x) * 1 if x.endswith('day') else int(x))\ndf['time_day'] = df['time_day'].astype(int)\ndf['time_day'] = df['time_day'].replace(0, 1)\ndf['time_day'] = df['time_day'].astype(str)\ndf['time_day'] = df['time_day'].str.replace(r'\\d+', r'\\d')\ndf['time_day'] = df['time_day'].str.replace(r'0', r'')\ndf['time_day'] = df['time_day'].str.replace(r'1', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'2', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'3', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'4', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'5', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'6', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'7', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'8', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'9', r'0')\ndf['time_day'] = df['time_day'].astype(int)\ndf['time_day'] = df['time_day'].astype(str)\ndf['time_day'] = df['time_day'].str.replace(r'\\d+', r'\\d')\ndf['time_day'] = df['time_day'].str.replace(r'0', r'')\ndf['time_day'] = df['time_day'].str.replace(r'1', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'2', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'3', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'4', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'5', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'6', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'7', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'8', r'0')\ndf['time_day'] = df['time_day'].str.replace(r'9', r'0')\ndf['time_day'] = df['time_day'].astype(int)\ndf['time_day'] = df['time_day'].astype(str)\ndf['time_day'] = df['time_day'].str.replace(r'\\d+', r'\\d')\ndf['time_day'] = df['time_day'].str.replace(r'0', r'')\ndf['time_day'] = df['time_day'].str.replace(r'1', r'0')",
        "\ncheck = np.where(df1[columns_check_list] != df2[columns_check_list], True, False)\n",
        "\ncheck = np.where(df1[columns_check_list] == df2[columns_check_list] | (df1[columns_check_list] != df2[columns_check_list]))\n",
        "\nimport pandas as pd\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                 names=('id', 'date'))\ndf = pd.DataFrame({'x': [100, 90, 80], 'y':[7, 8, 9]}, index=index)\n# Parse dates\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n# Assign parsed dates to index\ndf.index = pd.MultiIndex.from_tuples([('abc', date) for date in df.index.levels[1]])\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                 names=('name', 'datetime'))\ndf = pd.DataFrame({'fee': [100, 90, 80], 'credits':[7, 8, 9]}, index=index)\n# Parse datetime index\ndf.index = pd.to_datetime(df.index.levels[1])\n# Print result\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\ndef f(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(level=1)\n    df = df",
        "\ndf.index = pd.to_datetime(df.index.str.split().apply(lambda x: pd.to_datetime(x[0], format='%d/%m/%Y')))\ndf.index.names = ['date', 'id']\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   '2000': [12, 1, 20, 0],\n                   '2001': [15, 3, 23, 1],\n                   '2002': [18, 2, 25, 2],\n                   '2003': [17, 5, 29, 2],\n                   '2004': [23, 7, 31, 3],\n                   '2005': [29, 5, 32, 3]})\nresult = pd.melt(df, id_vars='Country', var_name='Variable', value_name='year')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   '2000': [12, 1, 20, 0],\n                   '2001': [15, 3, 23, 1],\n                   '2002': [18, 2, 25, 2],\n                   '2003': [17, 5, 29, 2],\n                   '2004': [23, 7, 31, 3],\n                   '2005': [29, 5, 32, 3]})\ndf = df.sort_values('year', ascending=False)\nresult = df\nprint(result)\n",
        "\n# filter the dataframe to keep only rows where absolute value of all columns is less than 1\nresult = df[df.abs() < 1]\n",
        "\n# filter the dataframe based on absolute value of columns\nfiltered_df = df[df.abs() > 1]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': ['Good &amp; bad', 'BB', 'CC', 'DD', 'Good &amp; bad'], 'B': range(5), 'C': ['Good &amp; bad'] * 5})\ndf['A'] = df['A'].replace(r'&amp;', '&', regex=True)\nresult = df\nprint(result)\n",
        "\ndf = df.replace('&lt;', '<', regex=True)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'A': ['Good &amp; bad', 'BB', 'CC', 'DD', 'Good &amp; bad'], 'B': range(5), 'C': ['Good &amp; bad'] * 5})\ndef f(df=example_df):\n    result = df.replace('&amp;', '&', regex=True)\n    return result\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &GT; bad'] * 5})\ndef replace_amp(s):\n    return s.replace('&AMP;', '&')\ndef replace_lt(s):\n    return s.replace('&LT;', '<')\ndef replace_gt(s):\n    return s.replace('&GT;', '>')\ndf['A'] = df['A'].apply(replace_amp)\ndf['A'] = df['A'].apply(replace_lt)\ndf['A'] = df['A'].apply(replace_gt)\nresult = df\nprint(result)\n",
        "\ndf = df.replace('&amp;', '&', regex=True)\n",
        "\nimport pandas as pd\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Juan de la Cruz']})\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if x is None else x)\nprint(df)\n",
        "\nimport pandas as pd\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Juan de la Cruz']})\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if validate_single_space_name(x) else None)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\ndf['first_name'] = df['name'].str.split().str[0]\ndf['middle_name'] = df['name'].str.split().str[1]\ndf['last_name'] = df['name'].str.split().str[2]\nprint(df)\n",
        "\nresult = pd.merge(df1, df2, on='Timestamp')\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n# Join the two dataframes based on timestamp\nresult = pd.merge(df1, df2, on='Timestamp')\n# Add the data from df2 to df1\nresult = result.merge(df1, on='Timestamp', suffixes=('_x', '_y'))\n# Rename the columns\nresult.columns = ['Timestamp', 'data_x', 'data_y', 'stuff']\n# Print the result\nprint(result)\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] + x['col3'] <= 150 else x['col1'].max(), axis=1)\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] + x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\nfor row in df.iterrows():\n    if not pd.api.types.is_numeric_dtype(row[1][\"Field1\"]):\n        result.append(row[1][\"Field1\"])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\nfor row in df.iterrows():\n    if not pd.api.types.is_numeric_dtype(row[1][\"Field1\"]):\n        row[1][\"Field1\"] = int(row[1][\"Field1\"])\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    errors = []\n    for row in df.iterrows():\n        if not isinstance(row[1][\"Field1\"], int):\n            errors.append(row[1][\"Field1\"])\n    return errors\nresult = f()\nprint(result)\n",
        "\ndf['val1'] /= df['val1'].sum()\ndf['val2'] /= df['val2'].sum()\ndf['val3'] /= df['val3'].sum()\ndf['val4'] /= df['val4'].sum()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\ndf['val1_percent'] = df['val1'] / df['val1'].sum()\ndf['val2_percent'] = df['val2'] / df['val2'].sum()\ndf['val3_percent'] = df['val3'] / df['val3'].sum()\ndf['val4_percent'] = df['val4'] / df['val4'].sum()\nresult = df\nprint(result)\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.select(test)\n",
        "\ndf.drop(test, inplace=True)\n",
        "\nimport pandas as pd\ndef f(df, test):\n    test_df = df.iloc[test]\n    return test_df\n",
        "\nfrom sklearn.neighbors import NearestNeighbors\n# Define the number of nearest neighbors to consider\nk = 1\n# Create a NearestNeighbors object with the given number of neighbors\nnearest_neighbors = NearestNeighbors(n_neighbors=k)\n# Fit the object to the dataframe\nnearest_neighbors.fit(df)\n# Get the nearest neighbor for each car\nnearest_neighbors_df = nearest_neighbors.kneighbors(df, return_distance=False)\nnearest_neighbors_df.columns = ['car', 'nearest_neighbour']\nresult = pd.merge(df, nearest_neighbors_df, on='car')\n",
        "\nfrom sklearn.neighbors import NearestNeighbors\n# Define the number of neighbors to consider\nk = 1\n# Create a NearestNeighbors object with the given number of neighbors\nnearest_neighbors = NearestNeighbors(n_neighbors=k)\n# Fit the object to the dataframe\nnearest_neighbors.fit(df)\n# Get the indices of the farmost neighbors for each car\nfarmost_neighbors = nearest_neighbors.kneighbors(df, k=k)\n# Calculate the euclidean distance between each car and its farmost neighbor\neuclidean_distances = [\n    pd.Series([(car, distance) for car, distance in zip(df['car'], df['x'] - df['x'].iloc[farmost_neighbors[i]])])\n    for i in range(len(df))\n]\n# Merge the euclidean distances with the dataframe\nresult = pd.concat([df, pd.DataFrame(euclidean_distances)], axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\nresult = df\nprint(result)\n",
        "\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Sample 20% of rows for each user\nresult = df.sample(frac=0.2, random_state=0)\n# Set Quantity column to zero for sampled rows\nresult.loc[result['UserId'].isin(df.groupby('UserId').ngroup()), 'Quantity'] = 0\n# Keep the indexes of the altered rows\nresult.index = df.index\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef f(df=example_df):\n    duplicate = df.loc[df.duplicated(subset=['col1','col2'], keep='first') == True]\n    duplicate['index_original'] = df.index[duplicate.index]\n    return duplicate\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].idxmax()\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].idxmin()\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].idxmax()\n",
        "\ndf.query(\"Category.isin(filter_list)\")\n",
        "\ndf.query(\"Category != \" + \" \".join(filter_list))\n",
        "\nimport pandas as pd\ndef melt_df(df, col_levels):\n    value_vars = [('{}{}{}'.format(col_level[0], col_level[1], col_level[2]), col_level[3]) for col_level in col_levels]\n    return pd.melt(df, id_vars=df.columns, value_vars=value_vars)\nresult = melt_df(df, [('A', 'B', 'E'), ('A', 'B', 'F'), ('A', 'C', 'G'), ('A', 'C', 'H'), ('A', 'D', 'I'), ('A', 'D', 'J')])\nprint(result)\n",
        "\nimport pandas as pd\ndef melt_df(df, col_levels):\n    value_vars = [tuple(col_levels[i]) for i in range(len(col_levels))]\n    result = pd.melt(df, id_vars=df.columns, var_name='variable_', value_name='value')\n    result['variable_'] = value_vars\n    return result\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\nresult = melt_df(df, ['AAAAAA', 'BBCCDD', 'EFGHIJ'])\nprint(result)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum())\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum('val')\n",
        "\ndf['cummax'] = df.groupby('id').cummax('val')\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum('val')\n",
        "\ndf.groupby('l')['v'].apply(lambda x: x.fillna(np.NaN) if pd.isna(x).any() else x)\n",
        "\ndf.groupby('r')['v'].apply(lambda x: x.fillna(np.NaN).sum())\n",
        "\ndf.groupby('l')['v'].apply(lambda x: x.fillna(np.NaN).sum(skipna=False))\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndef get_relationship_type(df):\n    relationships = []\n    for col1, col2 in zip(df.columns, df.columns[1:]):\n        if col1 == col2:\n            relationships.append(f'{col1} {col2} one-to-one')\n        elif col1 < col2:\n            relationships.append(f'{col1} {col2} one-to-many')\n        else:\n            relationships.append(f'{col1} {col2} many-to-one')\n    return relationships\nresult = get_relationship_type(df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ncorr_matrix = df.corr()\nresult = []\nfor i in range(len(corr_matrix)):\n    for j in range(i, len(corr_matrix)):\n        if corr_matrix[i][j] == 1:\n            result.append(f\"{df.columns[i]}{df.columns[j]}\")\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndef get_relationship_type(df):\n    one_to_one = []\n    one_to_many = []\n    many_to_one = []\n    many_to_many = []\n    for col1, col2 in zip(df.columns, df.columns[1:]):\n        if col1 == col2:\n            one_to_one.append(col1)\n        elif col1 != col2 and col1 != 'Column5':\n            one_to_many.append(col1)\n        elif col1 == 'Column5' and col2 != 'Column5':\n            many_to_one.append(col1)\n        elif col1 != 'Column5' and col2 == 'Column5':\n            many_to_many.append(col1)\n    return pd.DataFrame({\n        'Column1': one_to_one,\n        'Column2': one_to_many,\n        'Column3': one_to_many,\n        'Column4': one_to_one,\n        'Column5': many_to_many\n    })\nresult = get_relationship_type(df)\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar'],\n                   'bank': [np.nan, 'abc', 'xyz']})\n# Sort the DataFrame by bank account\ndf = df.sort_values('bank')\n# Drop the rows with NaN bank account\ndf = df.dropna(subset=['bank'])\n# Print the result\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.read_csv('path/to/data.csv')\ndf['Revenue'] = pd.to_numeric(df['Revenue'], errors='coerce')\ndf['Other, Net'] = pd.to_numeric(df['Other, Net'], errors='coerce')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\nnew_groups = (df['SibSp'] > 0) | (df['Parch'] > 0)\nnew_groups = new_groups.astype(int)\nresult = df.groupby(new_groups).mean()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\nnew_groups = (df['Survived'] > 0) | (df['Parch'] > 0)\nnew_groups_name = 'Has Family'\nnew_groups_mean = df[new_groups].mean()\nnew_groups_mean.name = new_groups_name\nno_family = (df['Survived'] == 0) & (df['Parch'] == 0)\nno_family_name = 'No Family'\nno_family_mean = df[no_family].mean()\nno_family_mean.name = no_family_name\nresult = pd.concat([new_groups_mean, no_family_mean], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n# Group the data based on the conditions\ngroups = {\n    'Has Family': (df['SibSp'] == 1) & (df['Parch'] == 1),\n    'New Family': (df['SibSp'] == 0) & (df['Parch'] == 0),\n    'No Family': (df['SibSp'] == 0) & (df['Parch'] == 1),\n    'Old Family': (df['SibSp'] == 1) & (df['Parch'] == 0)\n}\nresult = pd.DataFrame({\n    'Survived': [0,1,1,1,0],\n    'SibSp': [1,1,0,1,0],\n    'Parch': [0,0,0,0,1]\n})\nfor group_name, group in groups.items():\n    result = result.loc[group, 'Survived'] = 1\n    result = result.loc[~group, 'Survived'] = 0\nprint(result)\n",
        "\ndf.groupby('cokey').sort_values('A')\n",
        "\ndf.groupby('cokey').sort_values('A', ascending=False)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = pd.DataFrame(list(someTuple))\nresult.columns = ['birdType', 'birdCount']\nprint(result)\n",
        "\ndef stdMeann(x):\n    return np.std(np.mean(x))\nresult = df.groupby('a').b.apply(stdMeann)\n",
        "\nimport numpy as np\ndef stdMeann(x):\n    return np.std(np.mean(x))\nresult = df.groupby('b').apply(lambda x: pd.Series({'mean': x.mean(), 'std': stdMeann(x)})).reset_index()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n# Softmax normalization\ndf['b'] = df['b'].apply(lambda x: 1 / (1 + np.exp(-x)))\n# Min-max normalization\nresult = df\nprint(result)\n",
        "\ndf = df.dropna(axis=1, how='all')\n",
        "\ndf = df.dropna(axis=1, how='all')\n",
        "\ndf = df.loc[df.max(axis=1) == 2]\n",
        "\ndf = df.fillna(0)\n",
        "",
        "\nsorted_index = s.index.sort_values(ascending=True)\nresult = s.reindex(sorted_index)\n",
        "\ndf = df[df['A'].astype(int) | df['A'].astype(float)]\n",
        "\ndf = df[df['A'].astype(str) == 's']\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['Value'].idxmax()\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].max()\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].min()\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].idxmax()\n",
        "\nimport pandas as pd\nimport numpy as np\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\nfor key, value in dict.items():\n    df[key] = value\nresult = df\nprint(result)\n",
        "\nresult = df.assign(Date=dict.get(df['Member'], '17/8/1926'))\n",
        "\nimport pandas as pd\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\ndef f(dict=example_dict, df=example_df):\n    result = pd.DataFrame()\n    for key, value in dict.items():\n        df.loc[df['Member'] == key, 'Date'] = value\n    return df\n",
        "\nresult = df.assign(Date=dict.get(df['Member'], '17-Aug-1926'))\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n# Group by month and year\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\n# Merge with original dataframe\nresult = pd.merge(df, df1, on='Date')\n# Rename columns\nresult.columns = ['Date', 'Val', 'Count_d', 'Count_m', 'Count_y']\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n# Group by month and year\n# Group by month and year and val\ndf2 = df1.groupby(['year', 'Val']).agg({'count': 'sum'})\n# Merge with original dataframe\ndf = pd.merge(df, df2, on=['year', 'Val'])\n# Rename columns\ndf.columns = ['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_Val']\nprint(df)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n# Group by month and year\n# Group by weekday and year\n# Merge the two dataframes\ndf3 = pd.merge(df1, df2, on='Date')\n# Add the missing columns\ndf3['Count_d'] = df3['year'] + df3['weekday'] + df3['Val']\ndf3['Count_m'] = df3['year'] + df3['weekday'] + df3['Val']\ndf3['Count_y'] = df3['year'] + df3['weekday'] + df3['Val']\ndf3['Count_w'] = df3['weekday'] + df3['Val']\ndf3['Count_Val'] = df3['Val']\n# Rename the columns\ndf3.columns = ['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_w', 'Count_Val']\n# Print the result\nprint(df3)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\nresult1 = df.groupby('Date')['B'].sum().reset_index(name='B')\nresult2 = df.groupby('Date')['C'].sum().reset_index(name='C')\nprint(result1)\nprint(result2)\n",
        "\nresult1 = df.groupby('Date')['B'].sum().reset_index(name='B')\nresult1['C'] = df.groupby('Date')['C'].sum().reset_index(name='C')\nresult1 = result1.pivot(index='Date', columns='B', values='C')\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult = pd.concat([result, pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)], axis=1)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=lambda x: np.max(x) if x.dtype == np.float64 else np.min(x))\n",
        "\ndf = df.explode('var2')\n",
        "\nimport dask.dataframe as dd\nimport pandas as pd\n# Read the csv file into a dask dataframe\ndf = dd.read_csv('your_file.csv')\n# Explode the 'var2' column\ndf = df.explode('var2')\n# Print the resulting dataframe\nprint(df)\n",
        "",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df.apply(count_special_char, axis=0)\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df.apply(count_special_char, axis=0)\n",
        "\ndf['fips'] = df['row'].str.split(', ', expand=True)\ndf['row'] = df['row'].str.split(', ', expand=True)\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\n",
        "\ndf['fips'] = df['row'].str.split().iloc[:, 0]\ndf['medi'] = df['row'].str.split().iloc[:, 1]\ndf['row'] = df['row'].str.split().iloc[:, 2]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n# Calculate cumulative average ignoring zeros\ndf['Cumulative Average'] = df.cumsum() / (df != 0).cumsum()\n# Print the result\nprint(df)\n",
        "\n# Calculate cumulative average for each row from end to head\n# Ignore values that are zero\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\ndef f(df=example_df):\n    result = []\n    for name, row in df.iterrows():\n        if row['2001'] == 0:\n            continue\n        else:\n            result.append(row['2001'] + (row['2002'] - row['2001']) / 2 + (row['2003'] - row['2002']) / 2 + (row['2004'] - row['2003']) / 2 + (row['2005'] - row['2004']) / 2 + (row['2006'] - row['2005']) / 2)\n    return pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n",
        "\n# Calculate cumulative average for each row from end to head\n# Ignore values that are zero\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07'],\n                   'Close': [1460, 1470, 1480, 1450]})\ndf['label'] = (df['Close'] - df['Close'].shift(1)) > 1\nprint(df)\n",
        "\ndf['label'] = 1\ndf.iloc[0] = 0\ndf.iloc[1] = 1\ndf.iloc[2] = 1\ndf.iloc[3] = 0\ndf.iloc[4] = -1\n",
        "\ndf['label'] = (df['Close'] - df['Close'].shift()) / df['Close'].shift()\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\nimport pandas as pd\nimport datetime\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\ndf['Duration'] = df.apply(lambda x: x.departure_time - x.arrival_time, axis=1)\nprint(df)\n",
        "\n# Calculate the time difference in seconds\ndf['Duration'] = df.departure_time.diff().dt.seconds\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], format='%d-%b-%Y %H:%M:%S')\n",
        "\ndf.groupby('key1')['key2'].value_counts()\n",
        "\ndf.groupby('key1')['key2'].value_counts()\n",
        "\ndf.groupby(['key1']).size().reset_index(name='count')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\nmin_date = df.index.min()\nmax_date = df.index.max()\nprint(min_date, max_date)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\nmode_date = df.index.mode().date\nmedian_date = df.index.median().date\nprint(mode_date, median_date)\n",
        "\ndf = df[df['closing_price'].between(99, 101)]\n",
        "\ndf = df[~(99 <= df['closing_price'] <= 101)]\n",
        "\ndf1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\ndf1 = df1.drop(df1[df1[\"diff\"] != df1[\"diff\"].min()].index)\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n",
        "\nimport pandas as pd\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\nexample_df = pd.DataFrame(data={'SOURCE_NAME': strs})\ndef f(df=example_df):\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n    return df\nresult = f()\nprint(result)\n",
        "",
        "",
        "",
        "\na_b = pd.DataFrame([[(a.iloc[i][0], b.iloc[i][0]) for i in range(len(a))],[(a.iloc[i][1], b.iloc[i][1]) for i in range(len(a))]], columns=['one', 'two'])\n",
        "\nresult = pd.DataFrame([[(a.iloc[i][0], b.iloc[i][0], c.iloc[i][0]) for i in range(len(a))] for j in range(len(b))], columns=['one', 'two'])\n",
        "\na_b = pd.DataFrame([[(1, 5), (2, 6)],[(3, 7), (4, 8)],[(np.nan,9),(np.nan,10)]], columns=['one', 'two'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\ngroups = df.groupby(pd.cut(df.views, bins))\nresult = groups.username.count()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\ngroups = df.groupby(pd.cut(df.views, bins))\nresult = groups.username.count()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['tom', 'tom', 'tom', 'tom', 'jack', 'jack', 'jack', 'jack'],\n                   'post_id': [10, 8, 7, 6, 5, 4, 3, 2],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\ngroups = df.groupby(pd.cut(df.views, bins))\nresult = groups.username.count()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = ''\nfor text in df['text']:\n    result += text + ', '\nresult = result[:-2]  # remove the last comma and space\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = ''\nfor i in range(len(df)):\n    result += df['text'][i] + '-'\nresult = result[:-1]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = ''\nfor i in range(len(df)):\n    result += df.iloc[i]['text'] + ', '\nresult = result[:-2]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = pd.Series(df['text'].str.join(', '))\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = ''\nfor i in range(len(df)):\n    result += df.iloc[i]['text'] + '-'\nresult = result[:-1]\nprint(result)\n",
        "",
        "",
        "",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\nresult = pd.merge(C, D, how='outer', on='A')\nresult.loc[C.index, 'B'] = D.B\nprint(result)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult.loc[C.index, 'B'] = result.loc[C.index, 'B'].fillna(result.loc[D.index, 'B'])\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['dulplicated'] = result['A'].duplicated()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\nresult = df.groupby('user')['time'].apply(list).sort_values().reset_index()\nprint(result)\n",
        "\ndf.sort_values(['time', 'amount'], inplace=True)\n",
        "\ndf.sort_values(['time', 'amount'], inplace=True)\n",
        "\ndf = pd.DataFrame(series.values.reshape(-1, 4), columns=['file1', 'file2', 'file3'])\n",
        "\ndf = pd.DataFrame(series.values.reshape(-1, 4), columns=['name', '0', '1', '2', '3'])\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\nresult = []\nfor col in df.columns:\n    if s in col:\n        result.append(col)\nprint(result)\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\nresult = []\nfor col in df.columns:\n    if s in col:\n        result.append(col)\nprint(result)\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\ndef find_column_names(df, s):\n    return [col for col in df.columns if s in col]\nresult = find_column_names(df, s)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\ndf['code_0'] = df['codes'].explode().fillna(np.nan)\ndf['code_1'] = df['codes'].explode().fillna(np.nan)\ndf['code_2'] = df['codes'].explode().fillna(np.nan)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\ndf['code_1'] = df['codes'].explode().fillna(np.nan)\ndf['code_2'] = df['codes'].explode().fillna(np.nan)\ndf['code_3'] = df['codes'].explode().fillna(np.nan)\nprint(df)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\nresult = df['col1'].apply(lambda x: ''.join(str(y) for y in x))\nprint(result)\n",
        "\nresult = ''.join(str(x) for x in df.loc[0:index, 'User IDs'].values.tolist())\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\nresult = ''.join(str(x) for x in df.loc[0:index, 'User IDs'].values.tolist())\nprint(result)\n",
        "\nimport pandas as pd\ndef bin_and_average(df, time_bin_size):\n    bins = {}\n    for time, value in df.iterrows():\n        bin_key = time.strftime('%H:%M:%S')\n        if bin_key not in bins:\n            bins[bin_key] = [value]\n        else:\n            bins[bin_key].append(value)\n    for bin_key, values in bins.items():\n        bin_start = pd.to_datetime(bin_key, format='%H:%M:%S')\n        bin_end = bin_start + pd.Timedelta(time_bin_size)\n        bin_values = [value for value in values if bin_start <= pd.to_datetime(value, format='%H:%M:%S') <= bin_end]\n        if len(bin_values) > 0:\n            bin_values = pd.Series(bin_values)\n            bin_values.index.name = 'Time'\n            result = pd.concat([result, bin_values])\n    return result\ndf = pd.DataFrame({'Time': ['2015-04-24 06:38:49', '2015-04-24 06:39:19', '2015-04-24 06:43:49', '2015-04-24 06:44:18',\n                            '2015-04-24 06:44:48', '2015-04-24 06:45:18', '2015-04-24 06:47:48', '2015-04-24 06:48:18',\n                            '2015-04-24 06:50:48', '2015-04-24 06:51:18', '2015-04-24 06:51:48', '2015-04-24 06:52:18',\n                            '2015-04-24 06:52:48', '2015-04-24 06:53:48', '2015-04-24 06:55:18', '2015-04-24 07:00:47',\n                            '2015-04-24 07:01:17', '2015-04-24 07:01:47'],\n                   'Value': [0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075]})\ntime_bin_size = pd.Timedelta('2T')\nresult = bin_and_average(df, time_bin_size)\nprint(result)\n",
        "\nimport pandas as pd\ndef bin_time(df, bin_size):\n    bins = []\n    for time in df['Time']:\n        bin_start = time.strftime('%H:%M:%S')\n        bin_end = time + pd.Timedelta(minutes=bin_size)\n        bins.append((bin_start, bin_end))\n    return pd.DataFrame(bins, columns=['bin_start', 'bin_end'])\ndef sum_bins(df, bins):\n    grouped = df.groupby(bins['bin_start'])\n    result = grouped.sum()\n    return result\ndef interpolate_time(df, bins):\n    grouped = df.groupby(bins['bin_start'])\n    result = grouped.resample('3T').interpolate()\n    return result\ndef main():\n    df = pd.DataFrame({'Time': ['2015-04-24 06:38:49', '2015-04-24 06:39:19', '2015-04-24 06:43:49', '2015-04-24 06:44:18',\n                            '2015-04-24 06:44:48', '2015-04-24 06:45:18', '2015-04-24 06:47:48', '2015-04-24 06:48:18',\n                            '2015-04-24 06:50:48', '2015-04-24 06:51:18', '2015-04-24 06:51:48', '2015-04-24 06:52:18',\n                            '2015-04-24 06:52:48', '2015-04-24 06:53:48', '2015-04-24 06:55:18', '2015-04-24 07:00:47',\n                            '2015-04-24 07:01:17', '2015-04-24 07:01:47'],\n                   'Value': [0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075]})\n    bins = bin_time(df, 3)\n    result = sum_bins(df, bins)\n    interpolated = interpolate_time(result, bins)\n    print(interpolated)\nif __name__ == '__main__':\n    main()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\nprint(df)\n",
        "\ndf = df.loc[filt]\n",
        "\nfiltered_df = df.loc[filt]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\nresult = []\nfor i in range(len(df)):\n    for j in range(i+1, len(df)):\n        if df.iloc[i,j] != df.iloc[i,j]:\n            result.append((df.iloc[i,j], df.iloc[j,j]))\nprint(result)\n",
        "\nimport pandas as pd\ndates = ['2016-1-{}'.format(i)for i in range(1,21)]\nvalues = [i for i in range(20)]\ndata = {'Date': dates, 'Value': values}\ndf = pd.DataFrame(data)\ndf['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\nts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n",
        "\nresult = df.iloc[0]\n",
        "\nresult = df.iloc[0]\n",
        "\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .18)],\n                  columns=['dogs', 'cats'])\ndf['dogs'] = df['dogs'].apply(lambda x: x.round(2) if not pd.isna(x) else x)\ndf['cats'] = df['cats'].apply(lambda x: x.round(2) if not pd.isna(x) else x)\nprint(df)\n",
        "\nresult = df[list_of_my_columns].sum(axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\nresult = df[list_of_my_columns].mean(axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndata = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}\ndf = pd.DataFrame(data)\navg_cols = ['A', 'B', 'C']\navg_df = df[avg_cols].mean(axis=1)\nprint(avg_df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\ndf.sort_values('time', inplace=True)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\ndf.sort_values('VIM', inplace=True)\n",
        "\n# Define the dates to be deleted\ndelete_dates = ['2020-02-17', '2020-02-18']\n# Drop the rows with the specified dates\nsp = sp.drop(sp[sp.index.isin(delete_dates)].index)\n",
        "\ndf = df.drop(['2020-02-17', '2020-02-18'], axis=0, inplace=True)\n",
        "\n# [Missing Code]\n",
        "\nresult = corr.loc[corr > 0.3, :]\n",
        "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\n",
        "\ndf.columns[0] = 'Test'\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4],\n                   'bit6': [3, 0, 5]})\n# Find the frequent values in each row\nfrequent_values = df.apply(lambda x: x.value_counts().index.tolist(), axis=1)\n# Create a new DataFrame with the frequent values and their counts\nresult = pd.DataFrame({'bit1': [0, 2, 4],\n                       'bit2': [0, 2, 0],\n                       'bit3': [3, 0, 4],\n                       'bit4': [3, 0, 4],\n                       'bit5': [0, 2, 4],\n                       'bit6': [3, 0, 5]},\n                       index=df.index)\nresult['frequent'] = frequent_values\nresult['freq_count'] = result.groupby('bit1')['frequent'].cumsum()\nprint(result)\n",
        "\nres = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\n",
        "\nres = df.groupby(['id1', 'id2'])['foo', 'bar'].agg(numpy.mean)\nres.reset_index(inplace=True)\nres.rename(columns={'foo': 'foo_mean', 'bar': 'bar_mean'})\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum')\nresult = result.drop(['b_col'], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum')\nresult = result.drop(['a_col'], axis=1)\nprint(result)\n"
    ],
    "Numpy": [
        "\nprint(a.shape)\n",
        "\nx = np.delete(x, np.isnan(x).argmax())\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nx = np.nan_to_num(x)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.zeros((len(a), 4))\nb[np.arange(len(a)), a] = 1\nprint(b)\n",
        "\nb = np.zeros_like(a)\nb[np.arange(len(a)), a] = 1\n",
        "\nimport numpy as np\na = np.array([-1, 0, 3])\nb = np.zeros((len(a), len(np.unique(a))), dtype=int)\nb[np.arange(len(a)), a] = 1\nprint(b)\n",
        "\nb = np.zeros_like(a)\nb[np.argsort(a)[::-1]] = 1\n",
        "\nb = np.zeros_like(a)\nb[np.arange(a.shape[0]), a.argmin(axis=1)] = 1\n",
        "",
        "\nB = np.reshape(A, (ncol, -1))\n",
        "\nB = np.reshape(A, (nrow, -1))\n",
        "\nB = np.reshape(A, (ncol, -1))\n",
        "\nB = np.reshape(A, (ncol, -1))\n",
        "\ndef shift(arr, shift_val):\n    if shift_val < 0:\n        return np.roll(arr, shift_val)\n    else:\n        return np.roll(arr, -shift_val)\n",
        "\nresult = np.roll(a, shift, axis=0)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nimport numpy as np\ndef generate_array():\n    r_old = np.random.randint(3, size=(100, 2000)) - 1\n    r_new = np.random.randint(3, size=(100, 2000)) - 1\n    return r_old, r_new\nr_old, r_new = generate_array()\nprint(r_old, r_new)\n",
        "\nresult = np.argmax(a, axis=0)\n",
        "\nresult = np.argmin(a)\n",
        "\nresult = np.argmax(a, axis=0)\n",
        "\nresult = np.argmax(a, axis=0)\n",
        "\nimport numpy as np\ndef f(a):\n    max_val = np.max(a)\n    max_idx = np.argmax(a)\n    return np.ravel_multi_index(max_idx, a.shape)\nexample_a = np.array([[10,50,30],[60,20,40]])\nresult = f(example_a)\nprint(result)\n",
        "\nresult = np.argmax(a, axis=1)\n",
        "\ndelete(a, z, axis=1)\n",
        "\na = np.delete(a, np.isnan(a).any(axis=1), axis=0)\n",
        "\nresult = np.array(a)\n",
        "\na = np.transpose(a, permutation)\n",
        "\nresult = np.transpose(a, permutation)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = np.argmin(a, axis=1)\n",
        "",
        "\nresult = np.cos(degree)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nvalue = 1.0\nresult = np.degrees(np.arcsin(value))\nprint(result)\n",
        "\nresult = np.zeros(length - A.size) + A\n",
        "\nresult = np.zeros(length - len(A) + 1)\nresult[:len(A)] = A\n",
        "\na = np.arange(4).reshape(2, 2)\npower = 5\na **= power\nprint(a)\n",
        "\ndef f(a = example_a, power = 5):\n    result = np.power(a, power)\n    return result\n",
        "\nresult = np.divide(numerator, denominator)\n",
        "\nresult = np.lcm(numerator, denominator) / np.gcd(numerator, denominator)\n",
        "\nresult = np.divide(numerator, denominator)\n",
        "\nresult = np.mean(a, axis=0) + np.mean(b, axis=0) + np.mean(c, axis=0)\n",
        "\nresult = np.maximum(a, b) + np.maximum(b, c) + np.maximum(c, a)\n",
        "\nresult = a[np.diag_indices(a.shape[0]-1, 0)]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4, 5],\n              [ 5,  6,  7,  8,  9, 10],\n              [10, 11, 12, 13, 14, 15],\n              [15, 16, 17, 18, 19, 20],\n              [20, 21, 22, 23, 24, 25]])\ndiagonal = np.diag_indices(a.shape[0], axis1=-1)\nresult = a[diagonal]\nprint(result)\n",
        "\nresult = np.diag_indices(a.shape[0], k=1, dtype=int)\nresult = np.concatenate([result, np.diag_indices(a.shape[0]-1, k=1, dtype=int)], axis=0)\nresult = np.concatenate([result, np.diag_indices(a.shape[0]-1, k=1, dtype=int)], axis=1)\nresult = np.concatenate([result, np.diag_indices(a.shape[0]-1, k=1, dtype=int)], axis=2)\nresult = np.concatenate([result, np.diag_indices(a.shape[0]-1, k=1, dtype=int)], axis=3)\nresult = np.concatenate([result, np.diag_indices(a.shape[0]-1, k=1, dtype=int)], axis=4)\n",
        "\nresult = np.diag_indices(a.shape[0], a.shape[1])[0]\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nresult = np.array([int(x) for x in mystr])\n",
        "\nresult = np.cumsum(a[:, col] * multiply_number)\n",
        "\nresult = a[row] * multiply_number\ncumulative_sum = np.cumsum(result)\n",
        "\nresult = a[row] / divide_number\n",
        "\nresult = np.linalg.matrix_rank(a)\n",
        "\nrow_size = a.shape[0]\n",
        "",
        "\n# [Missing Code]\n",
        "",
        "\noutput = np.setdiff1d(A, B)\n",
        "\noutput = np.setdiff1d(A, B)\noutput = np.setdiff1d(B, A)\noutput = np.vstack(output)\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nresult = b[np.argsort(a, axis=1)]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0, 2], axis=0)\n",
        "\nresult = a[:, del_col]\n",
        "\na[pos] = element\n",
        "\na[pos] = np.insert(a[pos], 1, element, axis=0)\n",
        "\nimport numpy as np\ndef f(a = example_a, pos=2, element = 66):\n    a[pos] = element\n    return a\n",
        "\na[pos] = element\n",
        "\nresult = np.array([np.copy(a) for a in array_of_arrays])\n",
        "\nif np.all(a == a[0]):\n    print(\"All rows are equal\")\nelse:\n    print(\"Not all rows are equal\")\n",
        "\nif np.all(a == a[0]):\n    print(\"All columns are equal\")\nelse:\n    print(\"Not all columns are equal\")\n",
        "\ndef f(a = example_a):\n    return np.all(a == np.tile(np.arange(1, 6), 3))\n",
        "\nfrom scipy.interpolate import RectBivariateSpline\ndef integral_2d(f, x, y):\n    x, y = np.meshgrid(x, y)\n    return RectBivariateSpline(x, y, f(x, y)).integral(x, y)\nresult = integral_2d(lambda x, y: (x**4 + y**2), x, y)\nprint(result)\n",
        "\nimport numpy as np\ndef f(x = example_x, y = example_x):\n    x, y = np.meshgrid(x, y)\n    return np.exp(x**2 + y**2)\n",
        "\n# [Missing Code]\n",
        "\ndef ecdf(x):\n    x = x / np.sum(x)\n    return np.cumsum(x)\necdf_grades = ecdf(grades)\necdf_eval = ecdf(eval)\nresult = np.interp(ecdf_eval, ecdf_grades, grades)\n",
        "\n# Compute the cumulative distribution function (CDF) of the grades\ncdf = np.cumsum(grades)\n# Find the index of the first grade that satisfies ECDF(x) < threshold\ni = np.argmax(cdf < threshold)\n# Compute the longest interval [low, high) that satisfies ECDF(x) < threshold\nlow = grades[i]\nhigh = grades[i+1]\n",
        "",
        "\nimport torch\nimport numpy as np\na = torch.ones(5)\na_np = np.array(a.numpy())\nprint(a_np)\n",
        "\nimport torch\nimport numpy as np\na = np.ones(5)\na_pt = torch.tensor(a)\nprint(a_pt)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = tf.ones([2,3,4])\na_np = tf.convert_to_tensor(a)\nprint(a_np)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)[::-1][:N]\n",
        "\nresult = A**n\n",
        "\nresult = np.array([[a[i][j], a[i][j+1]] for i in range(len(a)) for j in range(len(a[i]))])\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\n",
        "\nresult = np.array([[a[i][j], a[i+1][j]] for i in range(len(a)-2) for j in range(len(a[0])-2)])\n",
        "\nresult = np.array([[a[i:i+patch_size,j:j+patch_size] for j in range(patch_size)] for i in range(patch_size)])\n",
        "\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i, j] = a[i][j]\nprint(result)\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size])\n",
        "",
        "",
        "\nresult = a[:, low:high]\n",
        "\nimport numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(eval(string))\nprint(a)\n",
        "\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\ndef log_uniform_distribution(min, max, base):\n    x = np.logspace(np.log(min), np.log(max), num=n)\n    return x\nresult = log_uniform_distribution(min, max, base)\nprint(result)\n",
        "\nimport numpy as np\nmin = 0\nmax = 1\nn = 10000\ndef log_uniform_distribution(min, max, base):\n    x = np.exp(np.random.uniform(min, max, size=n))\n    return x\nresult = log_uniform_distribution(min, max, base)\nprint(result)\n",
        "\nimport numpy as np\ndef f(min=1, max=np.e, n=10000):\n    # Generate a random number between min and max\n    x = np.random.uniform(min, max)\n    # Calculate the log of x\n    log_x = np.log(x)\n    # Generate a random number between 0 and 1\n    u = np.random.uniform(0, 1)\n    # Calculate the exponent of x\n    exp_x = np.exp(log_x)\n    # Calculate the exponent of u\n    exp_u = np.exp(u)\n    # Calculate the probability of x\n    prob_x = exp_x / (exp_x + exp_u)\n    # Return the probability of x\n    return prob_x\n",
        "\nB = np.zeros_like(A)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = np.zeros(len(A))\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nimport numpy as np\ndemod4 = np.empty((0,), dtype=int)\n",
        "\nimport numpy as np\ndemod4 = np.zeros((3, 0))\n",
        "\nresult = np.sub2ind(dims, index, 0)\n",
        "\nrow_index = index[0] * dims[0] + index[1] * dims[1] + index[2] * dims[2]\ncol_index = index[0] * dims[1] + index[1] * dims[2] + index[2] * dims[0]\nlinear_index = row_index + col_index\n",
        "\nimport numpy as np\nimport pandas as pd\nindex = ['x', 'y']\ncolumns = ['a', 'b', 'c']\nvalues = np.zeros((2, 3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df)\n",
        "\nresult = np.cumsum(accmap * a)\n",
        "\nresult = np.max(a[index], axis=0)\n",
        "\nresult = np.zeros_like(a)\nresult[accmap] = 1\n",
        "\nresult = np.min(a[index])\n",
        "\nimport numpy as np\ndef elementwise_function(element_1, element_2):\n    return element_1 + element_2\nx = [[2, 2, 2],\n     [2, 2, 2],\n     [2, 2, 2]]\ny = [[3, 3, 3],\n     [3, 3, 3],\n     [3, 3, 1]]\nz = [[5, 5, 5],\n     [5, 5, 5],\n     [5, 5, 3]]\nfor i in range(len(x)):\n    for j in range(len(y)):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\nprint(z)\n",
        "\narr = np.array(lista_elegir)\n",
        "\nresult = np.zeros((3, 3))\nresult[low_index:high_index, low_index:high_index] = a[low_index:high_index, low_index:high_index]\n",
        "\nresult = np.delete(x, np.where(x < 0))\n",
        "\nresult = x[x != 0]\n",
        "\nbin_data = np.array_split(data, int(np.ceil(len(data) / bin_size)))\n",
        "\nbin_data = np.array_split(data, int(np.ceil(len(data) / bin_size)))\n",
        "\nbin_data = np.array([[4,2,5,6,7],\n[5,4,3,5,7]])\nbin_size = 3\nbin_data_mean = np.zeros((bin_size, len(bin_data[0])))\nfor i in range(bin_size):\n    bin_data_mean[i] = np.mean(bin_data[i], axis=0)\nprint(bin_data_mean)\n",
        "\nbin_data = np.array_split(data, np.arange(0, len(data), bin_size))\n",
        "\nbin_data = np.array([[data[i][j:j+bin_size] for j in range(len(data[i]))] for i in range(len(data))])\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\ndef bin_data(data, bin_size):\n    n_rows, n_cols = data.shape\n    n_bins = int(np.ceil(n_cols / bin_size))\n    bin_data = np.zeros((n_bins, bin_size))\n    for i in range(n_bins):\n        start_row = i * bin_size\n        end_row = min(start_row + bin_size, n_rows)\n        bin_data[i] = data[start_row:end_row]\n    return bin_data\nbin_data = bin_data(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=0)\nprint(bin_data_mean)\n",
        "",
        "",
        "\nresult = np.correlate(a, b, mode='full')\n",
        "\nresult = np.array(df.values.reshape(15, 4, 5))\n",
        "",
        "\nimport numpy as np\ndef binary_array(num, m):\n    binary_array = np.unpackbits(np.uint8(num))\n    binary_array = np.reshape(binary_array, (m, -1))\n    return binary_array\na = np.array([1, 2, 3, 4, 5])\nm = 8\nresult = binary_array(a, m)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = np.zeros((len(a), m))\nfor i in range(len(a)):\n    result[i, :] = np.unpackbits(np.uint8(a[i]))[:m]\nprint(result)\n",
        "\nimport numpy as np\ndef convert_to_binary_array(num, m):\n    binary_array = np.unpackbits(np.uint8(num))\n    binary_array = np.reshape(binary_array, (m, -1))\n    return binary_array\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = np.zeros((m, m))\nfor i in range(m):\n    binary_array = convert_to_binary_array(a, m)\n    result += binary_array[i]\nprint(result)\n",
        "",
        "\nend = np.mean(a) + 2*np.std(a)\n",
        "\nimport numpy as np\ndef f(a = example_a):\n    mean = np.mean(a)\n    std = np.std(a)\n    third_std = mean + 3 * std\n    return (mean - third_std, mean + third_std)\n",
        "\n# [Missing Code]\n",
        "\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[zero_rows, zero_cols] = 0\n",
        "",
        "\nmask = np.zeros_like(a)\nmask[np.argmax(a, axis=1)] = True\n",
        "\nmask = np.min(a, axis=1)\n",
        "\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nresult = np.zeros((N, M, M))\nfor i in range(M):\n    result[:, i, i] = X[:, i].dot(X[:, i].T)\n",
        "\nX = np.dot(Y, Y.T)\n",
        "\nif np.all(a == number):\n    is_contained = True\nelse:\n    is_contained = False\n",
        "\nC = np.delete(A, np.in1d(A, B))\n",
        "\nC = np.intersect1d(A, B)\n",
        "\nC = np.intersect1d(A[A >= 1], A[A <= 4])\n",
        "\nresult = rankdata(a, method='dense')\n",
        "\nresult = np.zeros_like(a)\nfor i in range(len(a)):\n    if a[i] == a[i-1]:\n        result[i] = result[i-1] + 1\n    else:\n        result[i] = rankdata(a[i-1:i+1])[1]\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\ndef f(a = example_a):\n    a = np.array(a)\n    a = np.sort(a)[::-1]\n    return rankdata(a)\n",
        "\ndists = np.vstack(np.hstack(x_dists, y_dists))\n",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ndists = np.vstack(np.hstack(x_dists, y_dists))\nprint(dists)\n",
        "\nresult = a[:][second][third].flatten()\n",
        "\narr = np.zeros((20, 10, 10, 2))\n",
        "\nresult = X.sum(axis=1)\nprint(result)\n",
        "\nresult = np.array([LA.norm(v,ord=2) for v in X])\n",
        "\nresult = np.array([LA.norm(v, ord=np.inf) for v in X])\n",
        "\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nchoices = ['XX']\nconditions = np.array([a['properties_path'].str.contains('blog'),\n                       a['properties_path'].str.contains('credit-card-readers/|machines|poss|team|transaction_fees'),\n                       a['properties_path'].str.contains('signup|sign-up|create-account|continue|checkout'),\n                       a['properties_path'].str.contains('complete'),\n                       a['properties_path'] == '/za/|/',\n                       a['properties_path'].str.contains('promo')])\na['page_type'] = np.select(conditions, choices, default=np.nan)\nprint(a)\n",
        "\n# Calculate distance between each point and all other points\nresult = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(i+1, len(a)):\n        distance = np.linalg.norm(a[i] - a[j])\n        result[i][j] = distance\n        result[j][i] = distance\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10), dim)\n# Initialize distance matrix\ndistance_matrix = np.zeros((len(a), len(a)))\n# Iterate through all points\nfor i in range(len(a)):\n    # Calculate distance between current point and all other points\n    for j in range(i, len(a)):\n        distance_matrix[i][j] = np.linalg.norm(a[i] - a[j])\n        distance_matrix[j][i] = np.linalg.norm(a[j] - a[i])\n# Print distance matrix\nprint(distance_matrix)\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10), dim)\n# Initialize distance matrix\ndistance_matrix = np.zeros((len(a), len(a)))\n# Iterate through all possible pairs of points\nfor i in range(len(a)):\n    for j in range(i+1, len(a)):\n        # Calculate distance between points\n        distance = np.linalg.norm(a[i] - a[j])\n        # Add distance to distance matrix\n        distance_matrix[i, j] = distance\n        distance_matrix[j, i] = distance\n# Print distance matrix\nprint(distance_matrix)\n",
        "\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nNA = np.asarray(A)\n",
        "\nresult = np.unique(a[~np.iszero(a)])\n",
        "",
        "\nimport numpy as np\nimport pandas as pd\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\nprint(df)\n",
        "\nimport numpy as np\nimport pandas as pd\nexample_lat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\nexample_lon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\nexample_val=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\ndef f(lat = example_lat, lon = example_lon, val = example_val):\n    df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\n    df.columns = ['lat', 'lon', 'val']\n    return df\n",
        "\nimport numpy as np\nimport pandas as pd\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\nprint(df)\n",
        "\ndef rolling_window(a, size):\n    n, m = a.shape\n    result = []\n    for i in range(n):\n        for j in range(m):\n            center = (i - size[0] // 2, j - size[1] // 2)\n            if center[0] >= 0 and center[1] >= 0 and center[0] < n and center[1] < m:\n                window = a[center[0]:center[0] + size[0], center[1]:center[1] + size[1]]\n                result.append(window)\n    return result\n",
        "\ndef rolling_window(arr, size, step=1):\n    if step == 1:\n        return arr\n    else:\n        return np.roll(arr, step, axis=0) + np.roll(arr, step, axis=1)\nresult = rolling_window(a, size)\n",
        "\nresult = np.mean(a)\n",
        "\nresult = np.mean(a)\n",
        "\nresult = Z[:, -1:]\n",
        "\nresult = a[-1:, :, :]\n",
        "\nimport numpy as np\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ 57, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  78, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ 57, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, 727]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ 66, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\nresult = any(np.array(c) == np.array(numpy_array) for numpy_array in CNTS)\nprint(result)\n",
        "\nimport numpy as np\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ np.nan, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  np.nan, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, np.nan]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ np.nan, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\nresult = False\nfor cnt in CNTS:\n    if np.isclose(c, cnt, rtol=1e-6).all():\n        result = True\n        break\nprint(result)\n",
        "\ninterp_func = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = interp_func(x_new, y_new)\n",
        "\ndf[name] = df.groupby('D')[name].cumsum()\n",
        "\ni = np.diag(i)\n",
        "\na[np.triu_indices(a.shape[0], k=1)] = 0\n",
        "\nimport numpy as np\nimport pandas as pd\nstart = pd.to_datetime(start)\nend = pd.to_datetime(end)\nn = int(end - start) // pd.Timedelta(seconds=1)\nresult = pd.period_range(start=start, end=end, periods=n)\nprint(result)\n",
        "\nresult = np.where(x == a)[0][0]\n",
        "\nresult = []\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result.append(i)\nprint(result)\n",
        "\nimport numpy as np\ndef minimize_error(x, y):\n    a, b, c = 0, 0, 0\n    for i in range(len(x)):\n        y_pred = a * x[i]**2 + b * x[i] + c\n        error = y[i] - y_pred\n        squared_error = error**2\n        a += (x[i]**2 * error) / squared_error\n        b += (x[i] * error) / squared_error\n        c += (error) / squared_error\n    return a, b, c\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\nresult = minimize_error(x, y)\nprint(result)\n",
        "\nimport numpy as np\ndef fit_polynomial(x, y, degree):\n    coeffs = np.polyfit(x, y, degree)\n    return np.poly1d(coeffs)\n",
        "\ntemp_arr = [0,1,2,3]\ntemp_df = pd.DataFrame(np.repeat([1, 1, 1, 1], 4).reshape(4, -1))\nfor i in range(len(temp_df)):\n    temp_df.iloc[i] = temp_df.iloc[i] - temp_arr[i]\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\narr = np.array([[1.0,2.0,3.0],[0.1, 5.1, 100.1],[0.01, 20.1, 1000.1]])\nscaler = MinMaxScaler()\nscaler.fit(arr)\nresult = scaler.transform(arr)\nprint(result)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n",
        "\nimport numpy as np\narr = (np.random.rand(100, 50)-0.5) * 50\nmask = arr < -10\nmask2 = arr < 15\nmask3 = mask & mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\nprint(arr)\n",
        "\nimport numpy as np\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\nmask = arr < n1\narr[mask] = 0\nmask2 = arr >= n2\narr[mask2] = 30\nmask3 = mask2 ^ mask\narr[mask3] = arr[mask3] + 5\nprint(arr)\n",
        "\nimport numpy as np\nn = 10\nm = 4\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\n# use numpy.allclose to compare the two arrays\nresult = np.allclose(s1, s2)\nprint(result)\n",
        "\ns1 = np.append(s1, np.nan)\ns2 = np.append(s2, np.nan)\n",
        "\n# [Missing Code]\n",
        "\nresult = all(np.isnan(arr) for arr in a)\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\ndef f(arr, shape):\n    if arr.shape != shape:\n        arr = np.zeros(shape)\n    return arr\n",
        "\nresult = np.zeros((shape[0], shape[1]))\nresult[:a.shape[0], :a.shape[1]] = a\nresult[a.shape[0]:, :shape[1]-a.shape[1]] = np.zeros((shape[0]-a.shape[0], shape[1]-a.shape[1]))\nresult[:shape[0], :shape[1]-a.shape[1]] = np.zeros((shape[0]-a.shape[0], shape[1]-a.shape[1]))\nresult[a.shape[0]:, shape[1]-a.shape[1]:] = np.zeros((shape[0]-a.shape[0], shape[1]-a.shape[1]))\nresult[:shape[0], shape[1]-a.shape[1]:] = np.zeros((shape[0]-a.shape[0], shape[1]-a.shape[1]))\n",
        "\n# [Missing Code]\n",
        "\nresult = np.zeros_like(a)\nresult[b] = a[b]\n",
        "\nresult = a[b]\n",
        "\nimport numpy as np\na = np.array(\n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array(\n    [[0, 1, 2],\n    [2, 1, 3],\n    [1, 0, 3]]\n)\nresult = np.zeros_like(a)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        for k in range(a.shape[2]):\n            result[i, j, k] = a[i, j, b[i, j]]\nprint(result)\n",
        "\nimport numpy as np\na = np.array(\n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n     [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n     [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array(\n    [[0, 1, 2],\n     [2, 1, 3],\n     [1, 0, 3]]\n)\nresult = np.sum(a[b], axis=0)\nprint(result)\n",
        "\nresult = np.sum(a[b[:, 0], b[:, 1], b[:, 2]], axis=0)\nprint(result)\n",
        "\ny = np.where(df['a'] >= 1, df['b'], np.nan)\n",
        "\nresult = np.delete(im, 0, axis=0)\n",
        "\n# [Missing Code]\n",
        "",
        "\nresult = np.delete(im, 0, axis=0)\n"
    ],
    "Matplotlib": [
        "\nx = 10 * np.random.randn(10)\ny = x\n# plot x vs y, label them using \"x-y\" in the legend\nplt.plot(x, y, label='x-y')\nplt.legend()\nplt.show()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# turn on minor ticks on y-axis only\ny_axis = plt.gca().yaxis\ny_axis.set_minor_locator(plt.MultipleLocator(1))\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# turn on minor ticks\nax = plt.gca()\nax.tick_params(axis='x', which='minor', length=2)\nax.tick_params(axis='y', which='minor', length=2)\nplt.show()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# turn on minor ticks on x-axis only\nax = plt.gca()\nax.set_xticks(np.arange(0, 11, 1))\nax.set_xticklabels(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\n",
        "",
        "",
        "\nx = np.arange(10)\ny = np.random.randn(10)\n# line plot x and y with a thin diamond marker\nplt.plot(x, y, marker='o', markersize=5, linestyle='', color='blue')\n# add a legend\nplt.legend(['Data'])\n# show the plot\nplt.show()\n",
        "",
        "\nsns.set_style(\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips, ylim=(0, 40))\n",
        "\nx = 10 * np.random.randn(10)\nplt.plot(x)\n# highlight in red the x range 2 to 4\nplt.plot([2, 4], [0, 1], color='red')\n# add a legend\nplt.legend(['x range 2 to 4'])\nplt.show()\n",
        "\n# draw a full line from (0,0) to (1,2)\nx = np.linspace(0, 1, 100)\ny = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.sqrt(X**2 + Y**2)\nfig, ax = plt.subplots()\nax.plot_surface(X, Y, Z, rstride=10, cstride=10, color='blue')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.show()\n",
        "\n# draw a line segment from (0,0) to (1,2)\nx = np.linspace(0, 1, 100)\ny = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.sqrt(X**2 + Y**2)\nfig, ax = plt.subplots()\nax.plot_surface(X, Y, Z, rstride=10, cstride=10, color='blue')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.show()\n",
        "",
        "\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n# draw a regular matplotlib style plot using seaborn\nsns.set_style(\"whitegrid\")\nsns.set_context(\"notebook\", font_scale=1.5)\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xlabel(\"x-axis label\")\nax.set_ylabel(\"y-axis label\")\nax.set_title(\"Title of the plot\")\nplt.show()\n",
        "",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n# in plt.plot(x, y), use a plus marker and give it a thickness of 7\nplt.plot(x, y, marker='+', markersize=7)\n",
        "\nplt.legend(loc='upper left', fontsize=20)\n",
        "",
        "",
        "\nl.set_edgecolor(\"black\")\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30)\nplt.set_facecolor(\"red\")\nplt.set_edgecolor(\"red\")\nplt.set_linewidth(5)\nplt.show()\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels clockwise by 45 degrees\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels counter clockwise by 45 degrees\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nplt.xticks(np.arange(0, 2 * np.pi, 2 * np.pi / 6))\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\nsns.distplot(x, label=\"a\", color=\"0.25\", legend=\"Distribution of x\")\nsns.distplot(y, label=\"b\", color=\"0.25\", legend=\"Distribution of y\")\n# add legends\nsns.distplot(x, label=\"a\", color=\"0.25\", legend=\"Distribution of x\")\nsns.distplot(y, label=\"b\", color=\"0.25\", legend=\"Distribution of y\")\n",
        "",
        "",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n# set xlabel as \"X\"\nplt.xlabel('X')\n# put the x label at the right end of the x axis\nplt.gca().xaxis.set_label_position('right')\n",
        "\ndf = sns.load_dataset(\"planets\")\ng = sns.boxplot(x=\"method\", y=\"orbital_period\", data=df)\n# rotate the x axis labels by 90 degrees\ng.set_xticklabels(g.get_xticklabels().rotate(90))\nplt.show()\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nplt.plot(x, y)\nmyTitle = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\n# fit a very long title myTitle into multiple lines\nplt.title(myTitle, wrap=True)\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n# make the y axis go upside down\nplt.gca().invert_yaxis()\nplt.plot(x, y)\nplt.show()\n",
        "\nx = np.random.randn(10)\ny = x\nplt.scatter(x, y)\n# put x ticks at 0 and 1.5 only\nplt.xticks(rotation=45)\n",
        "\nx = np.random.randn(10)\ny = x\nplt.scatter(x, y)\nplt.yticks([-1, 1])\n# put y ticks at -1 and 1 only\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n# plot x, then y then z, but so that x covers y and y covers z\nfig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, sharey=False)\naxs[0].plot(x)\naxs[1].plot(y)\naxs[2].plot(z)\nplt.show()\n",
        "",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n# make all axes ticks integers\nax.tick_params(axis='x', which='both', bottom=0, top=10, labelbottom=True, labeltop=True, labelleft=True, labelright=True, labelpad=10)\nax.tick_params(axis='y', which='both', bottom=0, top=10, labelbottom=True, labeltop=True, labelleft=True, labelright=True, labelpad=10)\nplt.show()\n",
        "",
        "\n",
        "\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n# plot x vs y1 and x vs y2 in two subplots, sharing the x axis\nfig, ax1 = plt.subplots()\nax1.plot(x, y1)\nax1.set_title('y1')\nfig, ax2 = plt.subplots()\nax2.plot(x, y2)\nax2.set_title('y2')\n# add a shared x-axis label\nax1.set_xlabel('x')\nax2.set_xlabel('x')\n# show the plot\nplt.show()\n",
        "",
        "\nplt.xticks([])\n",
        "\nx = np.arange(10)\ny = np.sin(x)\ndf = pd.DataFrame({\"x\": x, \"y\": y})\nsns.lineplot(x=\"x\", y=\"y\", data=df)\n# remove x tick labels\nax = plt.gca()\nax.set_xticklabels([])\n",
        "\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n# show xticks and vertical grid at x positions 3 and 4\nplt.xticks(np.arange(3, 5), [3, 4])\nplt.yticks(np.arange(3, 5), [3, 4])\nplt.grid(True, which='both', color='gray', linestyle='-', linewidth=0.5)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n# show yticks and horizontal grid at y positions 3 and 4\nplt.yticks([3, 4], labels=['Position 3', 'Position 4'])\nplt.grid(which='y', linestyle=':', color='gray')\nplt.show()\n",
        "\nplt.yticks([3, 4], ['3', '4'])\nplt.yticks(np.arange(0, 5, 1), [str(i) for i in np.arange(0, 5, 1)])\nplt.grid(True, which='y', axis='y')\n",
        "",
        "\nx = 10 * np.random.randn(10)\ny = x\nplt.plot(x, y, label=\"x-y\")\n# put legend in the lower right\nplt.legend(loc='lower right')\nplt.show()\n",
        "",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x\n# Label the x-axis as \"X\"\n# Set the space between the x-axis label and the x-axis to be 20\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(np.arange(10), [str(i) for i in range(10)])\nplt.yticks(np.arange(10), [str(i) for i in range(10)])\nplt.xlim(0, 9)\nplt.ylim(0, 9)\nplt.gca().invert_yaxis()\nplt.gca().set_xlabel('X', fontsize=20)\nplt.gca().set_ylabel('Y', fontsize=20)\nplt.gca().set_xticklabels(np.arange(10), fontsize=20)\nplt.gca().set_yticklabels(np.arange(10), fontsize=20)\nplt.gca().set_xlim(0, 9)\nplt.gca().set_ylim(0, 9)\nplt.gca().set_aspect('equal', 'box')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# plot y over x\n# do not show xticks for the plot\nplt.plot(y, x)\nplt.xticks([])\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x\n# move the y axis ticks to the right\nplt.plot(x, y)\nplt.xticks(np.arange(10), [str(i) for i in range(10)])\nplt.yticks(np.arange(10), [str(i) for i in range(10)])\nplt.xlabel('X Label')\nplt.ylabel('Y Label')\nplt.show()\n",
        "",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", xlim=(0, 100), ylim=(0, 100), color=\"green\", ax=plt.gca())\nplt.scatter(tips[\"total_bill\"], tips[\"tip\"], color=\"green\", alpha=0.5)\nplt.plot(tips[\"total_bill\"], tips[\"tip\"], color=\"blue\", linestyle=\":\", linewidth=1)\n",
        "\ntips = sns.load_dataset(\"tips\")\n# Make a seaborn joint regression plot (kind='reg') of 'total_bill' and 'tip' in the tips dataframe\n# change the line color in the regression to green but keep the histograms in blue\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"blue\", size=5)\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")\nsns.set_style(\"darkgrid\")\nsns.set_style(\"ticks\")\nsns.set_style(\"whitegrid\")",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", size=5)\n",
        "\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n# For data in df, make a bar plot of s1 and s2 and use celltype as the xlabel\n# Make the x-axis tick labels horizontal\nplt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"])\nplt.bar(df[\"celltype\"], df[\"s2\"])\nplt.xticks(rotation=45)\nplt.xlabel(\"Cell Type\")\nplt.ylabel(\"Value\")\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.bar(df[\"celltype\"], df[\"s1\"])\nax.bar(df[\"celltype\"], df[\"s2\"])\nax.set_xticklabels(df[\"celltype\"], rotation=45)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and label the x axis as \"X\"\n# Make both the x axis ticks and the axis label red\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.xticks(rotation=45, color=\"red\")\nplt.show()\n",
        "",
        "",
        "",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make two side-by-side subplots and and in each subplot, plot y over x\n# Title each subplot as \"Y\"\nfig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=False)\naxs[0].plot(y, x)\naxs[0].set_title(\"Y\")\naxs[1].plot(y, x)\naxs[1].set_title(\"Y\")\nplt.show()\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n# make a seaborn scatter plot of bill_length_mm and bill_depth_mm\n# use markersize 30 for all data points in the scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, size=30)\nplt.show()\n",
        "\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n# make scatter plot of a over b and annotate each data point with correspond numbers in c\nfig, ax = plt.subplots()\nax.scatter(b, a, c=c)\nax.annotate(str(c[0]), (0.5, 0.5), xytext=(0, -10), ha='center', va='center', fontsize=10)\nax.annotate(str(c[1]), (0.5, 0.5), xytext=(0, -20), ha='center', va='center', fontsize=10)\nax.annotate(str(c[2]), (0.5, 0.5), xytext=(0, -30), ha='center', va='center', fontsize=10)\nplt.show()\n",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart and label the line \"y over x\"\n# Show legend of the plot and give the legend box a title  \"Legend\"\n# Bold the legend title\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('y over x')\nax.legend(['y over x'], loc='upper left')\nplt.show()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\n# Make a histogram of x and show outline of each bar in the histogram\n# Make the outline of each bar has a line width of 1.2\nplt.hist(x, bins=10, alpha=0.5, edgecolor='none', linewidth=1.2)\nplt.show()\n",
        "",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nbins = np.linspace(-1, 1, 100)\n# Plot two histograms of x and y on a single chart with matplotlib\n# Set the transparency of the histograms to be 0.5\nfig, ax = plt.subplots()\nax.hist2d(x, y, bins=bins, alpha=0.5)\nax.set_xlabel('X')\nax.set_ylabel('Y')\nplt.show()\n",
        "",
        "",
        "",
        "\nx = np.random.random((10, 2))\n# Plot each column in x as an individual line and label them as \"a\" and \"b\"\nplt.plot(x[:, 0], x[:, 1], label='a')\nplt.plot(x[:, 0], -x[:, 1], label='b')\nplt.legend()\nplt.show()\n",
        "\nfig, ax1 = plt.subplots()\nax1.plot(x, y)\nax1.set_title('Y over X')\nfig, ax2 = plt.subplots()\nax2.plot(z, a)\nax2.set_title('Z over A')\nplt.suptitle('Y and Z', fontsize=20)\nplt.show()\n",
        "",
        "",
        "",
        "",
        "\nx = np.arange(0, 1000, 50)\ny = np.arange(0, 1000, 50)\n# plot y over x on a log-log plot\n# mark the axes with numbers like 1, 10, 100. do not use scientific notation\nplt.loglog(x, y, 'o-', label='Data')\nplt.xlabel('x', labelpad=20)\nplt.ylabel('y', labelpad=20)\nplt.xticks(np.arange(1, 11, 1), [1, 10, 100, 1000])\nplt.yticks(np.arange(1, 11, 1), [1, 10, 100, 1000])\nplt.legend()\nplt.show()\n",
        "",
        "",
        "",
        "",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n# Make 2 subplots.\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\n# Do not share y axix for the subplots\nfig, ax1 = plt.subplots(figsize=(10, 6))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nax1.set_xlabel(\"Bill Length (mm)\")\nax1.set_ylabel(\"Bill Depth (mm)\")\nax1.set_title(\"Bill Depth vs Bill Length\")\nfig, ax2 = plt.subplots(figsize=(10, 6))\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\nax2.set_xlabel(\"Bill Length (mm)\")\nax2.set_ylabel(\"Flipper Length (mm)\")\nax2.set_title(\"Flipper Length vs Bill Length\")\nplt.show()\n",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x\n# Show legend and use the greek letter lambda as the legend label\nplt.plot(x, y)\nplt.legend(['Lambda'])\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(y, x)\nplt.xticks(range(0, 10, 2))\n# Add extra ticks [2.1, 3, 7.6] to existing xticks\nplt.xticks(np.arange(0, 10, 2) + 0.1, ['2.1', '3', '7.6'])\n",
        "",
        "",
        "",
        "",
        "",
        "",
        "\nvalues = [[1, 2], [3, 4]]\ndf = pd.DataFrame(values, columns=[\"Type A\", \"Type B\"], index=[\"Index 1\", \"Index 2\"])\n# Plot values in df with line chart\n# label the x axis and y axis in this plot as \"X\" and \"Y\"\nplt.plot(df.index, df[\"Type A\"], label=\"Type A\")\nplt.plot(df.index, df[\"Type B\"], label=\"Type B\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Make a scatter plot with x and y\n# Use star hatch for the marker\nplt.scatter(x, y, marker='*')\n# Add a title and axis labels\nplt.title('Scatter Plot with Star Hatch')\nplt.xlabel('X Label')\nplt.ylabel('Y Label')\n# Show the plot\nplt.show()\n",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\nplt.plot(y, x, 'b-')\nplt.show()\n",
        "",
        "\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\nfig, ax = plt.subplots()\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', pctdistance=0.8, startangle=90)\nplt.axis('equal')\nplt.title(\"Activity Distribution\")\nplt.xlabel(\"Activity\")\nplt.ylabel(\"Percentage\")\nplt.legend(labels)\nplt.show()\n",
        "\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\nfig, ax = plt.subplots()\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', pctdistance=0.8, startangle=90)\nplt.axis('equal')\nplt.legend(labels)\nplt.show()\n",
        "",
        "",
        "",
        "",
        "\nfig, ax = plt.subplots()\nax.scatter(x, y, c=y, cmap='Spectral')\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# plot y over x\n# use a tick interval of 1 on the a-axis\nplt.plot(x, y)\nplt.xticks(np.arange(10), [str(i) for i in range(10)])\nplt.yticks(np.arange(10), [str(i) for i in range(10)])\nplt.show()\n",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and use the greek letter phi for title. Bold the title and make sure phi is bold.\nplt.plot(x, y)\nplt.title('$\\phi$', fontweight='bold')\nplt.show()\n",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y, label=\"Line\")\nplt.plot(y, x, label=\"Flipped\")\n# Show a two columns legend of this plot\nplt.legend(['Line', 'Flipped'])\n",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x. Give the plot a title \"Figure 1\". bold the word \"Figure\" in the title but do not bold \"1\"\nplt.plot(x, y)\nplt.title(\"Figure 1\", fontweight=\"bold\")\nplt.show()\n",
        "\ndf = pd.DataFrame(\n    {\n        \"id\": [\"1\", \"2\", \"1\", \"2\", \"2\"],\n        \"x\": [123, 22, 356, 412, 54],\n        \"y\": [120, 12, 35, 41, 45],\n    }\n)\n# Use seaborn to make a pairplot of data in `df` using `x` for x_vars, `y` for y_vars, and `id` for hue\n# Hide the legend in the output figure\nsns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\", diag_kind=\"kde\", diag_kws={\"shade\": 0.5})\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and invert the x axis\nplt.plot(y, x)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xticks(np.arange(0, 11, 1))\nplt.yticks(np.arange(0, 11, 1))\nplt.show()\n",
        "",
        "",
        "",
        "\nx = np.random.rand(100) * 10\n# Make a histogram of x\n# Make the histogram range from 0 to 10\n# Make bar width 2 for each bar in the histogram and have 5 bars in total\nplt.hist(x, bins=5, range=(0, 10), width=2)\nplt.show()\n",
        "",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n# Plot y over x and z over a in two side-by-side subplots\n# Make \"Y\" the title of the first subplot and \"Z\" the title of the second subplot\n# Raise the title of the second subplot to be higher than the first one\nfig, ax1 = plt.subplots()\nax1.plot(y, z)\nax1.set_title('Y')\nax1.set_ylabel('Z')\nfig, ax2 = plt.subplots()\nax2.plot(x, a)\nax2.set_title('Z')\nax2.set_ylabel('X')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make 4 by 4 subplots with a figure size (5,5)\n# in each subplot, plot y over x and show axis tick labels\n# give enough spacing between subplots so the tick labels don't overlap\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(5, 5))\nfor i in range(4):\n    axs[i, 0].plot(x, y)\n    axs[i, 0].set_xticks(np.arange(10))\n    axs[i, 0].set_yticks(np.arange(10))\n    axs[i, 0].set_xticklabels(np.arange(10))\n    axs[i, 0].set_yticklabels(np.arange(10))\n    axs[i, 0].set_xlabel('X Label')\n    axs[i, 0].set_ylabel('Y Label')\nplt.show()\n",
        "\nd = np.random.random((10, 10))\n# Use matshow to plot d and make the figure size (8, 8)\nfig, ax = plt.subplots(figsize=(8, 8))\nax.matshow(d)\nplt.show()\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n].head(10)\n# Plot df as a matplotlib table. Set the bbox of the table to [0, 0, 1, 1]\nfig, ax = plt.subplots()\nsns.table_plot(data=df, ax=ax, bbox=[0, 0, 1, 1])\nplt.show()\n",
        "",
        "",
        "",
        "\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=axs.flat[0], data=df)\naxs.flat[0].set_title(\"Group: Fat\")\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=axs.flat[1], data=df)\naxs.flat[1].set_title(\"Group: No Fat\")\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=ax)\nax.set_xlabel(\"Exercise Time\")\nax.set_ylabel(\"Exercise Time\")\nplt.show()\n",
        "\ndf = sns.load_dataset(\"exercise\")\n# Make catplots of scatter plots by using \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\n# Do not show any ylabel on either subplot\nfig, ax1 = plt.subplots()\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=ax1, data=df, kind=\"scatter\", legend=False)\nax1.set_xlabel(\"Time\")\nax1.set_ylabel(\"Pulse\")\nax1.set_title(\"Scatter Plot by Time and Pulse\")\nfig, ax2 = plt.subplots()\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", ax=ax2, data=df, kind=\"scatter\", legend=False)\nax2.set_xlabel(\"Time\")\nax2.set_ylabel(\"Pulse\")\nax2.set_title(\"Scatter Plot by Time and Pulse\")\nplt.show()\n",
        "",
        "",
        "",
        "",
        "\ndf = sns.load_dataset(\"penguins\")[[\"bill_length_mm\", \"species\", \"sex\"]]\n# Make a stripplot for the data in df. Use \"sex\" as x, \"bill_length_mm\" as y, and \"species\" for the color\n# Remove the legend from the stripplot\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, size=5, edgecolor=\"white\", ax=plt.gca())\nplt.legend(loc=\"upper left\")\nplt.show()\n",
        "",
        "",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart and name axis with labels (\"x\" and \"y\")\n# Hide tick labels but keep axis labels\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n",
        ""
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx = tf.Variable(114514)\n",
        "\nresult = tf.one_hot(tf.stack(labels), depth=10)\n",
        "\nresult = tf.one_hot(tf.stack(labels), depth=10)\n",
        "\nresult = tf.one_hot(tf.range(10), depth=10)\n",
        "\nimport tensorflow as tf\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    one_hot_labels = tf.one_hot(labels, depth=10)\n    return one_hot_labels\n",
        "\nresult = tf.one_hot(tf.range(10), depth=10)\n",
        "\n# [Missing Code]\n",
        "\nresult = []\nfor i in input:\n    result.extend(my_map_func(i))\nreturn result\n",
        "\nresult = tf.zeros_like(lengths)\nresult = tf.concat([tf.ones_like(lengths[:4]), result], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths[4:])], axis=0)\nresult = tf",
        "\nresult = tf.zeros_like(lengths)\nresult = tf.pad(result, [[0, 1]], 'CONSTANT')\nresult = tf.one_hot(result, depth=8)\n",
        "\nresult = tf.zeros_like(lengths)\nresult = tf.concat([result, tf.ones_like(lengths[:4])], axis=0)\n",
        "\nresult = tf.zeros_like(lengths)\nresult = tf.concat([result, tf.ones_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)\nresult = tf.concat([result, tf.zeros_like(lengths)], axis=0)",
        "\nresult = tf.zeros_like(lengths)\nresult = tf.concat([tf.ones_like(lengths[:1]), result, lengths, tf.zeros_like(lengths)], axis=0)\n",
        "\nresult = tf.TensorArray(tf.int32, size=len(a) * len(b))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        result.write(i, j, a[i] * b[j])\n",
        "\nresult = tf.stack([tf.math.multiply(a, b) for a, b in zip(example_a, example_b)], axis=1)\n",
        "\n# Reshape the tensor\na = tf.reshape(a, (50, 100, 512))\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\nresult = tf.expand_dims(a, axis=3)\nprint(result)\n",
        "\na = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\nresult = tf.math.reduce_sum(A, axis=1)\n",
        "",
        "",
        "\nresult = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(a - b), axis=1))\n",
        "\nresult = tf.reduce_sum(tf.square(a - b), axis=1)\n",
        "\nresult = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(tf.sub(A,B)), axis=1))\n",
        "\nm = tf.gather(x, y, axis=0)\n",
        "\nm = x[[row,col]]\n",
        "\n    m = x[y,z]\n    ",
        "\nC = tf.einsum('ijk,jkl->i', A, B)\n",
        "\nC = tf.einsum('ijk,jk->i', A, B)\n",
        "\nimport tensorflow as tf\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\nresult = []\nfor byte_string in x:\n    byte_string = byte_string.decode('utf-8')\n    result.append(byte_string)\nprint(result)\n",
        "\nresult = []\nfor x in example_x:\n    result.append(tf.compat.as_str(x))\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef f(x=example_x):\n    x = tf.math.reduce_sum(x, axis=2)\n    x = x / tf.math.reduce_sum(x, axis=2, keepdims=True)\n    return x\n",
        "",
        "\nimport tensorflow as tf\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n# Convert scores tensor to desired format\nscores_flat = tf.argmax(a, axis=1)\nscores_flat = tf.gather(scores_flat, a.indices)\nscores_flat = tf.stack(scores_flat, axis=0)\n# Compute accuracy\naccuracy = tf.reduce_mean(tf.equal(scores_flat, y))\nprint(accuracy)\n",
        "\nimport tensorflow as tf\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n# Convert scores tensor to desired format\nscores_flat = tf.argmax(a, axis=1)\nscores_flat = tf.stack(scores_flat, axis=1)\nprint(scores_flat)\n",
        "\nimport tensorflow as tf\nexample_a = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\ndef f(a=example_a):\n    scores = tf.math.argmax(a, axis=1)\n    return tf.math.reduce_sum(tf.math.equal(scores, tf.range(10)))\n",
        "\nresult = tf.argmin(a, axis=1)\n",
        "\nmodel.save(\"my_model\", include_optimizer=False)\n",
        "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(10,), dtype=tf.int32, minval=1, maxval=4)\n",
        "\nimport tensorflow as tf\nseed_x = 10\n### return the tensor as variable 'result'\nresult = tf.random.uniform(shape=(114,), dtype=tf.int32, minval=2, maxval=5, seed=seed_x)\nprint(result)\n",
        "\nimport tensorflow as tf\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    return tf.random.uniform(shape=(10,), dtype=tf.int32, minval=1, maxval=4)\n",
        "\nimport tensorflow as tf\ntf.version.VERSION\n"
    ],
    "Scipy": [
        "\nA, B = np.polyfit(x, y, deg=1)\n",
        "\nA, B = np.polyfit(x, y, deg=1)\n",
        "\nfrom scipy.optimize import curve_fit\ndef exponential_fit(y, x, p0):\n    def func(p, x):\n        return p[0] * np.exp(p[1] * x) + p[2]\n    res = curve_fit(func, x, y, p0=p0)\n    return res[0]\nresult = exponential_fit(y, x, p0)\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = optimize.minimize(f, initial_guess, method='SLSQP', bounds=[(0, 10), (0, 10), (0, 10)])\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores) - scipy.stats.norm.cdf(-z_scores)\n",
        "\nimport numpy as np\nimport scipy.stats\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\nz_scores = []\nfor p in p_values:\n    z_score = scipy.stats.norm.cdf(p) - scipy.stats.norm.cdf(-p)\n    z_scores.append(z_score)\nprint(z_scores)\n",
        "\ndist = stats.lognorm(mu, stddev)\nresult = dist.cdf(x)\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nresult = sa * sb\n",
        "\nresult = sp.sparse.csr_matrix(m) * sp.sparse.csr_matrix(c)\n",
        "",
        "",
        "\n# [Missing Code]\n",
        "\nresult = M.diagonal()\n",
        "\nfrom scipy.stats import kstest\nresult = kstest(times, \"uniform\")\nprint(result)\n",
        "\ndef f(times, rate, T):\n    times_uniform = np.linspace(0, T, num=len(times))\n    kstest_result = stats.kstest(times, times_uniform, 'uniform')\n    return kstest_result\n",
        "\nfrom scipy.stats import kstest\ndef test_uniformity(times):\n    return kstest(times, \"uniform\")\nresult = test_uniformity(times)\nprint(result)\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.vstack([c1, c2])\n",
        "\nFeature = sparse.vstack(c1, c2)\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nb.setdiag(0)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\n# [Missing Code]\n",
        "\nM.make_symmetric()\n",
        "\ndef f(sA = example_sA):\n    sA = sA.make_symmetric()\n    return sA\n",
        "",
        "",
        "\nmean = np.mean(col)\nstandard_deviation = np.std(col)\n",
        "",
        "",
        "\ndef fourier(x, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15):\n    return a1 * np.cos(1 * np.pi / tau * x) + \\\n           a2 * np.cos(2 * np.pi / tau * x) + \\\n           a3 * np.cos(3 * np.pi / tau * x) + \\\n           a4 * np.cos(4 * np.pi / tau * x) + \\\n           a5 * np.cos(5 * np.pi / tau * x) + \\\n           a6 * np.cos(6 * np.pi / tau * x) + \\\n           a7 * np.cos(7 * np.pi / tau * x) + \\\n           a8 * np.cos(8 * np.pi / tau * x) + \\\n           a9 * np.cos(9 * np.pi / tau * x) + \\\n           a10 * np.cos(10 * np.pi / tau * x) + \\\n           a11 * np.cos(11 * np.pi / tau * x) + \\\n           a12 * np.cos(12 * np.pi / tau * x) + \\\n           a13 * np.cos(13 * np.pi / tau * x) + \\\n           a14 * np.cos(14 * np.pi / tau * x) + \\\n           a15 * np.cos(15 * np.pi / tau * x)\n",
        "",
        "",
        "",
        "\nresult = interpolate.splrep(x_val, y_int, k = 2, s = 4)\nresult = interpolate.splev(x_val, result, der = 0)\n",
        "\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x1, x2, x3, x4)\n",
        "\n# Calculate Anderson-Darling statistic for x1\nresult1 = ss.anderson(x1)\n# Calculate Anderson-Darling statistic for x2\nresult2 = ss.anderson(x2)\n# Compare the results\nif result1 < result2:\n    print(\"The two datasets are significantly different.\")\nelse:\n    print(\"The two datasets are not significantly different.\")\n",
        "\ndef tau1(x):\n    y = np.array(A['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\nA['AB'] = pd.rolling_apply(A['B'], 3, lambda x: tau1(x))\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nonzero()[0]\n",
        "\nblock_diag = np.zeros((3,3,3))\nfor i in range(3):\n    block_diag[i] = np.diag(a[i])\nresult = block_diag\n",
        "\np_value = stats.ranksums(pre_course_scores, during_course_scores)[1]\n",
        "\np_value = stats.ranksums(pre_course_scores, during_course_scores)[1]\n",
        "\n# [Missing Code]\nkurtosis_result = (numpy.sum(a**4) - 3*numpy.sum(a**2*a**2)) / (numpy.sum(a**2)**2)\n",
        "",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Get the indices of the Voronoi cells\ncell_indices = vor.regions\n# Initialize an empty list to store the indices of the cells occupied by each extra point\noccupied_cells = []\n# Loop through the extra points\nfor point in extraPoints:\n    # Find the Voronoi cell that contains the point\n    cell_index = vor.point_region(point)\n    # Add the cell index to the list of occupied cells\n    occupied_cells.append(cell_index)\n# Print the list of occupied cells\nprint(occupied_cells)\n",
        "\n# [Missing Code]\n",
        "\nb = nd.median_filter(a, 3, origin=(1, 1))\n",
        "\nresult = M.getrow(row).toarray()[0][column]\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = [2, 1]\ncolumn = [3, 0]\nrow_vector = M.getrow(row)\nrow_vector = np.array(row_vector)\nrow_vector = row_vector.reshape(-1, 1)\ncolumn_vector = M.getcol(column)\ncolumn_vector = np.array(column_vector)\ncolumn_vector = column_vector.reshape(1, -1)\nresult = np.array([row_vector, column_vector])\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.interpolate\narray = np.random.randint(0, 9, size=(100, 100, 100))\nx = np.linspace(0, 10, 10)\nx_new = np.linspace(0, 10, 100)\ndef interpolate_array(x_new, array):\n    f = scipy.interpolate.interp1d(x, array, kind='linear')\n    return f(x_new)\nnew_array = np.zeros((1000, 100, 100))\nfor i in range(100):\n    for j in range(100):\n        new_array[:, i, j] = interpolate_array(x_new, array[:, i, j])\nprint(new_array)\n",
        "\nimport scipy.integrate\nimport math\nimport numpy as np\n",
        "",
        "\nresult = np.zeros((N, N))\nfor i in range(N):\n    result[i, i] = 1\n",
        "\nfrom scipy.sparse import diags\nimport numpy as np\nv1 = [3*i**2 +(i/2) for i in range(1, 6)]\nv2 = [-(6*i**2 - 1) for i in range(1, 6)]\nv3 = [3*i**2 -(i/2) for i in range(1, 6)]\nmatrix = np.array([v1, v2, v3])\nresult = diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)\n",
        "\nM = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        M[i,j] = scipy.stats.binom.pmf(i, N, p)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\ndef zscore_column(df, column):\n    zscores = stats.zscore(df[column])\n    return pd.Series(zscores, index=df.index, name=column)\nresult = []\nfor probegenes in df.index:\n    sample1_zscore = zscore_column(df, 'sample1')\n    sample2_zscore = zscore_column(df, 'sample2')\n    sample3_zscore = zscore_column(df, 'sample3')\n    result.append(pd.Series([sample1_zscore, sample2_zscore, sample3_zscore], index=['sample1', 'sample2', 'sample3'], name=probegenes))\nprint(pd.concat(result, axis=1))\n",
        "\nzscores = []\nfor gene in df.index:\n    data = df.loc[gene, 'sample1'] + df.loc[gene, 'sample2'] + df.loc[gene, 'sample3']\n    zscores.append(stats.zscore(data))\nresult = pd.DataFrame({'sample1': df['sample1'], 'sample2': df['sample2'], 'sample3': df['sample3'], 'zscore': zscores})\n",
        "\nz_scores = []\nfor col in df.columns:\n    data = df[col].values\n    z_scores.append(stats.zscore(data))\nresult = pd.DataFrame({'sample1': df['sample1'].values, 'sample2': df['sample2'].values, 'sample3': df['sample3'].values}, index=df.index)\nresult['zscore'] = z_scores\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ndef get_distance_2(y, x):\n    mid = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1], [0, 1], [0, -1]])\n    return distance.cdist(np.dstack((y, x)), mid)\nshape = (6, 6)\nresult = np.zeros(shape)\nfor i in range(shape[0]):\n    for j in range(shape[1]):\n        result[i, j] = get_distance_2(shape[0] - i - 1, shape[1] - j - 1)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ndef get_distance_2(y, x):\n    mid = np.stack((y, x), axis=-1)\n    return distance.cdist(mid, mid)\nshape = (6, 6)\nresult = np.zeros(shape)\nfor i in range(shape[0]):\n    for j in range(shape[1]):\n        result[i, j] = get_distance_2(i, j)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ndef get_distance_2(y, x):\n    mid = np.stack((y, x), axis=-1)\n    return distance.cdist(mid, mid)\ndef f(shape = (6, 6)):\n    y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n    return np.stack((y, x), axis=-1)\nresult = f()\nprint(result)\n",
        "",
        "\n# [Missing Code]\n",
        "\nfit_params = Parameters()\nfit_params.add('x', value=x0)\nout = minimize(residual, fit_params, args=(a, y))\n",
        "\nt = np.linspace(0, 1, 1000)\ny = np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=lambda t, y: -100*y + y, t_span=time_span, y0=[N0,], method='BDF')\nresult = sol.y\nprint(result)\n",
        "\ndef dN1_dt_sin(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = solve_ivp(fun=dN1_dt_sin, t_span=[0, 10], y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\ndef dN1_dt_sinusoid(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = solve_ivp(fun=dN1_dt_sinusoid, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.optimize import minimize\ndef function(x):\n    return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\nI=np.array((20,50,50,80))\nx0=I\ncons=[]\nsteadystate={'type':'eq', 'fun': lambda x: x.sum()-I.sum() }\ncons.append(steadystate)\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n",
        "\nresult = sa.vstack(sb)\n",
        "\nresult = sa.dot(sb)\n",
        "\ndefinite_integral = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n",
        "",
        "\nV = V.add(x)\n",
        "\nV = sparse.coo_matrix(V.tocoo()) + x\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density=0.05, format='coo', random_state=42)\nx = 100\ny = 99\nV_sparse = sparse.coo_matrix(V.nonzero())\nV_sparse = V_sparse.add(x)\nV_sparse = V_sparse.add(y)\nprint(V_sparse)\n",
        "\n# [Missing Code]\n",
        "\n# update the original column of the matrix\nsa.set_diag(sa.diagonal().data, 0)\n",
        "\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\na = np.where(a > 0, 1, 0)\nprint(a)\n",
        "\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\na = np.where(a == 0, 0, 1)\nprint(a)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = fsolve(eqn, [a, b], args=(xdata, bdata))\n",
        "\nresult = []\nfor x, a in zip(xdata, adata):\n    b = fsolve(eqn, x0=0.5, args=(a,))[0]\n    result.append([b, -b])\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\nimport scipy.interpolate\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\neval = [(2.7, 2.3)]\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.optimize as sciopt\nx = np.array([[ 1247.04,  1274.9 ,  1277.81,  1259.51,  1246.06,  1230.2 ,\n     1207.37,  1192.  ,  1180.84,  1182.76,  1194.76,  1222.65],\n   [  589.  ,   581.29,   576.1 ,   570.28,   566.45,   575.99,\n      601.1 ,   620.6 ,   637.04,   631.68,   611.79,   599.19]])\ny = np.array([ 1872.81,  1875.41,  1871.43,  1865.94,  1854.8 ,  1839.2 ,\n    1827.82,  1831.73,  1846.68,  1856.56,  1861.02,  1867.15])\ndef f(p, x, y):\n    return p[0] * x[0] + p[1] * x[1] - y\npmin = np.array([0.5, 0.7])\npmax = np.array([1.5, 1.8])\nresult = sciopt.fminbound(f, pmin, pmax, args=(x, y))\nprint(result)\n",
        "",
        "\nresult = []\nfor i in range(len(arr)):\n    for j in range(len(arr[i])):\n        if arr[i][j] <= arr[i][j-1] and arr[i][j] <= arr[i][j+1] and arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i+1][j]:\n            result.append([i, j])\n",
        "\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data)\n",
        "\ndata1 = pd.DataFrame(data.data)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_boston\nimport pandas as pd\ndata = load_boston()\ndata1 = pd.DataFrame(data.data)\nprint(data1)\n",
        "\ndef solve(data):\n    iris_data = pd.DataFrame(data.data)\n    iris_data.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']\n    iris_data.index = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n    return iris_data\n",
        "\n# [Missing Code]\n",
        "\ndf_out = pd.DataFrame(columns=list(set(df['Col2'].unique())))\nfor i in range(len(df)):\n    df_out.loc[i, 'Apple'] = 1 if 'Apple' in df['Col2'][i] else 0\n    df_out.loc[i, 'Orange'] = 1 if 'Orange' in df['Col2'][i] else 0\n    df_out.loc[i, 'Banana'] = 1 if 'Banana' in df['Col2'][i] else 0\n    df_out.loc[i, 'Grape'] = 1 if 'Grape' in df['Col2'][i] else 0\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Convert decision scores to probabilities\nproba = 1 / (1 + np.exp(-predicted_test_scores))\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\n# merge the sparse matrix with the DataFrame\ndf = pd.merge(df_origin, transform_output, on='column_name')\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndef solve(df_origin, transform_output):\n    transformed_data = transform_output.toarray()\n    result = pd.concat([df_origin, pd.DataFrame(transformed_data)], axis=1)\n    return result\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Delete the 'reduce_dim' step\ndel clf.named_steps['reduce_dim']\nprint(len(clf.named_steps))\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nestimators = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())]\nclf = Pipeline(estimators)\n# Delete the 'reduce_poly' step\nclf.steps.pop(0)\nprint(clf.steps)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Delete the second step\ndel clf.named_steps['pOly']\nprint(clf.named_steps)\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nestimators = [('reduce_dim', PCA()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Delete the 'reduce_dim' step\nclf.steps.pop(0)\nprint(clf.steps)\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nestimators = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())]\nclf = Pipeline(estimators)\n# Insert a new step into the pipeline\nnew_estimator = ('new_step', SVC())\nnew_steps = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('new_step', SVC())]\nnew_clf = Pipeline(new_steps)\n# Print the steps in the original and new pipelines\nprint(clf.named_steps())\nprint(new_clf.named_steps())\n",
        "\nclf.steps['t1919810'] = ('t1919810', PCA())\n",
        "",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n",
        "\nproba = logreg.predict_proba(X_test)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef solve(data, scaler, scaled):\n    inverse_scaled = scaler.inverse_transform(scaled)\n    return inverse_scaled\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].named_steps['select'].fit_transform(data, target)\n",
        "\n# [Missing Code]\n",
        "",
        "",
        "",
        "\ndef prePro(text):\n    return text.lower()\n",
        "\ndf_out = preprocessing.scale(data)\n",
        "\n# Scale the data\nscaled_data = preprocessing.scale(data)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nselected_indices = model.get_support()\nselected_columns = X.columns[selected_indices]\n",
        "\nselected_features = model.get_support()\ncolumn_names = [X.columns[i] for i in selected_features]\nprint(column_names)\n",
        "\nselected_indices = model.get_support()\nselected_columns = X.columns[selected_indices]\n",
        "\nselected_columns = model.get_support()\ncolumn_names = [X.columns[i] for i in selected_columns]\nprint(column_names)\n",
        "",
        "",
        "",
        "\ndef get_samples(p, X, km):\n    samples = km.predict(X)\n    distances = km.inertia_\n    idx = np.argsort(distances)[::-1][:50]\n    return X[idx]\n",
        "\n# convert categorical variable to matrix and merge back with original training data\nX_train = pd.get_dummies(X_train, columns=['0'])\n",
        "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n# merge back with original training data\nX_train = pd.concat([X_train, X_train.iloc[:, 1:]], axis=1)\n",
        "\n# fit, then predict X\nsvm_reg = sklearn.svm.SVR(kernel='linear', C=0.1, epsilon=0.1)\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n",
        "\nfrom sklearn.datasets import make_regression\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\n# Load data\nX, y = make_regression(n_samples=100, n_features=10, n_informative=5, n_redundant=5, random_state=42)\n# Fit SVM with gaussian kernel\nsvm = SVR(kernel='linear', C=1, gamma='auto')\nsvm.fit(X, y)\n# Use polynomial features\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n# Predict values\ny_pred = svm.predict(X_poly)\n",
        "\n# fit, then predict X\nsvm_reg = sklearn.svm.SVR(kernel='poly', degree=2)\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n",
        "\nfrom sklearn.datasets import make_regression\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\n# Load data\nX, y = make_regression(n_samples=100, n_features=10, n_informative=5, n_redundant=5, random_state=42)\n# Fit SVM with polynomial kernel\nsvm = SVR(kernel='poly', degree=2)\nsvm.fit(X, y)\n# Predict values\nX_new = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\ny_pred = svm.predict(X_new)\nprint(y_pred)\n",
        "\n# calculate cosine similarity between query and documents\ncosine_similarities_of_queries = np.dot(tfidf.transform(queries), tfidf.transform(documents))\n",
        "\n# calculate cosine similarity between query and documents\ncosine_similarities_of_queries = np.dot(tfidf.transform(queries), tfidf.transform(documents))\n",
        "\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = 1 - np.sqrt(1 - query_tfidf @ query_tfidf.T)\n    return cosine_similarities_of_queries\n",
        "\nnew_features = np.zeros((len(features), 6))\nfor i, sample in enumerate(features):\n    for j, feature in enumerate(sample):\n        new_features[i, j] = 1 if feature == 'f1' else 0\n",
        "\nnew_f = np.zeros((len(f), len(set(f[0]))))\nfor i in range(len(f)):\n    new_f[i, :] = [1 if x in f[i] else 0 for x in set(f[0])]\n",
        "\nnew_features = np.zeros((len(features), 6))\nfor i, sample in enumerate(features):\n    for j, feature in enumerate(sample):\n        new_features[i, j] = 1 if feature == 'f1' else 0\n",
        "\ndef solve(features):\n    new_features = np.zeros((len(features), len(features[0])))\n    for i in range(len(features)):\n        for j in range(len(features[0])):\n            if features[i][j] == '1':\n                new_features[i][j] = 1\n    return new_features\n",
        "\nnew_features = np.zeros((len(features), len(set(features)))).astype(int)\n",
        "\n# convert the distance matrix to a graph\nG = nx.Graph()\nG.add_edges_from(data_matrix)\n# perform hierarchical clustering on the graph\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2).fit(G)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\n# [Missing Code]\n",
        "\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nfrom sklearn.preprocessing import BoxCoxTransformer\nbox_cox_transformer = BoxCoxTransformer()\nbox_cox_transformer.fit(data)\nbox_cox_data = box_cox_transformer.transform(data)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop('target', axis=1), test_size=0.2, random_state=42)\n",
        "\n# Split the dataframe into training and testing sets\ntrain_size = int(0.8 * len(data))\nx_train, x_test = data.iloc[:train_size, :-1], data.iloc[train_size:, :-1]\ny_train, y_test = data.iloc[:train_size, -1], data.iloc[train_size:, -1]\n# Split each of the sets into x and y\nx_train, y_train = x_train.values.reshape(-1, 1), y_train.values.reshape(-1, 1)\nx_test, y_test = x_test.values.reshape(-1, 1), y_test.values.reshape(-1, 1)\n",
        "\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop('target', axis=1), test_size=0.2, random_state=42)\n",
        "\n# [Missing Code]\n",
        "\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nselected_feature_names = [feature_name for feature_name, coef in zip(vectorizer.get_feature_names(), model.coef_) if coef != 0]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\n",
        "\n# [Missing Code]\n",
        "",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('titanic.csv')\ny = df['Sex']\nenc = LabelEncoder()\nenc.fit(y)\ndf['Sex'] = enc.transform(y)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('data.csv')\ny = df['Sex']\nenc = LabelEncoder()\nenc.fit(y)\ndf['Sex'] = enc.transform(y)\nprint(df)\n",
        "\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('titanic.csv')\nlabels = pd.Series(['male', 'female'])\ndf['Sex'] = LabelEncoder().fit_transform(labels)\n",
        "",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nnp_array = load_data()\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\nprint(transformed)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n",
        "\ndef Transform(a):\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a)\n    return new_a\n",
        "\nfrom sklearn import tree\nimport pandas as pd\nimport pandas_datareader as web\nimport numpy as np\ndf = web.DataReader('goog', 'yahoo', start='2012-5-1', end='2016-5-20')\ndf['B/S'] = (df['Close'].diff() < 0).astype(int)\nclosing = (df.loc['2013-02-15':'2016-05-21'])\nma_50 = (df.loc['2013-02-15':'2016-05-21'])\nma_100 = (df.loc['2013-02-15':'2016-05-21'])\nma_200 = (df.loc['2013-02-15':'2016-05-21'])\nbuy_sell = (df.loc['2013-02-15':'2016-05-21'])  # Fixed\nclose = pd.DataFrame(closing)\nma50 = pd.DataFrame(ma_50)\nma100 = pd.DataFrame(ma_100)\nma200 = pd.DataFrame(ma_200)\nbuy_sell = pd.DataFrame(buy_sell)\nclf = tree.DecisionTreeRegressor()\nx = np.concatenate([close, ma50, ma100, ma200], axis=1)\ny = buy_sell.reshape(-1, 1)  # reshape to 1D array\ny = y.repeat(4)  # repeat to 4D array\nclf.fit(x, y)\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\nclf.predict([close_buy1, m5, m10, ma20])\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\n# Reshape the dataframe\nX = dataframe.iloc[-1:].values.reshape(-1, 1)\ny = dataframe.iloc[:,-1].values.reshape(-1, 1)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].scaler.fit_transform(df[cols])\n",
        "\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].scaler.fit_transform(myData[cols])\n",
        "\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names())\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        ""
    ],
    "Pytorch": [
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\ndef train_model(model, optimizer, train_loader, criterion, epoch):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\ndef scheduler(optimizer, epoch):\n    if epoch < 10:\n        lr = 0.01\n    elif epoch < 20:\n        lr = 0.001\n    else:\n        lr = 0.0001\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n    for epoch in range(100):\n        train_model(model, optimizer, train_loader, criterion, epoch)\n        scheduler(optimizer, epoch)\n        print(f\"Epoch {epoch}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\nif __name__ == '__main__':\n    main()\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\ndef train_model(model, optimizer, train_loader, criterion, epoch):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\ndef scheduler(optimizer, epoch):\n    if epoch < 10:\n        lr = 0.1\n    elif epoch < 20:\n        lr = 0.01\n    else:\n        lr = 0.001\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n    train_loader = ...\n    criterion = ...\n    for epoch in range(100):\n        train_model(model, optimizer, train_loader, criterion, epoch)\n        scheduler(optimizer, epoch)\nif __name__ == '__main__':\n    main()\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = torch.optim.SGD(..., lr=0.005)\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.1)\nfor epoch in range(100):\n    # train for one epoch\n    optim.zero_grad()\n    # ...\n    # update the learning rate\n    scheduler.step()\n",
        "\n# Load the pre-trained word2vec embedding\nword2vec_embedding = word2vec.wv.vectors\n# Convert the input data to PyTorch tensors\ninput_Tensor = torch.tensor(input_data)\n# Embedding the input data using the pre-trained word2vec embedding\nembedded_input = torch.mm(input_Tensor, word2vec_embedding)\n",
        "\ndef get_embedded_input(input_Tensor):\n    word2vec_model = Word2Vec.load(\"path/to/word2vec/model\")\n    embedded_input = word2vec_model.infer_vector(input_Tensor, steps=word2vec.max_sentence_length)\n    return embedded_input\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.detach().numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nimport torch\nC = torch.LongTensor([[1, 3], [4, 6]])\n# 1 3\n# 4 6\nA_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log.byte()] # convert A_log to ByteTensor\n",
        "\nimport torch\nA_logical, B = load_data()\nC = B[:, torch.ByteTensor.argmax(A_logical, dim=1)]\nprint(C)\n",
        "\nA_log = torch.tensor([1, 1, 0])\n",
        "\nimport torch\nC = torch.LongTensor([[1, 3], [4, 6]])\n# 1 3\n# 4 6\nA_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, torch.LongTensor(A_log)] # convert ByteTensor to LongTensor\nprint(C)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    B_long = B.long()\n    C = B_long[:, A_log]\n    return C\nC = solve(A_log, B)\nprint(C)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\nA_log = A_log.long() # convert ByteTensor to LongTensor\nC = B[:, A_log]\nprint(C)\n",
        "\nC = torch.index_select(B, 0, idx)\n",
        "\nimport torch\nimport numpy as np\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=float16),\n   np.array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nx_tensor = torch.tensor(x_array)\nprint(x_tensor)\n",
        "\nx_tensor = torch.tensor(x)\n",
        "\ndef Convert(a):\n    x_tensor = torch.tensor(a)\n    return x_tensor\n",
        "\nmask = torch.tensor([[1, 1, 1, 0, 0],\n                    [1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 0]], dtype=torch.long)\n",
        "\nmask = torch.zeros_like(lens)\nfor i in range(lens.shape[0]):\n    mask[i, :lens[i]] = 1\n",
        "\nmask = torch.tensor([[0, 0, 1, 1, 1],\n                    [1, 1, 1, 1, 1],\n                    [0, 1, 1, 1, 1]])\n",
        "\ndef get_mask(lens):\n    mask = torch.zeros(len(lens), max(lens) + 1)\n    for i, len_ in enumerate(lens):\n        mask[i, :len_] = 1\n    return mask\n",
        "\nTensor_3D = torch.diag(Tensor_2D)\n",
        "\nresult = torch.diag(Tensor_2D)\n",
        "\nab = torch.stack((a, b), 0)\n",
        "\nab = torch.stack((a, b), 0)\n",
        "\nab = torch.stack((a, b), 0)\n",
        "\na[ : , lengths : , : ]  = 0\n",
        "\na[ : , lengths : , : ]  = 2333\n",
        "\na[:lengths, :, :] = 0\n",
        "\na[ : , : lengths , : ]  = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nimport torch\nlist = [torch.randn(3), torch.randn(3), torch.randn(3)]\nnew_tensors = torch.stack(list)\n",
        "\ndef Convert(lt):\n    tensor_of_tensors = torch.tensor(lt)\n    return tensor_of_tensors\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = x.gather(1, ids)\n",
        "\nresult = x.gather(1, ids)\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmin(softmax_output, dim=1)\n",
        "\ndef solve(softmax_output):\n    max_prob = softmax_output.max(dim=1)\n    y = torch.argmax(max_prob, dim=1)\n    return y\n",
        "\ndef solve(softmax_output):\n    y = torch.argmin(softmax_output, dim=1)\n    return y\n",
        "\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, w, z), target: (n, w, z)\n    n, c, w, z = input.size()\n    # log_p: (n, c, w, z)\n    log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*w*z, c)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)  # make class dimension last dimension\n    log_p = log_p[\n       target.view(n, w, z, 1).repeat(0, 0, 0, c) >= 0]  # this looks wrong -> Should rather be a one-hot vector\n    log_p = log_p.view(-1, c)\n    # target: (n*w*z,)\n    mask = target >= 0\n    target = target[mask]\n    loss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False)\n    if size_average:\n        loss /= mask.data.sum()\n    return loss\n",
        "\ncnt_equal = np.count_nonzero(A == B)\n",
        "\ncnt_equal = np.count_nonzero(A == B)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_not_equal = np.count_nonzero(A != B)\nprint(cnt_not_equal)\n",
        "\ndef Count(A, B):\n    cnt_equal = 0\n    for i in range(len(A)):\n        if A[i] == B[i]:\n            cnt_equal += 1\n    return cnt_equal\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_equal = np.count_nonzero(A == B)\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_not_equal = np.sum(A != B)\nprint(cnt_not_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.randn(1, 3, 10, 40, 1)\nchunk_dim = 10\na_split = torch.chunk(a, chunk_dim, dim=3)\ntensors_31 = []\nfor i in range(31):\n    tensor = a_split[i]\n    tensor = tensor.reshape(1, 3, 10, 10, 1)\n    tensor = tensor.permute(0, 2, 3, 4, 1)\n    tensor = tensor.reshape(1, 10, 10, 1)\n    tensors_31.append(tensor)\nfor tensor in tensors_31:\n    print(tensor)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = load_data()\nassert a.shape == (1, 3, 10, 40, 1)\nchunk_dim = 10\ntensors_31 = []\nfor i in range(0, 31):\n    tensor = a[:, :, i*chunk_dim:(i+1)*chunk_dim, :, :]\n    tensors_31.append(tensor)\nfor tensor in tensors_31:\n    print(tensor)\n",
        "\noutput = torch.where(mask, clean_input_spectrogram, output)\n",
        "\noutput = torch.where(mask, clean_input_spectrogram, output)\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\n",
        "\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min = torch.min(torch.abs(x), torch.abs(y))\n    return torch.mul(sign_x, min) + torch.mul(sign_y, min)\n",
        "\noutput = MyNet(input)\noutput = output.reshape(1, 3)\noutput = torch.softmax(output, dim=1)\nconf = output.max(dim=1)[0]\n",
        "\nresult = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n",
        "\nresult = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n",
        "\nnew = torch.tensor([[[0., 0., 0., 0.]]])\n",
        "\nnew = torch.tensor([[[0., 0., 0., 0.]]])\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\nnew = torch.tensor([-1, -1, -1, -1])\nresult = torch.stack([t, new])\nprint(result)\n",
        "\nresult = torch.bmm(data, W).squeeze()\n"
    ]
}