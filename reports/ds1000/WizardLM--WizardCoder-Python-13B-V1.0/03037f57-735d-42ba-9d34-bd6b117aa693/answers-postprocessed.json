{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\nresult = df.iloc[List]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    result = df.copy()\n    for col in ['Qu1', 'Qu2', 'Qu3']:\n        value_counts = df[col].value_counts()\n        others_mask = value_counts[value_counts >= 2].index\n        others_values = value_counts[value_counts < 2].index\n        result[col].replace(others_values, 'others', inplace=True)\n    ",
        "\n# [Missing Code]\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x not in df['Qu1'].value_counts()[df['Qu1'].value_counts() >= 3].index else x)\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if x not in df['Qu2'].value_counts()[df['Qu2'].value_counts() >= 2].index else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x not in df['Qu3'].value_counts()[df['Qu3'].value_counts() >= 2].index else x)\n",
        "\n# [Missing Code]\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\n",
        "\ndf = df.drop_duplicates(subset='url', keep='last')\ndf = df[df['keep_if_dup'] == 'Yes']\nresult = df.drop_duplicates(subset='url', keep='first')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\nresult = {}\nfor i, row in df.iterrows():\n    if row['name'] not in result:\n        result[row['name']] = {}\n    if row['v1'] not in result[row['name']]:\n        result[row['name']][row['v1']] = {}\n    result[row['name']][row['v1']][row['v2']] = row['v3']\nprint(result)\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n    result = df['datetime'].dt.tz_localize(None)\n    ",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\nimport pandas as pd\nimport ast\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\n# Extract key-value pairs from the message column\ndf['message'] = df['message'].apply(lambda x: ast.literal_eval(x.replace(\" \", \"\")))\n# Create new columns for each key-value pair\nfor key in df['message'][0].keys():\n    df[key] = df['message'].apply(lambda x: x[key])\n# Drop the original message column\ndf = df.drop('message', axis=1)\nprint(df)\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [[1069104, 1069105], [1066489, 1066491]]\nfor product_list in products:\n    for product in product_list:\n        df.loc[df['product'] == product, 'score'] *= 10\nresult = df\n",
        "\nmin_max_normalized_scores = []\nfor index, row in df.iterrows():\n    if row['product'] in products:\n        min_max_normalized_scores.append(1)\n    else:\n        min_max_normalized_scores.append(0)\ndf['score'] = min_max_normalized_scores\n",
        "\ncategory_cols = ['A', 'B', 'C', 'D']\ncategory_df = df[category_cols].idxmax(axis=1)\ndf['category'] = category_df\n",
        "\n# [Missing Code]\n",
        "\ndf['category'] = df.apply(lambda x: [col for col in df.columns if x[col] == 1], axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Month'] = df['Date'].dt.strftime('%m')\ndf['Year'] = df['Date'].dt.strftime('%Y')\ndf['Day'] = df['Date'].dt.strftime('%A')\nresult = df\n",
        "\ndf.loc[0, '#1'] = df.loc[4, '#1']\ndf.loc[4, '#1'] = df.loc[0, '#1']\nresult = df\n",
        "\ndf.loc[df.index[-1], '#1'] = df.loc[df.index[0], '#1']\ndf.loc[df.index[0], '#1'] = df.loc[df.index[-1], '#1']\nresult = df\n",
        "\ndf.loc[df.index[-1], '#1'] = df.loc[df.index[0], '#1']\ndf.loc[df.index[0], '#1'] = df.loc[df.index[-1], '#1']\ndf.loc[df.index[-1], '#2'] = df.loc[df.index[0], '#2']\ndf.loc[df.index[0], '#2'] = df.loc[df.index[-1], '#2']\n",
        "\n# [Missing Code]\ndf.iloc[0, 0] = df.iloc[-1, 0]\ndf.iloc[-1, 0] = df.iloc[0, 0]\ndf = df.iloc[1:]\nresult = df\n",
        "\ndf.columns = [col + 'X' for col in df.columns]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = ['X' + col if col != 'HeaderX' else col for col in df.columns]\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\"})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\"})\n",
        "\ndef get_agg_dict(df):\n    agg_dict = {}\n    for col in df.columns:\n        if col.endswith('2'):\n            agg_dict[col] = 'mean'\n        else:\n            agg_dict[col] = 'sum'\n    return agg_dict\nresult = df.groupby('group').agg(get_agg_dict(df))\n",
        "\nresult = df.loc[row_list, column_list].mean()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.apply(pd.value_counts)\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor col in df.columns:\n    value_counts = df[col].value_counts()\n    result += f\"---- {col} ----\\n\"\n    for index, value in value_counts.items():\n        result += f\"{index}    {value}\\n\"\n    result += f\"Name: {col}, dtype: int64\\n\"\n",
        "\nresult = df.iloc[[0]].combine_first(df.iloc[[1]])\n",
        "\nresult = df.iloc[[0]].combine_first(df.iloc[[1]])\n",
        "\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n",
        "\nresult = df.copy()\nresult.loc['X'] = df.loc[df['value'] < thresh].sum()\nresult.loc[df['value'] < thresh] = 0\nresult = result.dropna()\n",
        "\nresult = df.copy()\nresult.loc[result['value'] > thresh, 'value'] = 'X'\nresult = result.groupby(result['value']).mean()\nresult.loc['X', 'value'] = result.loc['X', 'value'].mean()\nresult = result.reset_index()\nresult.loc[result['value'] == 'X', 'value'] = result.loc[result['value'] == 'X', 'value'].mean()\nresult = result.set_index('value')\nresult = result.rename(columns={'lab': 'value'})\nresult = result.reset_index()\nresult = result.rename(columns={'index': 'lab'})\n",
        "\nresult = df.copy()\nresult.loc['X'] = df.loc[df['value'].between(section_left, section_right)].mean()\nresult.loc[df['value'].between(section_left, section_right), 'value'] = float('nan')\nresult = result.dropna()\n",
        "\nresult = df.assign(**{f\"inv_{col}\": 1/df[col] for col in df.columns})\n",
        "\nresult = pd.concat([df, pd.DataFrame({f\"exp_{col}\": [math.exp(x) for x in df[col]] for col in df.columns})], axis=1)\n",
        "\nresult = pd.DataFrame({\"A\": df[\"A\"], \"B\": df[\"B\"], \"inv_A\": [1/x if x != 0 else 0 for x in df[\"A\"]], \"inv_B\": [1/x if x != 0 else 0 for x in df[\"B\"]]})\n",
        "\nresult = df.assign(**{f\"sigmoid_{col}\": sigmoid(df[col]) for col in df.columns})\n",
        "\nresult = df.idxmax()\n",
        "\nresult = df.idxmin()\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': df['user'].unique()})\nresult = result.merge(df, on=['dt', 'user'], how='left').fillna(0)\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': df['user'].unique()})\nresult = result.merge(df, on=['dt', 'user'], how='left').fillna(0)\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': df['user'].unique(), 'val': 233})\nresult = result.merge(df, on=['user', 'dt'], how='left').fillna(233)\nresult = result[['dt', 'user', 'val']]\n",
        "\nresult = df.set_index('dt').groupby('user').resample('D').max().reset_index()\nresult['val'] = result.groupby('user')['val'].transform(lambda x: x.fillna(x.max()))\nresult = result.reset_index(drop=True)\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D'), 'user': df['user'].unique(), 'val': df.groupby(['user', 'dt'])['val'].transform('max')})\nresult = result.pivot_table(index='dt', columns='user', values='val').reset_index()\nresult.columns = ['dt', 'a', 'b']\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n# Create a dictionary to map names to unique IDs\nname_to_id = {}\nfor i, name in enumerate(df['name'].unique()):\n    name_to_id[name] = i + 1\n# Replace names with IDs\ndf['name'] = df['name'].map(name_to_id)\nprint(df)\n",
        "\ndf['a'] = df.groupby('name')['a'].transform(lambda x: range(1, len(x)+1))\n",
        "\n    unique_names = df['name'].unique()\n    name_to_id = {name: i+1 for i, name in enumerate(unique_names)}\n    df['name'] = df['name'].map(name_to_id)\n    ",
        "\ndf['ID'] = df.groupby(['name', 'a']).ngroup()\ndf = df.drop(['name', 'a'], axis=1)\nresult = df.rename(columns={'ID': 'name'})\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\nresult = df\n",
        "\ndf = pd.melt(df, id_vars=['user'], var_name='others', value_name='value')\nresult = df\n",
        "\ndf = pd.melt(df, id_vars=['user', 'someBool'], var_name='date', value_name='value')\nresult = df\n",
        "\nresult = df[df.c > 0.5][columns]\n",
        "\nresult = df[df.c > 0.45][columns]\n",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    ",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    result['sum'] = result.b + result.e\n    ",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df.loc[df.c > 0.5, columns]\n    ",
        "\nimport pandas as pd\nfrom datetime import timedelta\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 120\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n# Create a new column with the date shifted by X days\ndf['shifted_date'] = df['date'] + pd.DateOffset(days=X)\n# Filter the rows where the shifted date is greater than the current date\nresult = df[df['shifted_date'] > df['date']]\n# Drop the shifted_date column\nresult = result.drop(columns=['shifted_date'])\nprint(result)\n",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n# Function to check if two dates overlap\ndef overlap(date1, date2, weeks):\n    return abs((date1 - date2).days) <= weeks * 7\n# Filter out overlapping rows\nfilter_dates = []\nfor index, row in df.iterrows():\n    for i in range(1, X):\n        filter_dates.append((row['date'] + timedelta(weeks=i)).date())\ndf = df[~df['date'].isin(filter_dates)]\nprint(df)\n",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n# Function to check if two dates overlap\ndef overlap(date1, date2, weeks):\n    return abs((date1 - date2).days) <= weeks * 7\n# Filter out overlapping rows\nfilter_dates = []\nfor index, row in df.iterrows():\n    for i in range(1, X):\n        filter_dates.append((row['date'] + timedelta(weeks=i)).date())\ndf = df[~df['date'].isin(filter_dates)]\n# Convert date column back to string format\ndf['date'] = df['date'].dt.strftime('%m-%d-%Y')\nprint(df)\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 3):\n    result = result.append(df.iloc[i:i+3].mean())\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 3):\n    result = result.append(pd.DataFrame({'col1': [df['col1'][i:i+3].sum()]}), ignore_index=True)\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 4):\n    result = result.append(pd.DataFrame({'col1': df['col1'][i:i+4].sum()}, index=[i//4]))\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 3):\n    result.loc[i//3] = df.iloc[i:i+3].mean()\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 5):\n    result.loc[i//5] = df.iloc[i:i+3].sum()\n    result.loc[i//5+1] = df.iloc[i+3:i+5].mean()\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 5):\n    if i + 3 <= len(df):\n        result.loc[i//5] = df.iloc[i:i+3].sum()\n    if i + 1 <= len(df):\n        result.loc[i//5+1] = df.iloc[i+2:i+4].mean()\n",
        "\ndf.fillna(method='ffill', inplace=True)\n",
        "\ndf.fillna(method='ffill', inplace=True)\n",
        "\ndf['A'] = df['A'].replace(0, float('nan'))\ndf['A'] = df['A'].fillna(method='ffill').fillna(method='bfill')\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\ndf['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n",
        "\ndf['time_number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n",
        "\n    df['number'] = df.duration.str.extract(r'(\\d+)')\n    df['time'] = df.duration.str.extract(r'(\\D+)')\n    df['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n    ",
        "\ndf['time_number'] = df['duration'].str.split(expand=True)\ndf['time'] = df['time_number'].iloc[:, 0]\ndf['number'] = df['time_number'].iloc[:, 1].astype(int)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\ndf['time_day'] *= df['number']\n",
        "\ncheck = []\nfor column in columns_check_list:\n    check.append(df1[column] == df2[column])\nresult = np.all(check, axis=0)\n",
        "\ncheck = []\nfor column in columns_check_list:\n    check.append(df1[column] == df2[column])\nresult = np.all(check, axis=0)\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1], format='%m/%d/%Y')\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\n    df.index = pd.to_datetime(df.index)\n    df = df.reset_index()\n    df = df.melt(id_vars=['id', 'date'], var_name='column', value_name='value')\n    df = df.dropna()\n    df = df.drop('id', axis=1)\n    df = df.drop('column', axis=1)\n    df = df.values\n    ",
        "\n    df.index = pd.MultiIndex.from_tuples([(x[1], x[0]) for x in df.index])\n    df.index.names = ['id', 'date']\n    df.index = pd.to_datetime(df.index.get_level_values('date'))\n    df = df.swaplevel()\n    ",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'], value_name='Value', var_name='year')\nresult = df\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'], value_name='Value', var_name='year')\nresult = df.pivot_table(index=['Country', 'Variable', 'year'], columns='Value', aggfunc='first')\nresult = result.reset_index()\nresult = result.rename(columns={'Value': 'var1', 0: 'var2'})\nresult = result[['Variable', 'Country', 'year', 'var1', 'var2']]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\ndf = df.loc[df.abs() > 1]\ndf.columns = df.columns.str.replace('Value_', '')\nresult = df\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT;', '<', regex=True)\ndf = df.replace('&LT', '<', regex=True)\n",
        "\n    result = df.replace('&AMP;', '&', regex=True)\n    ",
        "\ndf = df.replace(['&AMP;', '&LT;', '&GT;'], ['&', '<', '>'], regex=True)\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['C'] = df['C'].str.replace('&AMP;', '&')\n",
        "\nimport pandas as pd\nimport re\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: x.split()[-1] if x.count(' ') == 1 else None)\nresult = df\n",
        "\nimport pandas as pd\nimport re\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['1_name'] = df['name'].apply(lambda x: x.split(' ')[0])\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1] if len(x.split(' ')) > 1 else None)\nresult = df.dropna()\nprint(result)\n",
        "\nvalid_names = df['name'].apply(validate_single_space_name)\nvalid_names = valid_names[valid_names.notnull()]\ndf = df[df['name'].isin(valid_names)]\ndf[['first_name', 'middle_name', 'last_name']] = df['name'].str.split(expand=True)\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\nresult = pd.merge_asof(df2.sort_values('Timestamp'), df1.sort_values('Timestamp'), on='Timestamp', direction='nearest')\nresult = result.drop('Timestamp', axis=1)\nresult = result.rename(columns={'data': 'data_df1'})\nresult['data'] = result['data_df1'].fillna(result['data_df2'])\nresult = result.drop(['data_df1', 'data_df2'], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\nresult = pd.merge_asof(df1, df2, on='Timestamp', direction='nearest')\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] <= 50 and x['col3'] <= 50 else max(x['col1'], x['col2'], x['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] > 50 and x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'], axis=1)\n",
        "\nerror_values = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        error_values.append(row['Field1'])\n",
        "\nresult = []\nfor index, row in df.iterrows():\n    if type(row['Field1']) == int:\n        result.append(row['Field1'])\n",
        "\n    error_values = []\n    for index, row in df.iterrows():\n        if not isinstance(row['Field1'], int):\n            error_values.append(row['Field1'])\n    ",
        "\ntotal_values = df.sum(axis=1)\npercentages = df.div(total_values, axis=0)\nresult = percentages\n",
        "\ntotal = df.sum()\nresult = df.div(total)\n",
        "\nimport pandas as pd\nimport io\ndata = io.StringIO(\"\"\"\nrs  alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP7      A/T      0    7      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP15     C/A      0   15      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n\"\"\")\ndf = pd.read_csv(data, delim_whitespace=True).set_index('rs')\ntest = ['TP3', 'TP7', 'TP18']\nresult = df.loc[test]\nprint(result)\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\n",
        "\n    result = df.loc[test]\n    ",
        "\nimport pandas as pd\nfrom scipy.spatial.distance import cdist\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Group by time\ndf_grouped = df.groupby('time')\n# Calculate pairwise distances between cars\ndistances = df_grouped.apply(lambda x: cdist(x[['x', 'y']], x[['x', 'y']], metric='euclidean'))\n# Find nearest neighbour for each car\nnearest_neighbours = df_grouped.apply(lambda x: distances.apply(lambda y: y.argmin()))\n# Calculate euclidean distance between each car and its nearest neighbour\neuclidean_distances = df_grouped.apply(lambda x: distances.apply(lambda y: y.min()))\n# Combine results\nresult = pd.concat([nearest_neighbours, euclidean_distances], axis=1)\nresult.columns = ['nearest_neighbour', 'euclidean_distance']\nprint(result)\n",
        "\n# Calculate pairwise distances between cars\ndistances = cdist(df[['x', 'y']], df[['x', 'y']], metric='euclidean')\n# Find the farmost neighbour for each car\nfarmost_neighbours = distances.apply(lambda x: distances.columns[x.argmax()], axis=1)\n# Calculate euclidean distance between each car and its farmost neighbour\neuclidean_distances = distances.apply(lambda x: x[x.argmax()], axis=1)\n# Combine the results into a new dataframe\nresult = pd.DataFrame({'time': df['time'], 'car': df['car'], 'farmost_neighbour': farmost_neighbours, 'euclidean_distance': euclidean_distances})\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df.apply(lambda row: \",\".join([str(x) for x in row if pd.notna(x)]), axis=1)\n",
        "\ncols = [df['keywords_0'], df['keywords_1'], df['keywords_2'], df['keywords_3']]\ndf['keywords_all'] = df.apply(lambda row: '-'.join([str(x) for x in row if pd.notna(x)]), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df.apply(lambda row: \"-\".join([str(x) for x in row[cols].dropna()]), axis=1)\n",
        "\ncols = [df['keywords_0'], df['keywords_1'], df['keywords_2'], df['keywords_3']]\ndf['keywords_all'] = df.apply(lambda row: '-'.join([str(x) for x in row[cols].dropna()]), axis=1)\n",
        "\nsample_rows = df.sample(frac=0.2, random_state=0)\nsample_rows['Quantity'] = 0\nresult = df.mask(df.index.isin(sample_rows.index), sample_rows)\n",
        "\nsample_rows = df.sample(frac=0.2, random_state=0)\nsample_rows['ProductId'] = 0\nresult = df.mask(df.index.isin(sample_rows.index), sample_rows)\n",
        "\n# [Missing Code]\n",
        "\nduplicate['index_original'] = df.index[duplicate_bool].tolist()\n",
        "\nduplicate['index_original'] = df.index[duplicate_bool].tolist()\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.index.map(lambda x: df.index[df.duplicated(subset=['col1','col2'])].tolist()[x])\n    result = duplicate\n    ",
        "\nduplicate['index_original'] = duplicate.index.to_series().map(lambda x: df.index[df.duplicated(subset=['col1','col2', '3col'], keep='first')].tolist().index(x))\n",
        "\nduplicate['index_original'] = df.index[duplicate_bool].tolist()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n# Group by ['Sp', 'Mt'] columns\ngrouped_df = df.groupby(['Sp', 'Mt'])\n# Get the max count for each group\nmax_count = grouped_df['count'].max()\n# Filter the rows with max count for each group\nresult = df[df['count'].isin(max_count)]\n# Reset the index\nresult = result.reset_index(drop=True)\n# Output the result\nprint(result)\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'])\nresult = result.drop_duplicates(['Sp','Mt'])\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\nresult = df.merge(result, on=['Sp', 'Mt', 'count'])\nresult = result.drop_duplicates(['Sp', 'Mt'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n# Group by Sp and Value\ngrouped = df.groupby(['Sp', 'Value'])\n# Get the max count for each group\nmax_count = grouped['count'].max()\n# Filter the rows where count equals max_count\nresult = df[df['count'].isin(max_count)]\n# Reset the index\nresult = result.reset_index(drop=True)\nprint(result)\n",
        "\nresult = df.query(\"Category in @filter_list\")\n",
        "\nresult = df.query(\"Category not in @filter_list\")\n",
        "\nresult = pd.melt(df, value_vars=df.columns.tolist())\n",
        "\nresult = pd.melt(df, var_name=['variable_0', 'variable_1', 'variable_2'], value_name='value')\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum()['val']\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum()['val']\n",
        "\ndf['cummax'] = df.groupby('id').cummax()\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum()['val']\ndf['cumsum'] = df['cumsum'].apply(lambda x: 0 if x < 0 else x)\n",
        "\nresult = df.groupby('l')['v'].sum()\n",
        "\nresult = df.groupby('r')['v'].sum()\n",
        "\nresult = df.groupby('l')['v'].sum()\nresult.loc['right'] = np.nan\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndef relationship_type(col1, col2):\n    if len(df[col1].unique()) == len(df[col2].unique()) and len(df[col1].unique()) == 1:\n        return \"one-to-one\"\n    elif len(df[col1].unique()) == len(df[col2].unique()) and len(df[col1].unique()) > 1:\n        return \"many-to-many\"\n    elif len(df[col1].unique()) > len(df[col2].unique()):\n        return \"one-to-many\"\n    else:\n        return \"many-to-one\"\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            result.append(f\"{col1} {col2} {relationship_type(col1, col2)}\")\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndef relationship_type(df):\n    result = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                col1_unique = len(df[col1].unique())\n                col2_unique = len(df[col2].unique())\n                if col1_unique == col2_unique:\n                    result.append(f\"{col1} {col2} one-2-many\")\n                elif col1_unique < col2_unique:\n                    result.append(f\"{col1} {col2} many-2-one\")\n                elif col1_unique > col2_unique:\n                    result.append(f\"{col1} {col2} one-2-many\")\n                else:\n                    result.append(f\"{col1} {col2} many-2-many\")\n    return result\nresult = relationship_type(df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n# Define a function to check the relationship between two columns\ndef check_relationship(col1, col2):\n    if len(df[col1].unique()) == len(df[col2].unique()):\n        return \"one-to-one\"\n    elif len(df[col1].unique()) < len(df[col2].unique()):\n        return \"one-to-many\"\n    elif len(df[col1].unique()) > len(df[col2].unique()):\n        return \"many-to-one\"\n    else:\n        return \"many-to-many\"\n# Create an empty DataFrame to store the relationships\nresult = pd.DataFrame(columns=df.columns, index=df.columns)\n# Iterate through each pair of columns and calculate the relationship\nfor col1 in df.columns:\n    for col2 in df.columns:\n        result.loc[col1, col2] = check_relationship(col1, col2)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n# Define a function to check the relationship between two columns\ndef check_relationship(col1, col2):\n    if col1.nunique() == col2.nunique():\n        return \"one-2-one\"\n    elif col1.nunique() < col2.nunique():\n        return \"one-2-many\"\n    else:\n        return \"many-2-one\"\n# Create an empty DataFrame to store the relationships\nresult = pd.DataFrame(columns=df.columns, index=df.columns)\n# Iterate through each pair of columns and calculate the relationship\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            relationship = check_relationship(df[col1], df[col2])\n            result.loc[col1, col2] = relationship\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nresult = pd.to_numeric(s.str.replace(',',''), errors='coerce')\n",
        "\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\nresult = df.groupby('Family')['Survived'].mean()\n",
        "\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\ndf['NoFamily'] = (df['Survived'] == 0) & (df['Parch'] == 0)\nresult = df.groupby(['Family', 'NoFamily'])['SibSp'].mean()\n",
        "\ndf['Group'] = pd.cut(df['SibSp'] + df['Parch'], bins=[-1, 0, 1, 2, 3], labels=['No Family', 'New Family', 'Has Family', 'Old Family'])\nresult = df.groupby('Group')['Survived'].mean()\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(by='A'))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(by='A'))\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\ndf = df.stack(0).swaplevel(0,1).sort_index()\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\ndf = df.stack(0).stack()\ndf.index.names = ['Caps', 'Middle', 'Lower']\ndf = df.unstack().unstack()\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\nresult = df.swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
        "\nbird_df = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\ndef stdMeann(x):\n    return np.std(np.mean(x))\nresult = pd.Series(df.groupby('a').b.apply(stdMeann))\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\ndef stdMeann(x):\n    return np.std(np.mean(x))\nresult = pd.Series(df.groupby('b').a.apply(stdMeann))\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n# Group by column 'a'\ngroups = df.groupby('a')\n# Calculate softmax and min-max normalization for each group\nsoftmax = groups['b'].apply(lambda x: np.exp(x)/np.sum(np.exp(x)))\nmin_max = groups['b'].apply(lambda x: (x - x.min())/(x.max() - x.min()))\n# Combine the results with the original dataframe\nresult = df.join(softmax.to_frame('softmax'))\nresult = result.join(min_max.to_frame('min-max'))\n",
        "\nresult = df[(df != 0).any(axis=1)]\nresult = result[(result.T != 0).any()]\n",
        "\nresult = df[(df.sum(axis=1) != 0) & (df.sum(axis=0) != 0)]\n",
        "\nresult = df[(df.A <= 1) & (df.B <= 1) & (df.C <= 1) & (df.D <= 1)]\n",
        "\nresult = df.apply(lambda x: x.replace(2, 0))\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n          index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n# Sort by value and index\nresult = s.sort_values(ascending=True)\nprint(result)\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n              index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\ndf = pd.DataFrame(s)\ndf.columns = ['1']\ndf = df.sort_values(by=['1', df.index], ascending=[True, True])\nresult = df\nprint(result)\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n# Group by ['Sp', 'Mt'] columns\ngrouped_df = df.groupby(['Sp', 'Mt'])\n# Get the max count for each group\nmax_count = grouped_df['count'].max()\n# Filter the rows with max count for each group\nresult = df[df['count'].isin(max_count)]\n# Reset the index\nresult = result.reset_index(drop=True)\n# Output the result\nprint(result)\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'])\nresult = result.drop_duplicates(['Sp','Mt'])\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\nresult = df.merge(result, on=['Sp', 'Mt', 'count'])\nresult = result.drop_duplicates(['Sp', 'Mt'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n# Group by Sp and Value\ngrouped = df.groupby(['Sp', 'Value'])\n# Get the max count for each group\nmax_count = grouped['count'].max()\n# Filter the rows where count equals max_count\nresult = df[df['count'].isin(max_count)]\n# Reset the index\nresult = result.reset_index(drop=True)\nprint(result)\n",
        "\ndf['Date'] = df['Member'].map(dict)\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\n",
        "\n    result = df.copy()\n    for key, value in dict.items():\n        result.loc[result['Member'] == key, 'Date'] = value\n    ",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926').apply(pd.to_datetime, format='%d/%m/%Y').dt.strftime('%d-%b-%Y')\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).size()\ndf['Count_y'] = df.groupby(df['Date'].dt.year).size()\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).size().reset_index(name='Count_Val')\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month'), 'Val']).size().reset_index(name='Count_m')\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'), 'Val']).size().reset_index(name='Count_y')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.map(df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).size())\ndf['Count_y'] = df.Date.map(df.groupby([df['Date'].dt.year]).size())\ndf['Count_Val'] = df.groupby(['Val'])['Date'].transform('count')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n# result1: zero\nresult1 = df.groupby('Date').apply(lambda x: x.eq(0).sum())\n# result2: non-zero\nresult2 = df.groupby('Date').apply(lambda x: x.ne(0).sum())\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n# result1: even\nresult1 = df.groupby('Date').apply(lambda x: x.apply(lambda y: y[y % 2 == 0].sum()))\n# result2: odd\nresult2 = df.groupby('Date').apply(lambda x: x.apply(lambda y: y[y % 2 != 0].sum()))\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\nresult = df.explode('var2')\n",
        "\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2')\n",
        "\nimport dask.dataframe as dd\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=1)\ndef split_column(row):\n    return row.str.split(\"-\")\nresult = df.apply(split_column, axis=1, result_type='expand')\nprint(result.compute())\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\nprint(result)\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[6:]\n",
        "\ndf['fips'] = df['row'].str[:3]\ndf['row'] = df['row'].str[3:]\n",
        "\ndf[['fips', 'medi', 'row']] = df['row'].str.split(expand=True)\n",
        "\ndf = df.set_index('Name')\ndf = df.cumsum(axis=1)\ndf = df.div(df.count(axis=1), axis=0)\nresult = df.reset_index()\n",
        "\ndf = df.set_index('Name')\ndf = df.cumsum(axis=1)\ndf = df.div(df.sum(axis=1), axis=0)\nresult = df.reset_index()\n",
        "\n    df = df.replace(0, pd.np.nan)\n    df = df.cumsum(axis=1) / (pd.Series(range(1, len(df.columns)+1)) * df.shape[0])\n    df = df.fillna(0)\n    result = df.round(3)\n    ",
        "\ndf = df.set_index('Name')\ndf = df.cumsum(axis=1)\ndf = df.div(df.sum(axis=1), axis=0)\nresult = df.reset_index()\n",
        "\ndf['Label'] = 0\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 1)\ndf.iloc[0, df.columns.get_loc('Label')] = 1\n",
        "\ndf['label'] = 1\nfor i in range(1, len(df)):\n    diff = df.loc[i, 'Close'] - df.loc[i-1, 'Close']\n    if diff > 0:\n        df.loc[i, 'label'] = 1\n    elif diff == 0:\n        df.loc[i, 'label'] = 0\n    else:\n        df.loc[i, 'label'] = -1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else 0 if x == 0 else -1)\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.shift(-1) - df.arrival_time\ndf.iloc[0, 2] = pd.NaT\ndf.iloc[-1, 2] = pd.NaT\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.shift(-1) - df.arrival_time\ndf.iloc[0, 3] = np.nan\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.shift(-1) - df.arrival_time\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['Duration'] = df['Duration'].dt.seconds\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'one').sum())\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'two').sum())\n",
        "\ndf['key2_end_with_e'] = df['key2'].apply(lambda x: x.endswith('e'))\nresult = df.groupby(['key1']).agg({'key2_end_with_e': 'sum'})\n",
        "\nmax_result = df.index.max()\nmin_result = df.index.min()\n",
        "\nmode_result = df.idxmax()\nmedian_result = df.median().idxmax()\n",
        "\nresult = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]\n",
        "\ndf = df[~(df['closing_price'] < 99) & ~(df['closing_price'] > 101)]\n",
        "\nresult = df.groupby(\"item\", as_index=False).min()\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n",
        "\ndef parse_source_name(name):\n    parts = name.split('_')\n    if len(parts) > 1:\n        return parts[-1]\n    else:\n        return name\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(parse_source_name)\n",
        "\n    result = df['SOURCE_NAME'].str.split('_').str[0]\n    ",
        "\ndf['Column_x'].fillna(pd.Series(np.where(df['Column_x'].isnull(), df['Column_x'].fillna(0.5), df['Column_x']), index=df.index), inplace=True)\n",
        "\n",
        "\ndf['Column_x'].fillna(pd.Series(np.random.choice([0, 1], size=df['Column_x'].isnull().sum(), p=[0.5, 0.5])), inplace=True)\n",
        "\na_b = pd.DataFrame([[(x, y) for x, y in zip(a.to_numpy().flatten(), b.to_numpy().flatten())]], columns=['one', 'two'])\n",
        "\na_b_c = pd.DataFrame()\nfor i in range(len(a)):\n    a_b_c.loc[i] = tuple(a.iloc[i].values) + tuple(b.iloc[i].values) + tuple(c.iloc[i].values)\n",
        "\na_b = pd.DataFrame()\nfor i in range(len(a)):\n    if i < len(b):\n        a_b.loc[i] = [(a.iloc[i]['one'], b.iloc[i]['one']), (a.iloc[i]['two'], b.iloc[i]['two'])]\n    else:\n        a_b.loc[i] = [(a.iloc[i]['one'], np.nan), (a.iloc[i]['two'], np.nan)]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack(level=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack(fill_value=0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['tom', 'tom', 'tom', 'tom', 'jack', 'jack', 'jack', 'jack'],\n                   'post_id': [10, 8, 7, 6, 5, 4, 3, 2],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack(fill_value=0)\nprint(result)\n",
        "\nresult = pd.DataFrame({'text': [' '.join(df['text'])]})\n",
        "\nresult = pd.DataFrame({'text': ['-'.join(df['text'])]})\n",
        "\nresult = pd.DataFrame({'text': [' '.join(df['text'])]})\n",
        "\nresult = pd.Series([' '.join(df['text'])])\n",
        "\nresult = pd.Series(['-'.join(df['text'])])\n",
        "\ndf2['city'] = df1.loc[df1['id'].isin(df2['id']), 'city']\ndf2['district'] = df1.loc[df1['id'].isin(df2['id']), 'district']\nresult = pd.concat([df1, df2], axis=0)\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date'], format='%Y/%m/%d')\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby('id').first().reset_index()\nresult = result.sort_values(['id', 'date'])\n",
        "\ndf2['city'] = df1.groupby('id')['city'].apply(lambda x: x.iloc[0])\ndf2['district'] = df1.groupby('id')['district'].apply(lambda x: x.iloc[0])\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult = result.drop_duplicates(['id'], keep='first')\nresult = result.reset_index(drop=True)\n",
        "\nresult = pd.merge(C, D, how='left', on='A')\nresult['B'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(['B_x', 'B_y'], axis=1)\n",
        "\nresult = pd.merge(C, D, how='left', on='A')\nresult['B'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(['B_x', 'B_y'], axis=1)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['dulplicated'] = result['B_x'] != result['B_y']\nresult['B'] = result['B_y'].fillna(result['B_x'])\nresult = result[['A', 'B', 'dulplicated']]\n",
        "\nresult = df.groupby('user').agg(lambda x: x.tolist())\n",
        "\nresult = df.groupby('user').agg(lambda x: x.tolist())\n",
        "\nresult = df.groupby('user').agg(lambda x: x.tolist())\n",
        "\ndf_concatenated = pd.DataFrame(series.tolist(), index=series.index)\n",
        "\ndf_concatenated = pd.DataFrame(series.tolist(), index=series.index, columns=['0', '1', '2', '3'])\nresult = df_concatenated\n",
        "\nresult = []\nfor col in df.columns:\n    if s in col:\n        result.append(col)\n",
        "\nresult = df.filter(regex=f'{s}.*')\n",
        "\nresult = []\nfor col in df.columns:\n    if s in col:\n        result.append(col)\n",
        "\ndf = pd.DataFrame(df['codes'].tolist(), columns=['code_' + str(i) for i in range(len(df['codes'][0]))])\n",
        "\ndf = pd.DataFrame(df['codes'].tolist(), columns=['code_1', 'code_2', 'code_3'])\n",
        "\ndf = pd.DataFrame(df['codes'].tolist(), columns=['code_1', 'code_2', 'code_3'])\n",
        "\nresult = []\nfor i in range(len(df)):\n    result.extend(df.loc[i, 'col1'])\n",
        "\nresult = ''\nfor index, row in df.iterrows():\n    result += ','.join(str(x) for x in row['col1'][::-1])\n",
        "\nresult = ''\nfor row in df['col1']:\n    result += ','.join(map(str, row))\n",
        "\ndf = df.groupby(pd.Grouper(key='Time', freq='2Min')).mean()\nresult = df\n",
        "\ndf = df.groupby(pd.Grouper(key='Time', freq='3min')).sum()\nresult = df\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\nresult = df\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\nresult = df\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\nresult = df\n",
        "\nresult = df[df.index.get_level_values('a').isin(filt[filt].index)]\n",
        "\nresult = df[df.index.get_level_values('a').isin(filt[filt].index) & df.index.get_level_values('b').isin(filt[filt].index)]\n",
        "\nresult = df.index[df.iloc[0] != df.iloc[8]].tolist()\n",
        "\nresult = []\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if pd.isna(df.iloc[i, j]) and pd.isna(df.iloc[8, j]):\n            result.append(df.columns[j])\nprint(result)\n",
        "\nresult = []\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if not np.isnan(df.iloc[i, j]) and not np.isnan(df.iloc[8, j]) and df.iloc[i, j] != df.iloc[8, j]:\n            result.append(df.columns[j])\n",
        "\nresult = []\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if not np.isnan(df.iloc[i, j]) and not np.isnan(df.iloc[8, j]) and df.iloc[i, j] != df.iloc[8, j]:\n            result.append((df.iloc[i, j], df.iloc[8, j]))\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n",
        "\nresult = pd.DataFrame(df.values.flatten(), columns=['_'.join(df.columns) + '_' + str(i) for i in range(1, df.shape[0] + 1)])\n",
        "\nresult = pd.DataFrame(np.array(df).reshape(1, -1), columns=df.columns + '_' + df.index.astype(str))\n",
        "\ndf['dogs'] = df['dogs'].round(2)\n",
        "\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\nresult = df.sort_index(level='time')\n",
        "\nresult = df.sort_values(by=['VIM', 'time'], ascending=[True, True])\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = corr.where(corr > 0.3)\nresult = result.dropna(how='all')\nresult = result.dropna(how='all', axis=1)\nresult = result.stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult = result.sort_values(['Col1', 'Col2'])\n",
        "\nresult = corr[corr > 0.3]\n",
        "\ndf.columns = [col if i != len(df.columns)-1 else 'Test' for i, col in enumerate(df.columns)]\n",
        "\ndf.columns.values[0] = 'Test'\n",
        "\ndef find_frequent(row):\n    freq_count = 0\n    frequent = row.value_counts().index[0]\n    for value in row:\n        if value == frequent:\n            freq_count += 1\n    return frequent, freq_count\ndf['frequent'], df['freq_count'] = zip(*df.apply(find_frequent, axis=1))\n",
        "\ndef find_frequent(row):\n    freq_value = row.value_counts().index[0]\n    freq_count = row.value_counts().values[0]\n    return pd.Series({'frequent': freq_value, 'freq_count': freq_count})\nresult = df.apply(find_frequent, axis=1)\n",
        "\ndef find_frequent(row):\n    freq_dict = {}\n    for value in row:\n        if value in freq_dict:\n            freq_dict[value] += 1\n        else:\n            freq_dict[value] = 1\n    max_count = max(freq_dict.values())\n    frequent = [key for key, value in freq_dict.items() if value == max_count]\n    return frequent\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df['frequent'].apply(len)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n# Convert 'NULL' values to NaN\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\n# Group by id1 and id2 and get the mean of foo and bar\nres = df.groupby([\"id1\",\"id2\"])[[\"foo\",\"bar\"]].mean()\n# Fill missing values with 0\nres.fillna(0, inplace=True)\n# Rename columns\nres.columns = ['foo', 'bar']\nprint(res)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n# Convert 'NULL' to 0\ndf['bar'] = df['bar'].replace('NULL', 0)\n# Group by id1 and id2 and get the mean of foo and bar\nres = df.groupby([\"id1\",\"id2\"])[[\"foo\",\"bar\"]].mean()\n# Fill missing values with 0\nres = res.fillna(0)\nprint(res)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'a_col']]\nprint(result)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'b_col']]\nprint(result)\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nresult = x[~np.isnan(x)].tolist()\n",
        "\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a] = 1\n",
        "\nb = np.zeros((len(a), np.max(a) + 1))\nb[np.arange(len(a)), a] = 1\n",
        "\nb = np.zeros((len(a), max(a) + 1))\nb[np.arange(len(a)), a + max(a)] = 1\n",
        "\nb = np.zeros((len(a), len(a)))\nb[np.arange(len(a)), np.argsort(a)] = 1\n",
        "\nb = np.zeros((a.shape[0], a.shape[1] * (a.max() + 1)), dtype=int)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, a.max() + a[i, j]] = 1\n",
        "\nresult = np.percentile(a, p)\n",
        "\nB = A.reshape((-1, ncol))\n",
        "\nB = A.reshape(nrow, A.shape[0] // nrow)\n",
        "\nB = A.reshape((-1, ncol))\n",
        "\nB = A.reshape((-1, ncol))\n",
        "\nresult = np.roll(a, shift)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.zeros_like(a)\nresult[:, shift[0]:] = a[:, :a.shape[1] - shift[0]]\nresult[:, :shift[1]] = np.nan\n",
        "\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    result = np.unravel_index(np.argmax(a), a.shape)\n    ",
        "\nresult = np.unravel_index(np.argsort(a.ravel())[-2], a.shape)\n",
        "\nz = np.any(np.isnan(a), axis=0)\na = np.delete(a, np.where(z)[0], axis=1)\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a, axis=None), a.shape)\n",
        "\nresult = np.sin(np.deg2rad(degree))\n",
        "\nresult = np.cos(np.deg2rad(degree))\n",
        "\nresult = 0 if np.sin(np.deg2rad(number)) > 0 else 1\n",
        "\nresult = np.degrees(np.arcsin(value))\n",
        "\nresult = np.pad(A, (0, length - A.size), 'constant', constant_values=0)\n",
        "\nresult = np.pad(A, (0, length - A.size), 'constant', constant_values=0)\n",
        "\na = a ** power\n",
        "\n    result = np.power(a, power)\n    ",
        "\nresult = fractions.Fraction(numerator, denominator).limit_denominator()\n",
        "\n    gcd = np.gcd(numerator, denominator)\n    result = (numerator // gcd, denominator // gcd)\n    ",
        "\nresult = (numerator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, np.maximum(b, c))\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[::-1][diagonal]\n",
        "\ndiagonal = np.diag_indices(a.shape[0])\nresult = a[::-1][diagonal]\n",
        "\ndiagonal = np.diag_indices(5)\nresult = np.concatenate((a[diagonal], a[::-1, diagonal[1]][::-1]), axis=1)\n",
        "\ndiagonal1 = np.diag_indices(5)\ndiagonal2 = np.diag_indices(6)\nresult = np.concatenate((a[diagonal1], a[diagonal2]), axis=0)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor row in X:\n    for element in row:\n        result.append(element)\nprint(result)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\nprint(result)\n",
        "\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i][j])\n    return result\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\nprint(result)\n",
        "\nresult = np.array([int(i) for i in mystr])\n",
        "\na[:, col] *= multiply_number\nresult = np.cumsum(a[:, col])\n",
        "\nresult = np.cumsum(a[row] * multiply_number)\n",
        "\na[row] /= divide_number\nresult = np.prod(a[row])\n",
        "\nresult = np.linalg.matrix_rank(a)\n",
        "\nresult = a.shape[0]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.stats\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n# Remove nans from data\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n# Calculate weighted mean and std dev\nwa = np.average(a, weights=np.ones(len(a))/len(a))\nwb = np.average(b, weights=np.ones(len(b))/len(b))\nsa = np.sqrt(np.average((a-wa)**2, weights=np.ones(len(a))/len(a)))\nsb = np.sqrt(np.average((b-wb)**2, weights=np.ones(len(b))/len(b)))\n# Calculate t-statistic\nt_stat = (wa-wb)/np.sqrt(sa**2/len(a) + sb**2/len(b))\n# Calculate p-value\np_value = 2*scipy.stats.t.cdf(-np.abs(t_stat), df=len(a)+len(b)-2)\n",
        "\n# [Missing Code]\n",
        "\noutput = A[~np.isin(A, B)]\n",
        "\noutput = np.concatenate((np.setdiff1d(A, B), np.setdiff1d(B, A)))\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\n# [Missing Code]\n",
        "\na = np.delete(a, 2, 1)\n",
        "\na = np.delete(a, 2, 0)\n",
        "\na = np.delete(a, [0, 2], 1)\n",
        "\nresult = np.delete(a, del_col, axis=1)\n",
        "\na = np.insert(a, pos, element)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\n    a = np.insert(a, pos, element)\n    ",
        "\nfor i in range(len(pos)):\n    a = np.insert(a, pos[i], element[i], axis=0)\n",
        "\nresult = np.array([np.copy(arr) for arr in array_of_arrays])\n",
        "\nresult = np.all(np.diff(a, axis=0) == 0)\n",
        "\nresult = np.all(a == a[0, :], axis=0)\n",
        "\n    result = np.all(np.apply_along_axis(lambda x: np.all(x == a[0]), 1, a), axis=0)\n    ",
        "\nfx = np.cos(x)**4\nfy = np.sin(y)**2\nresult = np.trapz(np.trapz(fx, x), y)\n",
        "\n    result = (np.cos(x)**4) + (np.sin(y)**2)\n    ",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\nresult = ecdf(grades)\n",
        "\nresult = ecdf(grades)(eval)\n",
        "\n# [Missing Code]\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\na_np = a.numpy()\n",
        "\na_tf = tf.constant(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[::-1][:N]\n",
        "\nresult = np.power(A, n)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\nresult = []\nfor i in range(0, a.shape[0]-1, 2):\n    for j in range(0, a.shape[1]-1, 2):\n        result.append(a[i:i+2, j:j+2])\nprint(result)\n",
        "\nresult = []\nfor i in range(0, a.shape[0]-1, 2):\n    for j in range(0, a.shape[1]-1, 2):\n        result.append(a[i:i+2, j:j+2])\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\nresult = []\nfor i in range(0, a.shape[0]-1, 2):\n    for j in range(0, a.shape[1]-1, 2):\n        result.append(a[i:i+2, j:j+2])\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size].tolist())\n",
        "\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i, j] = a[i // 2, j // 3, i % 2 * 3 + j % 3]\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size].tolist())\n",
        "\nresult = a[:, low:high+1]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:, low:high]\n",
        "\na = np.array(string.replace(\"[\", \"\").replace(\"]\", \"\").split(), dtype=float).reshape(2, 2)\n",
        "\ndef log_uniform(min, max, n):\n    log_min = math.log(min)\n    log_max = math.log(max)\n    log_range = log_max - log_min\n    log_values = np.random.uniform(0, 1, n) * log_range + log_min\n    values = np.exp(log_values)\n    return values\nresult = log_uniform(min, max, n)\n",
        "\ndef log_uniform(min, max, n):\n    log_min = math.log(min)\n    log_max = math.log(max)\n    log_range = log_max - log_min\n    log_values = np.random.uniform(log_min, log_max, n)\n    values = np.exp(log_values)\n    return values\nresult = log_uniform(min, max, n)\n",
        "\n    log_min = math.log(min)\n    log_max = math.log(max)\n    result = np.exp(np.random.uniform(log_min, log_max, n))\n    ",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a*A[0]\nB[1] = a*A[1]+b*B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3, 0))\n",
        "\nresult = np.ravel_multi_index(index, dims)\n",
        "\nresult = np.ravel_multi_index(index, dims)\n",
        "\nvalues = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nunique_indices, inverse_indices = np.unique(accmap, return_inverse=True)\nresult = np.zeros(unique_indices.shape)\nnp.add.at(result, inverse_indices, a)\n",
        "\nresult = np.zeros(np.max(index) + 1)\nfor i in range(len(a)):\n    result[index[i]] += a[i]\n",
        "\nresult = np.zeros(np.max(accmap) + 1)\nfor i in range(len(a)):\n    result[accmap[i]] += a[i]\n",
        "\nresult = np.zeros(np.max(index) + 1)\nfor i in range(len(a)):\n    result[index[i]] = min(result[index[i]], a[i])\n",
        "\nimport numpy as np\nx = [[2, 2, 2],\n     [2, 2, 2],\n     [2, 2, 2]]\ny = [[3, 3, 3],\n     [3, 3, 3],\n     [3, 3, 1]]\ndef elementwise_function(element_1,element_2):\n    return (element_1 + element_2)\nz = np.zeros_like(x)\nfor i in range(len(x)):\n    for j in range(len(x[0])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\nprint(z)\n",
        "\nimport numpy as np\nprobabilit = np.array([0.333, 0.333, 0.334])\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\nresult = np.pad(a, ((low_index, high_index - a.shape[0]), (low_index, high_index - a.shape[1])), 'constant', constant_values=0)\n",
        "\nresult = x[x >= 0]\n",
        "\nresult = x[np.iscomplex(x)]\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size)\nbin_data_max = np.max(bin_data, axis=1)\n",
        "\nbin_data = np.array_split(data, data.shape[1] // bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = data[-bin_size:].reshape(1, bin_size)\nbin_data_mean = np.mean(bin_data)\nfor i in range(len(data) - bin_size, 0, -bin_size):\n    bin_data = data[i-bin_size:i].reshape(1, bin_size)\n    bin_data_mean = np.append(bin_data_mean, np.mean(bin_data))\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = []\nbin_data_mean = []\nfor row in data:\n    bins = np.split(row, np.arange(bin_size, len(row), bin_size))\n    bins = [bin for bin in bins if len(bin) == bin_size]\n    bin_data.append(bins)\n    bin_data_mean.append([np.mean(bin) for bin in bins])\n",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\ndef smoothclamp(x):\n    if x < x_min:\n        return x_min\n    elif x > x_max:\n        return x_max\n    else:\n        return x\nresult = smoothclamp(x)\nprint(result)\n",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\nN = 5\ndef smoothclamp(x, N=5):\n    x = np.clip(x, x_min, x_max)\n    return N * x * (1 - x)\nresult = smoothclamp(x, N=N)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\n# Circular cross-correlation\nresult = np.correlate(a, b, mode='full')\nresult = np.roll(result, len(result)//2)\nresult = result[:len(a)]\n# Circular auto-correlation\nresult = np.correlate(a, a, mode='full')\nresult = np.roll(result, len(result)//2)\nresult = result[:len(a)]\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('colu'))\nresult = df.to_numpy().reshape(4, 15, 5)\n",
        "\nresult = df.to_numpy().reshape((15, 4, 5))\n",
        "\nresult = np.unpackbits(a.astype(np.uint8)).reshape(-1, m)\n",
        "\ndef int_to_binary(num, m):\n    binary = bin(num)[2:].zfill(m)\n    return np.array([int(bit) for bit in binary])\nresult = np.array([int_to_binary(num, m) for num in a])\n",
        "\ndef int_to_binary(num, m):\n    binary = np.zeros(m, dtype=int)\n    for i in range(m):\n        binary[m-1-i] = num % 2\n        num //= 2\n    return binary\nresult = np.array([int_to_binary(num, m) for num in a])\n",
        "\n# [Missing Code]\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 2*std, mean + 2*std)\n",
        "\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3*std, mean + 3*std)\n    ",
        "\n# [Missing Code]\n",
        "\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\nprob = np.percentile(masked_data, percentile)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[1, :] = 0\na[:, 0] = 0\n",
        "\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(a.shape[0]), np.argmax(a, axis=1)] = True\n",
        "\nmask = np.zeros_like(a, dtype=bool)\nmask[np.argmin(a, axis=1)] = True\n",
        "\nimport scipy.stats as stats\nresult = stats.pearsonr(post, distance)\n",
        "\nresult = np.zeros((X.shape[1], X.shape[0], X.shape[0]))\nfor i in range(X.shape[1]):\n    result[i] = np.dot(X[:, i][:, np.newaxis], X[:, i][np.newaxis, :])\n",
        "\nX = np.zeros((4,3))\nfor i in range(4):\n    for j in range(3):\n        for k in range(3):\n            X[i][j] += Y[i][j][k]\n",
        "\nis_contained = np.any(a == number)\n",
        "\nC = np.setdiff1d(A, B)\n",
        "\nC = np.array([x for x in A if x in B])\n",
        "\nC = A[(A >= B[0]) & (A <= B[1])]\n",
        "\nresult = rankdata(a, method='max').astype(int)\nresult = len(a) - result + 1\n",
        "\nresult = rankdata(a, method='ordinal')\nresult = np.argsort(result)\nresult = result.astype(int)\nresult = result[::-1]\n",
        "\n    ranks = rankdata(a, method='dense')\n    result = np.max(ranks) - ranks + 1\n    ",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\nresult = a[:, second, third]\n",
        "\narr = np.zeros((20, 10, 10, 2))\n",
        "\nl1 = X.sum(axis=1)\nresult = X / l1.reshape(5,1)\n",
        "\nx = np.linalg.norm(X, axis=1)\nresult = X / x[:, np.newaxis]\n",
        "\nx = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = X / x[:, None]\n",
        "\nconditions = [df['a'].str.contains(target)]\nresult = np.select(conditions, choices, default='')\n",
        "\nfrom scipy.spatial.distance import cdist\nresult = cdist(a, a, 'euclidean')\n",
        "\nresult = squareform(pdist(a))\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\nfrom scipy.spatial.distance import pdist, squareform\nresult = squareform(pdist(a))\nprint(result)\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA.astype(float), axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nA = [np.inf if x == 'inf' else float(x) for x in A]\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nfor i in range(len(A)):\n    NA[i] = eval(A[i])\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nresult = a[np.concatenate(([True], np.not_equal(a[1:], a[:-1])))]\n",
        "\nresult = a[np.insert(np.diff(a.ravel()).nonzero()[0] + 1, 0, 0)]\n",
        "\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n",
        "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ",
        "\nimport numpy as np\nimport pandas as pd\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n# Create a pandas dataframe with columns 'lat', 'lon', 'val'\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n# Add a column 'maximum' with the maximum value of each row\ndf['maximum'] = df[['lat', 'lon', 'val']].max(axis=1)\n# Reorder the rows based on the positions in each array\ndf = df.sort_index(axis=0)\n# Reset the index to start from 0\ndf = df.reset_index(drop=True)\n# Output the dataframe\nprint(df)\n",
        "\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        result.append(a[i:i+size[0], j:j+size[1]])\n",
        "\nresult = []\nfor i in range(a.shape[0] - size[0] + 1):\n    for j in range(a.shape[1] - size[1] + 1):\n        result.append(a[i:i+size[0], j:j+size[1]])\n",
        "\nresult = np.nan_to_num(np.mean(a))\n",
        "\n    mask = np.isfinite(a)\n    result = np.mean(a[mask])\n    ",
        "\nresult = Z[..., -1:]\n",
        "\nresult = a[-1:, ...]\n",
        "\nresult = False\nfor cnt in CNTS:\n    if np.array_equal(c, cnt):\n        result = True\n        break\n",
        "\nimport numpy as np\ndef is_member(arr, lst):\n    for item in lst:\n        if np.array_equal(arr, item):\n            return True\n    return False\nc = np.array([[[ 75, 763],\n              [ 57, 763],\n              [ np.nan, 749],\n              [ 75, 749]]])\nCNTS = [np.array([[[  np.nan, 1202],\n                  [  63, 1202],\n                  [  63, 1187],\n                  [  78, 1187]]]),\n        np.array([[[ 75, 763],\n                  [ 57, 763],\n                  [ np.nan, 749],\n                  [ 75, 749]]]),\n        np.array([[[ 72, 742],\n                  [ 58, 742],\n                  [ 57, 741],\n                  [ 57, np.nan],\n                  [ 58, 726],\n                  [ 72, 726]]]),\n        np.array([[[ np.nan, 194],\n                  [ 51, 194],\n                  [ 51, 179],\n                  [ 66, 179]]])]\nresult = is_member(c, CNTS)\nprint(result)\n",
        "\nf = intp.interp2d(np.arange(2), np.arange(2), a, kind='linear')\nresult = f(x_new, y_new)\n",
        "\nimport pandas as pd\nimport numpy as np\ndata = {'D':[2015,2015,2015,2015,2016,2016,2016,2017,2017,2017], 'Q':np.arange(10)}\nname= 'Q_cum'\ndf = pd.DataFrame(data)\ndf[name] = df.groupby('D')['Q'].apply(lambda x: np.cumsum(x))\nprint(df)\n",
        "\ni_diag = np.diag(i)\n",
        "\na[1:, :-1] = 0\na[:-1, 1:] = 0\n",
        "\nseries = pd.date_range(start=t0, end=tf, periods=n)\nresult = pd.DatetimeIndex(series)\n",
        "\nresult = np.where((x == a) & (y == b))[0][0]\n",
        "\nresult = np.where((x == a) & (y == b))\n",
        "\na, b, c = np.polyfit(x, y, 2)\nresult = [a, b, c]\n",
        "\ncoefficients = np.polyfit(x, y, degree)\nresult = np.flip(coefficients)\n",
        "\ndef subtract_arr(x):\n    return x - a[x.name]\ndf = df.apply(subtract_arr)\n",
        "\nresult = np.einsum('ij,kjl->kil', B, A)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(-1, 1))\n",
        "\nresult = MinMaxScaler().fit_transform(arr)\n",
        "\nresult = []\nfor i in range(a.shape[0]):\n    scaler = MinMaxScaler()\n    scaler.fit(a[i].reshape(-1, 1))\n    result.append(scaler.transform(a[i].reshape(-1, 1)).reshape(a[i].shape))\nresult = np.array(result)\n",
        "\narr_temp = arr.copy()\nmask = arr_temp < -10\nmask2 = arr_temp < 15\narr[mask] = 0\narr[mask2] = arr_temp[mask2] + 5\narr[~mask2] = 30\n",
        "\nimport numpy as np\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\nfor i in range(5):\n    arr_temp = arr[i].copy()\n    mask = arr_temp < n1[i]\n    mask2 = arr_temp < n2[i]\n    mask3 = mask ^ mask2\n    arr[i][mask] = 0\n    arr[i][mask3] = arr_temp[mask3] + 5\n    arr[i][~mask2] = 30\nprint(arr)\n",
        "\nresult = np.nonzero(s1 != s2)[0].shape[0]\n",
        "\nresult = np.count_nonzero(np.isnan(s1) != np.isnan(s2))\n",
        "\nresult = True\nfor i in range(len(a)-1):\n    if not np.array_equal(a[i], a[i+1]):\n        result = False\n        break\n",
        "\nresult = all(np.isnan(arr).all() for arr in a)\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), mode='constant', constant_values=element)\n",
        "\n    result = np.zeros(shape)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n    ",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\na = a.reshape(3, 4)\n",
        "\nresult = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        result[i, j] = a[i, j, b[i, j]]\n",
        "\nresult = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        result[i, j] = a[i, j, b[i, j]]\n",
        "\nresult = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        result[i, j] = a[i, j, b[i, j]]\n",
        "\nresult = np.sum(a[np.arange(b.shape[0]), np.arange(b.shape[1]), b], axis=-1)\n",
        "\nresult = 0\nfor i in range(b.shape[0]):\n    for j in range(b.shape[1]):\n        result += a[i, j, b[i, j]]\n",
        "\nresult = df.loc[df['a'].between(1, 4, inclusive=True), 'b']\nresult = result.fillna(np.nan)\n",
        "\nresult = np.array([])\nfor i in range(1, len(im)-1):\n    row = np.array([])\n    for j in range(1, len(im[0])-1):\n        if im[i][j] != 0:\n            row = np.append(row, im[i][j])\n    result = np.append(result, row)\nresult = result.reshape(len(im)-2, len(im[0])-2)\n",
        "\nresult = A[np.any(A, axis=1), :][:, np.any(A, axis=0)]\n",
        "\nresult = np.zeros((4, 5), dtype=int)\nresult[1:4, 1:5] = im[1:4, 1:5]\n",
        "\nresult = np.array([row[row != 0] for row in im if any(row)])\n"
    ],
    "Matplotlib": [
        "\nplt.scatter(x, y, label='x-y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n",
        "\nplt.minorticks_on()\nplt.grid(which='minor', axis='y', linestyle='--')\n",
        "\nax = plt.gca()\nax.minorticks_on()\n",
        "\nax = plt.gca()\nax.minorticks_on()\n",
        "\nstyles = ['-', '--', '-.', ':']\nfor style in styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, style, label=style)\nplt.legend()\nplt.show()\n",
        "\nstyles = ['-', '--', '-.', ':']\nfor style in styles:\n    y = np.random.rand(10)\n    plt.plot(x, y, style, label=style)\nplt.legend()\nplt.show()\n",
        "\nplt.plot(x, y, 'o-')\nplt.show()\n",
        "\nplt.plot(x, y, 'D', markersize=10, linewidth=3)\n",
        "\nsns.set_style(\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n# set the y axis limit to be 0 to 40\nax.set_ylim(0, 40)\n",
        "\nx = 10 * np.random.randn(10)\nplt.plot(x)\n# highlight in red the x range 2 to 4\nplt.axvspan(2, 4, color='red', alpha=0.5)\nplt.show()\n",
        "\n# draw a full line from (0,0) to (1,2)\nplt.plot([0, 1], [0, 2])\nplt.show()\n",
        "\n# draw a line segment from (0,0) to (1,2)\nplt.plot([0, 1], [0, 2])\nplt.show()\n",
        "\nsns_plot = seaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df)\nsns_plot.set(xlabel=\"Height (cm)\", ylabel=\"Weight (kg)\", title=\"Relation Plot of Height and Weight by Gender\")\nsns_plot.fig.set_size_inches(10, 6)\n",
        "\nsns.set_style(\"darkgrid\")\nplt.plot(x, y)\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.title(\"Regular Matplotlib Style Plot\")\nplt.show()\n",
        "\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n",
        "\nplt.plot(x, y, '+', linewidth=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', fontsize=20)\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30)\n# set the face color of the markers to have an alpha (transparency) of 0.2\nl.set_facecolor('r')\nl.set_alpha(0.2)\nplt.show()\n",
        "\n",
        "\nl.set_color('red')\nl.set_markerfacecolor('red')\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels clockwise by 45 degrees\nax = plt.gca()\nax.set_xticklabels(ax.get_xticks(), rotation=45)\nplt.show()\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels counter clockwise by 45 degrees\nax = plt.gca()\nax.set_xticklabels(ax.get_xticks(), rotation=45, ha='right')\nplt.show()\n",
        "\nplt.xticks(np.arange(0, 2 * np.pi + 1, 2))\n",
        "\n",
        "\nplt.imshow(H, cmap='viridis', interpolation='nearest')\nplt.colorbar()\nplt.show()\n",
        "\nH = np.random.randn(10, 10)\n# show the 2d array H in black and white\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n# set xlabel as \"X\"\n# put the x label at the right end of the x axis\nplt.xlabel(\"X\")\nplt.xlabel(\"X\", loc=\"right\")\nplt.plot(x, y)\nplt.show()\n",
        "\ndf = sns.load_dataset(\"planets\")\ng = sns.boxplot(x=\"method\", y=\"orbital_period\", data=df)\n# rotate the x axis labels by 90 degrees\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.show()\n",
        "\nmyTitle = textwrap.fill(myTitle, width=30)\n",
        "\ny = -1 * y\n",
        "\nx = np.random.randn(10)\ny = x\nplt.scatter(x, y)\n# put x ticks at 0 and 1.5 only\nplt.xticks([0, 1.5])\nplt.show()\n",
        "\nx = np.random.randn(10)\ny = x\nplt.scatter(x, y)\n# put y ticks at -1 and 1 only\nax = plt.gca()\nax.set_yticks([-1, 1])\nplt.show()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n# plot x, then y then z, but so that x covers y and y covers z\nplt.fill_between(x, y, color='blue', alpha=0.5)\nplt.fill_between(y, z, color='green', alpha=0.5)\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolor='black', facecolor='blue')\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n# make all axes ticks integers\nplt.xticks(np.arange(min(x), max(x)+1, 1.0))\nplt.yticks(np.arange(min(y), max(y)+1, 1.0))\nplt.plot(x, y)\nplt.show()\n",
        "\nformatter = mticker.ScalarFormatter()\nformatter.set_scientific(False)\nplt.gca().yaxis.set_major_formatter(formatter)\n",
        "\nax = sns.lineplot(x=x, y=y, linestyle='--')\n",
        "\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n# plot x vs y1 and x vs y2 in two subplots, sharing the x axis\nfig, axs = plt.subplots(2, sharex=True)\naxs[0].plot(x, y1)\naxs[1].plot(x, y2)\nplt.show()\n",
        "\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n# plot x vs y1 and x vs y2 in two subplots\n# remove the frames from the subplots\nfig, ax = plt.subplots(2, 1, figsize=(10, 8))\nax[0].plot(x, y1)\nax[0].set_title('Sine Wave')\nax[0].set_xlabel('X')\nax[0].set_ylabel('Y1')\nax[0].spines['top'].set_visible(False)\nax[0].spines['right'].set_visible(False)\nax[1].plot(x, y2)\nax[1].set_title('Cosine Wave')\nax[1].set_xlabel('X')\nax[1].set_ylabel('Y2')\nax[1].spines['top'].set_visible(False)\nax[1].spines['right'].set_visible(False)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.sin(x)\ndf = pd.DataFrame({\"x\": x, \"y\": y})\nsns.lineplot(x=\"x\", y=\"y\", data=df)\n# remove x axis label\nsns.lineplot(x=\"x\", y=\"y\", data=df, xlabel=\"\")\n",
        "\nx = np.arange(10)\ny = np.sin(x)\ndf = pd.DataFrame({\"x\": x, \"y\": y})\nax = sns.lineplot(x=\"x\", y=\"y\", data=df)\n# remove x tick labels\nax.set_xticklabels([])\nplt.show()\n",
        "\nplt.xticks([3, 4])\nplt.grid(axis='x')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y', linestyle='--')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y', linestyle='--')\nplt.xticks([1, 2])\nplt.grid(axis='x', linestyle='--')\n",
        "\n",
        "\nplt.legend(loc='lower right')\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\n# Copy the previous plot but adjust the subplot padding to have enough space to display axis labels\nfig.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.15)\nplt.show()\nplt.clf()\n",
        "\nplt.legend(['Y', 'Z'])\n",
        "\ncolumn_labels = list(\"ABCD\")\nrow_labels = list(\"WXYZ\")\ndata = np.random.rand(4, 4)\nfig, ax = plt.subplots()\nheatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n# Move the x-axis of this heatmap to the top of the plot\nax.set_xlabel(\"X-axis label\")\nax.set_ylabel(\"Y-axis label\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", labelpad=20)\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.yticks(np.arange(10), np.arange(10))\nplt.ylabel('y')\nplt.xlabel('x')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.yticks(left=True, right=False)\nplt.show()\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='green')\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='g')\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\n",
        "\nplt.bar(df['celltype'], df['s1'])\nplt.bar(df['celltype'], df['s2'])\nplt.xticks(rotation=90)\nplt.xlabel('celltype')\nplt.show()\n",
        "\nplt.bar(df['celltype'], df['s1'])\nplt.bar(df['celltype'], df['s2'])\nplt.xticks(rotation=45)\nplt.xlabel('celltype')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.xticks(color=\"red\")\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.axhline(color=\"red\")\n",
        "\nplt.plot(x, y)\nplt.xticks(fontsize=10, rotation=90)\nplt.show()\n",
        "\nplt.vlines([0.22058956, 0.33088437, 2.20589566], ymin=0, ymax=1)\n",
        "\nfig, ax = plt.subplots()\nim = ax.imshow(rand_mat, cmap='hot', interpolation='nearest')\n# We want to show all ticks...\nax.set_xticks(numpy.arange(len(xlabels)))\nax.set_yticks(numpy.arange(len(ylabels)))\n# ... and label them with the respective list entries\nax.set_xticklabels(xlabels)\nax.set_yticklabels(ylabels)\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n# Loop over data dimensions and create text annotations.\nfor i in range(len(xlabels)):\n    for j in range(len(ylabels)):\n        text = ax.text(j, i, rand_mat[i, j], ha=\"center\", va=\"center\", color=\"w\")\n# Invert the y-axis\nax.invert_yaxis()\n# Set the x-axis label\nax.set_xlabel(\"X-axis Label\")\n# Set the y-axis label\nax.set_ylabel(\"Y-axis Label\")\n# Set the title\nax.set_title(\"Heatmap Title\")\n# Turn off the grid\nax.grid(False)\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\n# Add legend for all three curves\nfig.legend(loc=\"upper right\")\nplt.show()\nplt.clf()\n",
        "\nfig, axs = plt.subplots(1, 2)\naxs[0].plot(x, y)\naxs[0].set_title(\"Y\")\naxs[1].plot(x, y)\naxs[1].set_title(\"Y\")\nplt.show()\n",
        "\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, s=30)\n",
        "\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n# make scatter plot of a over b and annotate each data point with correspond numbers in c\nplt.scatter(b, a)\nfor i, txt in enumerate(c):\n    plt.annotate(txt, (b[i], a[i]))\nplt.xlabel('b')\nplt.ylabel('a')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", prop={'weight': 'bold'})\nplt.show()\n",
        "\nplt.hist(x, edgecolor='black', linewidth=1.2)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\nplt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.hist([x, y], label=['x', 'y'], alpha=0.5)\nax.legend()\nplt.show()\n",
        "\n# calculate the slope of the line\nm = (d - b) / (c - a)\n# calculate the y-intercept of the line\nb = b - m * a\n# calculate the x and y values for the line\nx = [0, 5]\ny = [m * x[0] + b, m * x[1] + b]\n# plot the line\nplt.plot(x, y)\n# set the xlim and ylim to be between 0 and 5\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n# show the plot\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nim1 = ax1.imshow(x, cmap='viridis')\nim2 = ax2.imshow(y, cmap='magma')\nfig.colorbar(im1, ax=ax1)\nfig.colorbar(im2, ax=ax2)\nplt.show()\n",
        "\nplt.plot(x[:,0], label='a')\nplt.plot(x[:,1], label='b')\nplt.legend()\nplt.show()\n",
        "\nfig, axs = plt.subplots(2)\naxs[0].plot(x, y)\naxs[0].set_title('Y over X')\naxs[1].plot(a, z)\naxs[1].set_title('Z over A')\nfig.suptitle('Y and Z')\nplt.show()\n",
        "\nx = [p[0] for p in points]\ny = [p[1] for p in points]\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title('y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n",
        "\nax.set_xticks(np.arange(10))\nax.set_yticks(np.arange(10))\nax.set_xticklabels(np.arange(1, 11))\nax.set_yticklabels(np.arange(1, 11))\n",
        "\nlines = [[(0, 1), (1, 1)], [(2, 3), (3, 3)], [(1, 2), (1, 3)]]\nc = np.array([(1, 0, 0, 1), (0, 1, 0, 1), (0, 0, 1, 1)])\n# Plot line segments according to the positions specified in lines\n# Use the colors specified in c to color each line segment\nfor i, line in enumerate(lines):\n    plt.plot(line, color=c[i])\nplt.show()\n",
        "\nplt.loglog(x, y)\nplt.xticks([1, 10, 100])\nplt.yticks([1, 10, 100])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nfor col in df.columns:\n    ax.plot(df.index, df[col], label=col)\n    ax.scatter(df.index, df[col])\nax.legend()\nplt.show()\n",
        "\n# Make a histogram of data\nplt.hist(data, bins=np.arange(0, 21000, 1000))\n# Renormalize the data to sum up to 1\ndata = data / np.sum(data)\n# Format the y tick labels into percentage and set y tick labels as 10%, 20%, etc.\ny_ticks = np.arange(0, 1.1, 0.1)\ny_tick_labels = [str(int(tick * 100)) + '%' for tick in y_ticks]\nplt.yticks(y_ticks, y_tick_labels)\n# Set y tick labels as 10%, 20%, etc.\nplt.ylabel('Frequency (%)')\n# Set x tick labels as 0, 1000, 2000, etc.\nx_ticks = np.arange(0, 21000, 1000)\nx_tick_labels = [str(tick) for tick in x_ticks]\nplt.xticks(x_ticks, x_tick_labels)\n# Set x label\nplt.xlabel('Value')\n# Show the plot\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markersize=5, markerfacecolor='r', markeredgecolor='k', markeredgewidth=1, alpha=0.5, linewidth=1)\n",
        "\nfig, ax = plt.subplots(1, 2)\nax[0].plot(x, y, label='y')\nax[1].plot(z, a, label='a')\nfig.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2)\nplt.show()\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n# Make 2 subplots.\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\n# Do not share y axix for the subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nfig, ax = plt.subplots(1, 1)\nplt.xlim(1, 10)\nplt.xticks(range(1, 10))\nax.plot(y, x)\n# change the second x axis tick label to \"second\" but keep other labels in numerical\nax.set_xticklabels([''] + [str(i) for i in range(2, 10)] + ['second'])\nplt.show()\n",
        "\nplt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n",
        "\nplt.xticks(np.append(plt.xticks()[0], [2.1, 3, 7.6]))\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.xticks(rotation=60, ha='center')\nplt.yticks(rotation=-60)\n",
        "\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y)\n# Remove the margin before the first xtick but use greater than zero margin for the yaxis\nplt.margins(x=0, y=0.1)\nplt.show()\n",
        "\nplt.margins(x=0.1)\nplt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
        "\nfig, ax = plt.subplots(1, 2)\nax[0].plot(x, y)\nax[0].set_title('Subplot 1')\nax[1].plot(x, y)\nax[1].set_title('Subplot 2')\nplt.suptitle('Figure')\nplt.show()\n",
        "\nplt.plot(df)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n",
        "\nplt.scatter(x, y, marker='o', hatch='//', edgecolor='black', linewidth=1)\n",
        "\nplt.scatter(x, y, edgecolor='none', hatch='//')\n",
        "\nplt.scatter(x, y, marker='o', hatch='*')\n",
        "\nplt.scatter(x, y, s=100, hatch='*x')\n",
        "\nplt.imshow(data, cmap='hot', interpolation='nearest')\nplt.xlim(1, 5)\nplt.ylim(1, 4)\nplt.colorbar()\nplt.show()\n",
        "\nx = np.linspace(0.1, 2 * np.pi, 41)\ny = np.exp(np.sin(x))\n# make a stem plot of y over x and set the orientation to be horizontal\nplt.stem(x, y, use_line_collection=True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Stem Plot of y over x')\nplt.show()\n",
        "\nplt.bar(d.keys(), d.values(), color=[c[key] for key in d.keys()])\nplt.xticks(d.keys())\nplt.show()\n",
        "\nplt.axvline(x=3, color='k', linestyle='-', label='cutoff')\nplt.legend()\n",
        "\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection='polar')\nax.bar(labels, height, color='blue', alpha=0.5)\nax.set_title('Polar Bar Plot')\n",
        "\nfig, ax = plt.subplots()\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.4), startangle=90)\nax.legend(wedges, l,\n          title=\"Pie Chart\",\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1))\nplt.show()\n",
        "\nplt.plot(x, y, 'b--')\nplt.grid(True, which='both', color='blue', linestyle='--')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', color='gray', linestyle='--', linewidth=0.5)\nplt.grid(False)\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.legend(loc='upper left')\nplt.title(\"Pie Chart\")\nfor i, l in enumerate(labels):\n    plt.text(1, i, l, horizontalalignment='center', verticalalignment='center', fontsize=14, fontweight='bold')\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.legend(loc='upper left')\nplt.title(\"Pie Chart\")\nfor i, l in enumerate(labels):\n    plt.text(1, i, l, horizontalalignment='center', verticalalignment='center', fontsize=14, fontweight='bold')\nplt.show()\n",
        "\nplt.plot(x, y, 'o-', mfc='none', mec='black')\n",
        "\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\nsns.distplot(df[\"bill_length_mm\"], color=\"blue\")\n# Plot a vertical line at 55 with green color\nplt.axvline(x=55, color=\"green\")\nplt.show()\n",
        "\n# Create a list of x-axis values\nx = np.arange(len(blue_bar))\n# Create a bar plot with blue bars\nplt.bar(x, blue_bar, color='blue')\n# Create a bar plot with orange bars\nplt.bar(x, orange_bar, color='orange', bottom=blue_bar)\n# Set the x-axis labels\nplt.xticks(x, ['A', 'B', 'C'])\n# Set the y-axis label\nplt.ylabel('Value')\n# Set the title of the plot\nplt.title('Blue and Orange Bars')\n# Display the plot\nplt.show()\n",
        "\nfig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='y')\naxs[0].plot(x, z, label='z')\naxs[0].legend()\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y and z')\naxs[1].plot(a, z, label='z')\naxs[1].set_xlabel('a')\naxs[1].set_ylabel('z')\nplt.show()\n",
        "\ncmap = plt.cm.get_cmap('Spectral')\nscatter = plt.scatter(x, y, c=y, cmap=cmap)\ncbar = plt.colorbar(scatter)\ncbar.set_label('y-value')\nplt.xlabel('x-value')\nplt.ylabel('y-value')\nplt.title('Scatter plot of y over x')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(min(x), max(x)+1, 1.0))\nplt.show()\n",
        "\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=False)\nfor i, species in enumerate(df[\"species\"].unique()):\n    sns.barplot(x=\"sex\", y=\"bill_length_mm\", data=df[df[\"species\"] == species], ax=axes[i])\n    axes[i].set_title(species)\nplt.tight_layout()\nplt.show()\n",
        "\ncircle = plt.Circle((0.5, 0.5), 0.2, color='r')\nplt.gca().add_patch(circle)\nplt.axis('scaled')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\phi$', fontsize=18, fontweight='bold')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1), borderaxespad=0.1)\n",
        "\nplt.plot(x, y, label='Line')\nplt.legend(handlelength=0.3)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y, label=\"Line\")\nplt.plot(y, x, label=\"Flipped\")\n# Show a two columns legend of this plot\nplt.legend(ncol=2)\nplt.show()\n",
        "\nplt.legend()\nplt.plot(x, y, marker=\"o\", label=\"Markers\")\n",
        "\nplt.imshow(data, cmap='viridis', interpolation='nearest')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Figure 1\", fontdict={'fontname': 'Arial', 'fontweight': 'bold', 'fontsize': 16})\nplt.show()\n",
        "\ndf = pd.DataFrame(\n    {\n        \"id\": [\"1\", \"2\", \"1\", \"2\", \"2\"],\n        \"x\": [123, 22, 356, 412, 54],\n        \"y\": [120, 12, 35, 41, 45],\n    }\n)\n# Use seaborn to make a pairplot of data in `df` using `x` for x_vars, `y` for y_vars, and `id` for hue\n# Hide the legend in the output figure\nsns.pairplot(data=df, x_vars='x', y_vars='y', hue='id', legend=False)\nplt.show()\n",
        "\nplt.plot(y, x)\nplt.gca().invert_xaxis()\nplt.show()\n",
        "\nplt.scatter(x, y)\nplt.axis('equal')\nplt.gca().set_clip_on(False)\n",
        "\nplt.scatter(x, y, color='red', edgecolor='black')\n",
        "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor i in range(2):\n    for j in range(2):\n        axs[i, j].plot(x, y)\n",
        "\nplt.hist(x, bins=5, range=(0, 10), width=2)\nplt.show()\n",
        "\nplt.plot(x, y, color='blue', label='y')\nplt.fill_between(x, y-error, y+error, alpha=0.2, color='gray', label='error')\nplt.legend()\nplt.show()\n",
        "\nxvec = np.linspace(-5.0, 5.0, 100)\nx, y = np.meshgrid(xvec, xvec)\nz = -np.hypot(x, y)\nplt.contourf(x, y, z)\n# draw x=0 and y=0 axis in my contour plot with white color\nplt.axhline(0, color='white')\nplt.axvline(0, color='white')\nplt.show()\n",
        "\nbox_position, box_height, box_errors = np.arange(4), np.ones(4), np.arange(1, 5)\nc = [\"r\", \"r\", \"b\", \"b\"]\nfig, ax = plt.subplots()\nax.bar(box_position, box_height, color=\"yellow\")\n# Plot error bars with errors specified in box_errors. Use colors in c to color the error bars\nax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\nplt.show()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title(\"Y\", fontsize=18)\naxs[1].plot(a, z)\naxs[1].set_title(\"Z\", fontsize=20)\nplt.tight_layout()\nplt.show()\n",
        "\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].tick_params(labelbottom=True, labelleft=True)\nplt.tight_layout()\n",
        "\nplt.matshow(d)\nplt.show()\n",
        "\nax = plt.axes([0, 0, 1, 1])\nax.axis(\"off\")\ntable = ax.table(cellText=df.values, colLabels=df.columns, loc=\"center\")\ntable.auto_set_font_size(False)\ntable.set_fontsize(14)\n",
        "\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xticks(x)\nax.set_xticklabels(x)\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(x)\nplt.tick_params(axis='x', which='both', top=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(x)\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True)\nplt.show()\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", kind=\"scatter\", data=df)\ng.set(xlabel=\"Time\", ylabel=\"Pulse\")\ng.axes[0][0].set_title(\"Group: Fat\")\ng.axes[0][1].set_title(\"Group: No Fat\")\n",
        "\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", kind=\"scatter\", data=df)\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\nplt.show()\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\", height=4, aspect=1, sharey=False)\ng.set(ylabel=None)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.plot(t, a, label='sin(t)')\nax.plot(t, b, label='cos(t)')\nax.plot(t, c, label='sin(t) + cos(t)')\nax.set_xlabel('t')\nax.set_ylabel('y')\nax.set_title('sin(t), cos(t), sin(t) + cos(t)')\nax.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=True, dodge=True)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Bill Length (mm)\")\nplt.title(\"Stripplot for Penguin Bill Length\")\n",
        "\ng = sns.FacetGrid(df, row=\"b\", hue=\"b\", aspect=1.5)\ng.map(sns.pointplot, \"a\", \"c\", ci=None)\nfor ax in g.axes.flat:\n    ax.set_xticks(np.arange(1, 31, 2))\n    ax.set_xticklabels(np.arange(2, 31, 2))\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(100, 50)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n",
        "\ngs = gridspec.GridSpec(nrow, ncol)\ngs.update(wspace=0, hspace=0)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = plt.subplot(gs[i, j])\n        ax.imshow(x, cmap='gray')\n        ax.axis('off')\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(my_map_func(x)))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n",
        "\n    result = []\n    for i in input:\n        result.extend([i, i+1, i+2])\n    return result\n    ",
        "\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n# Create a tensor of 1s with the same shape as lengths\nones = tf.ones(shape=tf.shape(lengths), dtype=tf.int32)\n# Pad the tensor of 1s with 0s to a total length of 8\npadded_ones = tf.pad(ones, paddings=[[0, 0], [0, 4-tf.shape(ones)[1]]], constant_values=0)\n# Create a tensor of 0s with the same shape as lengths\nzeros = tf.zeros(shape=tf.shape(lengths), dtype=tf.int32)\n# Pad the tensor of 0s with 1s to a total length of 8\npadded_zeros = tf.pad(zeros, paddings=[[0, 0], [0, 4-tf.shape(zeros)[1]]], constant_values=1)\n# Combine the padded tensors of 1s and 0s\nresult = tf.concat([padded_ones, padded_zeros], axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n# Create a tensor of 1s with the same shape as lengths\nones = tf.ones(shape=tf.shape(lengths), dtype=tf.int32)\n# Create a tensor of zeros with the same shape as lengths\nzeros = tf.zeros(shape=tf.shape(lengths), dtype=tf.int32)\n# Create a tensor of 1s with the same shape as lengths, but with the maximum length\nmax_length = tf.reduce_max(lengths)\nmax_ones = tf.ones(shape=tf.shape(lengths), dtype=tf.int32) * max_length\n# Create a tensor of 1s with the same shape as lengths, but with the maximum length padded by 1s\nmax_ones_padded = tf.pad(max_ones, paddings=tf.constant([[0, 0], [0, 1]]), constant_values=1)\n# Create a tensor of 1s with the same shape as lengths, but with the maximum length padded by 1s and then truncated to the maximum length\nresult = tf.where(tf.less(lengths, max_length), zeros, ones) * max_ones_padded\nprint(result)\n",
        "\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n# Create a tensor of 1s and 0s with the same shape as lengths\nones = tf.ones(shape=lengths, dtype=tf.int32)\n# Pad the tensor with 0s in front\npadded_ones = tf.pad(ones, paddings=[[0, 0], [4, 0]], constant_values=0)\n# Create a tensor of 1s and 0s with the same shape as lengths\nzeros = tf.zeros(shape=lengths, dtype=tf.int32)\n# Pad the tensor with 0s in front\npadded_zeros = tf.pad(zeros, paddings=[[0, 0], [4, 0]], constant_values=0)\n# Combine the padded ones and zeros\nresult = tf.cast(tf.concat([padded_zeros, padded_ones], axis=1), dtype=tf.float32)\nprint(result)\n",
        "\n    max_length = tf.reduce_max(lengths)\n    mask = tf.sequence_mask(lengths, maxlen=max_length)\n    padded_mask = tf.pad(mask, paddings=tf.constant([[0, 0], [0, 8 - max_length]]))\n    result = tf.cast(padded_mask, tf.int32)\n    ",
        "\nresult = tf.constant([1] * 8)\nfor i, length in enumerate(lengths):\n    result = tf.tensor_scatter_nd_update(result, [[i, length - 1]], tf.zeros(length - 1))\n",
        "\nresult = tf.stack(tf.meshgrid(a, b, indexing='ij'), axis=-1)\n",
        "\n    a = tf.expand_dims(a, axis=-1)\n    b = tf.expand_dims(b, axis=0)\n    result = tf.matmul(a, b)\n    ",
        "\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\nresult = tf.expand_dims(a, axis=-2)\n",
        "\nresult = tf.expand_dims(a, axis=0)\nresult = tf.expand_dims(result, axis=-2)\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n",
        "\n    lhs = tf.square(tf.subtract(A, B))\n    result = tf.reduce_sum(lhs, axis=1)\n    ",
        "\nm = tf.gather_nd(x, tf.stack([y, z], axis=-1))\n",
        "\nm = tf.gather(x, row, axis=0)\nm = tf.gather(m, col, axis=1)\n",
        "\n    m = tf.gather_nd(x, tf.stack([y, z], axis=-1))\n    ",
        "\nC = tf.einsum('bij,bjk->bik', A, B)\n",
        "\nresult = tf.einsum('bns,bnk->bnk', A, B)\n",
        "\nresult = tf.constant(x)\n",
        "\n    result = tf.constant(x)\n    ",
        "\ndef count_nonzero(input_tensor):\n    return tf.math.count_nonzero(input_tensor, axis=-1)\ndef average_nonzero(input_tensor):\n    nonzero_count = count_nonzero(input_tensor)\n    nonzero_sum = tf.reduce_sum(input_tensor, axis=-1)\n    return nonzero_sum / nonzero_count\nresult = tf.map_fn(average_nonzero, x)\n",
        "\n# [Missing Code]\n",
        "\n    mask = tf.not_equal(x, 0)\n    masked_x = tf.where(mask, x, tf.zeros_like(x))\n    sum_x = tf.reduce_sum(masked_x, axis=-2)\n    count_x = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-2)\n    result = tf.math.divide_no_nan(sum_x, count_x)\n    ",
        "\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    result = tf.argmax(a, axis=1)\n    ",
        "\nresult = tf.argmin(a, axis=1)\n",
        "\ntf.saved_model.save(model, \"my_model\")\n",
        "\nresult = tf.random.uniform([10], minval=1, maxval=5, dtype=tf.int32)\n",
        "\nresult = tf.random.uniform([114], minval=2, maxval=6, dtype=tf.int32)\n",
        "\nimport tensorflow as tf\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform([10], minval=1, maxval=5, dtype=tf.int32)\n    return result\n",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\ndef f(x):\n    return np.log(x)\ndef g(x):\n    return A*np.log(x) + B\ndef h(x):\n    return A*np.log(x) + B\ndef j(x):\n    return A*np.log(x) + B\ndef k(x):\n    return A*np.log(x) + B\ndef l(x):\n    return A*np.log(x) + B\ndef m(x):\n    return A*np.log(x) + B\ndef n(x):\n    return A*np.log(x) + B\ndef o(x):\n    return A*np.log(x) + B\ndef p(x):\n    return A*np.log(x) + B\ndef q(x):\n    return A*np.log(x) + B\ndef r(x):\n    return A*np.log(x) + B\ndef s(x):\n    return A*np.log(x) + B\ndef t(x):\n    return A*np.log(x) + B\ndef u(x):\n    return A*np.log(x) + B\ndef v(x):\n    return A*np.log(x) + B\ndef w(x):\n    return A*np.log(x) + B\ndef x(x):\n    return A*np.log(x) + B\ndef y(x):\n    return A*np.log(x) + B\ndef z(x):\n    return A*np.log(x) + B\ndef a(x):\n    return A*np.log(x) + B\ndef b(x):\n    return A*np.log(x) + B\ndef c(x):\n    return A*np.log(x) + B\ndef d(x):\n    return A*np.log(x) + B\ndef e(x):\n    return A*np.log(x) + B\ndef f(x):\n    return A*np.log(x) + B\ndef g(x):\n    return A*np.log(x) + B\ndef h(x):\n    return A*np.log(x) + B\ndef i(x):\n    return A*np.log(x) + B\ndef j(x):\n    return A*np.log(x) + B\ndef k(x):\n    return A*np.log(x) + B\ndef l(x):\n    return A*np.log(x) + B\ndef m(x):\n    return A*np.log(x) + B\ndef n(x):\n    return A*np.log(x) + B\ndef o(x):\n    return A*np.log(x) + B\ndef p(x):\n    return A*np.log(x) + B\ndef q(x):\n    return A*np.log(x) + B\ndef r(x):\n    return A*np.log(x) + B\ndef s(x):\n    return A*np.log(x) + B\ndef t(x):\n    return A*np.log(x) + B\ndef u(x):\n    return A*np.log(x) + B\ndef v(x):\n    return A*np.log(x) + B\ndef w(x):\n    return A*np.log(x) + B\ndef x(x):\n    return A*np.log(x) + B\ndef y(x):\n    return A*np.log(x) + B\ndef z(x):\n    return A*np.log(x) + B\ndef a(x):\n    return A*np.log(x) + B\n",
        "\ndef log_polyfit(x, y, degree):\n    x_log = np.log(x)\n    coeffs = np.polyfit(x_log, y, degree)\n    return coeffs\nresult = log_polyfit(x, y, 1)\n",
        "\nimport numpy as np\nimport scipy.optimize\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\ndef func(x, A, B, C):\n    return A * np.exp(B * x) + C\npopt, pcov = scipy.optimize.curve_fit(func, x, y, p0=p0)\nresult = popt\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\ntest_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value > alpha\n",
        "\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\ndef f(x):\n    a, b, c = x\n    return ((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4\nresult = optimize.minimize(f, initial_guess, method='SLSQP')\nprint(result.x)\n",
        "\np_values = scipy.stats.norm.sf(z_scores)\n",
        "\np_values = scipy.stats.norm.sf(z_scores)\n",
        "\nz_scores = [-np.sqrt(2) * scipy.stats.norm.ppf(1 - p_values) for p in p_values]\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\nfrom scipy.stats import lognorm\nstddev = 2.0785\nmu = 1.744\ndist = lognorm(stddev, loc=0, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nresult = sa.dot(sb)\n",
        "\n    result = sA.dot(sB)\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, 'uniform')\n",
        "\n    # [Missing Code]\n    ",
        "\nresult = stats.kstest(times, 'uniform')\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n# Calculate the distance matrix\ndist_matrix = scipy.spatial.distance.cdist(points1, points2, 'euclidean')\n# Find the optimal assignment using the Hungarian algorithm\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(dist_matrix)\n# Create the result array\nresult = np.zeros(N, dtype=int)\nresult[row_ind] = col_ind\nprint(result)\n",
        "\ndef total_distance(x):\n    return np.sum(np.abs(points1[x] - points2[x]))\nresult = scipy.optimize.minimize(total_distance, np.arange(N), method='SLSQP', constraints={'type': 'eq', 'fun': lambda x: np.sum(x) == N-1})\n",
        "\nb.setdiag(0)\n",
        "\nresult = ndimage.label(img > threshold)[1]\n",
        "\nresult = ndimage.label(img < threshold)[1]\n",
        "\n    labeled_array, num_features = ndimage.label(img > threshold)\n    result = num_features\n    ",
        "\nlabeled_array, num_features = ndimage.label(img > threshold)\ncenters_of_mass = ndimage.center_of_mass(img, labeled_array, range(1, num_features+1))\ndistances = [np.sqrt((0-x[0])**2 + (0-x[1])**2) for x in centers_of_mass]\n",
        "\nM = M.tolil()\nM = M.transpose()\nM = M.tolil()\nM = M + M.transpose()\n",
        "\n    sA = sA.tolil()\n    sA = sA.transpose()\n    sA = sA.tolil()\n    sA.setdiag(0)\n    sA = sA.tocsr()\n    sA = sA + sA.transpose()\n    ",
        "\n# [Missing Code]\n",
        "\n# Fill in the missing code here\n",
        "\nmean = col.mean()\nstandard_deviation = np.sqrt(col.power(2).mean() - mean**2)\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nMedian = np.median(col.toarray().ravel())\nMode = mode(col.toarray().ravel())[0][0]\n",
        "\nfrom scipy.optimize import curve_fit\nimport numpy as np\ns = '''1.000000000000000021e-03,2.794682735905079767e+02\n4.000000000000000083e-03,2.757183469104809888e+02\n1.400000000000000029e-02,2.791403179603880176e+02\n2.099999999999999784e-02,1.781413355804160119e+02\n3.30000000000000155e-02,-2.798375517344049968e+02\n4.19999999999999567e-02,-2.770513900380149721e+02\n5.10000000000000366e-02,-2.713769422793179729e+02\n6.90000000000000577e-02,1.280740698304900036e+02\n7.79999999999999989e-02,2.800801708984579932e+02\n8.99999999999999667e-02,2.790400329037249776e+02'''.replace('\\n', ';')\narr = np.matrix(s)\nz = np.array(arr[:, 0]).squeeze()\nUa = np.array(arr[:, 1]).squeeze()\ntau = 0.045\ndegree = 15\ndef fourier_series(x, *coeffs):\n    result = 0\n    for i, coeff in enumerate(coeffs):\n        result += coeff * np.cos(i * np.pi / tau * x)\n    return result\npopt, pcov = curve_fit(fourier_series, z, Ua, maxfev=100000, p0=[1] * degree)\nprint(popt)\n",
        "\nimport numpy as np\nimport scipy.spatial.distance\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n# Convert array to raster\nraster = example_array.astype(int)\n# Define unique IDs\nunique_ids = np.unique(raster)\nunique_ids = unique_ids[unique_ids != 0]\n# Calculate distances between unique IDs\nresult = []\nfor i in range(len(unique_ids)):\n    for j in range(i+1, len(unique_ids)):\n        id1 = unique_ids[i]\n        id2 = unique_ids[j]\n        mask1 = raster == id1\n        mask2 = raster == id2\n        distance = scipy.spatial.distance.cdist(np.argwhere(mask1), np.argwhere(mask2), 'euclidean').min()\n        result.append([id1, id2, distance])\n# Output result\nresult = np.array(result)\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n    unique_ids = np.unique(example_array)\n    result = []\n    for i in range(len(unique_ids)):\n        for j in range(i+1, len(unique_ids)):\n            id1 = unique_ids[i]\n            id2 = unique_ids[j]\n            mask1 = example_array == id1\n            mask2 = example_array == id2\n            dist = scipy.spatial.distance.cdist(np.argwhere(mask1), np.argwhere(mask2), 'euclidean')\n            min_dist = np.min(dist)\n            result.append([id1, id2, min_dist])\n    ",
        "\ntck = interpolate.splrep(x, y, k = 2, s = 4)\nresult = interpolate.splev(x_val, tck, der = 0)\n",
        "\ndata = [x1, x2, x3, x4]\nstatistic, critical_values, significance_level = ss.anderson_ksamp(data)\n",
        "\nresult = ss.anderson_ksamp([x1, x2])\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\ndef tau1(x):\n    y = np.array(df['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\ndf['AC'] = pd.rolling_apply(df['C'], 3, lambda x: tau1(x))\ndf['BC'] = pd.rolling_apply(df['C'], 3, lambda x: tau1(x))\nprint(df)\n",
        "\nresult = sa.count_nonzero() == 0\n",
        "\nresult = sa.count_nonzero() == 0\n",
        "\nresult = block_diag(*a)\n",
        "\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\n    ranksums_result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = ranksums_result.pvalue\n    ",
        "\ndef kurtosis(a):\n    n = len(a)\n    m = np.mean(a)\n    s = np.std(a)\n    k = (n*np.sum((a-m)**4))/(s**4*n*(n-1)) - 3\n    return k\nkurtosis_result = kurtosis(a)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False, bias=False)\n",
        "\nresult = scipy.interpolate.interp2d(x, y, z, kind='cubic')(s, t)\n",
        "\n    # [Missing Code]\n    ",
        "\nresult = np.zeros(extraPoints.shape[0])\nfor i, extra_point in enumerate(extraPoints):\n    region_index = vor.find_simplex(extra_point)\n    result[i] = region_index\n",
        "\nresult = []\nfor extraPoint in extraPoints:\n    region_index = vor.point_region[np.where(np.all(vor.vertices == extraPoint, axis=1))[0][0]]\n    result.append(region_index)\n",
        "\npadded_vectors = [np.pad(v, (0, max_vector_size - len(v)), 'constant', constant_values=0) for v in vectors]\nresult = sparse.csr_matrix(padded_vectors)\n",
        "\nb = scipy.ndimage.median_filter(a, 3, origin=-1)\n",
        "\nresult = M[row, column]\n",
        "\nresult = []\nfor i in range(len(row)):\n    result.append(M[row[i], column[i]])\n",
        "\nnew_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[:, i, j])\n        new_array[:, i, j] = f(x_new)\n",
        "\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nprob = NormalDistro(u,o2,x)\n",
        "\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    ",
        "\nresult = sf.dct(np.eye(N), norm='ortho')\n",
        "\nresult = sparse.diags(matrix, [-1, 0, 1], (5, 5)).toarray()\n",
        "\nM = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        M[i,j] = scipy.stats.binom.pmf(j, i, p)\n",
        "\nresult = df.apply(lambda x: stats.zscore(x))\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\ny, x = np.mgrid[:shape[0], :shape[1]]\nmid = np.array([shape[0] // 2, shape[1] // 2])\nresult = distance.cdist(np.dstack((y, x)), mid)\n",
        "\ny, x = np.indices(shape)\nmid = np.array([shape[0] // 2, shape[1] // 2])\nresult = distance.cdist(np.dstack((y, x)), mid)\n",
        "\n    y, x = np.indices(shape)\n    mid = np.array([shape[0] / 2, shape[1] / 2])\n    result = distance.cdist(np.dstack((y, x)), mid)\n    ",
        "\nzoom_factor = (shape[0]/x.shape[0], shape[1]/x.shape[1])\nresult = scipy.ndimage.zoom(x, zoom_factor, order=1)\n",
        "\nimport scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\ndef objective(x):\n    return a.dot(x ** 2)\nout = scipy.optimize.minimize(objective, x0, method='SLSQP', bounds=((0, None),)*len(x0))\nprint(out.x)\n",
        "\nimport scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\ndef objective(x):\n    return a.dot(x ** 2)\nout = scipy.optimize.minimize(objective, x0, method='L-BFGS-B', bounds=tuple([(lb, None) for lb in x_lower_bounds]))\nprint(out)\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 1\ntime_span = [0, 10]\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + t-np.sin(t) if 0 < t < 2*np.pi else 2*np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0])\nresult = sol.y\nprint(result)\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\ndef dN1_dt_varying(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_varying, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nfor t in range(4):\n    def const(x):\n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\ndef f(x):\n    return 2*x*c\nresult = scipy.integrate.quad(f, low, high)\n",
        "\n    def integrand(x):\n        return 2 * x * c\n    result, error = scipy.integrate.quad(integrand, low, high)\n    ",
        "\n# [Missing Code]\n",
        "\nV = V.tocoo()\nV.data += x\n",
        "\nA = V + x\nB = A + y\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\na = a > 0\n",
        "\na = a > 0\n",
        "\ndef closest_to_centroid(data, centroids):\n    result = []\n    for i in range(centroids.shape[0]):\n        distances = scipy.spatial.distance.cdist(data, centroids[i].reshape(1, -1))\n        closest_index = np.argmin(distances)\n        result.append(closest_index)\n    return result\nresult = closest_to_centroid(data, centroids)\n",
        "\ndef closest_to_centroid(data, centroids):\n    result = []\n    for i in range(centroids.shape[0]):\n        distances = scipy.spatial.distance.cdist(data, centroids[i].reshape(1, -1))\n        closest_index = np.argmin(distances)\n        result.append(data[closest_index])\n    return np.array(result)\nresult = closest_to_centroid(data, centroids)\n",
        "\ndef get_k_closest_to_centroid(data, centroids, k):\n    result = []\n    for i in range(centroids.shape[0]):\n        distances = scipy.spatial.distance.cdist(data, centroids[i].reshape(1, -1))\n        k_closest_indices = np.argsort(distances, axis=0)[:k]\n        result.append(k_closest_indices)\n    return result\nresult = get_k_closest_to_centroid(data, centroids, k)\n",
        "\nresult = np.zeros((len(xdata), len(bdata)))\nfor i in range(len(xdata)):\n    for j in range(len(bdata)):\n        result[i,j] = fsolve(eqn, x0=0.5, args = (xdata[i], bdata[j]))\n",
        "\nresult = []\nfor i in range(len(xdata)):\n    x0 = 0.5\n    a = adata[i]\n    b = fsolve(eqn, x0, args=(xdata[i], a))\n    result.append([xdata[i], a, b])\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\n",
        "\ndef log_likelihood(weights, data):\n    return np.sum(np.log(np.dot(weights, data)))\ndef mle(data):\n    n_categories = len(data)\n    init_guess = np.ones(n_categories) / n_categories\n    result = sciopt.minimize(lambda x: -log_likelihood(x, data), init_guess, method='SLSQP')\n    return result.x\nweights = mle(a)\n",
        "\npopt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\n",
        "\nresult = signal.argrelmax(arr, order=n)\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if j < n or j >= arr.shape[1] - n:\n            continue\n        if arr[i, j] <= arr[i, j-n:j+n+1].min() and arr[i, j] <= arr[i, j-n:j+n+1].max():\n            result.append([i, j])\n",
        "\ndf = df[(np.abs(stats.zscore(df[['NUM1', 'NUM2', 'NUM3']])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "",
        "\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "\n    result = pd.DataFrame(data=np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'])\n    ",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder\ndf = pd.DataFrame({'Col1': ['C', 'A', 'B'], 'Col2': [33, 2.5, 42], 'Col3': [['Apple', 'Orange', 'Banana'], ['Apple', 'Grape'], ['Banana']]})\n# Create a list of unique elements in Col3\nunique_elements = list(set([item for sublist in df['Col3'] for item in sublist]))\n# Create a one-hot-encoded matrix for the unique elements\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(np.array(unique_elements).reshape(-1, 1))\nencoded_matrix = encoder.transform(np.array(unique_elements).reshape(-1, 1))\n# Create a new dataframe with the encoded columns\ndf_out = pd.concat([df, pd.DataFrame(encoded_matrix, columns=unique_elements)], axis=1)\n# Set the values in the encoded columns based on the presence of the elements in Col3\nfor index, row in df_out.iterrows():\n    for element in unique_elements:\n        if element in row['Col3']:\n            df_out.at[index, element] = 1\n        else:\n            df_out.at[index, element] = 0\n# Drop the original Col3 column\ndf_out = df_out.drop('Col3', axis=1)\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndef load_data():\n    data = {'Col1': ['C', 'A', 'B'], 'Col2': [33, 2.5, 42], 'Col3': [['Apple', 'Orange', 'Banana'], ['Apple', 'Grape'], ['Banana']]}\n    return pd.DataFrame(data)\ndf = load_data()\n# Convert Col3 into a list of unique names\nunique_names = set()\nfor row in df['Col3']:\n    for name in row:\n        unique_names.add(name)\n# Create a new dataframe with the unique names as columns\ndf_out = pd.DataFrame(index=df.index, columns=unique_names)\n# Fill in the new dataframe with 1s and 0s based on the presence of each name in Col3\nfor i, row in df.iterrows():\n    for name in unique_names:\n        if name in row['Col3']:\n            df_out.at[i, name] = 1\n        else:\n            df_out.at[i, name] = 0\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder\ndf = pd.DataFrame({'Col1': ['C', 'A', 'B', 'D'],\n                   'Col2': [33, 2.5, 42, 666],\n                   'Col3': [11, 4.5, 14, 1919810],\n                   'Col4': [['Apple', 'Orange', 'Banana'], ['Apple', 'Grape'], ['Banana'], ['Suica', 'Orange']]})\n# Create a one-hot encoder\nencoder = OneHotEncoder(sparse=False)\n# Convert the last column to a numpy array\nX = df['Col4'].apply(lambda x: np.array(x)).to_numpy()\n# Fit the encoder to the data\nencoder.fit(X)\n# Transform the data\nX_transformed = encoder.transform(X)\n# Create a new dataframe with the transformed data\ndf_out = pd.DataFrame(X_transformed, columns=encoder.get_feature_names())\n# Add the other columns to the new dataframe\ndf_out['Col1'] = df['Col1']\ndf_out['Col2'] = df['Col2']\ndf_out['Col3'] = df['Col3']\n# Reorder the columns\ndf_out = df_out[['Col1', 'Col2', 'Col3'] + encoder.get_feature_names()]\nprint(df_out)\n",
        "\n# Convert Col3 to a list of lists\ndf['Col3'] = df['Col3'].apply(lambda x: [x])\n# Create a one-hot encoder\nencoder = OneHotEncoder(sparse=False)\n# Fit the encoder to the data\nencoder.fit(df['Col3'])\n# Transform the data\nencoded_data = encoder.transform(df['Col3'])\n# Create new columns for each unique element\nfor i, col in enumerate(encoder.categories_[0]):\n    df[col] = encoded_data[:, i]\n# Drop the original column\ndf = df.drop('Col3', axis=1)\n",
        "\n# [Missing Code]\nunique_elements = set([item for sublist in df['Col3'] for item in sublist])\nfor element in unique_elements:\n    df[element] = df['Col3'].apply(lambda x: 1 if element in x else 0)\ndf = df.drop('Col3', axis=1)\n",
        "\n# [Missing Code]\n",
        "\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\n# Convert the sparse matrix back to a DataFrame\ntransform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n# Merge the transformed data with the original DataFrame\ndf = pd.concat([df_origin, transform_output], axis=1)\n",
        "\ndf = pd.DataFrame(transform_output.toarray())\ndf.columns = df_origin.columns\ndf.index = df_origin.index\n",
        "\n    # Convert the sparse matrix back to a dataframe\n    df_transformed = pd.DataFrame.sparse.from_spmatrix(transform_output)\n    # Merge the original dataframe with the transformed dataframe\n    result = pd.concat([df, df_transformed], axis=1)\n    ",
        "\n# [Missing Code]\nclf.steps.pop(1)\n",
        "\n# [Missing Code]\n",
        "\ndel clf.named_steps['pOly']\n",
        "\nclf.steps.insert(1, ('poly', PolynomialFeatures()))\n",
        "\nclf.steps.insert(1, ('new_step', LinearSVC()))\n",
        "\n# [Missing Code]\n",
        "\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 7],\n    'min_child_weight': [1, 3, 5]\n}\nmodel = xgb.XGBRegressor()\nGridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=1, iid=False).fit(trainX,trainY, eval_metric=\"mae\", eval_set=[(testX, testY)], early_stopping_rounds=42)\n",
        "\nparamGrid = {'max_depth': [3, 4, 5], 'min_child_weight': [1, 2, 3], 'learning_rate': [0.1, 0.2, 0.3]}\nmodel = xgb.XGBRegressor()\nGridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]}).fit(trainX, trainY)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nmodel_name = str(model).split('(')[0]\n",
        "\nmodel_name = str(model).split('(')[0]\n",
        "\nmodel_name = str(model).split('(')[0]\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\ndata = pd.DataFrame([[\"Salut comment tu vas\", \"Hey how are you today\", \"I am okay and you ?\"]]).T\ndata.columns = [\"test\"]\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\ntf_idf_out = pipe.fit_transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nclf = GridSearchCV(bc, param_grid)\nclf.fit(X_train, y_train)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nX, y, X_test = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(X_test) == np.ndarray\n# Reshape X to have two dimensions\nX = X.reshape(-1, 1)\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = regressor.predict(X_test.reshape(-1, 1))\nprint(predict)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nX, y, X_test = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(X_test) == np.ndarray\n# Reshape X to have two dimensions\nX = X.reshape(-1, 1)\nX_test = X_test.reshape(-1, 1)\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = regressor.predict(X_test)\nprint(predict)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef preprocess(s):\n    return s.upper()\ntfidf = TfidfVectorizer(preprocessor=preprocess)\nprint(tfidf.preprocessor)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef prePro(text):\n    return text.lower()\ntfidf = TfidfVectorizer(preprocessor=prePro)\nprint(tfidf.preprocessor)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns)\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ncolumn_names = X.columns\n",
        "\ncolumn_names = X.columns[model.get_support()]\n",
        "\ncolumn_names = X.columns\n",
        "\ncolumn_names = X.columns\n",
        "\nclosest_50_samples = []\nfor i in range(len(km.cluster_centers_)):\n    distances = np.linalg.norm(X - km.cluster_centers_[i], axis=1)\n    closest_50_samples.append(X[np.argsort(distances)[:50]])\n",
        "\nkm.fit(X)\nclosest_50_samples = km.cluster_centers_[p-1]\n",
        "\nclosest_100_samples = []\nfor i in range(len(km.cluster_centers_)):\n    distances = np.linalg.norm(X - km.cluster_centers_[i], axis=1)\n    closest_100_samples.append(X[np.argsort(distances)[:100]])\n",
        "\n    center = km.cluster_centers_[p]\n    distances = np.linalg.norm(X - center, axis=1)\n    closest_50_indices = np.argsort(distances)[:50]\n    closest_50_samples = X[closest_50_indices]\n    ",
        "\n# [Missing Code]\n",
        "\nX_train = pd.get_dummies(X_train, columns=[0])\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nsvm = SVC(kernel='rbf', gamma='auto')\nsvm.fit(X_train, y_train)\npredict = svm.predict(X_test)\nprint(predict)\n",
        "\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n# fit, then predict X\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nsvm = SVC(kernel='rbf', gamma='scale')\nsvm.fit(X_scaled, y)\npredict = svm.predict(X_scaled)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\n# fit, then predict X\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nsvr = SVR(kernel='poly', degree=2, C=1.0, epsilon=0.1)\nsvr.fit(X_poly, y)\npredict = svr.predict(X_poly)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nsvr = SVR(kernel='poly', degree=2, C=1.0, epsilon=0.1)\nsvr.fit(X_poly, y)\npredict = svr.predict(X_poly)\n",
        "\nquery_tfidf_matrix = tfidf.transform(queries)\ncosine_similarities_of_queries = cosine_similarity(query_tfidf_matrix, tfidf_matrix)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(documents)\n    \n    query_tfidf_matrix = tfidf.transform(queries)\n    \n    cosine_similarities = cosine_similarity(query_tfidf_matrix, tfidf_matrix)\n    \n    return cosine_similarities\ncosine_similarities_of_queries = solve(queries, documents)\n",
        "\nnew_features = pd.DataFrame(features)\nnew_features = new_features.fillna(0)\nnew_features = new_features.astype(int)\n",
        "\nnew_f = pd.DataFrame(f)\nnew_f = new_f.apply(lambda x: pd.get_dummies(x))\nnew_f = new_f.apply(lambda x: x.astype(int), axis=1)\nnew_f = new_f.values\n",
        "\nnew_features = pd.DataFrame(features)\nnew_features = new_features.T.reset_index(drop=True)\nnew_features = new_features.set_index(0)\nnew_features = new_features.T.reset_index(drop=True)\nnew_features = new_features.values\n",
        "\n    df = pd.DataFrame(features)\n    df = df.set_index(0)\n    df = df.transpose()\n    new_features = df.to_numpy()\n    ",
        "\nnew_features = pd.DataFrame(features)\nnew_features = new_features.T.reset_index(drop=True)\nnew_features = new_features.set_index(0)\nnew_features = new_features.T.reset_index(drop=True)\nnew_features = new_features.values\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='complete')\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='complete')\ncluster_labels = model.fit_predict(simM)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n# Perform hierarchical clustering\nZ = sch.linkage(data_matrix, 'ward')\n# Define the number of clusters\nk = 2\n# Generate the cluster labels\ncluster_labels = sch.fcluster(Z, k, criterion='maxclust')\nprint(cluster_labels)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n# Perform hierarchical clustering\nZ = sch.linkage(data_matrix, method='ward')\n# Define the number of clusters\nk = 2\n# Generate the cluster labels\ncluster_labels = sch.fcluster(Z, k, criterion='maxclust')\nprint(cluster_labels)\n",
        "\nZ = sch.linkage(simM, method='ward')\ncluster_labels = sch.fcluster(Z, 2, criterion='maxclust')\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nassert type(data) == np.ndarray\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\nprint(centered_scaled_data)\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PowerTransformer\ndata = load_data()\nassert type(data) == np.ndarray\nbox_cox_data = PowerTransformer(method='box-cox').fit_transform(data)\nprint(box_cox_data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\nbox_cox_data = PowerTransformer(method='box-cox').fit_transform(data)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PowerTransformer\ndata = load_data()\nassert type(data) == np.ndarray\n# Apply Yeo-Johnson transformation\ntransformer = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = transformer.fit_transform(data)\nprint(yeo_johnson_data)\n",
        "\ntransformer = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = transformer.fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b|[!?\\\"']\")\ntransformed_text = vectorizer.fit_transform(text)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n# Split the dataset into training and testing sets\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = load_data()\n# Split the dataset into training and testing sets\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n# Output the split datasets\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n# Split the dataset into training and testing sets\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef load_data():\n    dataset = pd.read_csv('example.csv', header=None, sep=',')\n    return dataset\ndef solve(data):\n    # Split the dataset into features and target\n    X = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, y_train, X_test, y_test\ndataset = load_data()\nx_train, y_train, x_test, y_test = solve(dataset)\n",
        "\nfrom sklearn.cluster import KMeans\nimport pandas as pd\ndf = pd.read_csv(\"generate_csv/all_data_device.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nX = f1.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\nprint(labels)\n",
        "\nX = load_data()\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n",
        "\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[LinearSVC(penalty='l1').fit(X, y).coef_.nonzero()[1]]\n",
        "\nfeatureSelector = LinearSVC(penalty='l1', dual=False)\nfeatureSelector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.coef_ != 0]\n",
        "\n    clf = LinearSVC(penalty='l1')\n    clf.fit(X, y)\n    selected_features = np.where(clf.coef_ != 0)[1]\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_features]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef load_data():\n    return pd.DataFrame({\n        'Time': [1.00, 2.00, 3.00, 4.00, 5.00, 5.50, 6.00],\n        'A1': [6.64, 6.70, np.nan, 7.15, np.nan, 7.44, 7.62],\n        'A2': [6.82, 6.86, np.nan, 7.26, np.nan, 7.63, 7.86],\n        'A3': [6.79, 6.92, np.nan, 7.26, np.nan, 7.58, 7.71],\n        'B1': [6.70, np.nan, 7.07, 7.19, np.nan, 7.54, np.nan],\n        'B2': [6.95, np.nan, 7.27, np.nan, 7.51, np.nan, np.nan],\n        'B3': [7.02, np.nan, 7.40, np.nan, np.nan, np.nan, np.nan]\n    })\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    if col in ['Time', 'A1', 'A2', 'A3', 'B1', 'B2', 'B3']:\n        df2 = df1[~np.isnan(df1[col])] #removes NaN values for each column to apply sklearn function\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y) # either this or the next line\n        m = slope.coef_[0]\n        slopes.append(m)\nprint(slopes)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef load_data():\n    return pd.DataFrame({\n        'Time': [5.00, 5.50, 6.00],\n        'A1': [np.nan, 7.44, 7.62],\n        'A2': [np.nan, 7.63, 7.86],\n        'A3': [np.nan, 7.58, 7.71],\n        'B1': [np.nan, 7.54, np.nan],\n        'B2': [np.nan, np.nan, np.nan],\n        'B3': [np.nan, np.nan, np.nan]\n    })\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\nprint(slopes)\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import linear_model\nimport statsmodels.api as sm\nX_train, y_train, X_test, y_test = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nassert type(X_test) == np.ndarray\nassert type(y_test) == np.ndarray\n# ElasticNet Regression\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\nprint(training_set_score)\nprint(test_set_score)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a)\n    ",
        "\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict([close_buy1, m5, m10, ma20])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['asdf', '1'], ['asdf', '0']]\nclf = DecisionTreeClassifier()\n# One-hot encoding\nX = pd.get_dummies(X)\n# Convert to numpy array\nnew_X = np.array(X)\nclf.fit(new_X, ['2', '3'])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['asdf', '1'], ['asdf', '0']]\nclf = DecisionTreeClassifier()\n# One-hot encoding\nX = pd.get_dummies(X)\n# Convert to numpy array\nnew_X = np.array(X)\nclf.fit(new_X, ['2', '3'])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['dsa', '2'], ['sato', '3']]\nclf = DecisionTreeClassifier()\n# Convert string values to numerical values\nle = LabelEncoder()\nnew_X = np.array(X).T\nnew_X[0] = le.fit_transform(new_X[0])\nnew_X[1] = le.fit_transform(new_X[1])\nnew_X = new_X.T\nclf.fit(new_X, ['4', '5'])\n",
        "\n# [Missing Code]\nX = dataframe.iloc[:, :-1].values\ny = dataframe.iloc[:, -1].values\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Load data\nfeatures_dataframe = load_data()\n# Split data to train and test set\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n# Sort dataframes\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n# Output dataframes\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Load data\nfeatures_dataframe = load_data()\n# Split data to train and test set\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n# Sort dataframes\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n# Output dataframes\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort_values(by=\"date\")\n    test_dataframe = test_dataframe.sort_values(by=\"date\")\n    ",
        "\ncols = ['X2', 'X3']\nfor col in cols:\n    df[col + '_scale'] = df.groupby('Month')[col].apply(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\n",
        "\ncols = ['A2', 'A3']\nmyData[cols] = scaler.fit_transform(myData[cols])\nmyData['new_A2'] = myData['A2']\nmyData['new_A3'] = myData['A3']\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nwords = \"Hello @friend, this is a good day. #good.\"\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n",
        "\nparam_grid = GridSearch_fitted.param_grid\ncv_results = GridSearch_fitted.cv_results_\nfull_results = pd.DataFrame(cv_results)\n",
        "\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'gamma': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n}\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\nfull_results = pd.DataFrame(grid_search.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\n",
        "\n# [Missing Code]\n",
        "\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = 1 - cosine_similarity(tfidf_matrix)\n"
    ],
    "Pytorch": [
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\nembedding_weights = torch.FloatTensor(word2vec.wv.vectors)\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights)\nembedded_input = embedding_layer(input_Tensor)\n",
        "\n    # [Missing Code]\n    ",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.detach().numpy())\n",
        "\nC = B[:, A_log]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    A_logical = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_logical, B\nA_logical, B = load_data()\nC = B[:, A_logical]\nprint(C)\n",
        "\nC = B[:, A_log]\n",
        "\nC = B[:, A_log]\n",
        "\n    C = B[:, A_log]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    A_log = torch.ByteTensor([0, 0, 1])\n    B = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\n    return A_log, B\nA_log, B = load_data()\nC = B[:, A_log]\nprint(C)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    idx = torch.LongTensor([1, 2])\n    B = torch.LongTensor([[2, 1, 3], [5, 4, 6]])\n    return idx, B\nidx, B = load_data()\nC = B[:, idx]\nprint(C)\n",
        "\nx_tensor = torch.tensor(x_array.tolist())\n",
        "\nx_tensor = torch.tensor(x_array.tolist())\n",
        "\n    t = torch.tensor(a.tolist())\n    ",
        "\nmask = []\nfor length in lens:\n    mask.append([1]*length + [0]*(len(lens)-length))\nmask = torch.LongTensor(mask)\n",
        "\nmask = []\nfor length in lens:\n    mask.append([1] * length + [0] * (10 - length))\nmask = torch.LongTensor(mask)\n",
        "\nmask = torch.zeros(len(lens), max(lens)).long()\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = [3, 5, 4]\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len, dtype=torch.long)\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\nmask = get_mask(lens)\nprint(mask)\n",
        "\nTensor_3D = torch.zeros((Tensor_2D.shape[0], Tensor_2D.shape[1], Tensor_2D.shape[1]))\nfor i in range(Tensor_2D.shape[0]):\n    diag_matrix = torch.diag(Tensor_2D[i])\n    Tensor_3D[i] = diag_matrix\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef Convert(t):\n    index_in_batch, diag_ele = t.shape\n    result = torch.zeros((index_in_batch, diag_ele, diag_ele))\n    for i in range(index_in_batch):\n        for j in range(diag_ele):\n            result[i][j][j] = t[i][j]\n    return result\nTensor_2D = load_data()\nTensor_3D = Convert(Tensor_2D)\nprint(Tensor_3D)\n",
        "\nif a.shape == (2, 11) and b.shape == (1, 11):\n    b = b.expand(2, 11)\n    ab = torch.stack((a, b), 0)\nelif a.shape == (1, 11) and b.shape == (2, 11):\n    a = a.expand(2, 11)\n    ab = torch.stack((a, b), 0)\nelse:\n    raise ValueError(\"Invalid shapes for a and b\")\n",
        "\nab = torch.cat((a, b), 0)\n",
        "\n    if a.shape[1] == b.shape[1]:\n        ab = torch.stack((a,b),0)\n    else:\n        ab = torch.cat((a,b),0)\n    ",
        "\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 0\n",
        "\na[ : , lengths : , : ]  = 2333\n",
        "\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 0\n",
        "\na[:, :, :] = 2333\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\nprint(tensor_of_tensors)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist = [torch.randn(3), torch.randn(3), torch.randn(3)]\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\nprint(tensor_of_tensors)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    t = torch.tensor([[-0.2,  0.3],\n                        [-0.5,  0.1],\n                        [-0.4,  0.2]])\n    idx = np.array([1, 0, 1])\n    return t, idx\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = t[idx]\nprint(result)\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx, range(idx.shape[0])]\n",
        "\nresult = x.gather(1,ids)\n",
        "\nresult = x.gather(1,ids)\n",
        "\nresult = np.zeros((70, 2))\nfor i in range(ids.shape[0]):\n    result[i] = x[i, ids[i].argmax()]\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmin(softmax_output, dim=1)\n",
        "\n    y = torch.argmax(softmax_output, dim=1)\n    ",
        "\n    y = torch.argmin(softmax_output, dim=1)\n    ",
        "\nimages, labels = load_data()\nloss = cross_entropy2d(images, labels)\n",
        "\ncnt_equal = 0\nfor i in range(len(A)):\n    if A[i] == B[i]:\n        cnt_equal += 1\n",
        "\ncnt_equal = 0\nfor i in range(len(A)):\n    if A[i] == B[i]:\n        cnt_equal += 1\n",
        "\ncnt_not_equal = 0\nfor i in range(len(A)):\n    if A[i] != B[i]:\n        cnt_not_equal += 1\n",
        "\n    cnt_equal = 0\n    for i in range(len(A)):\n        if A[i] == B[i]:\n            cnt_equal += 1\n    ",
        "\nx = A.shape[1]\ncnt_equal = 0\nfor i in range(x):\n    if A[-1, i] == B[-1, i]:\n        cnt_equal += 1\n",
        "\nx = A.shape[1]\ncnt_not_equal = 0\nfor i in range(x):\n    if A[-1, i] != B[-1, i]:\n        cnt_not_equal += 1\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.randn(1, 3, 10, 40, 1)\nassert a.shape == (1, 3, 10, 40, 1)\nchunk_dim = 10\na_split = torch.chunk(a, chunk_dim, dim=3)\ntensors_31 = []\nfor i in range(len(a_split)):\n    tensors_31.append(a_split[i][:, :, :, :10, :])\nfor tensor in tensors_31:\n    print(tensor)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = torch.randn(1, 3, 40, 10, 1)\nchunk_dim = 10\na_split = torch.chunk(a, chunk_dim, dim=2)\ntensors_31 = []\nfor i in range(len(a_split)):\n    tensors_31.append(a_split[i])\nfor tensor in tensors_31:\n    print(tensor)\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    x = torch.tensor([[1, -2, 3], [4, -5, 6]])\n    y = torch.tensor([[2, -3, 4], [5, -6, 7]])\n    return x, y\nx, y = load_data()\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_abs = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.zeros_like(x)\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        if torch.abs(x[i][j]) == min_abs[i][j]:\n            signed_min[i][j] = sign_x[i][j] * x[i][j]\n        else:\n            signed_min[i][j] = sign_y[i][j] * y[i][j]\nprint(signed_min)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n    return x, y\nx, y = load_data()\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = max * sign_x\nprint(signed_max)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_abs = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = sign_x * min_abs\n    return signed_min\nx, y = load_data()\nsigned_min = solve(x, y)\n",
        "\noutput = MyNet(input)\nconfidence_score = torch.nn.functional.softmax(output, dim=1)\n",
        "\nresult = torch.zeros((2, 5))\nresult[:, :2] = a\nresult[:, 2] = (a[:, -1] + b[:, 0]) / 2\nresult[:, 3:] = b[:, 1:]\n",
        "\n    result = torch.zeros((2, 5))\n    result[:, :2] = a[:, :2]\n    result[:, 2:4] = (a[:, 2] + b[:, 0]) / 2\n    result[:, 4:] = b[:, 1:]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(8).reshape(1,4,2).float()\nprint(t)\nnew=torch.tensor([0., 0., 0.,0.]).reshape(1,4,1).float()\nprint(new)\nr = torch.cat([t,new], dim=2)\nprint(r)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(4).reshape(1,2,2).float()\nprint(t)\nnew=torch.tensor([0., 0., 0.,0.]).reshape(1,2,2).float()\nprint(new)\nr = torch.stack([t,new])\nprint(r)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(8).reshape(1,4,2).float()\nprint(t)\nnew=torch.tensor([-1, -1, -1, -1]).reshape(1,4,1).float()\nprint(new)\nr = torch.cat([t,new], dim=2)\nprint(r)\n",
        "\nresult = torch.bmm(data, W.unsqueeze(2)).squeeze()\n"
    ]
}