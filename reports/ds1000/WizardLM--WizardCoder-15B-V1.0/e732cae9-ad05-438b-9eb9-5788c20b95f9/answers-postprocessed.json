{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\ndf = df.iloc[List]\nresult = df[df['Type'] != df['Type'].shift()]\n",
        "\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# create a new column 'others' for Qu1\ndf['others'] = df['Qu1'].apply(lambda x: 'other' if x not in ['cheese', 'potato', 'banana'] else x)\n\n# create a new column 'others' for Qu3\ndf['others'] = df['others'].apply(lambda x: 'other' if x == 'apple' else x)\n\n# create a new column 'others' for Qu3\ndf['others'] = df['others'].apply(lambda x: 'other' if x == 'egg' else x)\n\nresult = df\n",
        "```python\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# create a new column 'others' for Qu2\ndf['others'] = df['Qu2']\n\n# create a new column 'others' for Qu3\ndf['others'] = df['Qu3']\n\n# replace values in Qu1 according to value_counts() when value count great or equal 3\ndf['Qu1'] = df['Qu1'].mask(pd.value_counts(df['Qu1']) >= 3, 'cheese')\n\n# replace values in Qu2 according to value_counts() when value count great or equal 3\ndf['Qu2'] = df['Qu2'].mask(pd.value_counts(df['Qu2']) >= 3, 'others')\n\n# replace values in Qu3 according to value_counts() when value count great or equal 3\ndf['Qu3'] = df['Qu3'].mask(pd.value_counts(df['Qu3']) >= 3, 'others')\n\n# replace values in Qu1 according to value_counts() when value count great or equal 3\ndf['Qu1'] = df['Qu1'].mask(pd.value_counts(df['Qu1']) >= 3, 'others')\n\n# replace values in Qu2 according to value_counts() when value count great or equal 3\ndf['Qu2'] = df['Qu2'].mask(pd.value_counts(df['Qu2']) >= 3, 'others')\n\n# replace values in Qu3 according to value_counts() when value count great or equal 3\ndf['Qu3'] = df['Qu3'].mask(pd.value_counts(df['Qu3']) >= 3, 'others')\n\n# replace values in Qu1 according to value_counts() when value count great or equal 3\ndf['Qu1'] = df['Qu1'].mask(pd.value_counts(df['Qu1']) >= 3, 'others')\n\n# replace values in Qu2 according to value_counts() when value count great or equal 3\ndf['Qu2'] = df['Qu2'].mask(pd.value_counts(df['Qu2']) >= 3, 'others')\n\n# replace values in Qu3 according to value_counts() when value count great or equal 3\ndf['Qu3'] = df['Qu3'].mask(pd.value_counts(df['Qu3']) >= 3, 'others')\n\n# replace values in Qu1 according to value_counts() when value count great or equal 3\ndf['Qu1'] = df['Qu1'].mask(pd.value_counts(df['Qu1']) >= 3, 'others')\n\n# replace values in Qu2 according to value_counts() when value count great or equal 3\ndf['Qu2'] = df['Qu2'].mask(pd.value_counts(df['Qu2']) >= 3, 'others')\n\n# replace values in Qu3 according to value_counts() when value count great or equal 3\ndf['Qu3'] = df['Qu3'].mask(pd.value_counts(df['Qu3']) >= 3, 'others')\n\n# replace values in Qu1 according to value_counts() when value count great or equal 3\ndf['Qu1'] = df['Qu1'].mask(pd.value_counts(df['Qu1']) >= 3, 'others')\n\n# replace values in Qu2 according to value_counts() when value count great or equal 3\ndf['Qu2'] = df['Qu2'].mask(pd.value_counts(df['Qu2']) >= 3, 'others')\n\n# replace values in Qu3 according to value_counts() when value count great or equal 3\ndf['Qu3'] = df['Qu3'].mask(pd.value_counts(df['Qu3']) >= 3, 'others')\n\n# replace values in Qu1 according to value_counts() when value count great or equal 3\ndf['Qu1'] = df['Qu1'].mask",
        "\n\ndef f(df=example_df):\n    value_counts = pd.value_counts(df.Qu1) >= 2\n    result = df.copy()\n    result.loc[value_counts, 'Qu1'] = 'other'\n    value_counts = pd.value_counts(df.Qu3) >= 2\n    result.loc[value_counts, 'Qu3'] = 'other'\n    return result\n",
        "\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# create a new column 'others' for Qu1\ndf['others'] = df['Qu1']\n\n# replace values in Qu1 according to value_counts() when value count great or equal 3\nvalue_counts = pd.value_counts(df['Qu1'])\ndf.loc[value_counts >= 3, 'others'] = 'cheese'\n\n# replace values in Qu2 and Qu3 according to value_counts() when value count great or equal 2\nvalue_counts = pd.value_counts(df['Qu2'])\ndf.loc[value_counts >= 2, 'Qu2'] = 'others'\nvalue_counts = pd.value_counts(df['Qu3'])\ndf.loc[value_counts >= 2, 'Qu3'] = 'others'\n\n# create a new dataframe with the final result\nresult = df[['others', 'Qu2', 'Qu3']]\n\n",
        "\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# create a new column 'others' for Qu1\ndf['others'] = df['Qu1']\n\n# replace 'apple' with 'others' for Qu1\ndf.loc[df['Qu1'] == 'apple', 'others'] = 'others'\n\n# replace 'egg' with 'others' for Qu1\ndf.loc[df['Qu1'] == 'egg', 'others'] = 'others'\n\n# replace 'potato' with 'others' for Qu1\ndf.loc[df['Qu1'] == 'potato', 'others'] = 'others'\n\n# replace 'cheese' with 'others' for Qu1\ndf.loc[df['Qu1'] == 'cheese', 'others'] = 'others'\n\n# replace 'banana' with 'others' for Qu1\ndf.loc[df['Qu1'] == 'banana', 'others'] = 'others'\n\n# replace 'sausage' with 'others' for Qu2\ndf.loc[df['Qu2'] == 'sausage', 'others'] = 'others'\n\n# replace 'apple' with 'others' for Qu2\ndf.loc[df['Qu2'] == 'apple', 'others'] = 'others'\n\n# replace 'banana' with 'others' for Qu2\ndf.loc[df['Qu2'] == 'banana', 'others'] = 'others'\n\n# replace 'cheese' with 'others' for Qu3\ndf.loc[df['Qu3'] == 'cheese', 'others'] = 'others'\n\n# replace 'potato' with 'others' for Qu3\ndf.loc[df['Qu3'] == 'potato', 'others'] = 'others'\n\n# replace 'sausage' with 'others' for Qu3\ndf.loc[df['Qu3'] == 'sausage', 'others'] = 'others'\n\n# replace 'apple' with 'others' for Qu3\ndf.loc[df['Qu3'] == 'apple', 'others'] = 'others'\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\ndf = df.drop_duplicates(subset='url', keep='first')\n\nresult = df[df['keep_if_dup'] == 'Yes']\n\n",
        "\ndf = df.drop_duplicates(subset='url', keep='first')\ndf = df[df['drop_if_dup'] == 'Yes']\nresult = df\n",
        "\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\ndf = df.sort_values(by=['url', 'keep_if_dup'], ascending=[True, False])\ndf = df.drop_duplicates(subset='url', keep='last')\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\n\nresult = {}\nfor name, group in df.groupby('name'):\n    result[name] = {}\n    for v1, group1 in group.groupby('v1'):\n        result[name][v1] = {}\n        for v2, v3 in zip(group1['v2'], group1['v3']):\n            result[name][v1][v2] = v3\n\n",
        "\nresult = df['datetime'].dt.tz_localize(None)\n",
        "\n\nexample_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\nexample_df['datetime'] = pd.to_datetime(example_df['datetime'])\ndef f(df=example_df):\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    return df['datetime']\n",
        "\nresult['datetime'] = result['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\nresult['datetime'] = result['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\n\n# Extracting key value pairs from message column\ndf['message'] = df['message'].str.replace('[', '').str.replace(']', '').str.split(',', expand=True)\ndf[['job', 'money', 'wife']] = df['message'].str.split(':', expand=True)\ndf['wife'] = df['wife'].str.strip()\ndf['money'] = df['money'].str.strip()\ndf['job'] = df['job'].str.strip()\n\n# Creating new columns for group and kids\ndf['group'] = None\ndf['kids'] = None\n\n# Filling in new columns\ndf.loc[df['message'].str.contains('group'), 'group'] = df['message'].str.extract(r'group: (.*)')[0]\ndf.loc[df['message'].str.contains('kids'), 'kids'] = df['message'].str.extract(r'kids: (.*)')[0]\n\n# Dropping message column\ndf.drop('message', axis=1, inplace=True)\n\n# Reordering columns\nresult = df[['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids']]\n\n",
        "\nresult.loc[result['product'].isin(products), 'score'] = result.loc[result['product'].isin(products), 'score'] * 10\n",
        "\nresult.loc[~result['product'].isin(products), 'score'] *= 10\n",
        "\nfor product_list in products:\n    mask = (df['product'] >= product_list[0]) & (df['product'] <= product_list[1])\n    df.loc[mask, 'score'] = df.loc[mask, 'score'] * 10\n",
        "\nresult = df.loc[df['product'].isin(products), 'score']\nresult = (result - result.min()) / (result.max() - result.min())\ndf.loc[df['product'].isin(products), 'score'] = result\n",
        "\n\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 0, 1],\n                   'C': [0, 0, 1, 0, 0, 0],\n                   'D': [0, 0, 0, 1, 0, 0]})\n\n# Convert binary columns to categorical column\ndf['category'] = df.apply(lambda x: ''.join(x.astype(int).astype(str)), axis=1)\n\n# Reverse pd.get_dummies()\nresult = pd.get_dummies(df['category'])\n\n",
        "\n\ndf = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\n\n# Convert binary columns to categorical\ndf['category'] = df.apply(lambda x: ''.join(x.astype(int).astype(str)), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 1, 1],\n                   'C': [1, 1, 1, 0, 1, 0],\n                   'D': [0, 0, 0, 1, 1, 0]})\n\n# Convert binary columns to categorical column of lists\ndf['category'] = df.apply(lambda x: [col for col, val in x.items() if val == 1], axis=1)\n\n# Drop binary columns\ndf.drop(['A', 'B', 'C', 'D'], axis=1, inplace=True)\n\nresult = df\n",
        "\nresult['Date'] = result['Date'].dt.to_period('M')\nresult['Date'] = result['Date'].apply(lambda x: x.strftime('%b-%Y'))\n",
        "\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y')\n",
        "\nresult = df[(df['Date'].dt.to_period(\"M\") >= '2017-08-17') & (df['Date'].dt.to_period(\"M\") <= '2018-01-31')]\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y')\nresult['Day'] = result['Date'].apply(lambda x: pd.to_datetime(x).strftime('%A'))\nresult = result[['Date', 'Day']]\nresult.columns = ['Date', 'Day']\n",
        "\nresult = df.shift(1, axis=0)\nresult.iloc[0, 0] = df.iloc[1, 0]\nresult.iloc[0, 1] = df.iloc[1, 1]\n",
        "\nresult = df.shift(1, axis=0)\nresult.iloc[-1, 0] = df.iloc[0, 0]\nresult.iloc[0, 0] = df.iloc[-1, 0]\n",
        "\nresult = df.shift(1, axis=0)\nresult.iloc[0, 0] = df.iloc[1, 0]\nresult.iloc[-1, 0] = df.iloc[-2, 0]\nresult.iloc[0, 1] = df.iloc[-1, 1]\nresult.iloc[-1, 1] = df.iloc[0, 1]\n",
        "\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\ndf.iloc[0] = df.iloc[1]\ndf.iloc[-1] = df.iloc[0]\n\nresult = df\n",
        "\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\ndf.rename(columns={'HeaderA': 'HeaderAX',\n                  'HeaderB': 'HeaderBX',\n                  'HeaderC': 'HeaderCX'}, inplace=True)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\ndf.columns = ['X' + col for col in df.columns]\n\nresult = df\n",
        "\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457],\n     \"HeaderX\": [345]})\n\ndf.rename(columns=lambda x: 'X' + x if x[-1] != 'X' else x, inplace=True)\n\nresult = df\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", [[Missing]]})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", [[Missing]]})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val32\": \"mean\"})\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].mean(axis=0)\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].sum(axis=0)\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].sum(axis=0)\nresult = result.drop(result.idxmax())\n\n",
        "\n\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\nresult = df.apply(lambda x: pd.value_counts(x.dropna()), axis=0)\n\n",
        "\n\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\nresult = df.isnull().sum()\n\n",
        "\nresult = ''\nfor col in df.columns:\n    if df[col].nunique() > 1:\n        result += '---- ' + col + ' ---\\n'\n        result += str(df[col].value_counts()) + '\\n'\n",
        "\ndf = df.head().combine_first(df.iloc[[0]])\n",
        "\ndf = df.head().combine_first(df.iloc[[0]])\n",
        "```python\ndf.fillna(method='ffill',axis=1,inplace=True)\ndf.fillna(method='bfill',axis=1,inplace=True)\ndf.fillna(method='ffill',axis=0,inplace=True)\ndf.fillna(method='bfill',axis=0,inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.fillna(df.median(),inplace=True)\ndf.fillna(df.mode().iloc[0],inplace=True)\ndf.fillna(df.",
        "\ndf.fillna(method='ffill',axis=1,inplace=True)\ndf.fillna(method='bfill',axis=1,inplace=True)\ndf.fillna(method='ffill',axis=0,inplace=True)\ndf.fillna(method='bfill',axis=0,inplace=True)\n",
        "\ndf.fillna(method='ffill',inplace=True)\ndf.fillna(method='bfill',inplace=True)\ndf.fillna(0,inplace=True)\n",
        "\nresult = df.loc[df['value'] < thresh].sum()\nresult.name = 'X'\nresult = pd.DataFrame(result).T\ndf.loc[df['value'] >= thresh] = result\n",
        "\nresult = df.loc[df['value'] >= thresh].groupby(level=0).mean().reset_index()\nresult.columns = ['lab', 'value']\nresult = result.append({'lab': 'X', 'value': df.loc[df['value'] >= thresh]['value'].mean()}, ignore_index=True)\n",
        "\nresult = df.loc[(df['value'] < section_left) | (df['value'] > section_right)]\nresult = result.groupby(level=0).mean()\nresult.loc['X'] = result.mean(axis=0)\nresult = result.drop(['value'], axis=1)\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nfor col in df.columns:\n    result[f\"inv_{col}\"] = 1/df[col]\n\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nfor col in df.columns:\n    result[f\"exp_{col}\"] = df[col].apply(lambda x: math.exp(x))\n\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 0]})\n\nresult = df.copy()\n\nfor col in df.columns:\n    if df[col].sum() != 0:\n        result[f\"inv_{col}\"] = 1/df[col]\n\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nfor col in df.columns:\n    df[f\"sigmoid_{col}\"] = 1/(1+np.exp(-df[col]))\n\n",
        "\nresult = df.idxmin().apply(lambda x: df.index[df[x] == df[x].min()][-1])\n",
        "\nresult = df.idxmin(axis=0).where(df.idxmin(axis=0) != df.idxmax(axis=0), df.idxmax(axis=0))\n",
        "\n\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['a']*len(pd.date_range(min_dt, max_dt)) + ['b']*len(pd.date_range(min_dt, max_dt)), 'val': [0]*len(pd.date_range(min_dt, max_dt))})\nresult = result.merge(df, on=['dt', 'user'], how='left')\nresult['val'] = result['val'].fillna(0)\n",
        "\n\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Finding the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Expanding the date column to have all the dates there while simultaneously filling in 0 for the val column\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date), 'user': ['abc']*len(pd.date_range(min_date, max_date)), 'val': [0]*len(pd.date_range(min_date, max_date))})\n\n# Merging the result with the original data frame\nresult = pd.merge(result, df, on='dt', how='left')\n\n# Filling in the missing values with 0\nresult['val'] = result['val'].fillna(0)\n\n",
        "\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# create a new dataframe with all dates between the min and max date in the original dataframe\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndates = pd.date_range(min_date, max_date)\n\n# create a new dataframe with all dates and fill in 233 for the val column\nresult = pd.DataFrame({'dt': dates, 'user': ['a']*len(dates), 'val': [233]*len(dates)})\n\n# merge the original dataframe with the new dataframe on the dt column\nresult = pd.merge(result, df, on='dt', how='left')\n\n# fill in missing values with 233\nresult['val'] = result['val'].fillna(233)\n\n# sort the result dataframe by dt and user\nresult = result.sort_values(['dt', 'user'])\n\n",
        "\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Finding the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Expanding the date column to have all the dates there\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date), 'user': ['a']*len(pd.date_range(min_date, max_date)), 'val': [33]*len(pd.date_range(min_date, max_date))})\n\n# Filling in the maximum val of the user for the val column\nresult.loc[result['user'] == 'b', 'val'] = df.loc[df['user'] == 'b', 'val'].max()\n\n",
        "\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['a']*len(pd.date_range(min_dt, max_dt)), 'val': [df[df['user'] == 'a']['val'].max()]*len(pd.date_range(min_dt, max_dt))})\n\nresult = pd.merge(result, df, on=['dt', 'user'], how='left')\nresult = result.fillna(method='ffill')\nresult = result.fillna(method='bfill')\nresult = result.fillna(0)\n\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n",
        "\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a new column 'id' with unique IDs for each name\ndf['id'] = df['name'].astype('category').cat.codes\n\n# Replace the original 'name' column with the new 'id' column\ndf.drop(columns=['name'], inplace=True)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a dictionary to map each unique value of 'a' to a unique ID\na_map = {a: i for i, a in enumerate(sorted(df['a'].unique()), 1)}\n\n# Replace each 'a' value with its corresponding ID\ndf['a'] = df['a'].map(a_map)\n\nresult = df\n",
        "\n\nexample_df = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\ndef f(df=example_df):\n    name_to_id = {}\n    for i, name in enumerate(df['name'].unique()):\n        name_to_id[name] = i + 1\n    df['name'] = df['name'].apply(lambda x: name_to_id[x])\n    return df\n",
        "\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a new dataframe with unique IDs for each name and a\ndf_new = pd.DataFrame({'ID': df['name'].astype('category').cat.codes + 1,\n                       'b': df['b'],\n                       'c': df['c']})\n\n# Replace the original 'name' and 'a' columns with the new ID column\ndf_new.insert(0, 'name', df['name'])\ndf_new.insert(1, 'a', df['a'])\n\nresult = df_new\n",
        "\nresult = pd.melt(df, id_vars=['user', 'someBool'], value_vars=['01/12/15', '02/12/15'], var_name='date', value_name='value')\n",
        "\nresult = pd.concat([df.drop(['someBool'], axis=1), df['someBool'].apply(pd.Series)], axis=1)\nresult.columns = ['user', '01/12/15', '02/12/15', 'value']\nresult = result[['user', '01/12/15', 'others', 'value']]\nresult['others'] = result['02/12/15'] + result['someBool'].apply(lambda x: ' ' + str(x))\nresult = result.drop(['02/12/15', 'someBool'], axis=1)\n",
        "\nresult = pd.melt(df, id_vars=['user', 'someBool'], value_vars=['01/12/15', '02/12/15'], var_name='date', value_name='value')\n",
        "\nresult = df[df.c > 0.5][columns].values\n",
        "\nresult = df[df.c > 0.45][columns].values\n",
        "\n\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    return result.values\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    result['sum'] = result.apply(lambda x: x.sum(), axis=1)\n    return result\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    return result\n",
        "\nfrom datetime import timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 120\n\nfilter_dates = []\nfor index, row in df.iterrows():\n    if X == 0:\n        filter_dates.append(index)\n    else:\n        for i in range(1, X+1):\n            filter_dates.append((index.date() + timedelta(days=i)))\n\nresult = df[~df.index.isin(filter_dates)]\n\n",
        "\nfrom datetime import timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n\nfilter_dates = []\nfor index, row in df.iterrows():\n    if X == 1:\n        filter_dates.append(index)\n    else:\n        for i in range(1, X):\n            filter_dates.append((index.date() + timedelta(weeks=i)))\n\nresult = df[~df.index.isin(filter_dates)]\n\n",
        "\nfrom datetime import datetime, timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n\n# Convert date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n\n# Create a new column with the date in the desired format\ndf['date_formatted'] = df['date'].dt.strftime('%d-%b-%Y')\n\n# Create a new dataframe with only the first row of each date\ndf_grouped = df.groupby('date_formatted').first().reset_index()\n\n# Create a new dataframe with only the rows that are not within X weeks of another row\nresult = df_grouped.sort_values(by='date').reset_index(drop=True)\nfor i in range(1, len(result)):\n    if (result.iloc[i]['date'] - result.iloc[i-1]['date']).days <= X*7:\n        result.drop(i, inplace=True)\n\n# Print the result\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = df.groupby(df.index // 3).mean()\n",
        "\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\nresult = df.groupby(df.index // 3).sum()\n",
        "\nresult = df.groupby(df.index // 4, sort=False).sum()\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = df.rolling(3).mean()\nresult = result.shift(-1)\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\nresult = pd.DataFrame()\n\nfor i in range(0, len(df), 3):\n    if i+2 <= len(df):\n        result = result.append({'col1': df.iloc[i:i+3]['col1'].sum()}, ignore_index=True)\n    if i+1 <= len(df):\n        result = result.append({'col1': df.iloc[i:i+2]['col1'].mean()}, ignore_index=True)\n\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\n# Create a new dataframe with the binned values\nresult = pd.DataFrame({'col1': [df.iloc[i:i+3].sum().values[0] for i in range(0, len(df), 3)] + \n                               [df.iloc[i:i+2].mean().values[0] for i in range(len(df)-2, len(df), 2)]})\n\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# fill the zeros with the previous non-zero value using pandas\ndf = df.fillna(method='ffill')\n\nresult = df\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# fill the zeros with the posterior non-zero value using pandas\ndf = df.fillna(method='ffill')\n\nresult = df\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# fill the zeros with the maximun between previous and posterior non-zero value using pandas\ndf = df.fillna(method='ffill').fillna(method='bfill')\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\n\ndf['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\ndf['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n\nresult = df\n",
        "\n\nexample_df = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\n\ndef f(df=example_df):\n    df['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\n    df['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\n    df['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n    return df\n",
        "\n\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\ndf['time_day'] *= df['number'].astype(int)\n\nresult = df\n",
        "\n\nresult = np.where([df1[column] != df2[column] for column in columns_check_list])\n",
        "\n\nresult = np.where([df1[column] == df2[column] for column in columns_check_list])\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_datetime64_any_dtype\nfrom pandas.api.types import is_numeric_dtype\nfrom pandas import Timestamp\n\ndef f(df):\n    if not isinstance(df.index, pd.MultiIndex):\n        raise ValueError(\"df must have a multi-index\")\n    if not is_string_dtype(df.index.levels[0]):\n        raise ValueError(\"first level of index must be string\")\n    if not is_datetime64_any_dtype(df.index.levels[1]):\n        raise ValueError(\"second level of index must be datetime\")\n    if not is_numeric_dtype(df.columns.get_level_values(0)):\n        raise ValueError(\"first level of columns must be numeric\")\n    if not is_numeric_dtype(df.columns.get_level_values(1)):\n        raise ValueError(\"second level of columns must be numeric\")\n    dates = df.index.levels[1].to_numpy()\n    x = df.columns.get_level_values(0).to_numpy()\n    y = df.columns.get_level_values(1).to_numpy()\n    data = df.to_numpy()\n    result = np.empty((len(dates), len(x), len(y)), dtype=data.dtype)\n    for i in range(len(dates)):\n        result[i] = data[i]\n    return np.array([dates, x, y]).T.reshape(-1, 3), result\n",
        "\ndef f(df):\n    df.index = pd.MultiIndex.from_tuples([(pd.to_datetime(date), id) for date, id in df.index], names=['date', 'id'])\n    df = df.swaplevel(0, 1)\n    return df\n",
        "\nresult = pd.wide_to_long(df, stubnames=['var1', 'var2'], i='Country', j='year', sep='_', suffix='\\d+')\nresult = result.reset_index()\nresult.columns = ['Variable', 'Country', 'year', 'var1', 'var2']\n",
        "\nresult = pd.wide_to_long(df, stubnames=['var1', 'var2'], i='Country', j='year', sep='_', suffix='\\d+')\nresult = result.reset_index()\nresult = result.sort_values(['year'], ascending=False)\nresult = result.rename(columns={'level_0': 'Variable'})\nresult = result[['Variable', 'Country', 'year', 'value']]\n",
        "\nresult = df[df.apply(lambda x: abs(x['Value_B']) < 1 and abs(x['Value_C']) < 1 and abs(x['Value_D']) < 1, axis=1)]\n",
        "\nresult = df[(abs(df['Value_B']) > 1) | (abs(df['Value_C']) > 1) | (abs(df['Value_D']) > 1)]\n",
        "\nresult = df.loc[df[['Value_B', 'Value_C', 'Value_D']].abs().max(axis=1) > 1, :]\nresult.columns = [col.replace('Value_', '') for col in result.columns]\n",
        "\n\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\ndf['A'] = df['A'].str.replace('&AMP;', '&')\nresult = df\n",
        "\n\ndf = pd.DataFrame({'A': ['Good &LT bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &LT; bad'] * 5})\ndf['A'] = df['A'].str.replace('&LT', '<')\nresult = df\n",
        "\n\nexample_df = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\ndef f(df=example_df):\n    df['A'] = df['A'].str.replace('&AMP;', '&')\n    return df\n",
        "\nresult = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\n\ndf = pd.DataFrame({'A': ['1 &AMP; 1', 'BB', 'CC', 'DD', '1 &AMP; 0'], 'B': range(5), 'C': ['0 &AMP; 0'] * 5})\ndf['A'] = df['A'].str.replace('&AMP;', '&')\nresult = df\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\ndf['first_name'] = df['name'].apply(lambda x: x.split()[0] if validate_single_space_name(x) else x)\ndf['last_name'] = df['name'].apply(lambda x: x.split()[1] if validate_single_space_name(x) else None)\n\nresult = df[['first_name', 'last_name']]\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\nresult = df.apply(lambda x: pd.Series({'1_name': validate_single_space_name(x['name']) or x['name'].split()[0], '2_name': validate_single_space_name(x['name']) or ' '.join(x['name'].split()[1:])}), axis=1)\n\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\n\n# Apply the validate_single_space_name function to each name in the DataFrame\ndf['name'] = df['name'].apply(validate_single_space_name)\n\n# Split the names into first_name, middle_name and last_name\ndf[['first_name', 'middle_name', 'last_name']] = df['name'].str.split(expand=True)\n\n# Replace NaN values with empty strings\ndf[['middle_name', 'last_name']] = df[['middle_name', 'last_name']].fillna('')\n\n# Rename the columns\ndf.columns = ['first_name', 'middle_name', 'last_name']\n\nresult = df\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\nresult['state'] = df.apply(lambda x: x['col1'] if x['col2'] <= 50 and x['col3'] <= 50 else max(x['col1'], x['col2'], x['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] > 50 and x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'], axis=1)\n",
        "\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\nerrors = []\nfor index, row in df.iterrows():\n    if not isinstance(row[\"Field1\"], int):\n        errors.append(row[\"Field1\"])\nresult = errors\n",
        "\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n\nresult = []\nfor i in df.itertuples():\n    if isinstance(i[2], int):\n        result.append(i[2])\n    else:\n        result.append(int(i[2]))\n\n",
        "\n\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    result = []\n    for i, row in df.iterrows():\n        if not isinstance(row[\"Field1\"], int):\n            result.append(row[\"Field1\"])\n    return result\n",
        "\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Compute the row total for each category\nrow_totals = df.groupby('cat').sum()\n\n# Compute the percentage of each value for each category\nresult = df.apply(lambda row: row / row_totals.loc[row['cat']] * 100, axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Compute the total of each column\ntotal = df.groupby('cat').sum()\n\n# Compute the percentage of each value\nresult = df.div(total, axis=0)\n\n# Print the result\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\ndf = df.drop(test)\nresult = df\n",
        "\n\ndef f(df, test):\n    # create a new dataframe with only the rows that are in the test list\n    result = df[df.index.isin(test)]\n    return result\n",
        "\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\ndef get_nearest_neighbour(df):\n    df2 = df.groupby('time')['car'].apply(lambda x: pd.DataFrame({'car': x, 'x': df[df['car'] == x.iloc[0]]['x'], 'y': df[df['car'] == x.iloc[0]]['y']}))\n    df2['euclidean_distance'] = df2.apply(lambda x: ((x['x'] - df2['x'])**2 + (x['y'] - df2['y'])**2)**0.5, axis=1)\n    df2['nearest_neighbour'] = df2['euclidean_distance'].idxmin()\n    return df2[['car', 'nearest_neighbour', 'euclidean_distance']]\n\nresult = get_nearest_neighbour(df)\n",
        "\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Calculate pairwise distances between cars\ndf['distance'] = df.groupby('car')['x'].transform(lambda x: np.sqrt((x - x.shift())**2 + (x.groupby(level=0).diff()**2).sum(axis=1)))\n\n# Get the farmost neighbour for each car\ndf2 = df.groupby(['time', 'car'])['distance'].agg(['min', 'idxmin']).reset_index()\ndf2.columns = ['time', 'car', 'farmost_neighbour', 'euclidean_distance']\n\n# Calculate the average distance for each time point\nresult = df2.groupby('time')['euclidean_distance'].mean().reset_index()\nresult.columns = ['time', 'average_distance']\n\n",
        "\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n\n# randomly select 20% of rows\nsample_size = int(len(df) * 0.2)\nsampled_df = df.sample(n=sample_size, random_state=0)\n\n# set Quantity of sampled rows to zero\nsampled_df['Quantity'] = 0\n\n# keep indexes of sampled rows\nsampled_indexes = sampled_df.index\n\n# merge sampled rows with original dataframe\nresult = pd.concat([df, sampled_df])\n\n# drop rows with indexes of sampled rows\nresult = result.drop(sampled_indexes)\n\n",
        "\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n\n# randomly select 20% of rows\nsample_size = int(len(df) * 0.2)\nsampled_df = df.sample(n=sample_size, random_state=0)\n\n# set ProductId of sampled rows to zero\nsampled_df['ProductId'] = 0\n\n# keep indexes of sampled rows\nsampled_indexes = sampled_df.index\n\n# merge sampled rows with original dataframe\nresult = pd.concat([df, sampled_df])\n\n# drop rows with indexes of sampled rows\nresult = result.drop(sampled_indexes)\n\n",
        "\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1, 1, 4, 7, 4, 2, 1, 1, 4, 7],\n                   'Quantity': [6, 1, 3, 2, 7, 2, 6, 1, 3, 2, 7, 2, 6, 1, 3]})\n\n# randomly select 20% of rows of each user\nresult = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0)).reset_index(drop=True)\n\n# set Quantity of selected rows to zero\nresult.loc[result['Quantity'] != 0, 'Quantity'] = 0\n\n# keep indexes of selected rows\nresult['index'] = result.index\n\n# merge selected rows with original dataframe\nresult = pd.merge(df, result[['index', 'Quantity']], on='index', how='left')\n\n# drop index column\nresult.drop(columns=['index'], inplace=True)\n\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\nresult = duplicate[['col1','col2','index_original']]\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\nresult = duplicate\n",
        "\n\nexample_df=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\ndef f(df=example_df):\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.index\n    return duplicate\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate[['val', 'col1', 'col2', '3col', 'index_original']]\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\nresult = duplicate\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg('max').reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(['min']).reset_index()\nresult = df[df['count']==result['min']]\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Value','count'], how='inner')\n",
        "\nresult=df.query(\"Category in @filter_list\")\n",
        "\ndf.query(\"Category not in filter_list\")\n",
        "\nresult = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                ('A', 'B', 'F'),\n                                ('A', 'C', 'G'),\n                                ('A', 'C', 'H'),\n                                ('A', 'D', 'I'),\n                                ('A', 'D', 'J')])\n",
        "\nresult = pd.melt(df, id_vars=[('A', 'B', 'C'), ('E', 'F', 'G', 'H', 'I', 'J')], value_vars=None, var_name='variable', value_name='value')\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum())\n",
        "\nresult = df.groupby('id')['val'].apply(lambda x: x.cumsum())\nresult = pd.concat([df, result], axis=1)\nresult.columns = ['id', 'stuff', 'val', 'cumsum']\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum())\n",
        "\ndf['cummax'] = df.groupby('id')['val'].transform(pd.Series.cummax)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.where(x >= 0, 0).cumsum())\n",
        "\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('l')['v'].sum(skipna=False)\nresult.loc['right'] = np.nan\n\n",
        "\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('r')['v'].sum(skipna=False)\n\n",
        "\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('l')['v'].apply(lambda x: x.sum(skipna=False))\n\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# Create an empty list to store the relationship information\nresult = []\n\n# Loop through each pair of columns\nfor col1 in df.columns:\n    for col2 in df.columns:\n        # Check if the columns are the same\n        if col1 == col2:\n            continue\n        # Check if the columns have the same number of unique values\n        if len(df[col1].unique()) == len(df[col2].unique()):\n            result.append(f'{col1} {col2} one-to-one')\n        elif len(df[col1].unique()) > len(df[col2].unique()):\n            result.append(f'{col1} {col2} one-to-many')\n        else:\n            result.append(f'{col1} {col2} many-to-one')\n\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            merged = pd.merge(df[[col1]], df[[col2]], how='inner')\n            if len(merged) == len(df):\n                result.append(f'{col1} {col2} one-2-many')\n            elif len(merged) == 0:\n                result.append(f'{col1} {col2} many-2-many')\n            else:\n                result.append(f'{col1} {col2} many-2-one')\n\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\nfor i in range(len(df.columns)):\n    for j in range(len(df.columns)):\n        if i == j:\n            result.iloc[i, j] = 'one-to-one'\n        elif len(df[df.columns[i]].unique()) == len(df[df.columns[j]].unique()):\n            result.iloc[i, j] = 'one-to-many'\n        elif len(df[df.columns[i]].unique()) == len(df[df.columns[j]].dropna().unique()):\n            result.iloc[i, j] = 'many-to-one'\n        else:\n            result.iloc[i, j] = 'many-to-many'\n\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\nresult = pd.crosstab(df.columns, df.columns)\n",
        "\n\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar', 'jim', 'john', 'mary', 'jim'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar', 'ryan', 'con', 'sullivan', 'Ryan'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar', 'jim@com', 'john@com', 'mary@com', 'Jim@com'],\n                   'bank': [np.nan, 'abc', 'xyz', np.nan, 'tge', 'vbc', 'dfg']})\n\n# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n             .applymap(lambda s: s.lower() if type(s) == str else s)\n             .applymap(lambda x: x.replace(\" \", \"\") if type(x) == str else x)\n             .drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# I wanted these duplicates to appear in the result:\n#   firstname  lastname     email bank\n# 2   Foo Bar   Foo Bar   Foo Bar  xyz\n# 6       jim      Ryan   Jim@com  dfg\n\n# remove the dupes that don't have an bank account\ndfiban_uniq = dfiban_uniq[dfiban_uniq['bank'].notna()]\n\nresult = dfiban_uniq\n",
        "\n\n# Set the locale to the user's default setting\nlocale.setlocale(locale.LC_ALL, '')\n\n# Create a sample DataFrame\ndf = pd.DataFrame({'Revenue': ['2,144.78', '2,036.62', '1,916.60', '1,809.40', '1,711.97', '6,667.22', '5,373.59', '4,071.00', '3,050.20', '-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0'],\n                   'Other, Net': ['-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0', '-0.8', '-1.12', '1.32', '-0.05', '-0.34', '-1.37', '-1.9', '-1.48', '0.1', '41.98', '35', '-11.66', '27.09', '-3.44', '14.13', '-18.69', '-4.87', '-5.7']})\n\n# Convert the Revenue column to float using pd.to_numeric\ndf['Revenue'] = pd.to_numeric(df['Revenue'].str.replace(',', ''), errors='coerce')\n\n# Print the resulting DataFrame\n",
        "\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\nresult = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0)).mean()['Survived']\n\n",
        "\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Group by the two conditions\ngrouped = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0), as_index=False)\n\n# Take the means of the two groups\nresult = grouped.mean()\n\n# Rename the columns\nresult.columns = ['Has Family', 'No Family']\n\n",
        "\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\nresult = df.groupby((df['SibSp'] == 1) & (df['Parch'] == 1), as_index=False).mean()['Survived']\nresult = result.append(df.groupby((df['SibSp'] == 0) & (df['Parch'] == 0), as_index=False).mean()['Survived'])\nresult = result.append(df.groupby((df['SibSp'] == 0) & (df['Parch'] == 1), as_index=False).mean()['Survived'])\nresult = result.append(df.groupby((df['SibSp'] == 1) & (df['Parch'] == 0), as_index=False).mean()['Survived'])\n\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(['A']))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(['A']))\n",
        "\nresult.columns = pd.MultiIndex.from_tuples([('Caps', 'Lower'), ('A', 'a'), ('A', 'b'), ('B', 'a'), ('B', 'b')])\nresult = result.stack().reset_index()\nresult.columns = ['index', 'Caps', 'Lower', 'Value']\nresult = result[['index', 'Caps', 'Lower', 'Value']]\n",
        "\nresult.columns = pd.MultiIndex.from_tuples([tuple(col.split(',')) for col in df.columns], names=['Caps', 'Middle', 'Lower'])\nresult.index.names = ['index']\n",
        "\nresult.columns = pd.MultiIndex.from_tuples([tuple(col.split(',')) for col in df.columns], names=['Caps', 'Middle', 'Lower'])\nresult = result.stack(level=['Caps', 'Middle', 'Lower']).reset_index()\nresult.columns = ['index', 'Caps', 'Middle', 'Lower', 'Value']\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('a')['b'].apply(stdMeann))\n\n",
        "\n\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('b')['a'].apply(stdMeann))\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\n# group by a\ngrouped = df.groupby('a')\n\n# calculate softmax and min-max normalization for each group\nresult = pd.DataFrame(columns=['a', 'b', 'softmax', 'min-max'])\nfor name, group in grouped:\n    softmax = np.exp(group['b']) / np.sum(np.exp(group['b']))\n    min_max = (group['b'] - group['b'].min()) / (group['b'].max() - group['b'].min())\n    result = result.append(pd.DataFrame({'a': name, 'b': group['b'], 'softmax': softmax, 'min-max': min_max}))\n\n",
        "\n\ndf = pd.DataFrame([[1,1,0,1],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n\n# Create a boolean mask to filter out rows with all zeros\nmask = (df.sum(axis=1) == 0)\n\n# Filter out the rows with all zeros\nresult = df[~mask]\n\n# Remove the columns with all zeros\nresult = result.loc[:, (result != 0).any(axis=0)]\n\n",
        "\n\ndf = pd.DataFrame([[-1,-1,0,2],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n\n# Create a new dataframe with only the rows and columns with sum of 0\nresult = df.loc[(df.sum(axis=1) == 0) | (df.sum(axis=0) == 0)]\n\n# Remove the rows and columns with sum of 0 from the original dataframe\ndf = df.loc[(df.sum(axis=1) != 0) & (df.sum(axis=0) != 0)]\n\n# Print the new dataframe\n",
        "\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df[df.max(axis=1) != 2]\nresult = result[result.max(axis=0) != 2]\n\n",
        "\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\ndf[df==2] = 0\n",
        "\nresult = s.sort_values(ascending=True)\n",
        "\nresult = s.sort_values(ascending=True, key=lambda x: (x.index, x))\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg('max').reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg('max').reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(['min']).reset_index()\nresult = df[df['count']==result['min']]\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Value','count'], how='inner')\n",
        "\nresult['Date'] = df['Date'].fillna(df['Member'])\nresult.loc[df['Member'].isin(dict.keys()), 'Date'] = df.loc[df['Member'].isin(dict.keys()), 'Member'].apply(lambda x: dict[x])\n",
        "\nresult['Date'] = df['Date'].fillna(df['Member'].apply(lambda x: dict.get(x, '17/8/1926')))\n",
        "\n\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\n\ndef f(dict=example_dict, df=example_df):\n    result = df.fillna(df['Member'])\n    for key, value in dict.items():\n        result.loc[result['Member'] == key, 'Date'] = value\n    return result\n",
        "\nresult['Date'] = df['Date'].fillna(df['Member'].apply(lambda x: dict.get(x, '17/8/1926')))\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).size()\ndf['Count_y'] = df.groupby(df['Date'].dt.year).size()\nresult = df[['Date', 'Val', 'Count_d', 'Count_m', 'Count_y']]\n",
        "\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month'), 'Val']).agg({'count'})\ndf1.reset_index(inplace=True)\ndf1.rename(columns={'count': 'Count_d'}, inplace=True)\ndf1['Count_m'] = df1.groupby(['year', 'month'])['Count_d'].transform('sum')\ndf1['Count_y'] = df1.groupby(['year'])['Count_d'].transform('sum')\ndf1['Count_Val'] = df1.groupby(['Val'])['Count_d'].transform('sum')\nresult = df1[['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_Val']]\n",
        "\n\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\n\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'Count_d': 'sum', 'Val': 'nunique'})\ndf1['Count_m'] = df1.groupby('year')['Count_d'].transform('sum')\ndf1['Count_y'] = df1.groupby('month')['Count_d'].transform('sum')\ndf1['Count_w'] = df1.groupby('month')['Count_d'].transform('sum')\ndf1['Count_Val'] = df1.groupby('month')['Val'].transform('sum')\n\nresult = df1\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: (x == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x != 0).sum())\n",
        "\nresult1 = df.groupby('Date')[['B', 'C']].apply(lambda x: (x % 2 == 0).astype(int))\nresult2 = df.groupby('Date')[['B', 'C']].apply(lambda x: (x % 2 != 0).astype(int))\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.sum, np.mean])\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.max, np.min])\n",
        "\n\ndf = dd.read_csv('file.csv')\n\nresult = df.assign(var2=df.var2.str.split(',').apply(pd.Series, 1).stack().reset_index(level=1, drop=True))\nresult = result.drop('var2', axis=1).join(result['var2'].apply(pd.Series).add_prefix('var2_'))\nresult = result.drop('var2', axis=1).join(result['var2_0'].str.split(',').apply(pd.Series, 1).stack().reset_index(level=1, drop=True).add_prefix('var2_'))\nresult = result.drop('var2_0', axis=1)\nresult = result.drop('var2_1', axis=1)\nresult = result.reset_index(drop=True)\n",
        "\n\ndf = dd.read_csv('file.csv')\n\nresult = df.var2.str.split(',').apply(pd.Series, 1).stack().reset_index(level=1, drop=True).to_frame('var2')\nresult['var1'] = df['var1']\nresult = result.reset_index(drop=True)\nresult = result.compute()\n",
        "\n\ndf = dd.read_csv('file.csv')\n\n# Split the string column into multiple rows\ndf['var2'] = df['var2'].str.split('-')\n\n# Create a new dataframe with the split values\nnew_df = df.var2.apply(pd.Series).stack().reset_index(level=1, drop=True).to_frame()\nnew_df.columns = ['var2']\n\n# Merge the new dataframe with the original dataframe\nresult = df.merge(new_df, left_index=True, right_index=True)\n\n# Drop the original string column\nresult = result.drop('var2', axis=1)\n\n# Rename the columns\nresult.columns = ['var1', 'var2']\n",
        "\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\n",
        "\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\n",
        "\nresult['fips'] = df['row'].str[:2]\nresult['row'] = df['row'].str[3:]\n",
        "\nresult['fips'] = df['row'].str[:2]\nresult['row'] = df['row'].str[3:]\n",
        "\n\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALAB AMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n\ndf[['fips', 'medi', 'row']] = df.row.str.split(expand=True)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\nresult = df.apply(lambda x: x.where(x != 0).mean(), axis=1)\n\n",
        "\nresult = df.apply(lambda x: x[x.notnull()].cumsum().divide(x.notnull().sum()), axis=1)\n",
        "\n\nexample_df = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\ndef f(df=example_df):\n    result = df.apply(lambda x: x.where(x != 0).mean(), axis=1)\n    return result\n",
        "\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\nresult = df.apply(lambda x: x.where(x != 0).mean(), axis=1).round(2)\n\n",
        "\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 1).astype(int)\ndf.loc[0, 'Label'] = 1\n",
        "\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\n\n# Create a new column 'label' with the difference between each row for Close column\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\n\n# Set the label of the first row to 1\ndf.loc[df.index[0], 'label'] = 1\n\nresult = df\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf.loc[0, 'label'] = 1\n",
        "\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\nfor i in range(len(df)-1):\n    df.loc[i+1, 'Duration'] = df.loc[i+1, 'departure_time'] - df.loc[i, 'arrival_time']\n\nresult = df\n",
        "\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\nfor i in range(len(df)-1):\n    df['Duration'][i] = (df['departure_time'][i+1] - df['arrival_time'][i]).total_seconds()\n\nresult = df\n",
        "\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\nfor i in range(len(df)-1):\n    df['Duration'][i] = (df['departure_time'][i+1] - df['arrival_time'][i]).total_seconds()\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'] == 'one'].shape[0])\n\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'] == 'two'].shape[0])\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'].str.endswith('e')].shape[0])\n",
        "\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\nmax_result = df.index.max()\nmin_result = df.index.min()\n\n",
        "\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\nmode_result = df.mode(axis=0)\nmedian_result = df.median(axis=0)\n\n",
        "\ndf = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\ndf = df[~(99 <= df['closing_price'] <= 101)]\n",
        "\ndf1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\n",
        "\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\nresult = df\n",
        "\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\nresult = df\n",
        "\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\nexample_df = pd.DataFrame(data={'SOURCE_NAME': strs})\ndef f(df=example_df):\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n    return df['SOURCE_NAME']\n",
        "\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# Filling the first 50% of NaN values with '0' and the last 50% with '1'\ndf['Column_x'].fillna(df['Column_x'].quantile(0.5), inplace=True)\ndf['Column_x'].fillna(df['Column_x'].quantile(0.5, interpolation='nearest'), inplace=True)\n\nresult = df\n",
        "\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace= True)\n",
        "\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,1,1,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# Fill all NaN values with 0\ndf['Column_x'].fillna(0, inplace=True)\n\n# Count the number of 0 and 1 in the column\ncount_0 = df['Column_x'].value_counts()[0]\ncount_1 = df['Column_x'].value_counts()[1]\n\n# Calculate the number of NaN values to be filled\nnum_nan = df['Column_x'].isna().sum()\n\n# Calculate the number of 0 and 1 to be filled\nnum_0 = int(num_nan/2)\nnum_1 = num_nan - num_0\n\n# Fill the remaining NaN values with 0\ndf['Column_x'].fillna(0, inplace=True)\n\n# Fill the remaining NaN values with 1\ndf['Column_x'].fillna(1, inplace=True)\n\n# Count the number of 0 and 1 in the column\ncount_0 = df['Column_x'].value_counts()[0]\ncount_1 = df['Column_x'].value_counts()[1]\n\n# Check if the counts are as expected\nif count_0 == num_0 and count_1 == num_1:\n    print(\"Success!\")\nelse:\n    print(\"Something went wrong.\")\n\nresult = df\n",
        "\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\n\nresult = pd.DataFrame(list(zip(a.values.tolist(), b.values.tolist())), columns=['one', 'two'])\n\n",
        "\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nc = pd.DataFrame(np.array([[9, 10],[11, 12]]), columns=['one', 'two'])\n\nresult = pd.DataFrame(list(zip(a.values.tolist(), b.values.tolist(), c.values.tolist())), columns=['one', 'two', 'three'])\n\n",
        "\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8],[9, 10]]), columns=['one', 'two'])\n\n# Create a list of tuples from the dataframes\ntuples = []\nfor i in range(len(a)):\n    if i < len(b):\n        tuples.append((tuple(a.iloc[i]), tuple(b.iloc[i])))\n    else:\n        tuples.append((tuple(a.iloc[i]), (np.nan, np.nan)))\n\n# Create a dataframe from the list of tuples\na_b = pd.DataFrame(tuples, columns=['one', 'two'])\n\n",
        "\n\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\ngroups = df.groupby('username')\nresult = groups.views.apply(lambda x: pd.cut(x, bins).value_counts().sort_index())\n\n",
        "\nresult = pd.crosstab(df.username, pd.cut(df.views, bins), normalize='index')\n",
        "\nresult = pd.crosstab(df.username, pd.cut(df.views, bins), normalize='index')\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = pd.DataFrame({'text': ['abc, def, ghi, jkl']})\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = pd.DataFrame({'text': ['-'.join(df['text'])]})\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = pd.DataFrame({'text': ['jkl, ghi, def, abc']})\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = pd.Series(df['text'].str.cat(sep=\", \"))\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = pd.concat([df1, df2], axis=0, ignore_index=True)\nresult['city'] = result['city'].fillna(result['city'].mode()[0])\nresult['district'] = result['district'].fillna(result['district'].mode()[0])\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)\nresult = result.fillna({'city': 'NaN', 'district': 'NaN'})\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby('id').apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.fillna({'city': 'NaN', 'district': 'NaN'})\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_left', '_right'))\nresult['B'] = result['B_left'].fillna(result['B_right'])\nresult = result.drop(columns=['B_left', 'B_right'])\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(columns=['B_x', 'B_y'])\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_c', '_d'))\nresult['duplicated'] = result.apply(lambda x: True if x['B_c'] == x['B_d'] else False, axis=1)\nresult = result[['A', 'B_c', 'B_d', 'duplicated']]\nresult.columns = ['A', 'B', 'B_d', 'duplicated']\nresult = result.sort_values(by=['A'])\nresult = result.reset_index(drop=True)\n",
        "\nresult = df.groupby('user').agg(lambda x: x.tolist())\n",
        "\nresult = df.groupby('user').agg(lambda x: list(zip(x['time'], x['amount']))).reset_index()\n",
        "\nresult = df.groupby('user').agg(lambda x: x.tolist())\n",
        "\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\ndf = pd.DataFrame(columns=range(4), index=series.index)\n\nfor i, arr in series.iteritems():\n    df.loc[i] = arr\n\nresult = df\n",
        "\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\ndf = pd.DataFrame(columns=['name'] + list(range(4)), index=series.index)\n\nfor i, (name, arr) in enumerate(series.iteritems()):\n    df.loc[name] = [name] + list(arr)\n\nresult = df\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\nresult = df.filter(like=s, axis=1)\n",
        "\n\ns = 'spike'\nresult = []\nfor col in df.columns:\n    if re.search(s, col):\n        result.append(col)\n\nnew_cols = []\nfor i, col in enumerate(result):\n    new_col = col.replace(s, '')\n    if new_col == '':\n        new_col = s\n    new_col += str(i+1)\n    new_cols.append(new_col)\n\ndf.columns = new_cols\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\n# create a new dataframe with the same columns as the original\nresult = pd.DataFrame(columns=['code_0', 'code_1', 'code_2'])\n\n# iterate over the rows of the original dataframe\nfor i, row in df.iterrows():\n    # get the list of codes for this row\n    codes = row['codes']\n    # create a new row in the result dataframe\n    new_row = {'code_0': None, 'code_1': None, 'code_2': None}\n    # iterate over the codes in the list\n    for j, code in enumerate(codes):\n        # assign the code to the corresponding column in the new row\n        new_row['code_' + str(j)] = code\n    # append the new row to the result dataframe\n    result = result.append(new_row, ignore_index=True)\n\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\n# create a new dataframe with the same columns as the original\nresult = pd.DataFrame(columns=['code_1', 'code_2', 'code_3'])\n\n# iterate over the rows of the original dataframe\nfor i, row in df.iterrows():\n    # split the list in the 'codes' column into columns\n    codes = row['codes']\n    if len(codes) == 1:\n        result.loc[i] = [codes[0], None, None]\n    elif len(codes) == 2:\n        result.loc[i] = [codes[0], codes[1], None]\n    elif len(codes) == 3:\n        result.loc[i] = [codes[0], codes[1], codes[2]]\n    else:\n        result.loc[i] = [codes[0], codes[1], codes[2]]\n\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99234, 99233, 99233]]})\n\n# Split the lists into columns\ndf = df.assign(\n    **{f'code_{i+1}': df['codes'].apply(lambda x: x[i] if len(x) > i else None) for i in range(3)}\n)\n\n# Fill NaNs with None\ndf = df.fillna(value=None)\n\n",
        "\nfrom ast import literal_eval\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nids = df.loc[0:1, 'col1'].values.tolist()\nresult = []\nfor i in ids:\n    result.extend(literal_eval(i))\n\n",
        "\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3],[4,5]]))\n\ndef reverse_list(lst):\n    return lst[::-1]\n\ndef reverse_lists(df):\n    result = ''\n    for i in range(len(df)):\n        result += str(reverse_list(df.loc[i, 'col1']))\n    return result\n\nresult = reverse_lists(df)\n",
        "\nfrom ast import literal_eval\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\n# Convert list values to string\ndf['col1'] = df['col1'].apply(lambda x: ','.join(map(str, x)))\n\n# Concatenate strings\nresult = ','.join(df['col1'].values.tolist())\n\n",
        "\npan = df.set_index('Time')\npan = pan.resample('2T').mean()\npan = pan.interpolate()\nresult = pan.reset_index()\n",
        "\npan = df.set_index('Time')\npan = pan.resample('3T').mean()\npan = pan.reset_index()\npan = pan.groupby(pd.Grouper(key='Time', freq='3T')).sum()\npan = pan.interpolate(method='linear')\nresult = pan\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])\n\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])\n\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n\nresult = df\n",
        "\nresult = df.loc[filt.index[filt], :]\n",
        "\nresult = df[filt]\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (np.isnan(x) and np.isnan(y))\n\nresult = df.apply(lambda x: equalp(x[0], x[8]), axis=1)\nresult = df.columns[result.values]\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (np.isnan(x) and np.isnan(y))\n\nresult = df.apply(lambda x: df.apply(lambda y: equalp(x[y.name], y[y.name]), axis=1), axis=1).apply(lambda x: x.index[x].tolist())\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (np.isnan(x) and np.isnan(y))\n\nresult = []\nfor i in range(2):\n    row = df.iloc[i]\n    for j in range(i+1, df.shape[0]):\n        if not equalp(row, df.iloc[j]):\n            result.append(list(set(row.index) & set(df.iloc[j].index)))\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n\nresult = []\nfor i in range(df.shape[0]):\n    for j in range(i+1, df.shape[0]):\n        if not all(equalp(df.iloc[i,k], df.iloc[j,k]) for k in range(df.shape[1])):\n            result.append([(df.iloc[i,k], df.iloc[j,k]) for k in range(df.shape[1]) if not equalp(df.iloc[i,k], df.iloc[j,k])])\n\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n",
        "\nresult = df.iloc[0]\n",
        "\nresult = df.iloc[0]\n",
        "\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, .03), (.21, .18),(pd.NA, .18)],\n                  columns=['dogs', 'cats'])\n\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n\nresult = df\n",
        "\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\n\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n\nresult = df\n",
        "\nresult['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\nresult['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\nresult['Avg'] = df[list_of_my_columns].mean(axis=1)\nresult['Min'] = df[list_of_my_columns].min(axis=1)\nresult['Max'] = df[list_of_my_columns].max(axis=1)\nresult['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_index(level='time')\n\n",
        "\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_values(by='VIM', kind='mergesort')\n\n",
        "\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\nsp = sp[(sp.index < hd1_from) | (sp.index > hd1_till)]\n",
        "\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\nsp = sp[(sp.index < hd1_from) | (sp.index > hd1_till)]\n",
        "\nresult = corr[corr > 0.3]\n",
        "\nresult = corr[(corr > 0.3) & (corr != 1)]\n",
        "\n\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\ndf.columns[-1] = 'Test'\n",
        "\n\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\ndf.columns[0] = 'Test'\n",
        "\n\ndf = pd.DataFrame({'bit1': [0, 1, 1],\n                   'bit2': [0, 1, 0],\n                   'bit3': [1, 0, 1],\n                   'bit4': [1, 0, 1],\n                   'bit5': [0, 1, 1]})\n\n# create a new dataframe with frequent and freq_count columns\nresult = pd.DataFrame(columns=['frequent', 'freq_count'])\n\n# iterate over each row of the dataframe\nfor index, row in df.iterrows():\n    # create a list of all values in the row\n    values = list(row)\n    # create a set of unique values in the row\n    unique_values = set(values)\n    # create a dictionary to store the count of each unique value\n    count_dict = {}\n    for value in unique_values:\n        count_dict[value] = values.count(value)\n    # find the frequent value and its count\n    frequent_value = max(count_dict, key=count_dict.get)\n    frequent_count = count_dict[frequent_value]\n    # add the frequent value and its count to the result dataframe\n    result.loc[index] = [frequent_value, frequent_count]\n\n",
        "\n\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4]})\n\n# Create a new dataframe with frequent and freq_count columns\nresult = df.groupby(df.columns.tolist()[:-2]).size().reset_index(name='freq_count')\n\n# Find the frequent value in each row\nresult['frequent'] = result.apply(lambda x: df[df.columns.tolist()[:-2]].apply(lambda y: y.value_counts().index[0] if y.value_counts().iloc[0] == x['freq_count'] else None).dropna().iloc[0], axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4],\n                   'bit6': [3, 0, 5]})\n\n# Group the rows by the values in each column\ngrouped = df.groupby(['bit1', 'bit2', 'bit3', 'bit4', 'bit5'])\n\n# Count the number of occurrences of each group\ncounts = grouped.size().reset_index(name='count')\n\n# Filter out the groups that have only one occurrence\nfrequent = counts[counts['count'] > 1]\n\n# Return the frequent values as a list\nresult = frequent['bit1'].tolist()\n",
        "\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\ngroupedFrame = df.groupby([\"id1\",\"id2\"])\naggrFrame = groupedFrame.aggregate(np.mean)\n\nresult = pd.concat([aggrFrame, groupedFrame[\"bar\"].mean().rename(\"bar\")], axis=1)\n\n",
        "\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean().fillna(0)\n\n",
        "\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(columns=['b_col'])\n\n",
        "\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(columns=['a_col'])\n\n"
    ],
    "Numpy": [
        "\na = np.array([[1,2],[3,4]])\nresult = a.shape\n",
        "\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = np.nan_to_num(x)\nx = x[~np.isnan(x)]\n",
        "\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = np.nan_to_num(x, nan=np.inf)\n",
        "\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nresult = x[~np.isnan(x)].tolist()\n",
        "\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a] = 1\n",
        "\nb = np.eye(max(a)+1)[a]\n",
        "\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a] = 1\n",
        "\na = np.array([1.5, -0.4, 1.3])\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a.astype(int)] = 1\n",
        "\nb = np.zeros((a.shape[0], a.max()+1), dtype=int)\nb[np.arange(a.shape[0]), a] = 1\n",
        "\na = np.array([1,2,3,4,5])\np = 25\nresult = np.percentile(a, p)\n",
        "\nB = A.reshape(-1,ncol)\n",
        "\nB = A.reshape(nrow,-1)\n",
        "\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = A.reshape(-1,ncol)\n",
        "\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = A.reshape(-1,ncol)\n",
        "\nresult = np.roll(a, shift)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.empty_like(a)\nfor i in range(a.shape[0]):\n    result[i] = np.roll(a[i], shift[i])\n",
        "\n\n# create a random array of shape (100,2000) with values of either -1,0, or 1\nr = np.random.randint(3, size=(100, 2000)) - 1\n\n# save the array to a file\nnp.save('random_array.npy', r)\n\n# load the array from the file\nr_new = np.load('random_array.npy')\n\n",
        "\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\na = np.array([[10,50,30],[60,20,40]])\nresult = np.ravel_multi_index(a.argmin(axis=None), a.shape)\n",
        "\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape, order='F')\n",
        "\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nexample_a = np.array([[10,50,30],[60,20,40]])\ndef f(a = example_a):\n    flat_a = a.ravel(order='C')\n    max_val = np.max(flat_a)\n    max_indices = np.where(flat_a == max_val)[0]\n    result = max_indices\n    return result\n",
        "\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argsort(a.flatten())[-2], a.shape)\n",
        "\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\nz = np.any(np.isnan(a), axis=0)\na = np.delete(a, np.where(z), axis=1)\n",
        "\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\n\n# Find the indices of the rows that contain NaN\nnan_rows = np.where(np.isnan(a).any(axis=1))[0]\n\n# Delete the rows that contain NaN\na = np.delete(a, nan_rows, axis=0)\n\n",
        "\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nresult = np.array(a)\n",
        "\n\na = np.array([[10, 20, 30, 40, 50],\n       [ 6,  7,  8,  9, 10]])\n\npermutation = [0, 4, 1, 3, 2]\n\n# Get the indices of the columns in the new order\nindices = np.argsort(permutation)\n\n# Transpose the array and reorder the columns\na = a.transpose()[indices].transpose()\n\n",
        "\na = np.array([[[10, 20],\n        [30, 40]],\n       [[6,  7],\n        [8,  9]],\n\t[[10, 11],\n\t [12, 13]]])\npermutation = [1, 0, 2]\nresult = a.transpose(permutation)\n",
        "\na = np.array([[1, 2], [3, 0]])\nresult = np.unravel_index(a.argmin(), a.shape)\n",
        "\n\na = np.array([[1, 2], [3, 0]])\n\n# Find the indices of the maximum element in the matrix\nmax_index = np.argmax(a)\n\n# Extract the row and column indices from the max_index\nrow_index = max_index // a.shape[1]\ncol_index = max_index % a.shape[1]\n\n# Print the result\n",
        "\na = np.array([[1, 0], [0, 2]])\nresult = np.stack([np.argmin(a, axis=0), np.argmin(a, axis=1)], axis=1)\n",
        "\ndegree = 90\nresult = np.degrees(np.arcsin(np.sin(np.radians(degree))))\n",
        "\ndegree = 90\nresult = np.cos(np.radians(degree))\n",
        "\n\nnumber = np.random.randint(0, 360)\n\nif np.sin(np.deg2rad(number)) > 0.5:\n    result = 0\nelse:\n    result = 1\n\n",
        "\n\nvalue = 1.0\nresult = np.degrees(np.arcsin(value))\n\n",
        "\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0))\n",
        "\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0))\n",
        "\na = np.arange(4).reshape(2, 2)\npower = 5\na = a**power\n",
        "\nexample_a = np.arange(4).reshape(2, 2)\ndef f(a = example_a, power = 5):\n    result = a ** power\n    return result\n",
        "\nnumerator = 98\ndenominator = 42\nresult = np.gcd(numerator, denominator)\nnumerator //= result\ndenominator //= result\n",
        "\ndef f(numerator = 98, denominator = 42):\n    result = np.gcd(numerator, denominator)\n    return (numerator // result, denominator // result)\n",
        "\nnumerator = 98\ndenominator = 42\nif denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // denominator, numerator % denominator)\n",
        "\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = (a + b + c) / 3\n",
        "\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = np.maximum(np.maximum(a, b), c)\n",
        "\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\ndiagonal = np.diag_indices(5, k=-1)\nresult = a[diagonal]\n",
        "\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i][j])\n    return result\n",
        "\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\n",
        "\nmystr = \"100110\"\nresult = np.fromstring(mystr, dtype=int, sep='')\n",
        "\na = np.random.rand(8, 5)\ncol = 2\nmultiply_number = 5.2\na[:, col] = a[:, col] * multiply_number\nresult = np.cumsum(a[:, col])\n",
        "\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\n\n# multiply the row-th row of the array by the multiply_number\na[row] *= multiply_number\n\n# calculate the cumulative sum of the numbers in the row\nresult = np.cumsum(a[row])\n\n",
        "\na = np.random.rand(8, 5)\nrow = 2\ndivide_number = 5.2\n\nrow_array = a[row]\nrow_array = row_array / divide_number\nresult = np.prod(row_array)\n\n",
        "\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\nnull_space = np.linalg.null_space(a)\n",
        "\na = np.random.rand(np.random.randint(5, 10), np.random.randint(6, 10))\nresult = a.shape[0]\n",
        "\n\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# combine the two arrays into one array\ndata = np.concatenate((a, b))\n\n# calculate the mean and standard deviation of the combined array\nmean = np.mean(data)\nstd_dev = np.std(data, ddof=1)\n\n# calculate the t-test and get the p-value\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n\n",
        "\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# Remove nans from data\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n\n# Calculate weighted t-test\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n\n",
        "\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\n\n# Calculate the weighted means\nwmean = (anobs*amean + bnobs*bmean)/(anobs+bnobs)\n\n# Calculate the weighted variances\nwvar = ((anobs-1)*avar + (bnobs-1)*bvar + (anobs*bnobs/((anobs+bnobs)**2))*(amean-bmean)**2)/(anobs+bnobs-2)\n\n# Calculate the degrees of freedom\ndf = anobs + bnobs - 2\n\n# Calculate the t-value\nt_value = (wmean - 0)/(np.sqrt(wvar)/np.sqrt(anobs+bnobs))\n\n# Calculate the p-value\np_value = 2*(1-scipy.stats.t.cdf(np.abs(t_value),df=df))\n\n",
        "\nA=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\n# Create a set of tuples from B\nB_set = set(tuple(x) for x in B)\n\n# Create a list of tuples from A\nA_list = [tuple(x) for x in A]\n\n# Use set difference to remove elements from A that are in B\noutput = [list(x) for x in A_list if tuple(x) not in B_set]\n\n",
        "\nA=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\n#get elements in A that are not in B\nnot_in_B = np.setdiff1d(A, B)\n\n#get elements in B that are not in A\nnot_in_A = np.setdiff1d(B, A)\n\n#combine the two arrays\noutput = np.concatenate((not_in_B, not_in_A))\n\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)[::-1]\nc = b[sort_indices]\n",
        "\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Sort b by the values of a\na_sum = np.sum(a, axis=(1, 2))\nb_sorted_indices = np.argsort(a_sum)\nresult = b[b_sorted_indices]\n\n",
        "\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\narr = np.delete(arr, 2, axis=1)\n",
        "\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\narr = np.delete(arr, 2, axis=0)\n",
        "\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\narr = np.delete(arr, [0, 2], axis=1)\n",
        "\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\ndel_col = [1, 2, 4, 5]\narr = np.delete(arr, del_col, axis=1)\ndef_col = np.array([1, 2, 4, 5])\nresult = arr[:, def_col-1]\n",
        "\na_l = a.tolist()\na_l.insert(pos, element)\na = np.asarray(a_l)\n",
        "\na = np.array([[1,2],[3,4]])\n\npos = 1\nelement = [3,5]\na = np.insert(a, pos, element, axis=0)\n\n",
        "\nexample_a = np.asarray([1,2,3,4])\ndef f(a = example_a, pos=2, element = 66):\n    a_l = a.tolist()\n    a_l.insert(pos, element)\n    a = np.asarray(a_l)\n    return a\n",
        "\na = np.array([[1,2],[3,4]])\npos = [1, 2]\nelement = np.array([[3, 5], [6, 6]])\n\nfor i in range(len(pos)):\n    a = np.insert(a, pos[i], element[i], axis=0)\n\n",
        "\npairs = [(2, 3), (3, 4), (4, 5)]\narray_of_arrays = np.array([np.arange(a*b).reshape(a,b) for (a, b) in pairs])\n\n# Create a deep copy of the array of arrays\nresult = np.array([np.copy(arr) for arr in array_of_arrays])\n\n",
        "\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\nresult = np.all(np.all(a == a[0], axis=1))\n",
        "\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\nresult = np.all(a == a[:,0].reshape(-1, 1), axis = 0)\n",
        "\nexample_a = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\ndef f(a = example_a):\n    result = np.all([np.array_equal(a[0], a[i]) for i in range(1,len(a))])\n    return result\n",
        "\nfrom scipy.integrate import trapz, dblquad\n\ndef f(x, y):\n    return np.cos(x)**4 + np.sin(y)**2\n\nx = np.linspace(0, 1, 20)\ny = np.linspace(0, 1, 30)\nX, Y = np.meshgrid(x, y)\n\n# Approximate the integral using trapezoidal rule\nresult = trapz(trapz(f(X, Y), x), y)\n\n# Multiply by the area of the rectangle\nresult *= (x[-1] - x[0]) * (y[-1] - y[0])\n\n",
        "\nfrom scipy.integrate import trapz\n\nexample_x = np.linspace(0, 1, 20)\nexample_y = np.linspace(0, 1, 30)\n\ndef f(x = example_x, y = example_y):\n    return np.cos(x)**4 + np.sin(y)**2\n\ndef integrate_2d(f, x, y):\n    dx = x[1] - x[0]\n    dy = y[1] - y[0]\n    area = dx * dy\n    result = trapz(trapz(f(x, y), x), y)\n    return result * area\n\nresult = integrate_2d(f, example_x, example_y)\n",
        "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n\nresult = ecdf(grades)\n",
        "\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\neval = np.array([88, 87, 62])\n\n# normalize grades to sum to 1\ngrades = grades / np.sum(grades)\n\n# generate ECDF function\ndef ecdf(x):\n  return np.cumsum(x)\n\n# apply ECDF function to eval array\nresult = ecdf(grades)[eval]\n\n",
        "\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\nthreshold = 0.5\n\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\n# find the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow = 0\nhigh = len(grades)\nwhile high - low > 1:\n    mid = (low + high) // 2\n    if ecdf(grades[:mid+1])[-1] < threshold:\n        low = mid\n    else:\n        high = mid\n\n",
        "\n\none_ratio = 0.9\nsize = 1000\n\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n\n",
        "\na_np = a.numpy()\n",
        "\na = np.ones(5)\na_pt = torch.from_numpy(a)\n",
        "\na_np = a.numpy()\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\nresult = np.argsort(a)[-N:][::-1]\n",
        "\nA = np.arange(16).reshape(4, 4)\nn = 5\nresult = np.linalg.matrix_power(A, n)\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        patch = a[i:i+2, j:j+2]\n        if not np.array_equal(patch[0], patch[1]):\n            result.append(patch)\n\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(a.shape[0] - 1):\n    for j in range(a.shape[1] - 1):\n        result.append(a[i:i+2, j:j+2])\n\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        patch = a[i:i+2, j:j+2]\n        if not np.array_equal(patch[0], patch[1]):\n            result.append(patch)\n\n",
        "\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\n# calculate the number of patches in each dimension\nnum_patches_x = a.shape[1] // patch_size\nnum_patches_y = a.shape[0] // patch_size\n\n# create an empty list to store the patches\npatches = []\n\n# loop through each patch and append it to the list\nfor i in range(num_patches_x):\n    for j in range(num_patches_y):\n        patch = a[j*patch_size:(j+1)*patch_size, i*patch_size:(i+1)*patch_size]\n        patches.append(patch)\n\n# convert the list to a numpy array\nresult = np.array(patches)\n\n",
        "\nresult = np.concatenate([a.reshape(h, w, -1)[:, :, i] for i in range(a.shape[-1])], axis=1)\n",
        "\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\n# calculate the number of patches in each dimension\nnum_patches_x = a.shape[1] // patch_size\nnum_patches_y = a.shape[0] // patch_size\n\n# create an empty array to store the patches\nresult = np.zeros((num_patches_y, num_patches_x, patch_size, patch_size))\n\n# loop through each patch and fill it with the corresponding elements from the original array\nfor i in range(num_patches_y):\n    for j in range(num_patches_x):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n\n",
        "\nresult = a[:, low-1:high]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:, low-1:high]\n",
        "\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.fromstring(string, dtype=float, sep=' ')\n",
        "\n\nmin = 1\nmax = np.e\nn = 10000\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\n\nmin = 0\nmax = 1\nn = 10000\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\ndef f(min=1, max=np.e, n=10000):\n    result = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n    return result\n",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a*A[0]\nB[1] = a*A[1]+b*B[0]\nfor t in range(2,len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nlinearInd = np.ravel_multi_index(index, dims, order='F') - 1\n",
        "\n\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\n\nresult = np.ravel_multi_index(index, dims, order='C')\n\n",
        "\nindex = ['x', 'y']\ncolumns = ['a','b','c']\nvalues = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nresult = np.bincount(accmap, weights=a, minlength=3)\n",
        "\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.zeros(3)\nfor i in range(len(index)):\n    if index[i] == 0:\n        result[0] = max(result[0], a[i])\n    elif index[i] == 1:\n        result[1] = max(result[1], a[i])\n    else:\n        result[2] = max(result[2], a[i])\n",
        "\nresult = np.bincount(accmap, weights=a)\n",
        "\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.zeros(len(index))\nfor i in range(len(index)):\n    if index[i] >= 0:\n        result[i] = a[index[i]]\n    else:\n        result[i] = np.min(a[:index[i]])\n",
        "\nx = np.array([[2, 2, 2],\n              [2, 2, 2],\n              [2, 2, 2]])\ny = np.array([[3, 3, 3],\n              [3, 3, 3],\n              [3, 3, 1]])\nz = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n",
        "\nprobabilit = [0.333, 0.334, 0.333]\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\na = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\n\n# pad the array with zeros\npadded_a = np.pad(a, ((0, 0), (0, 0)), 'constant', constant_values=0)\n\n# slice the padded array\nresult = padded_a[low_index:high_index, low_index:high_index]\n\n",
        "\nx = np.array([-2, -1.4, -1.1, 0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2])\nresult = np.delete(x, np.where(x < 0))\n",
        "\nx = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\nresult = np.delete(x, np.where(np.abs(x) == 0))\n",
        "\nbin_data = np.array_split(data, len(data)//bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nbin_data = np.array_split(data, len(data)//bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n",
        "\nbin_data = np.array([np.mean(data[i:i+bin_size], axis=0) for i in range(0, len(data), bin_size)])\nbin_data_mean = bin_data.mean(axis=1)\n",
        "\nbin_data = np.array([data[-i:] for i in range(bin_size, len(data)+1, bin_size)])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = []\nfor i in range(data.shape[1] - bin_size + 1):\n    bin_data.append(data[:, i:i+bin_size])\nbin_data = np.array(bin_data)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = []\nfor row in data:\n    # calculate the number of bins needed\n    num_bins = len(row) // bin_size\n    # calculate the last bin size\n    last_bin_size = len(row) % bin_size\n    # create the bins\n    bins = [row[i:i+bin_size] for i in range(0, len(row), bin_size)]\n    # add the last bin if it exists\n    if last_bin_size > 0:\n        bins[-1] = bins[-1][:last_bin_size]\n    # add the bins to the bin_data list\n    bin_data.append(bins)\n\nbin_data_mean = []\nfor row in bin_data:\n    # calculate the mean of each bin\n    bin_means = [np.mean(bin) for bin in row]\n    # add the bin means to the bin_data_mean list\n    bin_data_mean.append(bin_means)\n\n",
        "\ndef smoothclamp(x, x_min=0, x_max=1):\n    if x < x_min:\n        return x_min\n    elif x > x_max:\n        return x_max\n    else:\n        return 3*x**2 - 2*x**3\n",
        "\ndef smoothclamp(x, N=5):\n    x = np.clip(x, x_min, x_max)\n    x = (x - x_min) / (x_max - x_min)\n    result = 0\n    for i in range(N):\n        result += np.power(x, i) / np.math.factorial(i)\n    return result * (x_max - x_min) + x_min\n",
        "\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\nresult = np.correlate(a, b, mode='same', method='direct')\n",
        "\nresult = df.values.reshape(4, 15, 5)\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\na = np.array([1, 2, 3, 4, 5])\nm = 8\nresult = np.zeros((a.shape[0], m), dtype=np.uint8)\nfor i in range(a.shape[0]):\n    result[i] = np.unpackbits(np.uint8(a[i]), bitorder='little')[-m:]\n",
        "\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = np.zeros((a.shape[0], m), dtype=np.uint8)\nfor i in range(a.shape[0]):\n    num = a[i]\n    if num >= 0:\n        bits = np.unpackbits(np.uint8(num))\n        result[i, -len(bits):] = bits\n    else:\n        bits = np.unpackbits(np.uint8(2**m + num))\n        result[i, -len(bits):] = bits[-len(bits):]\n",
        "\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = np.zeros((a.shape[0], m))\nfor i in range(a.shape[0]):\n    binary = np.unpackbits(np.uint8(a[i]))\n    result[i][:binary.shape[0]] = binary\nresult = np.sum(result, axis=0) % 2\n",
        "\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\n# Calculate the standard deviation of the array\nstd_dev = np.std(a)\n\n# Calculate the mean of the array\nmean = np.mean(a)\n\n# Calculate the start and end of the 3rd standard deviation interval\nstart = mean - 3*std_dev\nend = mean + 3*std_dev\n\n# Print the result as a tuple\nresult = (start, end)\n",
        "\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\n# Calculate the mean\nmean = np.mean(a)\n\n# Calculate the 2nd standard deviation\nstd_dev = np.std(a)\nstd_dev_2 = std_dev * 2\n\n# Calculate the start and end of the 2nd standard deviation interval\nstart = mean - std_dev_2\nend = mean + std_dev_2\n\n# Print the result\nresult = (start, end)\n",
        "\nexample_a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\ndef f(a = example_a):\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3*std, mean + 3*std)\n    return result\n",
        "\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\nstd_dev = np.std(a)\nmean = np.mean(a)\nsecond_std_dev = std_dev * 2 + mean\nresult = (a > second_std_dev) | (a < mean - second_std_dev)\n",
        "\n\ndata = [[-5.5, -4.5, -3.5, -2.5, -1.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5]]\n\nDataArray = np.array(data)\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\npercentile = 50\nprob = np.percentile(masked_data, percentile)\n\n",
        "\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = 0\nzero_cols = 0\na = np.delete(a, zero_rows, axis=0)\na = np.delete(a, zero_cols, axis=1)\n",
        "\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = [1, 3]\nzero_cols = [1, 2]\na = np.delete(a, zero_rows, axis=0)\na = np.delete(a, zero_cols, axis=1)\n",
        "\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\na[1, :] = 0\na[:, 0] = 0\n",
        "\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(len(a)), np.argmax(a, axis=1)] = True\n",
        "\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.argmin(a, axis=1)\nmask = np.array([mask == i for i in range(a.shape[1])]).T\n",
        "\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n\n# Calculate the mean of each list\npost_mean = np.mean(post)\ndistance_mean = np.mean(distance)\n\n# Calculate the standard deviation of each list\npost_std = np.std(post)\ndistance_std = np.std(distance)\n\n# Calculate the numerator and denominator of the Pearson correlation coefficient formula\nnumerator = sum([(post[i] - post_mean) * (distance[i] - distance_mean) for i in range(len(post))])\ndenominator = post_std * distance_std\n\n# Calculate the Pearson correlation coefficient\nresult = numerator / denominator\n\n",
        "\nX = np.random.randint(2, 10, (5, 6))\nresult = np.array([np.dot(X[:, i].reshape(-1, 1), X[:, i].reshape(1, -1)) for i in range(X.shape[1])])\n",
        "\n\nY = np.array([[[81, 63, 63],\n        [63, 49, 49],\n        [63, 49, 49]],\n\n       [[ 4, 12,  8],\n        [12, 36, 24],\n        [ 8, 24, 16]],\n\n       [[25, 35, 25],\n        [35, 49, 35],\n        [25, 35, 25]],\n\n       [[25, 30, 10],\n        [30, 36, 12],\n        [10, 12,  4]]])\n\nX = np.linalg.inv(Y).dot(Y)\n\n",
        "\na = np.array([9, 2, 7, 0])\nnumber = 0\nis_contained = (number in a)\n",
        "\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\n# Find the indices of elements in B that are present in A\nidx = np.isin(B, A)\n\n# Remove the corresponding elements from A\nA = A[~idx]\n\n# The resulting array is C\nC = A\n\n",
        "\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = A[np.isin(A, B)]\n",
        "\nC = np.array([2,3,3,3,5,6,7])\n",
        "\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = rankdata(a, method='max').astype(int)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nfrom scipy.stats import rankdata\nexample_a = [1,2,3,4,3,2,3,4]\ndef f(a = example_a):\n    # Sort the list in descending order\n    a_sorted = sorted(a, reverse=True)\n    # Create a dictionary with the values as keys and their indices as values\n    a_dict = {value: index for index, value in enumerate(a_sorted)}\n    # Create a list with the indices of the original list in the sorted list\n    result = [a_dict[value] for value in a]\n    return result\n",
        "\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, 1, -2],\n                 [ -1,  0, 1],\n                 [ -2,  1,  0]])\n\ndists = np.stack((x_dists, y_dists), axis=-1)\n",
        "\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ndists = np.stack((x_dists, y_dists), axis=-1)\n",
        "\nresult = a[:, second, third]\n",
        "\narr = np.zeros((20,)*4)\narr = arr.reshape((20,10,10,2))\n",
        "\nfrom numpy import linalg as LA\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\nl1 = np.sum(np.abs(X), axis=1)\nresult = X / l1.reshape(-1, 1)\n",
        "\nfrom numpy import linalg as LA\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\nnorms = LA.norm(X, axis=1)\nnorms = norms.reshape(X.shape[0], 1)\nresult = X / norms\n\n",
        "\nfrom numpy import linalg as LA\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\nnorms = np.apply_along_axis(LA.norm, 1, X, ord=np.inf)\nresult = X / norms[:, np.newaxis]\n",
        "\n\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nchoices = ['XX']\n\nconditions = [df['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\n\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\n\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n\n# Calculate distance matrix using pdist\ndist_matrix = squareform(pdist(a))\n\n# Print distance matrix\n",
        "\n\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n\nresult = np.zeros((a.shape[0], a.shape[0]))\n\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        result[i][j] = np.linalg.norm(a[i]-a[j])\n        result[j][i] = result[i][j]\n\n",
        "\n\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n\nresult = np.zeros((a.shape[0], a.shape[0]))\n\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        result[i][j] = np.linalg.norm(a[i]-a[j])\n        result[j][i] = result[i][j]\n\n",
        "\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=float)\nAVG = np.mean(NA)\n",
        "\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA)\n",
        "\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=float)\nAVG = np.mean(NA, axis=0)\n",
        "\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\nresult = np.unique(a[a != 0])\n",
        "```python\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\n\n# Remove adjacent duplicate non-zero values and all zero values\nresult = np.concatenate([a[1:], a[1:] != a[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-1]], axis=0)\nresult = np.concatenate([result[1:], result[1:] != result[:-",
        "\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\ndf = pd.DataFrame(np.concatenate((lat, lon, val), axis=1), columns=['lat', 'lon', 'val'])\n\n",
        "\n\ndef f(lat = example_lat, lon = example_lon, val = example_val):\n    # Create a list of tuples with the lat, lon, and val values\n    data = list(zip(lat.flatten(), lon.flatten(), val.flatten()))\n    # Create a dataframe with the lat, lon, and val columns\n    df = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\n    # Set the index to the lat values\n    df.set_index('lat', inplace=True)\n    # Sort the index\n    df.sort_index(inplace=True)\n    # Return the dataframe\n    return df\n",
        "\n\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\n# Create a list of tuples with the lat, lon, and val values\ndata = [(lat[i][j], lon[i][j], val[i][j]) for i in range(len(lat)) for j in range(len(lat[i]))]\n\n# Create a dataframe with the lat, lon, and val values\ndf = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\n\n# Add a column with the maximum value of each row\ndf['maximum'] = df.apply(lambda row: max(row['lat'], row['lon'], row['val']), axis=1)\n\n",
        "\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\n\nsize = (3, 3)\n\n# Add padding to handle edge effects\npad_width = ((size[0] - 1) // 2, (size[0] - 1) // 2), ((size[1] - 1) // 2, (size[1] - 1) // 2)\na_pad = np.pad(a, pad_width, mode='edge')\n\n# Get the sliding window view\nresult = np.lib.stride_tricks.sliding_window_view(a_pad, size)\n\n# Slice the padded array to remove padding\nresult = result[:, :, size[0] // 2:-(size[0] // 2), size[1] // 2:-(size[1] // 2)]\n\n",
        "\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\n\nsize = (3, 3)\n\n# Add padding to handle edge effects\npad_width = ((size[0] - 1) // 2, (size[0] - 1) // 2), ((size[1] - 1) // 2, (size[1] - 1) // 2)\na_pad = np.pad(a, pad_width, mode='edge')\n\n# Get the sliding window view\nresult = np.lib.stride_tricks.sliding_window_view(a_pad, size)\n\n# Slice the padded array to remove padding\nresult = result[:, :, size[0] // 2:-(size[0] // 2), size[1] // 2:-(size[1] // 2)]\n\n",
        "\n\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\n\n# Replace complex infinity values with a finite value\na[np.isinf(a)] = 100\n\nresult = np.mean(a)\n\n",
        "\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    result = np.mean(a)\n    return result\n",
        "\nZ = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\nresult = Z[..., -1:]\n",
        "\na = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\nresult = a[[Missing]]\n",
        "\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ 57, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  78, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ 57, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, 727]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ 66, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\nif c in CNTS:\n    result = True\nelse:\n    result = False\n",
        "\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ np.nan, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  np.nan, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, np.nan]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ np.nan, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\n\nresult = any(np.array_equal(c, cnt) for cnt in CNTS)\n",
        "\nfrom scipy import interpolate as intp\n\na = np.arange(0, 4, 1).reshape(2, 2)\na = a.repeat(2, axis=0).repeat(2, axis=1)\n\nx_new = np.linspace(0, 2, 4)\ny_new = np.linspace(0, 2, 4)\n\nf = intp.interp2d(np.arange(a.shape[0]), np.arange(a.shape[1]), a, kind='linear')\nresult = f(x_new, y_new)\n\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].apply(lambda x: np.cumsum(x))\n",
        "\ni = np.diag(i)\n",
        "\na = np.array([[1,0,2,3],[0,5,3,4],[2,3,2,10],[3,4, 10, 7]])\nnp.fill_diagonal(a, 0)\na[np.triu_indices(a.shape[0], k=1)] = 0\n",
        "\n\nt0 = dateutil.parser.parse(start)\ntf = dateutil.parser.parse(end)\ndelta = (tf - t0) / n\nresult = pd.DatetimeIndex(start=t0, end=tf, periods=n, freq=delta)\n",
        "\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nresult = -1\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result = i\n        break\n",
        "\nresult = np.where(np.logical_and(x == a, y == b))[0]\n",
        "\n\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\n\n# Create a matrix A with the values of x and y\nA = np.vstack([x, np.ones(len(x))]).T\n\n# Solve the system of equations using numpy's lstsq function\ncoefficients, residuals, rank, singular_values = np.linalg.lstsq(A, y, rcond=None)\n\n# Extract the values of a, b, and c from the coefficients array\na, b, c = coefficients\n\n# Print the result\n",
        "\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\ndegree = 3\nA = np.vander(x, degree+1)\nc, res, rank, s = np.linalg.lstsq(A, y, rcond=None)\nresult = c[::-1]\n",
        "\ntemp_arr = [0,1,2,3]\ntemp_df = pd.DataFrame(np.repeat([1,2,3,4], 4).reshape(4, -1))\ntemp_df = temp_df.apply(lambda x: x-temp_arr[temp_df.index.values[0]])\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\na = np.array([[-1, 2], [-0.5, 6]])\nscaler = MinMaxScaler()\na_flat = a.reshape(-1, 1)\nresult = scaler.fit_transform(a_flat)\nresult = result.reshape(a.shape)\n",
        "\nresult = MinMaxScaler().fit_transform(arr)\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[[1, 0.5, -2], [-0.5,1, 6], [1,1,1]], [[-2, -3, 1], [-0.5, 10, 6], [1,1,1]]])\n\nscaler = MinMaxScaler(axis=None)\nresult = scaler.fit_transform(a)\n\n",
        "\narr = (np.random.rand(100, 50)-0.5) * 50\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] += 5\narr[~mask2] = 30\n",
        "\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\n\nfor i in range(len(n1)):\n    arr_temp = arr.copy()\n    mask = arr_temp < n1[i]\n    mask2 = arr_temp >= n2[i]\n    mask3 = mask ^ mask2\n    arr[mask] = 0\n    arr[mask3] = arr[mask3] + 5\n    arr[~mask2] = 30\n\n",
        "\n\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\n\nresult = np.sum(np.not_equal(s1, s2))\n\n",
        "\n\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\ns1 = np.append(s1, np.nan)\ns2 = np.append(s2, np.nan)\n\n# Use numpy.isclose() to compare s1 and s2 element-wise\nequal = np.isclose(s1, s2)\n\n# Count the number of truly different elements in s1 and s2\nresult = np.count_nonzero(~equal)\n\n",
        "\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\nresult = all(np.array_equal(a[0], a[i]) for i in range(1, len(a)))\n",
        "\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\nresult = all(np.isnan(arr).all() for arr in a)\n",
        "\n\na = np.ones((41, 13))\nshape = (93, 13)\n\nresult = np.pad(a, pad_width=((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant', constant_values=0)\n\n",
        "\n\na = np.ones((41, 12))\nshape = (93, 13)\n\nresult = np.pad(a, pad_width=((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant', constant_values=0)\n\n",
        "\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n",
        "\nexample_arr = np.ones((41, 13))\ndef f(arr = example_arr, shape=(93,13)):\n    result = np.pad(arr, pad_width=((0, 2), (0, 3)), mode='constant', constant_values=0)\n    return result\n",
        "\na = np.ones((41, 12))\nshape = (93, 13)\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant', constant_values=0)\n",
        "\na = np.array([i for i in range(0, 12)]).reshape(4, 3)\n",
        "\na = np.array( \n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\nb = np.array( \n    [[0, 1, 1],\n    [1, 0, 1],\n    [1, 1, 0]]\n)\ndesired = np.array(\n  [[ 0,  3,  5],\n   [ 7,  8, 11],\n   [13, 15, 16]]\n)\nresult = np.zeros((3,3))\nfor i in range(3):\n    for j in range(3):\n        if b[i][j] == 1:\n            result[i][j] = a[i][j][0]\n        else:\n            result[i][j] = a[i][j][1]\n",
        "\na = np.array( # dims: 3x3x2\n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\nb = np.array( # dims: 3x3\n    [[1, 1, 1],\n    [1, 1, 1],\n    [1, 1, 1]]\n)\n# select the elements in a according to b\n# to achieve this result:\ndesired = np.array(\n  [[ 1,  3,  5],\n   [ 7,  9, 11],\n   [13, 15, 17]]\n)\n\nresult = np.take_along_axis(a, b.astype(int), axis=2)\n",
        "\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\ndesired = np.array(\n  [[ 0,  3,  6],\n   [ 8,  9, 13],\n   [13, 14, 19]]\n)\nresult = np.take_along_axis(a, b[..., None], axis=2)\n",
        "\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\nresult = np.zeros(b.shape)\nfor i in range(b.shape[0]):\n    for j in range(b.shape[1]):\n        result[i][j] = np.sum(a[i][j][b[i][j]])\n",
        "\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\ndesired = 257\n\n# create a mask of the un-indexed elements\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(len(b)), b[:,0], b[:,1]] = True\n\n# compute the sum of the un-indexed elements\nresult = np.sum(a[~mask])\n\n",
        "\ndata = {'a': [1, 2, 3, 4, 5], 'b': [2, -6, 0, -4, 100]}\ndf = pd.DataFrame(data)\nresult = np.where((df['a'] > 1) & (df['a'] <= 4), df['b'], np.nan)\n",
        "\nim = np.array([[0,0,0,0,0,0],\n               [0,0,1,1,1,0],\n               [0,1,1,0,1,0],\n               [0,0,0,1,1,0],\n               [0,0,0,0,0,0]])\n\n# Create a mask to filter out the peripheral zeros\nmask = np.array([[1,1,1,1,1,1],\n                 [1,1,0,0,0,1],\n                 [1,0,0,0,0,1],\n                 [1,0,0,0,0,1],\n                 [1,1,1,1,1,1]])\n\n# Apply the mask to the image to filter out the peripheral zeros\nfiltered_im = im * mask\n\n# Create a new image with the filtered pixels\nresult = np.zeros((3,4))\nfor i in range(3):\n    for j in range(4):\n        result[i][j] = np.sum(filtered_im[i:i+2,j:j+2])\n\n",
        "\n\nA = np.array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]])\n\n# Find the bounding box of the nonzero data\nbbox = np.where(A != 0)\nmin_row, max_row = min(bbox[0]), max(bbox[0])\nmin_col, max_col = min(bbox[1]), max(bbox[1])\n\n# Truncate the array to the bounding box\nresult = A[min_row:max_row+1, min_col:max_col+1]\n\n",
        "\nim = np.array([[1,1,1,1,1,5],\n               [1,0,0,1,2,0],\n               [2,1,0,0,1,0],\n               [1,0,0,7,1,0],\n               [1,0,0,0,0,0]])\n\n# Create a mask that is True for all non-zero elements\nmask = (im != 0)\n\n# Create a masked array with the same shape as im\nmasked_im = np.ma.masked_array(im, mask=mask)\n\n# Create a new array with the same shape as im, filled with zeros\nresult = np.zeros_like(im)\n\n# Iterate over each row and column of the masked array\nfor i in range(masked_im.shape[0]):\n    for j in range(masked_im.shape[1]):\n        # If the element is not masked, copy it to the corresponding position in the result array\n        if not masked_im.mask[i,j]:\n            result[i,j] = masked_im[i,j]\n\n",
        "\nim = np.array([[0,0,0,0,0,0],\n               [0,0,5,1,2,0],\n               [0,1,8,0,1,0],\n               [0,0,0,7,1,0],\n               [0,0,0,0,0,0]])\n\n# Create a mask to filter out the black pixels\nmask = (im != 0)\n\n# Apply the mask to the image to filter out the black pixels\nfiltered_im = im[mask]\n\n# Reshape the filtered image to create the desired output\nresult = filtered_im.reshape((-1, im.shape[1]))\n\n"
    ],
    "Tensorflow": [
        "\n\nx = tf.Variable(0)\nx.assign(1)\nresult = x.numpy().astype(int)\n",
        "\n\nx = tf.Variable(0)\nx.assign(114514)\nresult = x.numpy().astype(int)\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\n",
        "\n\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    num_classes = 10\n    result = tf.one_hot(labels, num_classes, dtype=tf.int32)\n    return result\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\n",
        "\n\n\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [[i, i+1, i+2]]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n",
        "\ntf.compat.v1.disable_eager_execution()\n\nexample_input = [10, 20, 30]\ndef f(input=example_input):\n    result = []\n    for i in input:\n        result.extend([i, i+1, i+2])\n    return result\n",
        "\nmask = tf.sequence_mask(lengths, maxlen=8, dtype=tf.float32)\nresult = tf.pad(mask, [[1, 0], [0, 0]])[:, :-1]\n",
        "\nmask = tf.sequence_mask(lengths, maxlen=8, dtype=tf.float32)\nresult = tf.pad(mask, [[0, 0], [1, 0]], constant_values=1)\n",
        "\nmask = tf.sequence_mask(lengths, maxlen=8, dtype=tf.float32)\nresult = tf.pad(mask, [[0, 4], [0, 0]])\n",
        "\n\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    max_length = tf.reduce_max(lengths)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n    result = tf.pad(mask, [[0, 8-max_length], [0, 0]])\n    return result\n",
        "\n\nlengths = [4, 3, 5, 2]\nmax_length = 8\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\nresult = tf.pad(mask, [[0, max_length-tf.shape(lengths)[0]], [0, 0]])\n",
        "\n\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\nresult = tf.stack([tf.tile(a, [len(b)]), tf.repeat(b, len(a))], axis=1)\n",
        "\n\nexample_a = tf.constant([1,2,3])\nexample_b = tf.constant([4,5,6,7])\ndef f(a=example_a,b=example_b):\n    result = tf.stack(tf.meshgrid(a,b), axis=-1)\n    return result\n",
        "\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\n\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\nresult = tf.expand_dims(a, axis=-2)\n",
        "\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\n\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\nresult = tf.reciprocal(A)\n",
        "\n\na = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nb = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\n\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n\n",
        "\n\na = tf.constant([\n  [1,1,1],\n  [0,1,1]\n])\nb = tf.constant([\n  [0,0,1],\n  [1,1,1]\n])\n\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n",
        "\n\nexample_a = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nexample_b = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\ndef f(A=example_a,B=example_b):\n    result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n    return result\n",
        "\nm = tf.gather_nd(x, tf.stack([y,z], axis=1))\n",
        "\nm = tf.gather_nd(x, tf.stack([row, col], axis=1))\n",
        "\n\nexample_x = [[1,2,3],[4,5,6]]\nexample_y = [0,1]\nexample_z = [1,2]\nexample_x = tf.constant(example_x)\nexample_y = tf.constant(example_y)\nexample_z = tf.constant(example_z)\ndef f(x=example_x,y=example_y,z=example_z):\n    m = tf.gather_nd(x, tf.stack([y,z], axis=1))\n    return m\n",
        "\nresult = tf.tensordot(A, B, axes=[[2], [2]])\n",
        "\nresult = tf.tensordot(A, B, axes=[[2], [2]])\n",
        "\n\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\n\nresult = tf.strings.unicode_decode(x, \"UTF-8\")\n\n",
        "\n\nexample_x=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\ndef f(x=example_x):\n    result = tf.strings.unicode_decode(x, \"UTF-8\")\n    return result\n",
        "\n\nx = tf.constant([[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n                 [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n                 [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n                 [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n                [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n                 [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n                 [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n                 [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]], dtype=tf.float32)\n\nmask = tf.not_equal(x, tf.constant(0, dtype=tf.float32))\nmask = tf.reduce_any(mask, axis=-2)\nmask = tf.reduce_any(mask, axis=-1)\nmask = tf.expand_dims(mask, axis=-1)\nmask = tf.expand_dims(mask, axis=-1)\nmask = tf.expand_dims(mask, axis=-1)\n\nx = tf.where(mask, x, tf.constant(0, dtype=tf.float32))\n\nfeatures = tf.reduce_sum(x, axis=-2)\ncount = tf.reduce_sum(tf.cast(mask, tf.float32), axis=-2)\nresult = tf.math.divide_no_nan(features, count)\n\n",
        "\n\nx = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nx = tf.convert_to_tensor(x, dtype=tf.float32)\n\n# Get the non-zero entries of the second to last dimension\nnon_zero_entries = tf.boolean_mask(x, tf.not_equal(x, 0), axis=-2)\n\n# Calculate the mean of the non-zero entries\nmean = tf.reduce_mean(non_zero_entries, axis=-2, keepdims=True)\n\n# Calculate the variance of the non-zero entries\nvariance = tf.reduce_mean(tf.square(non_zero_entries - mean), axis=-2, keepdims=True)\n\n# Reshape the variance to match the shape of the original tensor\nvariance = tf.broadcast_to(variance, tf.shape(x))\n\n# Replace the zero padded values with the variance\nresult = tf.where(tf.equal(x, 0), variance, x)\n\n",
        "\n\nexample_x = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nexample_x = tf.convert_to_tensor(example_x, dtype=tf.float32)\n\ndef f(x=example_x):\n    # Get the non-zero entries of the second to last dimension\n    non_zero_entries = tf.reduce_sum(tf.cast(tf.not_equal(x, 0), tf.float32), axis=-2)\n    # Get the sum of the second to last dimension\n    sum_entries = tf.reduce_sum(x, axis=-2)\n    # Divide the sum by the number of non-zero entries\n    result = tf.divide(sum_entries, non_zero_entries)\n    return result\n",
        "\n\ntry:\n    Session = tf.Session\nexcept AttributeError:\n    Session = tf.compat.v1.Session\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nwith tf.Session() as sess:\n   result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmax(a, axis=1)\n\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmax(a, axis=1)\n\n",
        "\n\nexample_a = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\ndef f(a=example_a):\n    result = tf.argmax(a, axis=1)\n    return result\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmin(a, axis=1)\n",
        "\n#Save the model in \"export/1\"\ntf.saved_model.save(model, \"export/1\")\n",
        "\n\nseed_x = 10\ntf.random.set_seed(seed_x)\n\nresult = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25]).sample(10)\n",
        "\n\nseed_x = 10\ntf.random.set_seed(seed_x)\n\nresult = tfp.distributions.Categorical(probs=[0.2, 0.3, 0.2, 0.3]).sample(114)\n",
        "\n\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25]).sample(10)\n    return result\n",
        "\n\n# output the version of tensorflow into variable 'result'\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\ncoefficients = np.polyfit(np.log(x), y, 1)\nresult = coefficients[::-1]\n",
        "\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\ncoefficients = np.polyfit(np.log(x), y, 1)\nresult = coefficients[::-1]\n",
        "\n\ndef func(x, A, B, C):\n    return A*np.exp(B*x) + C\n\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\n\nresult, cov = scipy.optimize.curve_fit(func, x, y, p0)\n\n",
        "\nfrom scipy.stats import ks_2samp\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\nstatistic, p_value = ks_2samp(x, y)\n",
        "\nfrom scipy.stats import ks_2samp\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\nalpha = 0.01\nresult = ks_2samp(x, y)[1] < alpha\n",
        "\nfrom math import *\n\ninitial_guess = [-1, 0, -3]\n\ndef f(x):\n    return ((x[0]+x[1]-x[2])-2)**2 + ((3*x[0]-x[1]-x[2]))**2 + sin(x[1]) + cos(x[1]) + 4\n\nresult = optimize.minimize(f, initial_guess)\n\n",
        "\n\nz_scores = np.array([-3, -2, 0, 2, 2.5])\np_values = scipy.stats.norm.cdf(z_scores)\n\n",
        "\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\np_values = scipy.stats.norm.cdf(0 - np.array(z_scores))\n",
        "\n\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\nz_scores = [scipy.stats.norm.ppf(p) for p in p_values]\n\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\ntotal = 37\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nexpected_value, var, skew, kurt = dist.stats(moments='mvsk')\nmedian = dist.median()\n",
        "\nresult = sp.sparse.csr_matrix(sa.dot(sb))\n",
        "\nfrom scipy import sparse\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n    result = sA.dot(sB)\n    return result\n",
        "\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30]])\n\nresult = scipy.interpolate.griddata(points, V, request)\n",
        "\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\nresult = scipy.interpolate.griddata(points, V, request, method='linear')\n",
        "\nfrom scipy import misc\nfrom scipy.ndimage import rotate\ndata_orig = misc.face()\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nangle = np.random.randint(1, 360)\n\n# create rotation matrix\ntheta = np.deg2rad(angle)\nc, s = np.cos(theta), np.sin(theta)\nR = np.array(((c,-s), (s, c)))\n\n# create translation matrix\nx_trans = -x0\ny_trans = -y0\nT = np.array(((1, 0, x_trans), (0, 1, y_trans), (0, 0, 1)))\n\n# apply rotation and translation\ndata_rot = rotate(data_orig, angle, reshape=False)\ndata_rot = np.dot(data_rot, R)\ndata_rot = np.dot(data_rot, T)\n\n# get rotated coordinates\nxrot, yrot = np.round(np.dot(T, np.dot(R, np.array((x0, y0, 1)))).astype(int))\n\n",
        "\nresult = M.diagonal()\n",
        "\nfrom scipy import stats\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\nresult = stats.kstest(times, \"uniform\")\n",
        "\ndef f(times = example_times, rate = example_rate, T = example_T):\n    # Kolmogorov-Smirnov test for uniformity\n    D, p = stats.kstest(times, 'uniform')\n    result = (D, p)\n    return result\n",
        "\nfrom scipy import stats\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n\n# Kolmogorov-Smirnov test\nresult = stats.kstest(times, 'uniform')\n\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.hstack([c1, c2])\nFeature = sparse.csr_matrix(Feature)\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.hstack([c1, c2])\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.vstack([c1, c2])\n",
        "\n\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\n# calculate the distances between all pairs of points\nD = scipy.spatial.distance.cdist(points1, points2)\n\n# use the Hungarian algorithm to find the optimal assignment\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(D)\n\n# create the result by mapping the points from set 2 to set 1\nresult = [col_ind[i] for i in row_ind]\n\n",
        "\n\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\n# calculate the cost matrix\ncost_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\n\n# use the Hungarian algorithm to find the optimal assignment\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(cost_matrix)\n\n# create the result list\nresult = [col_ind[i] for i in range(N)]\n\n",
        "\nb.setdiag(0)\nb.eliminate_zeros()\n",
        "\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\nlabels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n",
        "\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Label connected regions of pixels with the same value\nlabels, num_labels = ndimage.label(img < threshold)\n\n# Count the number of regions with a value below the threshold\nresult = (labels == 0).sum()\n\n",
        "\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nexample_img /= img.max()\ndef f(img = example_img):\n    threshold = 0.75\n    labels, num_labels = ndimage.label(img >= threshold)\n    return num_labels\n",
        "\nregions = np.zeros_like(img)\nregions[img > threshold] = 1\n",
        "\nfrom scipy.sparse import lil_matrix\nfrom scipy import sparse\n\nM= sparse.random(10, 10, density=0.1, format='lil')\n\nfor i in range(M.shape[0]):\n    for j in range(i):\n        if M[i,j] != 0:\n            M[j,i] = M[i,j]\n\n",
        "\nfrom scipy.sparse import lil_matrix\nexample_sA = sparse.random(10, 10, density=0.1, format='lil')\ndef f(sA = example_sA):\n    for i in range(sA.shape[0]):\n        for j in range(i):\n            sA[j, i] = sA[i, j]\n    return sA\n",
        "\n\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\n\n# Perform binary dilation and erosion\nsquare = scipy.ndimage.binary_dilation(square)\nsquare = scipy.ndimage.binary_erosion(square)\n\n",
        "\n\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = np.random.randint(1, 255, size = (12, 12))\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = np.random.randint(1, 255, size = (20,))\n\n# Erosion and dilation to remove single cells\nsquare = scipy.ndimage.binary_erosion(square).astype(square.dtype)\nsquare = scipy.ndimage.binary_dilation(square).astype(square.dtype)\n\n# Remove blobs with size 1\nfor i in range(square.shape[0]):\n    for j in range(square.shape[1]):\n        if square[i, j] == 1 and (np.sum(square[i-1:i+2, j-1:j+2]) == 1):\n            square[i, j] = 0\n\n",
        "\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\ndata = col.data\nmean = np.mean(data)\nstandard_deviation = np.std(data)\n",
        "\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\ndata = col.data\nMax = np.max(data)\nMin = np.min(data)\n",
        "\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\ndata = col.data\n\nMedian = np.median(data)\nMode = np.bincount(data).argmax()\n\n",
        "\ndef fourier(x, *args):\n    a = args[:degree]\n    return np.sum([a[i] * np.cos(i * np.pi / tau * x) for i in range(degree)], axis=0)\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=np.ones(degree))\n",
        "\n\n# Sample study area array\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\n# Define raster resolution (in metres per cell)\nresolution = 10\n\n# Calculate pairwise Euclidean distances between all regions\ndistances = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean', p=2)\n\n# Convert distances from cells to metres\ndistances *= resolution\n\n# Print result\n",
        "\n\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\n# Calculate pairwise Manhattan distances using cdist\nresult = scipy.spatial.distance.cdist(example_array, metric=\"cityblock\")\n\n",
        "\n\nexample_arr = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                        [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                        [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                        [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                        [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                        [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                        [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                        [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                        [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\ndef f(example_array = example_arr):\n    # Calculate raster resolution\n    res = 1 # Assuming raster resolution is 1m per cell\n    # Calculate pairwise Euclidean distances using cdist\n    dist_matrix = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean', p=2)\n    # Convert distances from cells to metres\n    dist_matrix *= res\n    # Create result array\n    result = np.zeros((example_array.shape[0], example_array.shape[0], 3))\n    # Fill in result array with distances\n    for i in range(example_array.shape[0]):\n        for j in range(example_array.shape[0]):\n            result[i,j,0] = i\n            result[i,j,1] = j\n            result[i,j,2] = dist_matrix[i,j]\n    return result\n",
        "\nresult = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    y_int = interpolate.splev(x_val, tck, der = 0)\n    result[i, :] = y_int\n",
        "\n\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\nx3 = [34.0, 35.0, 39.0, 40.0, 43.0, 43.0, 44.0, 45.0]\nx4 = [34.0, 34.8, 34.8, 35.4, 37.2, 37.8, 41.2, 42.8]\n\n# concatenate the datasets into a single 2-dimensional array\nX = np.vstack((x1, x2, x3, x4)).T\n\n# calculate the Anderson-Darling test statistic and critical values\nstatistic, critical_values, significance_level = ss.anderson_ksamp(X)\n\n# print the results\n",
        "\nx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\nx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\nresult = ss.anderson_ksamp([np.concatenate((x1, x2))])\n",
        "\n\ndef tau1(x):\n    y = np.array(A['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\nA = pd.DataFrame([[1, 5, 1], [2, 4, 1], [3, 3, 1], [4, 2, 1], [5, 1, 1]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\n\nA['AB'] = pd.rolling_apply(A['B'], 3, lambda x: tau1(x))\n\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = len(sa.data) == 0\n",
        "\nfrom scipy.sparse import lil_matrix\n\ndef is_lil_matrix_only_zeroes(my_lil_matrix):\n    return(len(my_lil_matrix.nonzero()[0]) == 0)\n\nsa = lil_matrix([[1,2,0],[0,0,3],[4,0,5]])\nresult = is_lil_matrix_only_zeroes(sa)\n",
        "\nresult = block_diag(*a)\n",
        "\nfrom scipy import stats\nnp.random.seed(10)\npre_course_scores = np.random.randn(10)\nduring_course_scores = np.random.randn(10)\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\nfrom scipy import stats\nexample_pre_course_scores = np.random.randn(10)\nexample_during_course_scores = np.random.randn(10)\ndef f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n    _, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n    return p_value\n",
        "\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\nn = len(a)\nmean_x = np.mean(a)\nsum_deviation_squared = np.sum((a - mean_x)**2)\nkurtosis_result = (n-1)/(n-2)*(n+1)/((n-3)*(n-4)) * (sum_deviation_squared/n**2)\n",
        "\n\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\n\nn = len(a)\nmean = np.mean(a)\nstd = np.std(a, ddof=1)\n\nkurtosis_result = (n/(n-1)/(n-2)/(n-3)) * np.sum((a - mean)**4) / std**4\n\n",
        "\n\ns = np.linspace(-1, 1, 50)\nt = np.linspace(-2, 0, 50)\nx, y = np.ogrid[-1:1:10j,-2:0:10j]\nz = (x + y)*np.exp(-6.0 * (x * x + y * y))\n\nf = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n\ns_new = np.array([-1, 0, 1])\nt_new = np.array([-2, -1, 0])\n\nresult = f(s_new, t_new)\n\n",
        "\n\nexample_s = np.linspace(-1, 1, 50)\nexample_t = np.linspace(-2, 0, 50)\n\ndef f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    return scipy.interpolate.interp2d(s, t, z, kind='cubic')\n\nresult = f(example_s, example_t)\n\nreturn result(example_s, example_t)\n",
        "\n\n# Finding the regions occupied by the extra points\nresult = []\nfor point in extraPoints:\n    region = vor.point_region(point)\n    result.append(region)\n\n# Counting the number of extra points in each region\ncounts = np.bincount(result)\n\n# Printing the counts\n",
        "\n\n# Finding the regions occupied by the extra points\nresult = []\nfor point in extraPoints:\n    region = vor.point_region(point)\n    result.append(region)\n\n# Counting the number of extra points in each region\ncounts = np.bincount(result)\n\n# Printing the counts\n",
        "\n\nnp.random.seed(10)\nmax_vector_size = 1000\nvectors = [np.random.randint(100,size=900),np.random.randint(100,size=max_vector_size),np.random.randint(100,size=950)]\n\n# pad zeros to the end of elements for the vectors which are smaller than the maximum size\nvectors = [np.pad(v, (0, max_vector_size - len(v)), 'constant') for v in vectors]\n\n# create a sparse matrix using the vectors\ndata = []\nindices = []\nindptr = [0]\nfor v in vectors:\n    data.extend(v)\n    indices.extend(range(len(v)))\n    indptr.append(len(indices))\n\nresult = sparse.csr_matrix((data, indices, indptr), shape=(len(vectors), max_vector_size))\n\n",
        "\n\na = np.random.binomial(n=1, p=1/2, size=(9, 9))\nb = nd.median_filter(a, 3, origin=(1, 1))\n",
        "\nfrom scipy.sparse import csr_matrix\n\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = 2\ncolumn = 3\nresult = M[row,column]\n",
        "\nresult = [M.getrow(r).toarray()[0, c] for r, c in zip(row, column)]\n",
        "\nnew_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        for k in range(10):\n            f = scipy.interpolate.interp1d(x, array[:, i, j, k], kind='cubic')\n            new_array[:, i, j, k] = f(x_new)\n",
        "\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\nx = 2.5\nu = 1\no2 = 3\ndev = abs((x-u)/o2)\nP_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\nP_outer = 1 - P_inner\nP = P_inner + P_outer/2\nprob = P\n",
        "\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\ndef f(x = 2.5, u = 1, o2 = 3):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n",
        "\n\nN = 8\nresult = sf.dctn(np.eye(N), norm='ortho')\n",
        "\nfrom scipy.sparse import diags\nv1 = [3*i**2 +(i/2) for i in range(1, 6)]\nv2 = [-(6*i**2 - 1) for i in range(1, 6)]\nv3 = [3*i**2 -(i/2) for i in range(1, 6)]\nmatrix = np.array([v1, v2, v3])\nresult = diags(matrix, [-1,0,1], (5, 5)).toarray()\n",
        "\n\nN = 3\np = 0.5\n\nresult = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        result[i,j] = scipy.stats.binom.pmf(j, i, p)\n\n",
        "\nresult = stats.zscore(df, axis=1)\n",
        "\nresult = pd.DataFrame(stats.zscore(df), columns=df.columns)\n",
        "\nresult = pd.DataFrame(columns=['data', 'zscore'])\nfor col in df.columns:\n    z_scores = stats.zscore(df[col])\n    df[col] = df[col].apply(lambda x: 'data' if x != 0 else '')\n    df[col] = df[col].apply(lambda x: round(x, 2) if x != '' else '')\n    df[col] = df[col].apply(lambda x: str(x))\n    df[col + '_zscore'] = z_scores\n    result = result.append(df[[col, col + '_zscore']], ignore_index=True)\nresult.columns = ['probegenes', 'sample1', 'sample2', 'sample3']\nresult = result[['probegenes', 'sample1', 'sample2', 'sample3', 'sample1_zscore', 'sample2_zscore', 'sample3_zscore']]\n",
        "\nz_scores = stats.zscore(df)\nresult = pd.DataFrame(df)\nresult['zscore'] = z_scores\nresult = result.round(3)\nresult.columns = ['data', 'zscore']\nresult = result.reset_index()\nresult.columns = ['probegenes', 'sample1', 'sample2', 'sample3', 'data', 'zscore']\n",
        "\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\n\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\nresult = scipy.optimize.line_search(test_func,test_grad,starting_point,direction)\n",
        "\nmid = np.indices(shape).reshape(shape + (2,))\nresult = distance.cdist(mid, mid, metric='euclidean')\n",
        "\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([[[3, 3], [3, 3]], [[3, 3], [3, 3]]])\nresult = distance.cdist(np.indices(shape).reshape(shape + (2,)), mid)\n",
        "\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    mid = np.array([[[i,j] for j in range(shape[1])] for i in range(shape[0])])\n    result = distance.cdist(mid, mid, 'euclidean')\n    return result\n",
        "\n\nx = np.arange(9).reshape(3, 3)\n\n# Resample to (4,6)\nresult = scipy.ndimage.zoom(x, 0.5, order=1)\nresult = result[:4, :6]\n\n# Resample to (6,8)\nresult = scipy.ndimage.zoom(x, 0.5, order=1)\nresult = np.pad(result, ((0, 0), (0, 2)), mode='constant')\n\n# Resample to (6,10)\nresult = scipy.ndimage.zoom(x, 1.5, order=1)\nresult = np.pad(result, ((0, 0), (0, 2)), mode='constant')\n\n",
        "\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\ndef func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model) **2\ndef main():\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = minimize(residual, fit_params, args=(a, y))\n    print(out)\nif __name__ == '__main__':\n    main()\n",
        "\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\ndef func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model)**2\ndef main():\n    fit_params = Parameters()\n    fit_params.add('x', value=x0, min=x_lower_bounds)\n    out = minimize(residual, fit_params, args=(a, y), method='L-BFGS-B')\n    print(out)\nif __name__ == '__main__':\n    main()\n",
        "\ndef dN1_dt_simple(t, N1, t_input):\n    return -100 * N1 + np.sin(t_input * t)\n\nt_input = 2 * np.pi  # period of the input signal\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0], args=(t_input,))\nresult = sol.y\n",
        "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi - t + np.sin(2*np.pi - t)\n",
        "\n\nN0 = 10\ntime_span = [-0.1, 0.1]\n\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=lambda t: N0)\nresult = sol.y\n",
        "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sparse.vstack((sa, sb))\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sparse.vstack((sa, sb))\n",
        "\nc = 5\nlow = 0\nhigh = 1\nI = []\nfor n in range(len(c)):\n    # equation\n    eqn = lambda x: 2*x*c[n]\n    # integrate \n    result,error = scipy.integrate.quad(eqn,low,high)\n    I.append(result)\nI = numpy.array(I)\n",
        "\ndef f(c=5, low=0, high=1):\n    result,error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n",
        "\nfrom scipy import sparse\n\nV = sparse.random(10, 10, density = 0.05, format = 'dok', random_state = 42)\nx = 99\n\n# Create a full matrix with the scalar value x\nfull_matrix = np.full((10, 10), x)\n\n# Add the full matrix to V\nV = V + full_matrix\n\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\nV.data += x\n",
        "\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\ny = 99\n\n# Create a full matrix with the scalar value x\nfull_x = sparse.diags(x * np.ones(V.shape[0]), 0, format = 'coo')\n\n# Add the full matrix x to V\nV = V + full_x\n\n# Add the scalar values y to non-zero values in V\nV.data[V.data != 0] += y\n\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.3, format = 'csc', random_state = 42)\n#csc sparse matrix\nsa = sa.tocsc()\n#iterate through columns\nfor Col in range(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    #get the column length\n    Len = math.sqrt(sum(List))\n    #normalize the column\n    sa[:,Col] = sparse.csc_matrix(np.array([x/Len for x in Column]).reshape(-1,1))\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.3, format = 'csr', random_state = 42)\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    sa[:,Col] = (1/Len)*Column\n",
        "\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n\n# Convert decimal matrix to sparse matrix\nsparse_matrix = scipy.sparse.csr_matrix(a)\n\n# Convert sparse matrix to binary matrix\nbinary_matrix = sparse_matrix.toarray()\n\n",
        "\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n\n# Convert decimal matrix to sparse matrix\nsparse_matrix = scipy.sparse.csr_matrix(a)\n\n# Convert sparse matrix to binary matrix\nbinary_matrix = sparse_matrix.toarray()\n\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Calculate distance matrix between data and centroids\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\n\n# Cluster data using Ward's linkage\nlinkage_matrix = scipy.cluster.hierarchy.ward(dist_matrix)\n\n# Cut tree to get cluster assignments for each data point\ncluster_assignments = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=5)\n\n# Find index of closest element to each cluster centroid\nresult = []\nfor i in range(5):\n    centroid_index = np.argmin(np.sum((data - centroids[i])**2, axis=1))\n    result.append(np.where(cluster_assignments[:,0] == i)[0][centroid_index])\n\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Calculate distance matrix\ndistance_matrix = scipy.spatial.distance.cdist(data, centroids)\n\n# Cluster data using Ward linkage\nlinkage_matrix = scipy.cluster.hierarchy.ward(distance_matrix)\n\n# Cut tree to get cluster assignments\ncluster_assignments = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=5)\n\n# Find closest element to each cluster centroid\nclosest_elements = np.zeros((5, 3))\nfor i in range(5):\n    cluster_indices = np.where(cluster_assignments[:, 0] == i)[0]\n    cluster_data = data[cluster_indices]\n    centroid = centroids[i]\n    closest_element = cluster_data[np.argmin(np.linalg.norm(cluster_data - centroid, axis=1))]\n    closest_elements[i] = closest_element\n\n# Print closest elements to each cluster centroid\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Cluster the data using Ward's linkage\nZ = scipy.cluster.hierarchy.linkage(data, method='ward')\n\n# Cut the dendrogram to get the clusters\nT = scipy.cluster.hierarchy.cut_tree(Z)\n\n# Calculate the distance between each data point and its centroid\nD = scipy.spatial.distance.cdist(data, centroids)\n\n# Sort the distances to find the k-th closest for each cluster\nk = 3\nresult = []\nfor i in range(T.shape[0]):\n    cluster_indices = np.where(T[:, 0] == i)[0]\n    cluster_distances = D[cluster_indices]\n    sorted_indices = np.argsort(cluster_distances)\n    result.append(cluster_indices[sorted_indices[:k]])\n\n",
        "\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\nresult = fsolve(eqn, x0=0.5, args = (xdata, bdata))\n",
        "\nfrom scipy.optimize import fsolve\n\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\n\nresult = []\nfor i in range(len(xdata)):\n    roots = fsolve(lambda b: eqn(xdata[i], adata[i], b), 0)\n    result.append([roots[0], adata[i]])\n\nresult = np.array(result)\nresult = result[result[:,0].argsort()]\n\n",
        "\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n\n# Define the KS-test function\ndef KStest(sample_data, dist_func, args):\n    # Calculate the cumulative distribution function of the sample data\n    cdf_sample = np.array([dist_func(x, *args) for x in sample_data])\n    # Calculate the cumulative distribution function of the theoretical distribution\n    cdf_theor = np.array([dist_func(x, *args) for x in np.linspace(min(sample_data), max(sample_data), 1000)])\n    # Calculate the KS-statistic\n    D = np.max(np.abs(cdf_sample - cdf_theor))\n    # Calculate the p-value\n    p = 1 - stats.distributions.kstwobign.cdf(D)\n    return D, p\n\n# Calculate the KS-test for the Bekkers distribution\nresult = KStest(sample_data, bekkers, [estimated_a, estimated_m, estimated_d])\n\n",
        "\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n\n# Define the KS-test function\ndef KS_test(sample_data, dist_func, args, alpha=0.05):\n    \"\"\"\n    Perform the Kolmogorov-Smirnov test to check if the sample_data is drawn from the distribution specified by dist_func.\n    Args:\n        sample_data: a list of data points to be tested\n        dist_func: a function that takes a list of data points and returns a list of probabilities for each data point\n        args: a tuple of arguments to be passed to dist_func\n        alpha: the significance level (default 0.05)\n    Returns:\n        True if the null hypothesis can be rejected, False otherwise\n    \"\"\"\n    # Calculate the cumulative distribution function of the sample data\n    sample_cdf = np.cumsum(dist_func(sample_data, *args))\n    # Calculate the cumulative distribution function of the theoretical distribution\n    dist_cdf = np.cumsum(dist_func(np.arange(sample_data[0], sample_data[-1]+1e-6, (sample_data[-1]-sample_data[0])/len(sample_data)), *args))\n    # Calculate the maximum absolute difference between the two cumulative distribution functions\n    max_diff = np.max(np.abs(sample_cdf - dist_cdf))\n    # Calculate the K-S statistic\n    D = np.max(np.abs(sample_cdf - dist_cdf)) / np.sqrt(np.sum(dist_func(np.arange(sample_data[0], sample_data[-1]+1e-6, (sample_data[-1]-sample_data[0])/len(sample_data)), *args)))\n    # Calculate the p-value\n    p_value = 1 - stats.distributions.norm.cdf(D)\n    # Return True if the p-value is less than the significance level, False otherwise\n    return p_value < alpha\n\n# Test the KS-test function\nresult = KS_test(sample_data, bekkers, (estimated_a, estimated_m, estimated_d))\n",
        "\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(lambda x: integrate.trapz(x['A'], x.index))\n",
        "\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\neval = [(2.7, 2.3)]\nresult = scipy.interpolate.griddata(x, y, eval)\n",
        "\na=pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\ndef neg_log_likelihood(params, data):\n    weights = params\n    n = len(data)\n    freq = np.bincount(data, minlength=len(weights))\n    return -np.sum(weights * np.log(freq))\n\ndef neg_log_likelihood_grad(params, data):\n    weights = params\n    n = len(data)\n    freq = np.bincount(data, minlength=len(weights))\n    return -np.dot(weights / freq, freq - 1)\n\nweights = np.ones(a.max().max()+1)\nresult = sciopt.minimize(neg_log_likelihood, weights, args=(a.values.flatten()), jac=neg_log_likelihood_grad, method='BFGS')\nweights = result.x\n",
        "\npopt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\n",
        "\nresult = signal.argrelextrema(arr, np.less_equal, order=n)[0]\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if (i == 0 or arr[i-1][j] <= arr[i][j]) and (i == arr.shape[0]-1 or arr[i+1][j] <= arr[i][j]) and (j == 0 or arr[i][j-1] <= arr[i][j]) and (j == arr.shape[1]-1 or arr[i][j+1] <= arr[i][j]):\n            result.append([i, j])\n",
        "\ndf = df[(np.abs(stats.zscore(df[['NUM1', 'NUM2', 'NUM3']])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\nfrom sklearn.datasets import load_iris\ndata = load_iris()\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "\nfrom sklearn.datasets import load_iris\ndata = load_iris()\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=list(data['feature_names']) + ['target'])\n",
        "\nfrom sklearn.datasets import load_boston\ndata = load_boston()\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "\ndef solve(data):\n    data1 = pd.DataFrame(data.data, columns=data.feature_names)\n    data1['target'] = data.target\n    return data1\n",
        "\n\ndf = pd.DataFrame({'Col1': ['C', 'A', 'B'],\n                   'Col2': [33, 2.5, 42],\n                   'Col3': [['Apple', 'Orange', 'Banana'],\n                            ['Apple', 'Grape'],\n                            ['Banana']]})\n\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n\n",
        "\n\ndf = load_data()\n\n# create a new dataframe with one-hot-encoded columns for each unique name in the list\ndf_out = pd.get_dummies(df, columns=['Col3'])\n\n# print the new dataframe\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col4'])\n",
        "\ndf_out = pd.get_dummies(df[[Missing]], columns=[Missing])\n",
        "\ndf_out = pd.get_dummies(df.iloc[:, -1], prefix='[[Missing]]', prefix_sep='_', dummy_na=True)\ndf = pd.concat([df.iloc[:, :-1], df_out], axis=1)\n",
        "\nfrom sklearn.calibration import CalibratedClassifierCV\nsvmmodel = suppmach.LinearSVC()\ncalibrated_svm = CalibratedClassifierCV(svmmodel, cv=5)\ncalibrated_svm.fit(X, y)\nproba = calibrated_svm.predict_proba(x_test)[:, 1]\n",
        "\nfrom sklearn.calibration import CalibratedClassifierCV\nmodel = svm.LinearSVC()\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\npredicted_test = calibrated_model.predict(x_predict)\npredicted_test_scores = calibrated_model.decision_function(x_predict)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ndf = pd.DataFrame(transform_output.todense())\ndf = pd.concat([df_origin, df], axis=1)\n",
        "\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ndf = pd.DataFrame(transform_output.toarray())\ndf = pd.concat([df_origin, df], axis=1)\n",
        "\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ndef solve(df, transform_output):\n    # convert transform_output to a dense numpy array\n    transform_output = transform_output.toarray()\n    # create a new dataframe with the transformed data\n    df_transformed = pd.DataFrame(transform_output, columns=df.columns)\n    # concatenate the original dataframe with the transformed dataframe\n    result = pd.concat([df, df_transformed], axis=1)\n    return result\ndf = solve(df_origin, transform_output)\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\nclf.steps.insert(1, ('poly2', PolynomialFeatures()))\n",
        "\nclf.steps.insert(1, ('new_pca', PCA()))\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\nclf.named_steps.pop('poly')\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\nclf.steps.insert(1, ('poly2', PolynomialFeatures()))\n",
        "\nclf.steps.insert(1, ('new_pca', PCA()))\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\nclf.named_steps.pop('poly')\n",
        "\nmodel = xgb.XGBRegressor()\nGridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]}).fit(trainX,trainY)\n",
        "\nmodel = xgb.XGBRegressor()\nGridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]}).fit(trainX, trainY)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba, axis=0)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba, axis=0)\n",
        "\ninversed = scaler.inverse_transform(predicted_t)\n",
        "\ndef solve(data, scaler, scaled):\n    # inverse the StandardScaler to get back the real time\n    inversed = scaler.inverse_transform(scaled)\n    return inversed\n",
        "\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\n",
        "\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\n",
        "\nfrom sklearn.svm import LinearSVC\nmodel = LinearSVC()\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].transform(data)\n",
        "\nclf = GridSearchCV(bc, param_grid, cv=5)\nclf.fit(X_train, y_train)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y.reshape(-1,1))\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y.reshape(-1,1))\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n",
        "\nfrom sklearn import preprocessing\n\ndata = pd.read_csv(\"lala.csv\", delimiter=\",\")\n\n# create a new DataFrame with only the numeric columns\ndf_num = data.select_dtypes(include=['int64', 'float64'])\n\n# scale the numeric columns\nscaler = preprocessing.StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_num), columns=df_num.columns)\n\n# concatenate the scaled numeric columns with the non-numeric columns\ndf_out = pd.concat([data.select_dtypes(exclude=['int64', 'float64']), df_scaled], axis=1)\n\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n",
        "\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"model\", SGDClassifier(random_state=42))\n])\ngrid = GridSearchCV(pipe, param_grid={\"model__alpha\": [1e-3, 1e-2, 1e-1, 1]}, cv=5)\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"model\", RidgeClassifier(random_state=24))\n])\ngrid = GridSearchCV(pipe, param_grid={\"model__alpha\": [2e-4, 3e-3, 4e-2, 5e-1]}, cv=7)\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\nclosest_50_samples = pd.DataFrame(X[km.labels_ == p].argsort()[:50])\n",
        "\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\nclosest_50_samples = X[np.argsort(km.transform(X)[:,p])[0:50]]\n",
        "\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\ncenters = km.cluster_centers_\nclosest_100_samples = []\nfor i in range(len(centers)):\n    dist = np.linalg.norm(X - centers[i], axis=1)\n    closest_100_samples.append(X[np.argsort(dist)[:100]])\nclosest_100_samples = np.concatenate(closest_100_samples)\n",
        "\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\ndef get_samples(p, X, km):\n    km.fit(X)\n    centers = km.cluster_centers_\n    labels = km.labels_\n    p_index = np.where(labels == p)[0][0]\n    distances = np.linalg.norm(X - centers[p_index], axis=1)\n    closest_indices = np.argsort(distances)[:50]\n    samples = X[closest_indices]\n    return samples\nclosest_50_samples = get_samples(p, X, km)\n",
        "\n# Convert categorical variable to matrix using get_dummies in pandas.\nX_train = pd.get_dummies(X_train)\n\n# Merge back with original training data.\nX_train = pd.concat([X_train, pd.DataFrame(X_train[0])], axis=1)\nX_train.drop(columns=[0], inplace=True)\n\n# Model.\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n",
        "\nfrom sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# load data in the example\nX_train, y_train = load_data()\nX_train = pd.get_dummies(X_train)\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import load_boston\n\n# Load the Boston housing dataset\nX, y = load_boston(return_X_y=True)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVR model with a gaussian kernel\nsvr_model = SVR(kernel='rbf')\n\n# Fit the model to the training data\nsvr_model.fit(X_train, y_train)\n\n# Use the model to make predictions on the testing data\ny_pred = svr_model.predict(X_test)\n\n# Print the mean squared error (MSE)\nmse = mean_squared_error(y_test, y_pred)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\n# define the model\nmodel = SVR(kernel='rbf')\n\n# define the hyperparameters to tune\nparam_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n\n# define the grid search\ngrid_search = GridSearchCV(model, param_grid, cv=5)\n\n# fit the grid search\ngrid_search.fit(X, y)\n\n# get the best hyperparameters\nbest_params = grid_search.best_params_\n\n# define the model with the best hyperparameters\nmodel = SVR(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'])\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\n\n# Generate sample data\nX, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n\n# Create SVR model with polynomial kernel\nmodel = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Predict the output for a new input\nX_test = np.array([[50]])\ny_pred = model.predict(X_test)\n\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nmodel = make_pipeline(poly, SVR())\nmodel.fit(X, y)\n\npredict = model.predict(X)\n",
        "\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities_of_queries = cosine_similarity(tfidf.transform(documents), query_tfidf)\n    return cosine_similarities_of_queries\n",
        "\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities_of_queries = cosine_similarity(tfidf.transform(documents), query_tfidf)\n    return cosine_similarities_of_queries\n",
        "\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = cosine_similarity(query_tfidf, tfidf.transform(documents))\n    return cosine_similarities_of_queries\n",
        "\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Convert the list of features to a 2D-array\nvectorizer = CountVectorizer()\nnew_features = vectorizer.fit_transform(features).toarray()\n\n",
        "\nf = load_data()\nnew_f = pd.DataFrame(np.zeros((len(f), max(len(x) for x in f))), columns=[x for x in set().union(*f)])\nfor i, x in enumerate(f):\n    for j, y in enumerate(x):\n        new_f.iloc[i, new_f.columns.get_loc(y)] = 1\n",
        "\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Convert the list of features to a 2D-array\nvectorizer = CountVectorizer()\nnew_features = vectorizer.fit_transform(features).toarray()\n\n",
        "\nfeatures = load_data()\ndef solve(features):\n    # Convert the list of lists to a 2D numpy array\n    features_array = np.array(features)\n    # Transpose the array to get the features as columns and samples as rows\n    features_array = features_array.T\n    # Convert the array to a pandas dataframe\n    features_df = pd.DataFrame(features_array)\n    # Convert the categorical features to one-hot encoding\n    features_df = pd.get_dummies(features_df)\n    # Convert the dataframe back to a numpy array\n    new_features = features_df.values\n    return new_features\nnew_features = solve(features)\n",
        "\n\nfeatures = load_data()\n\n# Convert the list of lists to a 2D numpy array\nnew_features = np.array(features)\n\n# Transpose the array to get the features as columns and samples as rows\nnew_features = new_features.T\n\n",
        "\n\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create a distance matrix\ndistance_matrix = sklearn.metrics.pairwise_distances(data_matrix, metric='euclidean')\n\n# Perform hierarchical clustering\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(distance_matrix)\n\n",
        "\n\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create a distance matrix\ndist_matrix = sklearn.metrics.pairwise_distances(data_matrix)\n\n# Perform hierarchical clustering\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(dist_matrix)\n\n",
        "\n\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Create a distance matrix\ndistM = 1 - simM\n\n# Perform hierarchical clustering\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\ncluster_labels = model.fit_predict(distM)\n\n",
        "\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\nZ = linkage(data_matrix, 'ward')\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n",
        "\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\nZ = linkage(data_matrix, 'ward')\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n",
        "\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\nZ = linkage(simM, 'ward')\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n",
        "\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n\nscaler = StandardScaler()\nrobust_scaler = RobustScaler()\npower_transformer = PowerTransformer()\n\ncentered_scaled_data = scaler.fit_transform(data)\ncentered_scaled_data = robust_scaler.fit_transform(centered_scaled_data)\ncentered_scaled_data = power_transformer.fit_transform(centered_scaled_data)\n",
        "\nfrom sklearn.preprocessing import StandardScaler\n\ndata = load_data()\nassert type(data) == np.ndarray\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\nfrom scipy.stats import boxcox\ndata = load_data()\nassert type(data) == np.ndarray\nbox_cox_data, _ = boxcox(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntext = load_data()\n\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+[-\\'\\\"\\!\\?]+\\w+\\b')\ntransformed_text = vectorizer.fit_transform(text)\n\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.5, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\n\ndef solve(data):\n    # Split the data into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n\ndataset = pd.read_csv('example.csv', header=None, sep=',')\nx_train, y_train, x_test, y_test = solve(dataset)\n",
        "\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndf = pd.read_csv(\"generate_csv/all_data_device.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\n# generate another list\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n# Centroid values\ncentroids = kmeans.cluster_centers_\n#print(centroids)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(X[:, 0], X[:, 1], c=labels)\nax.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='#050505', s=1000)\nplt.title('K Mean Classification')\nplt.show()\n",
        "\nfrom sklearn.cluster import KMeans\n\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_indices = featureSelector.get_support(indices=True)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_indices = featureSelector.get_support(indices=True)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n\n# Sort the feature names in alphabetical order\nfeature_names = sorted(feature_names)\n\n# Create a new matrix with the sorted feature names\nnew_X = np.zeros((X.shape[0], len(feature_names)))\nfor i, feature in enumerate(feature_names):\n    new_X[:, i] = X[:, vectorizer.vocabulary_[feature]]\n\n# Replace the original matrix with the new matrix\nX = new_X\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n\n# Sort the feature names in alphabetical order\nfeature_names = sorted(feature_names)\n\n# Create a new matrix with the sorted feature names\nX_sorted = np.zeros((X.shape[0], len(feature_names)))\nfor i, feature in enumerate(feature_names):\n    X_sorted[:, i] = X[:, vectorizer.vocabulary_[feature]]\n\n# Replace the original matrix with the sorted matrix\nX = X_sorted\n",
        "\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] #removes NaN values for each column to apply sklearn function\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) # either this or the next line\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('titanic.csv')\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('titanic.csv')\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\nfrom sklearn import linear_model\nX_train, y_train, X_test, y_test = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nassert type(X_test) == np.ndarray\nassert type(y_test) == np.ndarray\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\n# Reshape the numpy array into a 1D array\nnp_array = np.reshape(np_array, (np_array.shape[0] * np_array.shape[1],))\n\n# Create a MinMaxScaler object and fit and transform the data\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n\n# Reshape the transformed data back into a 2D array\ntransformed = np.reshape(transformed, (np_array.shape[0], np_array.shape[1]))\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\nscaler = MinMaxScaler(feature_range=(0,1))\nnew_array = scaler.fit_transform(np_array)\n\n",
        "\npredict = clf.predict([close_buy1, m5, m10, ma20])\n",
        "\nnew_X = np.array(X, dtype=object)\nclf.fit(new_X, ['2', '3'])\n",
        "\nnew_X = np.array(X, dtype=object)\nclf.fit(new_X, ['2', '3'])\n",
        "\nnew_X = np.array(X, dtype=object)\nclf.fit(new_X, ['4', '5'])\n",
        "\nlogReg.fit(dataframe.iloc[-1:],dataframe.iloc[:,-1])\n",
        "\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\n",
        "\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = cross_validation.train_test_split(features_dataframe, train_size=train_size)\n\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n",
        "\ndf[cols + '_scale'] = df.groupby('Month')[cols].apply(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\n",
        "\nmyData['new_A2'] = myData.groupby('Month')['A2'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\nmyData['new_A3'] = myData.groupby('Month')['A3'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\n",
        "\ncount = CountVectorizer(lowercase = False)\n\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase = False)\n\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n\n",
        "\nfrom sklearn.model_selection import GridSearchCV\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results[['mean_fit_time', 'param_kernel', 'param_C', 'param_gamma', 'param_degree', 'param_coef0', 'param_shrinking', 'param_probability', 'param_tol', 'param_cache_size', 'param_class_weight', 'param_verbose', 'param_max_iter', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'mean_train_score', 'std_train_score', 'mean_score_time', 'std_score_time']]\nfull_results = full_results.sort_values(by='mean_fit_time')\n",
        "\n!pip install joblib\n",
        "\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity_matrix = cosine_similarity(tfidf_matrix)\n"
    ],
    "Pytorch": [
        "\n\n# Define the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Train the model\nfor epoch in range(10):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        output = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n        # Change the learning rate dynamically\n        if epoch == 5:\n            optimizer.set_lr(0.001)\n",
        "\n\ndef load_data():\n    # Load data here\n    return optim\n\noptim = load_data()\n\n# Define a custom learning rate scheduler\nclass CustomLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, lr_lambda, last_epoch=-1):\n        self.lr_lambda = lr_lambda\n        super(CustomLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        return [base_lr * self.lr_lambda(self.last_epoch) for base_lr in self.base_lrs]\n\n# Define a custom optimizer\nclass CustomOptimizer(torch.optim.SGD):\n    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0.0001):\n        super(CustomOptimizer, self).__init__(params, lr, momentum, weight_decay)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if group['weight_decay'] != 0:\n                    d_p.add_(group['weight_decay'], p.data)\n                p.data.add_(-group['lr'], d_p)\n        return loss\n\n# Create a custom learning rate scheduler\nlr_lambda = lambda epoch: 0.1 ** (epoch // 30)\nscheduler = CustomLR(optim, lr_lambda)\n\n# Create a custom optimizer\noptim = CustomOptimizer(optim.param_groups)\n\n# Update the learning rate of the optimizer when loss on training set increases\ndef update_lr(epoch, loss):\n    if loss < 0.5:\n        optim.param_groups[0]['lr'] *= 1.1\n    elif loss > 0.7:\n        optim.param_groups[0]['lr'] *= 0.9\n\n# Train the model\nfor epoch in range(100):\n    train()\n    loss = validate()\n    update_lr(epoch, loss)\n    scheduler.step()\n",
        "\n\n# create optimizer\noptim = torch.optim.SGD(model.parameters(), lr=0.005)\n\n# train the model\nfor epoch in range(10):\n    for batch in train_loader:\n        # do some training\n        ...\n        # change learning rate\n        if epoch == 5:\n            optim.set_lr(0.0005)\n",
        "\n\ndef load_data():\n    # Load data here\n    return optim\n\ndef update_lr(optim, loss):\n    # Update learning rate here\n    return optim\n\noptim = load_data()\n\nfor epoch in range(10):\n    # Train model here\n    loss = calculate_loss()\n    optim = update_lr(optim, loss)\n",
        "\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n\ninput_Tensor = load_data()\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n# create an embedding matrix with the same vocabulary as the input data\nvocab_size = len(word2vec.wv.vocab)\nembedding_matrix = np.zeros((vocab_size, word2vec.vector_size))\n\n# fill in the embedding matrix with the pre-trained word2vec weights\nfor i, word in enumerate(word2vec.wv.vocab):\n    embedding_matrix[i] = word2vec.wv[word]\n\n# create a PyTorch embedding layer with the embedding matrix\nembedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n\n# embed the input data using the embedding layer\nembedded_input = embedding_layer(input_Tensor)\n\n",
        "\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\ninput_Tensor = load_data()\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\ndef get_embedded_input(input_Tensor):\n    # Initialize an empty tensor to store the embedded input\n    embedded_input = torch.empty(len(input_Tensor), 100)\n    # Loop through each input sentence\n    for i, sentence in enumerate(input_Tensor):\n        # Convert the sentence to a list of words\n        words = sentence.split()\n        # Initialize an empty list to store the word embeddings\n        word_embeddings = []\n        # Loop through each word in the sentence\n        for word in words:\n            # Get the word embedding from the word2vec model\n            word_embedding = word2vec[word]\n            # Append the word embedding to the list\n            word_embeddings.append(word_embedding)\n        # Convert the list of word embeddings to a numpy array\n        word_embeddings_array = np.array(word_embeddings)\n        # Average the word embeddings to get the sentence embedding\n        sentence_embedding = np.mean(word_embeddings_array, axis=0)\n        # Append the sentence embedding to the embedded input tensor\n        embedded_input[i] = torch.from_numpy(sentence_embedding)\n    return embedded_input\nembedded_input = get_embedded_input(input_Tensor)\n",
        "\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n",
        "\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n",
        "\nx = torch.rand(6,6)\npx = pd.DataFrame(x.numpy())\n",
        "\nC = B[:, A_log] # Throws error\n",
        "\nC = B[:, A_logical]\n",
        "\nC = B[:, A_log] # Throws error\n",
        "\nC = B[:, A_log] # Throws error\n",
        "\nC = B[:, A_log] # Throws error\n",
        "\nC = B[:, A_log]\n",
        "\nC = torch.index_select(B, dim=1, index=idx)\n",
        "\nx_array = np.array([\n   [0.5, 1.0, 2.0],\n   [4.0, 6.0, 8.0]\n], dtype=object)\nx_tensor = torch.tensor(x_array)\n",
        "\nx_array = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n], dtype=object)\n\nx_tensor = torch.tensor(x_array)\n",
        "\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\ndef Convert(a):\n    return torch.tensor(a, dtype=torch.float16)\nx_tensor = Convert(x_array)\n",
        "\nlens = [3, 5, 4]\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmask = torch.zeros((len(lens), max(lens))).long()\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nlens = [3, 5, 4]\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nlens = [3, 5, 4]\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\n\nTensor_2D = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndiag_ele = torch.tensor([1, 2, 3])\nindex_in_batch = 2\n\n# Create diagonal matrix\ndiag_mat = torch.diag(diag_ele)\n\n# Repeat diagonal matrix along batch dimension\nTensor_3D = diag_mat.unsqueeze(0).repeat(index_in_batch, 1, 1)\n\n",
        "\nTensor_2D = load_data()\ndef Convert(t):\n    diag_ele = torch.diag(t)\n    drag_ele = torch.diag(torch.diag(t))\n    result = torch.zeros(t.shape[0], diag_ele.shape[0], drag_ele.shape[0])\n    for i in range(t.shape[0]):\n        result[i] = torch.diag(t[i])\n    return result\nTensor_3D = Convert(Tensor_2D)\n",
        "\na, b = load_data()\nab = torch.cat((a, b), 0)\n",
        "\na, b = load_data()\nab = torch.cat((a,b),0)\n",
        "\na, b = load_data()\ndef solve(a, b):\n    ab = torch.cat((a, b), 0)\n    return ab\nab = solve(a, b)\n",
        "\na[ : , lengths : , : ]  = 0\n",
        "\na[ : , lengths : , : ]  = 2333\n",
        "\na[ : , : lengths , : ]  = 0\n",
        "\na[ : , : lengths , : ]  = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nlist = load_data()\nnew_tensors = torch.stack(list)\n",
        "\n\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx].flatten()\n",
        "\nresult = t[idx].flatten()\n",
        "\nresult = t[idx, torch.arange(t.shape[1])]\n",
        "\nresult = x.gather(1,ids.unsqueeze(-1).expand(-1,-1,x.shape[-1]))\n",
        "\nresult = x.gather(1,ids.unsqueeze(-1).expand(-1,-1,x.shape[-1]))\n",
        "\nids, x = load_data()\nresult = np.zeros((70,2))\nfor i in range(70):\n    idx = np.argmax(ids[i])\n    result[i] = x[i][idx]\n",
        "\nsoftmax_output = torch.tensor([[0.2, 0.1, 0.7], [0.6, 0.2, 0.2], [0.1, 0.8, 0.1]])\ny = torch.argmax(softmax_output, dim=1)\n",
        "\nsoftmax_output = torch.tensor([[0.7, 0.2, 0.1], [0.2, 0.6, 0.2], [0.1, 0.1, 0.8]])\ny = torch.argmax(softmax_output, dim=1)\n",
        "\nsoftmax_output = torch.tensor([[0.2, 0.1, 0.7], [0.6, 0.3, 0.1], [0.15, 0.8, 0.05]])\ny = torch.argmin(softmax_output, dim=1)\ny = y.unsqueeze(1)\n",
        "\nsoftmax_output = load_data()\ndef solve(softmax_output):\n    y = torch.argmax(softmax_output, dim=1)\n    return y\ny = solve(softmax_output)\n",
        "\nsoftmax_output = load_data()\ndef solve(softmax_output):\n    y = torch.argmin(softmax_output, dim=1)\n    return y\ny = solve(softmax_output)\n",
        "\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, w, z), target: (n, w, z)\n    n, c, w, z = input.size()\n    # log_p: (n, c, w, z)\n    log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*w*z, c)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)  # make class dimension last dimension\n    log_p = log_p[\n       target.view(n, w, z, 1).repeat(0, 0, 0, c) >= 0]  # this looks wrong -> Should rather be a one-hot vector\n    log_p = log_p.view(-1, c)\n    # target: (n*w*z,)\n    mask = target >= 0\n    target = target[mask]\n    loss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False)\n    if size_average:\n        loss /= mask.data.sum()\n    return loss\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = np.count_nonzero(A != B)\n",
        "\nA, B = load_data()\ndef Count(A, B):\n    cnt_equal = np.sum(A == B)\n    return cnt_equal\ncnt_equal = Count(A, B)\n",
        "\nA, B = load_data()\ncnt_equal = np.sum(A[-len(B):] == B)\n",
        "\nA, B = load_data()\ncnt_not_equal = np.count_nonzero(A[-len(B):] != B)\n",
        "\ntensors_31 = []\nfor i in range(31):\n    start = i * 10\n    end = start + 10\n    tensor_i = a[:, :, :, start:end, :]\n    tensors_31.append(tensor_i)\n",
        "\ntensors_31 = []\nfor i in range(31):\n    start = i * 10\n    end = start + 10\n    tensor_31 = a[:, :, start:end, :, :]\n    tensors_31.append(tensor_31)\n",
        "\noutput[mask==1] = clean_input_spectrogram[mask==1]\n",
        "\nmask, clean_input_spectrogram, output= load_data()\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsigned_min = torch.where(torch.abs(x) < torch.abs(y), x, y) * torch.sign(x)\n",
        "\nx, y = load_data()\n\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\n\nsigned_max = torch.where(torch.abs(x) == max, sign_x, torch.where(torch.abs(y) == max, sign_y, 0))\n\n",
        "\nx, y = load_data()\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = torch.mul(sign_x, min)\n    return signed_min\nsigned_min = solve(x, y)\n",
        "\nMyNet = torch.nn.Sequential(torch.nn.Linear(4, 15),\n                            torch.nn.Sigmoid(),\n                            torch.nn.Linear(15, 3),\n                            )\nMyNet.load_state_dict(torch.load(\"my_model.pt\"))\ninput = load_data()\nassert type(input) == torch.Tensor\n\noutput = MyNet(input)\nconf, classes = torch.max(output.reshape(1, 3), 1)\nconfidence_score = conf.item()\n\n",
        "\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\n# concatenate the tensors along the columns axis\nc = torch.cat((a, b), dim=1)\n\n# extract the relevant columns and compute the average\nresult = (c[:, :2] + c[:, 3:]).div(2)\n\n",
        "\na, b = load_data()\ndef solve(a, b):\n    # Create a new tensor with zeros\n    result = torch.zeros((a.shape[0], a.shape[1] + b.shape[1]))\n    # Copy the first tensor into the first part of the result tensor\n    result[:, :a.shape[1]] = a\n    # Calculate the average of the last column of 'a' and the first column of 'b'\n    avg = (a[:, -1] + b[:, 0]) / 2\n    # Copy the average into the last part of the result tensor\n    result[:, a.shape[1]:] = avg.reshape(-1, 1)\n    return result\nresult = solve(a, b)\n",
        "\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[[0., 0., 0.,0.]]])\nr = torch.cat([t,new], dim=2)\n",
        "\nt = torch.arange(4).reshape(1,2,2).float()\nnew=torch.tensor([[[0., 0., 0.,0.]]])\nr = torch.cat([t,new], dim=0)\n",
        "\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[-1, -1, -1, -1,]])\nr = torch.cat([t,new], dim=1)\n",
        "\nresult = torch.bmm(data, W.unsqueeze(0).unsqueeze(0))\nresult = result.squeeze()\nresult = result.view(10, 2, 3)\n"
    ]
}