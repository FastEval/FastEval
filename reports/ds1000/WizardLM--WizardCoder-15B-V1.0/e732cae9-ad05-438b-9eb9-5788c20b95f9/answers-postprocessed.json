{
    "Pandas": [
        "\n\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\n\nList = np.random.permutation(len(df))\n\n# Shuffle the order of the DataFrame's rows according to the List\nresult = df.iloc[List]\n\n",
        "\n\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\n\n# Shuffle the DataFrame's rows according to the List\ndf = df.iloc[List]\n\n# Count the number of rows with different Type than the original DataFrame\nresult = len(df[df['Type'] != df.iloc[List]['Type']])\n\n",
        "\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# [Missing]\n\n# Create a new column 'others' for Qu1 with values apple and egg\ndf['others'] = df['Qu1'].apply(lambda x: 'other' if x in ['apple', 'egg'] else x)\n\n# Replace Qu1 values according to value_counts() when value count great or equal 2\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'cheese' if x == 'cheese' else 'potato' if x == 'potato' else 'banana' if x == 'banana' else 'other')\n\n# Replace Qu2 values according to value_counts() when value count great or equal 2\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'banana' if x == 'banana' else 'apple' if x == 'apple' else 'sausage' if x == 'sausage' else x)\n\n# Replace Qu3 values according to value_counts() when value count great or equal 2\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'cheese' if x == 'cheese' else 'potato' if x == 'potato' else 'other' if x in ['apple', 'egg'] else x)\n\nresult = df\n\n",
        "\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# [Missing]\n\n# Create a new dataframe with the updated values\nresult = df.copy()\nresult['Qu1'] = result['Qu1'].apply(lambda x: 'other' if pd.value_counts(df['Qu1'])[x] >= 3 else x)\nresult['Qu2'] = result['Qu2'].apply(lambda x: 'other' if pd.value_counts(df['Qu2'])[x] >= 3 else x)\nresult['Qu3'] = result['Qu3'].apply(lambda x: 'other' if pd.value_counts(df['Qu3'])[x] >= 3 else x)\n\n",
        "\n\nexample_df = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\ndef f(df=example_df):\n    # Create a new column 'others' for Qu1 column\n    df['others'] = df['Qu1'].apply(lambda x: 'other' if x not in ['cheese', 'potato', 'banana'] else x)\n    \n    # Replace Qu1 values according to value_counts()\n    value_counts = pd.value_counts(df['Qu1'])\n    for value in value_counts.index:\n        if value_counts[value] >= 2:\n            df['Qu1'] = df['Qu1'].replace(value, 'other')\n    \n    # Replace Qu2 values according to value_counts()\n    value_counts = pd.value_counts(df['Qu2'])\n    for value in value_counts.index:\n        if value_counts[value] >= 2:\n            df['Qu2'] = df['Qu2'].replace(value, 'other')\n    \n    # Replace Qu3 values according to value_counts()\n    value_counts = pd.value_counts(df['Qu3'])\n    for value in value_counts.index:\n        if value_counts[value] >= 2:\n            df['Qu3'] = df['Qu3'].replace(value, 'other')\n    \n    return df\n",
        "\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# [Missing]\n\n# create a new dataframe with the updated values\nresult = pd.DataFrame({'Qu1': ['other', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']})\n\n",
        "\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\n# [Missing]\n\n# Replace values in Qu1 column with 'other' when value count great or equal 3 and keep values cheese\ndf.loc[pd.value_counts(df.Qu1) >= 3, 'Qu1'] = 'other'\n\n# Replace values in Qu2 and Qu3 column with 'other' when value count great or equal 2\ndf.loc[pd.value_counts(df.Qu2) >= 2, 'Qu2'] = 'other'\ndf.loc[pd.value_counts(df.Qu3) >= 2, 'Qu3'] = 'other'\n\n# Reserve all the 'apple' in Qu1 column\ndf.loc[df.Qu1 == 'apple', 'Qu1'] = 'apple'\n\n# Replace values in Qu1 column with 'other' when value count great or equal 3 and keep values cheese\ndf.loc[pd.value_counts(df.Qu1) >= 3, 'Qu1'] = 'other'\n\n# Replace values in Qu2 and Qu3 column with 'other' when value count great or equal 2\ndf.loc[pd.value_counts(df.Qu2) >= 2, 'Qu2'] = 'other'\ndf.loc[pd.value_counts(df.Qu3) >= 2, 'Qu3'] = 'other'\n\n# Reserve all the 'apple' in Qu1 column\ndf.loc[df.Qu1 == 'apple', 'Qu1'] = 'apple'\n\nresult = df\n\n",
        "\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\n# Solution\ndf = df.drop_duplicates(subset='url', keep='first')\ndf = df[df['keep_if_dup'] == 'Yes']\n\n",
        "\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'drop_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\n# Solution\n\nresult = df.drop_duplicates(subset='url', keep='first', inplace=False)\nresult = result[result['drop_if_dup'] == 'Yes']\n\n",
        "\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\n# Solution\ndf = df.sort_values(by=['url', 'keep_if_dup'], ascending=[False, False])\ndf = df.drop_duplicates(subset='url', keep='first')\n\n",
        "\n\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\n\nresult = {}\nfor name, group in df.groupby('name'):\n    result[name] = {}\n    for v1, group1 in group.groupby('v1'):\n        result[name][v1] = {}\n        for v2, v3 in zip(group1['v2'], group1['v3']):\n            result[name][v1][v2] = v3\n\n",
        "\nresult = df.apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n\nexample_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\nexample_df['datetime'] = pd.to_datetime(example_df['datetime'])\ndef f(df=example_df):\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    return df['datetime']\n\nresult = f()\n",
        "\nresult = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\nresult = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\n\n# [Missing]\n\nresult = pd.DataFrame(columns=['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids'])\n\nfor i in range(len(df)):\n    message = df.loc[i, 'message']\n    message = message.replace('[', '').replace(']', '')\n    message = message.split(', ')\n    for j in range(len(message)):\n        message[j] = message[j].split(': ')\n    message = {x[0]: x[1] for x in message}\n    result.loc[i] = [df.loc[i, 'name'], df.loc[i, 'status'], df.loc[i, 'number'], message.get('job', 'none'), message.get('money', 'none'), message.get('wife', 'none'), message.get('group', 'none'), message.get('kids', 'none')]\n\n",
        "\nresult.loc[result['product'].isin(products), 'score'] = result.loc[result['product'].isin(products), 'score'] * 10\n",
        "\nresult.loc[~result['product'].isin(products), 'score'] *= 10\n",
        "\n\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [[1069104, 1069105], [1066489, 1066491]]\n\n# multiply scores corresponding to products in the products list by 10\nfor product_list in products:\n    df.loc[df['product'].isin(product_list), 'score'] *= 10\n\nresult = df\n",
        "\n# Solution\nresult = df.copy()\nresult.loc[result['product'].isin(products), 'score'] = (result.loc[result['product'].isin(products), 'score'] - result.loc[result['product'].isin(products), 'score'].min()) / (result.loc[result['product'].isin(products), 'score'].max() - result.loc[result['product'].isin(products), 'score'].min())\n",
        "\n\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 0, 1],\n                   'C': [0, 0, 1, 0, 0, 0],\n                   'D': [0, 0, 0, 1, 0, 0]})\n\n# Convert binary columns to categorical column\nresult = pd.get_dummies(df)\nresult.columns = ['A', 'B', 'C', 'D', 'category']\n\n",
        "\n\ndf = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\n\n# Convert binary columns into a single binary column\nbinary_cols = ['A', 'B', 'C', 'D']\ndf['binary'] = np.where(df[binary_cols].values.astype(bool), 1, 0)\n\n# Convert binary column into categorical column\nresult = pd.get_dummies(df['binary'])\n\n# Rename columns to match original dataframe\nresult.columns = ['A', 'B', 'C', 'D']\n\n",
        "\n\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 1, 1],\n                   'C': [1, 1, 1, 0, 1, 0],\n                   'D': [0, 0, 0, 1, 1, 0]})\n\n# Convert binary columns to categorical column of lists\ndf['category'] = df.apply(lambda x: [col for col, val in x.items() if val == 1], axis=1)\n\n# Drop binary columns\ndf = df.drop(['A', 'B', 'C', 'D'], axis=1)\n\nresult = df\n",
        "\nresult = df.copy()\nresult['Date'] = df['Date'].dt.to_period(\"M\")\nresult['Date'] = result['Date'].apply(lambda x: x.strftime(\"%b-%Y\"))\n",
        "\nresult = df.copy()\nresult['Date'] = df['Date'].dt.to_period(\"M\")\nresult['Date'] = result['Date'].apply(lambda x: x.strftime(\"%d-%b-%Y\"))\n",
        "\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\nList = ['2019-01-17', '2019-02-20']\n\n# [Missing]\n\nresult = df.loc[(df['Date'] >= pd.to_datetime(List[0])) & (df['Date'] <= pd.to_datetime(List[1])), ['Date']].copy()\nresult['Date'] = result['Date'].dt.to_period(\"M\")\nresult['Date'] = result['Date'].apply(lambda x: x.strftime(\"%d-%b-%Y\"))\nresult['Day'] = result['Date'].apply(lambda x: pd.to_datetime(x).strftime(\"%A\"))\nresult.columns = ['Date', 'Day']\nresult = result.reset_index(drop=True)\n\n",
        "\n# Solution code with missing line(s) of code\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the first row of the first column down 1 row\ndf.iloc[0,0] = df.iloc[1,0]\n\n# Shift the last row of the first column to the first row, first column\ndf.iloc[0,0] = df.iloc[-1,0]\n\nresult = df\n",
        "\nresult = df.shift(1, axis=0)\nresult.iloc[0, 0] = df.iloc[4, 0]\nresult.iloc[0, 1] = df.iloc[4, 1]\n",
        "\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the first row of the first column down 1 row\ndf.iloc[0,0] = df.iloc[1,0]\n\n# Shift the last row of the first column up 1 row\ndf.iloc[-1,0] = df.iloc[-2,0]\n\n# Shift the last row of the second column up 1 row\ndf.iloc[-1,1] = df.iloc[-2,1]\n\n# Shift the first row of the second column down 1 row\ndf.iloc[0,1] = df.iloc[1,1]\n\nresult = df\n",
        "\ndf = df.shift(periods=1, axis=0)\ndf.iloc[0, 0] = df.iloc[1, 0]\ndf.iloc[-1, 0] = df.iloc[-2, 0]\n",
        "\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\ndf.rename(columns={'HeaderA': 'HeaderAX',\n                  'HeaderB': 'HeaderBX',\n                  'HeaderC': 'HeaderCX'}, inplace=True)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\nresult = df.rename(columns=lambda x: 'X' + x)\n",
        "\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457],\n     \"HeaderX\": [345]})\n\n# Renaming all columns that don't end with \"X\" with \"X\" in the head\ndf.rename(columns={col: \"X\" + col if not col.endswith(\"X\") else col for col in df.columns}, inplace=True)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\n\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", \"val3\": \"mean\"})\n\n",
        "\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\n\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", df.columns.difference(['group', 'group_color']).tolist(): \"sum\"})\n\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"mean\"})\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].mean(axis=0)\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].sum(axis=0)\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].sum(axis=0)\nresult = result.drop(result.idxmax())\n\n",
        "\n\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\n# [Missing]\n\nresult = df.apply(lambda x: x.value_counts(), axis=0)\n\n",
        "\n\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\nresult = df.isnull().sum()\n\n",
        "\n\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\n# [Missing]\n\nresult = ''\nfor col in df.columns:\n    result += f'---- {col} ---\\n'\n    result += str(df[col].value_counts()) + '\\n'\n\n",
        "\nresult = df.head(2).combine_first(df.iloc[[0]])\n",
        "\ndf = df.drop('Unnamed: 2', axis=1)\ndf = df.combine_first(df.iloc[[0]])\nresult = df.head()\n",
        "\n\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\n\n# Solution\ndf.fillna(method='ffill',inplace=True)\ndf.fillna(method='bfill',inplace=True)\n\n",
        "\ndf.apply(lambda x : pd.Series(np.where(x.isnull(), np.nan, x)), axis=1)\n",
        "\ndf.fillna(method='ffill', inplace=True)\ndf.fillna(method='bfill', inplace=True)\n",
        "\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n\n# [Missing]\n\nresult = df.loc[df['value'] < thresh].sum()\nresult.name = 'X'\ndf.loc[df['value'] < thresh, 'value'] = result\n\n",
        "\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n\n# Select rows smaller than threshold\ndf_small = df.loc[df['value'] < thresh]\n\n# Compute average of selected rows\navg = df_small['value'].mean()\n\n# Create new row with average value\nnew_row = pd.DataFrame({'value':[avg]}, index=['X'])\n\n# Concatenate new row with rest of dataframe\nresult = pd.concat([df_small, new_row])\n\n",
        "\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nsection_left = 4\nsection_right = 38\n\n# [Missing]\n\nresult = df.loc[df['value'] < section_left, 'value'].mean()\nresult = pd.DataFrame({'lab':['X'], 'value':[result]})\nresult = result.set_index('lab')\ndf.loc[df['value'] >= section_right, 'value'] = result.loc['X', 'value']\n\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n# [Missing]\n\nresult = pd.concat([df, 1/df], axis=1)\nresult.columns = list(df.columns) + [f\"inv_{col}\" for col in df.columns]\n\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nfor col in df.columns:\n    result[f\"exp_{col}\"] = df[col].apply(lambda x: math.exp(x))\n\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [4, 5, 6]})\n\n# [Missing]\n\nresult = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [4, 5, 6]})\n\nfor col in df.columns:\n    if df[col].sum() != 0:\n        result[f\"inv_{col}\"] = 1/df[col]\n\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\ne = np.exp(1)\n\nresult = df.copy()\n\nfor col in df.columns:\n    result[f\"sigmoid_{col}\"] = 1 / (1 + np.exp(-df[col]))\n\n",
        "\n# Solution code\n\n# Get the index location of each respective column minimum\nmins = df.idxmin()\n\n# Get the index location of the last occurrence of the column-wise maximum, up to the location of the minimum\nresult = df.apply(lambda x: x.loc[x.idxmax():mins[x.name]], axis=1)\n",
        "\n# Solution\n\n# Get the index location of each respective column minimum\nidx_min = df.idxmin()\n\n# Get the index location of the first occurrence of the column-wise maximum, down to the location of the minimum\nresult = idx_min.apply(lambda x: df.index[df[x] == df[x].max()][0])\n",
        "\n\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# [Missing]\n# Create a new dataframe with the minimum and maximum date from the original dataframe\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nnew_dates = pd.date_range(start=min_date, end=max_date)\n\n# Create a new dataframe with all the dates and fill in 0 for the val column\nresult = pd.DataFrame({'dt': new_dates, 'user': ['a']*len(new_dates), 'val': [0]*len(new_dates)})\n\n# Merge the original dataframe with the new dataframe on the dt column\nresult = pd.merge(result, df, on='dt', how='left')\n\n",
        "\n\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Solution\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['abc']*len(pd.date_range(min_dt, max_dt)), 'val': [0]*len(pd.date_range(min_dt, max_dt))})\nresult = pd.merge(result, df, how='left', on=['dt', 'user'])\nresult = result.fillna(0)\n\n",
        "\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Fill in missing values\ndf = df.set_index('dt').resample('D').asfreq().reset_index()\ndf['val'] = 233\n\n# Add missing dates\nstart_date = df['dt'].min()\nend_date = df['dt'].max()\nmissing_dates = pd.date_range(start=start_date, end=end_date, freq='D')\nmissing_df = pd.DataFrame({'dt': missing_dates})\nmissing_df['user'] = 'a'\nmissing_df['val'] = 233\ndf = pd.concat([df, missing_df], ignore_index=True)\n\n# Sort by date and user\ndf = df.sort_values(['dt', 'user'])\n\n# Output result\nresult = df.pivot_table(index='dt', columns='user', values='val')\n",
        "\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Solution\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': df['user'].unique(), 'val': df.groupby('user')['val'].transform('max')})\nresult = result.merge(df, on=['dt', 'user'], how='left')\nresult = result.fillna(method='ffill')\nresult = result.fillna(method='bfill')\n\n",
        "\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Solution\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['a']*len(pd.date_range(min_dt, max_dt)), 'val': [df[df['user'] == 'a']['val'].max()] * len(pd.date_range(min_dt, max_dt))})\n\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n",
        "\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Replace each name with a unique ID\ndf['name'] = df['name'].astype('category').cat.codes + 1\n\n",
        "\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Replace each a with a unique ID\ndf['a'] = df['a'].astype('category').cat.codes + 1\n\n",
        "\n\nexample_df = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\ndef f(df=example_df):\n    # create a new dataframe with unique IDs for each name\n    name_ids = pd.DataFrame({'name': df['name'].unique(), 'id': range(len(df['name'].unique()))})\n    # merge the new dataframe with the original dataframe on name\n    result = pd.merge(df, name_ids, on='name')\n    # set the new ID as the index\n    result.set_index('id', inplace=True)\n    return result\n",
        "\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a new dataframe with unique IDs for each name and a combination of a and name\nnew_df = pd.DataFrame({'ID': df['name'] + df['a'].astype(str),\n                       'b': df['b'],\n                       'c': df['c']})\n\n# Replace the original dataframe with the new dataframe\ndf = new_df\n\n",
        "\nresult = pd.melt(df, id_vars=['user', 'someBool'], value_vars=['01/12/15', '02/12/15'], var_name='date', value_name='value')\n",
        "\nresult = pd.melt(df, id_vars=['user'], value_vars=['01/12/15', '02/12/15', 'someBool'], var_name='others', value_name='value')\n",
        "\nresult = pd.melt(df, id_vars=['user', 'someBool'], value_vars=['01/12/15', '02/12/15'], var_name='date', value_name='value')\n",
        "\nresult = df[df.c > 0.5][columns].values\n",
        "\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['a','b','e']\n\n# Selecting only the rows where the value for column 'c' is greater than 0.45\nresult = df[df.c > 0.45][columns]\n\n# Converting the result to a numpy array\ntraining_set = np.array(result)\n\n",
        "\n\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    return result.values\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    result['sum'] = result.apply(lambda x: x.sum(), axis=1)\n    return result\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    return result\n",
        "\nfrom datetime import timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 120\n\n# Solution\n\nresult = df.copy()\nfor index, row in df.iterrows():\n    if index == 0:\n        continue\n    if (df.iloc[index]['date'] - df.iloc[index-1]['date']).days <= X:\n        result.drop(index, inplace=True)\n\n",
        "\nfrom datetime import timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n\n# Remove overlapping rows\nresult = df.sort_values(by='date')\nfor i in range(1, len(result)):\n    if (result.iloc[i]['date'] - result.iloc[i-1]['date']).days <= X:\n        result = result.drop(index=result.index[i])\n\n",
        "\nfrom datetime import datetime, timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n\n# Create a list of dates to filter out\nfilter_dates = []\nfor index, row in df.iterrows():\n    observation_time = 'D'\n    observation_period = X\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(days=i)))\n\n# Filter out the dates\ndf = df[~df.index.isin(filter_dates)]\n\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n\n# Convert the date column to the desired format\ndf['date'] = df['date'].dt.strftime('%d-%b-%Y')\n\n# Rename the columns\ndf.columns = ['ID', 'date', 'close']\n\n# Sort the dataframe by date\ndf = df.sort_values(by='date')\n\n# Print the result\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\n\nresult = df.rolling(3).sum() / df.rolling(3).count()\n\n",
        "\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\n\nresult = df.groupby(df.index // 3).sum()\n\n",
        "\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1, 4]})\n\nresult = df.groupby(df.index // 4).sum()\n\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\n\n# Binning the dataframe for every 3 rows from back to front\nresult = df.iloc[::-3].mean()\n\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\n# Solution\nresult = pd.concat([df.iloc[:3].sum(), df.iloc[3:5].mean(), df.iloc[5:8].sum(), df.iloc[8:].mean()], axis=1)\nresult.columns = ['sum', 'avg']\n\n",
        "\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\n# Solution\nresult = df.groupby(df.index // 3).agg({'col1': ['sum', 'mean']}).unstack().fillna(0)\n\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Filling zeros with previous non-zero value using pandas\nresult = df.fillna(method='ffill')\n\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# fill the missing values with the posterior non-zero value\nresult = df.fillna(method='ffill')\n\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Filling the zeros with the maximun between previous and posterior non-zero value using pandas\nresult = df.fillna(method='ffill').fillna(method='bfill')\n\n",
        "\n\ndf = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\n\n# [Missing]\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\ndf['time_days'] = df['time'].replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\n# Separate numbers from time and put them in two new columns\ndf['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\n\n# Create another column based on the values of time column\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n\nresult = df\n",
        "\ndf['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\n",
        "\n\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\n# Separate numbers from time and put them in two new columns\ndf['number'] = df.duration.str.extract(r'(\\d+)')\ndf['time'] = df.duration.str.extract(r'(\\D+)')\n\n# Create another column based on the values of time column\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\ndf['time_day'] = df['time_day'].astype(int) * df['number'].astype(int)\n\n# Drop the original duration column\ndf.drop(columns=['duration'], inplace=True)\n\nresult = df\n",
        "\n\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\ncolumns_check_list = ['A','B','C','D','E','F']\n\ncheck = np.where([df1[column] != df2[column] for column in columns_check_list])\nresult = np.all(check[0] == [])\n\n",
        "\n\n# create a list of boolean values to represent the result of the check\nresult = []\n\n# iterate over the columns in the list\nfor column in columns_check_list:\n    # check if the values in both dataframes are equal for the current column\n    check = np.where(df1[column] == df2[column])\n    # if the values are equal, append True to the result list\n    if check[0].size == 0:\n        result.append(True)\n    # if the values are not equal, append False to the result list\n    else:\n        result.append(False)\n\n# print the result list\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\nfrom pandas import Timestamp\n\ndef f(df):\n    # parse date index\n    df.index = pd.to_datetime(df.index)\n    \n    # create numpy array of date, x and y\n    arr = np.array(df.reset_index())\n    \n    return arr\n",
        "\ndef f(df):\n    df.index = pd.MultiIndex.from_product([pd.to_datetime(df.index.get_level_values(0)), df.index.get_level_values(1)])\n    df = df.swaplevel(0, 1)\n    return df\n",
        "\nresult = pd.melt(df, id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'], var_name='year', value_name='value')\n",
        "\nresult = pd.wide_to_long(df, stubnames=['var1', 'var2'], i='Country', j='year', sep='_', suffix='\\d{4}')\nresult = result.reset_index()\nresult = result.sort_values(['year'], ascending=False)\nresult = result.rename(columns={'level_0': 'Variable'})\n",
        "\nresult = df[df.apply(lambda x: abs(x['Value_B']) < 1 and abs(x['Value_C']) < 1 and abs(x['Value_D']) < 1, axis=1)]\n",
        "\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\nresult = df[abs(df[['Value_B', 'Value_C', 'Value_D']]).max(axis=1) > 1]\n",
        "\nresult = df.loc[abs(df.filter(like='Value_').values) > 1]\nresult.columns = result.columns.str.replace('Value_', '')\n",
        "\n\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\n\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'A': ['Good &LT bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &LT; bad'] * 5})\n\ndf['A'] = df['A'].str.replace('&LT', '<')\n\nresult = df\n",
        "\n\nexample_df = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\ndef f(df=example_df):\n    df['A'] = df['A'].str.replace('&AMP;', '&')\n    return df\n",
        "\n\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &GT; bad'] * 5})\n\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['A'] = df['A'].str.replace('&LT;', '<')\ndf['A'] = df['A'].str.replace('&GT;', '>')\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'A': ['1 &AMP; 1', 'BB', 'CC', 'DD', '1 &AMP; 0'], 'B': range(5), 'C': ['0 &AMP; 0'] * 5})\n\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n\nresult = df\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\nresult = df.copy()\nresult['first_name'] = df['name'].apply(lambda x: x.split()[0] if validate_single_space_name(x) else x)\nresult['last_name'] = df['name'].apply(lambda x: x.split()[1] if validate_single_space_name(x) else None)\n\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\nresult = df['name'].apply(validate_single_space_name)\nresult = result.dropna()\nresult = result.str.split(expand=True)\nresult.columns = ['1_name', '2_name']\n\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\n\n# Apply the validate_single_space_name function to each name in the DataFrame\ndf['validated_name'] = df['name'].apply(validate_single_space_name)\n\n# Split the validated names into first_name, middle_name and last_name\ndf['first_name'] = df['validated_name'].apply(lambda x: x.split()[0] if x else None)\ndf['middle_name'] = df['validated_name'].apply(lambda x: x.split()[1] if len(x.split()) > 2 else None)\ndf['last_name'] = df['validated_name'].apply(lambda x: x.split()[-1] if x else None)\n\n# Drop the validated_name column\ndf.drop(columns=['validated_name'], inplace=True)\n\n# Output the final DataFrame\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\n\ndf = pd.DataFrame({'datetime': ['2021-04-10 01:00:00', '2021-04-10 02:00:00', '2021-04-10 03:00:00', '2021-04-10 04:00:00', '2021-04-10 05:00:00'],\n                   'col1': [25, 25, 25, 50, 100],\n                   'col2': [50, 50, 100, 50, 100],\n                   'col3': [50, 50, 50, 100, 100]})\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\n# [Missing]\n\nresult = df.assign(state=lambda x: x[['col1', 'col2', 'col3']].apply(lambda y: max(y), axis=1))\nresult.loc[result['col2'] <= 50, 'state'] = result['col1']\nresult.loc[result['col3'] <= 50, 'state'] = result['col1']\n\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] > 50 and x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'], axis=1)\n",
        "\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n\nerrors = []\nfor index, row in df.iterrows():\n    if not isinstance(row[\"Field1\"], int):\n        errors.append(row[\"Field1\"])\n\nresult = errors\n",
        "\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n\nresult = []\nfor i in df.itertuples():\n    if isinstance(i[2], int):\n        result.append(i[2])\n\n",
        "\n\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    result = []\n    for i, row in df.iterrows():\n        if not isinstance(row['Field1'], int):\n            result.append(row['Field1'])\n    return result\n\n",
        "\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Compute row totals\nrow_totals = df.sum(axis=1)\n\n# Compute percentage for each value\nresult = df.div(row_totals, axis=0) * 100\n\n",
        "\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Compute the total value of each column\ntotal_val = df.sum()\n\n# Compute the percentage of each value by dividing the value by the total value of the column\nresult = df.div(total_val, axis=1)\n\n# Rename the columns to include the percentage sign\nresult.columns = ['cat', 'val1 (%)', 'val2 (%)', 'val3 (%)', 'val4 (%)']\n\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\ndf = df.drop(test)\nresult = df\n",
        "\n\ndef f(df, test):\n    # create a set to remove duplicates\n    test_set = set(test)\n    # create a list of row names\n    row_names = list(df.index)\n    # create a list of row names that are in the test set\n    test_row_names = [name for name in row_names if name in test_set]\n    # create a list of row indices that are in the test set\n    test_row_indices = [df.index.get_loc(name) for name in test_row_names]\n    # select the rows using the row indices\n    result = df.iloc[test_row_indices]\n    return result\n",
        "\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Calculate pairwise euclidean distances between cars\ndf['euclidean_distance'] = df.groupby('car')['x', 'y'].apply(lambda x: np.sqrt((x.iloc[0]-x.iloc[1])**2 + (x.iloc[2]-x.iloc[3])**2))\n\n# Get nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('car')['euclidean_distance'].transform(lambda x: x.idxmin())\n\n# Calculate average distance for each time point\nresult = df.groupby('time')['euclidean_distance'].mean().reset_index()\nresult.columns = ['time', 'average_distance']\n\n",
        "\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Calculate euclidean distance between each pair of cars\ndf['distance'] = df.groupby('car')['x'].apply(lambda x: pd.Series(x).diff().pow(2).sum(axis=1).pow(0.5))\n\n# Get the index of the minimum distance for each car\ndf['farmost_neighbour'] = df.groupby('car')['distance'].transform(lambda x: x.idxmin())\n\n# Get the corresponding car as the farmost neighbour\ndf['farmost_neighbour'] = df['car'].apply(lambda x: df.loc[df['farmost_neighbour'] == x, 'car'].iloc[0])\n\n# Calculate the average distance for each time point\nresult = df.groupby('time')['distance'].mean().reset_index()\nresult.columns = ['time', 'average_distance']\n\n",
        "\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\n# [Missing]\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna().astype(str)), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n\n# randomly select 20% of rows and set Quantity to zero\nresult = df.sample(frac=0.2, random_state=0)\nresult['Quantity'] = 0\n\n# keep the indexes of the altered rows\nresult.index = df.index[df.index.isin(result.index)]\n\n",
        "\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n\n# randomly select 20% of rows and set ProductId to zero\nresult = df.sample(frac=0.2, random_state=0)\nresult['ProductId'] = 0\n\n# keep the indexes of the altered rows\nresult.index = df.index[df.index.isin(result.index)]\n\n",
        "\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1, 1, 4, 7, 4, 2, 1, 1, 4, 7],\n                   'Quantity': [6, 1, 3, 2, 7, 2, 6, 1, 3, 2, 7, 2, 6, 1, 3]})\n\n# randomly select 20% of rows of each user, set random_state=0 and change the value of the Quantity column of these rows to zero\nresult = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0)).reset_index(drop=True)\nresult['Quantity'] = 0\n\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\nresult = duplicate\n",
        "\n\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\n\n",
        "\n\nexample_df=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\ndef f(df=example_df):\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.index\n    return duplicate\n\nresult = f()\n",
        "\n\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\n\n",
        "\n\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = df.loc[duplicate_bool == True].index\n\nresult = duplicate\n\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\ngrouped = df.groupby(['Sp','Mt'])\nresult = grouped.apply(lambda x: x.loc[x['count'].idxmin()])\n",
        "\nresult = df.groupby(['Sp','Value']).apply(lambda x: x.loc[x['count'].idxmax()])\n",
        "\n\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\n\nresult=df.query(\"Category in @filter_list\")\n\n",
        "\n\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\n\nresult=df.query(\"Category not in filter_list\")\n\n",
        "\n\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n\n# Create a list of tuples where each tuple contains the column levels\ncolumn_levels = [(col1, col2, col3) for col1 in df.columns.levels[0] for col2 in df.columns.levels[1] for col3 in df.columns.levels[2]]\n\n# Use pd.melt() with the list of tuples as value_vars\nresult = pd.melt(df, value_vars=column_levels)\n\n",
        "\n\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n\n# melt the data frame\nresult = pd.melt(df, id_vars=[('A', 'B', 'C'), ('E', 'F', 'G', 'H', 'I', 'J')],\n                 value_vars=[('A', 'B', 'C'), ('E', 'F', 'G', 'H', 'I', 'J')],\n                 var_name=['variable_0', 'variable_1', 'variable_2'],\n                 value_name='value')\n\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum())\n",
        "\nresult = df.groupby('id')['val'].apply(lambda x: x.cumsum()).reset_index()\nresult.columns = ['id', 'cumsum']\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum())\n",
        "\ndf['cummax'] = df.groupby('id')['val'].transform(pd.Series.cummax)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.where(x >= 0, 0).cumsum())\n",
        "\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\n# [Missing]\n\nresult = df.groupby('l')['v'].sum(skipna=False)\nresult.loc['right'] = np.nan\n\n",
        "\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\n# Solution\n\nresult = df.groupby('r')['v'].sum(skipna=False)\n\n",
        "\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('l')['v'].sum(skipna=False)['right']\n\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# [Missing]\n\n# One-to-many\ndf['Column1']['Column2'] = 'one-to-many'\ndf['Column1']['Column3'] = 'one-to-many'\ndf['Column1']['Column5'] = 'one-to-many'\n\n# One-to-one\ndf['Column1']['Column4'] = 'one-to-one'\n\n# Many-to-one\ndf['Column2']['Column1'] = 'many-to-one'\ndf['Column2']['Column3'] = 'many-to-many'\ndf['Column2']['Column4'] = 'many-to-one'\ndf['Column2']['Column5'] = 'many-to-many'\n\n# Many-to-many\ndf['Column3']['Column1'] = 'many-to-one'\ndf['Column3']['Column2'] = 'many-to-many'\ndf['Column3']['Column4'] = 'many-to-one'\ndf['Column3']['Column5'] = 'many-to-many'\n\n# One-to-many\ndf['Column4']['Column1'] = 'one-to-one'\ndf['Column4']['Column2'] = 'one-to-many'\ndf['Column4']['Column3'] = 'one-to-many'\ndf['Column4']['Column5'] = 'one-to-many'\n\n# Many-to-one\ndf['Column5']['Column1'] = 'many-to-one'\ndf['Column5']['Column2'] = 'many-to-many'\ndf['Column5']['Column3'] = 'many-to-many'\ndf['Column5']['Column4'] = 'many-to-one'\n\nresult = []\n\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1][col2] == 'one-to-many':\n                result.append(f'{col1} {col2} one-to-many')\n            elif df[col1][col2] == 'one-to-one':\n                result.append(f'{col1} {col2} one-to-one')\n            elif df[col1][col2] == 'many-to-one':\n                result.append(f'{col1} {col2} many-to-one')\n            elif df[col1][col2] == 'many-to-many':\n                result.append(f'{col1} {col2} many-to-many')\n\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# Solution\n\n# Create a list to store the relationship between columns\nrelationship_list = []\n\n# Iterate through each column\nfor col in df.columns:\n    # Iterate through each row\n    for i in range(len(df)):\n        # Check if the current row has a non-null value in the current column\n        if df.iloc[i][col] is not None:\n            # Iterate through each other column\n            for other_col in df.columns:\n                # Check if the current row has a non-null value in the other column\n                if df.iloc[i][other_col] is not None:\n                    # Check if the current column and the other column have the same number of unique values\n                    if len(df[col].unique()) == len(df[other_col].unique()):\n                        # Check if the current column and the other column have the same number of non-null values\n                        if len(df[col].dropna()) == len(df[other_col].dropna()):\n                            # Check if the current column and the other column have the same data type\n                            if df[col].dtype == df[other_col].dtype:\n                                # Check if the current column and the other column have the same range of values\n                                if df[col].min() == df[col].max() and df[other_col].min() == df[other_col].max():\n                                    # If all conditions are met, add the relationship to the list\n                                    relationship_list.append(f'{col} {other_col} one-2-one')\n                                else:\n                                    # If the conditions are not met, add the relationship to the list\n                                    relationship_list.append(f'{col} {other_col} one-2-many')\n                            else:\n                                # If the conditions are not met, add the relationship to the list\n                                relationship_list.append(f'{col} {other_col} many-2-many')\n                        else:\n                            # If the conditions are not met, add the relationship to the list\n                            relationship_list.append(f'{col} {other_col} many-2-many')\n                    else:\n                        # If the conditions are not met, add the relationship to the list\n                        relationship_list.append(f'{col} {other_col} many-2-many')\n                else:\n                    # If the conditions are not met, add the relationship to the list\n                    relationship_list.append(f'{col} {other_col} many-2-many')\n        else:\n            # If the conditions are not met, add the relationship to the list\n            relationship_list.append(f'{col} {other_col} many-2-many')\n\n# Print the list of relationships\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# [Missing]\n\n# Create an empty DataFrame to store the relationship types\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\n# Loop through each column and check the relationship type\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 == col2:\n            result.loc[col1, col2] = 'one-to-one'\n        elif df[col1].nunique() == df[col2].nunique():\n            result.loc[col1, col2] = 'one-to-many'\n        elif df[col1].nunique() == df.shape[0]:\n            result.loc[col1, col2] = 'many-to-one'\n        else:\n            result.loc[col1, col2] = 'many-to-many'\n\n",
        "\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# Solution\n\n# Create an empty DataFrame to store the result\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\n# Loop through each column pair\nfor col1 in df.columns:\n    for col2 in df.columns:\n        # Check if the columns are the same\n        if col1 == col2:\n            result.loc[col1, col2] = 'one-2-one'\n        # Check if there is only one value in one column\n        elif df[col1].nunique() == 1 or df[col2].nunique() == 1:\n            result.loc[col1, col2] = 'one-2-many'\n        # Check if there is only one value in both columns\n        elif len(set(df[col1]).intersection(set(df[col2]))) == 1:\n            result.loc[col1, col2] = 'many-2-one'\n        # Otherwise, the relationship is many-2-many\n        else:\n            result.loc[col1, col2] = 'many-2-many'\n\n",
        "\n\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar', 'jim', 'john', 'mary', 'jim'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar', 'ryan', 'con', 'sullivan', 'Ryan'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar', 'jim@com', 'john@com', 'mary@com', 'Jim@com'],\n                   'bank': [np.nan, 'abc', 'xyz', np.nan, 'tge', 'vbc', 'dfg']})\n\n# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n             .applymap(lambda s: s.lower() if type(s) == str else s)\n             .applymap(lambda x: x.replace(\" \", \"\") if type(x) == str else x)\n             .drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# get the index of records with bank account\nbank_indx = dfiban_uniq[dfiban_uniq['bank'].notnull()].index\n\n# save records with bank account\ndfiban = dfiban_uniq.loc[bank_indx]\n\n# get the index of records without bank account\nnobank_indx = dfiban_uniq[dfiban_uniq['bank'].isnull()].index\n\n# save records without bank account\ndfnoban = dfiban_uniq.loc[nobank_indx]\n\n# concatenate the two dataframes\nresult = pd.concat([dfnoban, dfiban])\n\n",
        "\n\ndf = pd.DataFrame({'Revenue': ['2,144.78', '2,036.62', '1,916.60', '1,809.40', '1,711.97', '6,667.22', '5,373.59', '4,071.00', '3,050.20', '-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0'],\n                   'Other, Net': ['-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0', '-0.8', '-1.12', '1.32', '-0.05', '-0.34', '-1.37', '-1.9', '-1.48', '0.1', '41.98', '35', '-11.66', '27.09', '-3.44', '14.13', '-18.69', '-4.87', '-5.7']})\n\ndf['Revenue'] = pd.to_numeric(df['Revenue'].str.replace(',', ''), errors='coerce')\ndf['Other, Net'] = pd.to_numeric(df['Other, Net'].str.replace(',', ''), errors='coerce')\n\n",
        "\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Group by the two conditions\ngrouped = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0), as_index=False)\n\n# Take the means of the two groups\nresult = grouped.mean()\n\n# Rename the columns\nresult.columns = ['Has Family', 'No Family']\n\n",
        "\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Group by the two conditions\ngrouped = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0), as_index=False)\n\n# Take the means of the two groups\nresult = grouped.mean()\n\n# Rename the columns\nresult.columns = ['Has Family', 'No Family']\n\n",
        "\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Group by the conditions\ngrouped = df.groupby((df['SibSp'] == 1) & (df['Parch'] == 1), as_index=False)\nnew_group = grouped.get_group(True)\nold_group = grouped.get_group(False)\n\n# Take the means of the two groups\nresult = pd.concat([new_group.mean(), old_group.mean()], axis=1)\nresult.columns = ['Has Family', 'New Family', 'No Family', 'Old Family']\n\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(['A', 'B']))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(['A']))\n",
        "\nresult = df.reset_index().melt(id_vars=['index'], value_vars=df.columns, var_name=['Caps', 'Lower'], value_name='Value')\nresult = result.pivot_table(index=['index', 'Caps', 'Lower'], columns='variable', values='Value')\nresult.columns = result.columns.swaplevel(0, 1)\nresult.columns.names = ['Caps', 'Lower']\nresult = result.sort_index(level=['Caps', 'Lower'])\n",
        "\n\nl = [('A', '1', 'a'),  ('A', '1', 'b'), ('A', '2', 'a'), ('A', '2', 'b'), ('B', '1','a'),  ('B', '1','b')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\n\n# Solution\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\ndf.columns = df.columns.set_levels([df.columns.levels[0].str.capitalize(), df.columns.levels[1].str.capitalize()], level=0)\ndf.columns = df.columns.set_levels([df.columns.levels[0].str.upper(), df.columns.levels[1].str.upper()], level=1)\ndf.columns = df.columns.set_levels([df.columns.levels[0].str.capitalize(), df.columns.levels[1].str.capitalize()], level=2)\n\nresult = df\n",
        "\nresult = df.reset_index().melt(id_vars=['index'], var_name=['Caps', 'Middle', 'Lower'], value_name='Value')\nresult = result.pivot_table(index=['index', 'Caps', 'Middle', 'Lower'], columns='variable', values='Value')\nresult.columns = result.columns.map('_'.join)\nresult = result.reset_index()\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('a')['b'].apply(stdMeann))\n\n",
        "\n\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('b')['a'].apply(stdMeann))\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\n# softmax normalization\ndf['softmax'] = np.exp(df['b']) / np.sum(np.exp(df['b']))\n\n# min-max normalization\ndf['min-max'] = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n\nresult = df[['a', 'b', 'softmax', 'min-max']]\n",
        "\nresult = df.loc[(df['A'] == 0) | (df['B'] == 0) | (df['C'] == 0) | (df['D'] == 0), ['A', 'B', 'D']]\n",
        "\n\ndf = pd.DataFrame([[-1,-1,0,2],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n\n# Remove rows and columns with sum of 0\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf = df.loc[(df != 0).any(axis=1), :]\n\n# Remove rows and columns with sum of 0 again\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf = df.loc[(df != 0).any(axis=1), :]\n\n# Remove rows and columns with sum of 0 one last time\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf = df.loc[(df != 0).any(axis=1), :]\n\n# Print result\n",
        "\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df[df.max(axis=1) != 2]\n\n",
        "\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n\ndf[df == 2] = 0\n\n",
        "\nresult = s.sort_values(ascending=True)\n",
        "\ndf = s.reset_index().sort_values(['index', 1], ascending=[True, False]).set_index('index')\n",
        "\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(['max']).reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\nresult = result[result['count_x'] == result['max']]\nresult = result[['Sp','Mt','Value','count_x']]\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\nresult = df.groupby(['Sp', 'Mt'])['count'].agg(['min']).reset_index()\nresult = df[df['count'] == result['min']]\n",
        "\nresult = df.groupby(['Sp','Value']).agg({'count': 'max'}).reset_index()\nresult = df.merge(result, on=['Sp','Value','count'], how='inner')\n",
        "\nresult = df.fillna(df['Member'])\nresult.loc[df['Member'].isin(dict.keys()), 'Date'] = df['Date'].fillna(dict)\n",
        "\nresult['Date'] = df['Date'].fillna(df['Member'].apply(lambda x: dict.get(x, '17/8/1926')))\n",
        "\n\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\n\ndef f(dict=example_dict, df=example_df):\n    result = df.fillna(df['Member'])\n    for key, value in dict.items():\n        result.loc[result['Member'] == key, 'Date'] = value\n    return result\n",
        "\n# [Missing]\n\nresult['Date'] = df['Date'].fillna(df['Member'].apply(lambda x: dict.get(x, '17/8/1926')))\nresult['Date'] = pd.to_datetime(result['Date'], format='%d/%m/%Y').dt.strftime('%d-%b-%Y')\n",
        "\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).size()\ndf['Count_y'] = df.groupby(df['Date'].dt.year).size()\n\nresult = df[['Date', 'Val', 'Count_d', 'Count_m', 'Count_y']]\n",
        "\n\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\n\ndf1['Count_Val'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month'), 'Val']).size()\n\ndf1['Count_m'] = df1.groupby('month')['count'].transform('sum')\ndf1['Count_y'] = df1.groupby('year')['count'].transform('sum')\n\nresult = df1[['Date', 'Val', 'count', 'Count_m', 'Count_y', 'Count_Val']]\n\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.dt.month.map(df.groupby(df['Date'].dt.month).size())\ndf['Count_y'] = df.Date.dt.year.map(df.groupby(df['Date'].dt.year).size())\ndf['Count_w'] = df.Date.dt.weekday.map(df.groupby(df['Date'].dt.weekday).size())\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\n\nresult = df[['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_w', 'Count_Val']]\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: (x == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x != 0).sum())\n",
        "\nresult1 = df.groupby('Date').agg({'B': ['sum', lambda x: (x % 2 == 0).sum()], 'C': ['sum', lambda x: (x % 2 == 1).sum()]})\nresult2 = df.groupby('Date').agg({'B': ['sum', lambda x: (x % 2 != 0).sum()], 'C': ['sum', lambda x: (x % 2 != 1).sum()]})\n",
        "\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n\n",
        "\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.sum, np.mean])\n\n",
        "\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n    'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n    'B' : ['A', 'B', 'C'] * 4,\n    'D' : np.random.randn(12),\n    'E' : np.random.randn(12)\n})\n\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.max, 'E':np.min})\n",
        "\n\ndf = dd.read_csv('file.csv')\n\nresult = df.assign(var2=df.var2.str.split(',').explode())\n\nresult = result.drop('var2', axis=1).reset_index().rename(columns={'index': 'id'})\n\nresult = result.set_index(['id', 'var1'])\n\n",
        "\n\n# create example dataframe\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\nddf = dd.from_pandas(df, npartitions=2)\n\n# split var2 column into multiple rows using str.split\nsplit_df = ddf['var2'].str.split(',', expand=True).stack().reset_index(level=1, drop=True).to_frame('var2')\n\n# concatenate var1 and var2 columns back into a single column\nresult = dd.concat([ddf['var1'].to_frame('var1'), split_df], axis=1)\n\n# output result\n",
        "\n\ndf = dd.read_csv('file.csv')\n\n# split the string in var2 column into multiple rows\ndf['var2'] = df['var2'].str.split('-')\n\n# repeat each row in var2 column to match the length of var1 column\ndf = df.explode('var2')\n\n# concatenate var1 and var2 columns to form the new dataframe\nresult = dd.concat([df['var1'], df['var2']], axis=1)\n\n# rename the columns\nresult.columns = ['var1', 'var2']\n\n# output the result\nresult.compute()\n",
        "\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n\nresult = df\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n",
        "\nresult['fips'] = df['row'].str[:2]\nresult['row'] = df['row'].str[3:]\n",
        "\nresult['fips'] = df['row'].str[:2]\nresult['row'] = df['row'].str[3:]\n",
        "\n\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALAB AMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n\n# Split the row column into three columns using df.row.str.split()\ndf[['fips', 'medi', 'row']] = df.row.str.split(expand=True)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\n# [Missing]\n\nresult = df.apply(lambda x: x.where(x != 0).mean(), axis=1)\nresult = df.apply(lambda x: x.where(x != 0).expanding().mean(), axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\n# Calculate the cumulative average for each row from end to head\nresult = df.apply(lambda x: x[x.notnull()].cumsum().divide(x.notnull().sum()), axis=1)\n\n",
        "\n\nexample_df = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\ndef f(df=example_df):\n    result = df.apply(lambda x: x.where(x != 0).mean(), axis=1)\n    return result\n",
        "\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\n# Calculate cumulative average for each row from end to head\nresult = df.apply(lambda x: x[x.notnull()].expanding().mean(), axis=1)\n\n# Fill NaN values with 0\nresult = result.fillna(0)\n\n",
        "\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07'],\n                   'Close': [1460, 1470, 1480, 1450]})\n\ndf['Label'] = 0\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 1).astype(int)\ndf.loc[0, 'Label'] = 1\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\n\n# [Missing]\n\nresult = df.assign(label=df['Close'].diff().apply(lambda x: 1 if x > 0 else 0 if x == 0 else -1))\nresult.iloc[0, result.columns.get_loc('label')] = 1\n",
        "\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\n\n# [Missing]\n\nresult = pd.DataFrame({'DateTime': df['DateTime'].dt.strftime('%d-%b-%Y'),\n                       'Close': df['Close'].diff(),\n                       'label': df['Close'].diff().apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))})\nresult['DateTime'] = pd.to_datetime(result['DateTime'])\nresult.iloc[0, 2] = 1\n\n",
        "\n# [Missing]\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\n",
        "\nfrom datetime import datetime\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\n# [Missing]\nfor i in range(len(df)-1):\n    df.loc[i+1, 'Duration'] = (datetime.strptime(df.loc[i+1, 'departure_time'], '%Y-%m-%d %H:%M:%S') - datetime.strptime(df.loc[i, 'arrival_time'], '%Y-%m-%d %H:%M:%S')).total_seconds()\n\nresult = df\n",
        "\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\n",
        "\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'one').sum())\n\n",
        "\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'two').sum())\n\n",
        "\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'gee', 'two', 'three', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'].str.endswith('e')].shape[0])\n\n",
        "\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\n# Get the min and max dates from the dataframe's major axis\nmin_result = df.index.min()\nmax_result = df.index.max()\n\n",
        "\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\n# Solution\n\nmode_result = df.mode(axis=0)\nmedian_result = df.median(axis=0)\n\n",
        "\n\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\n\nresult = df[df['closing_price'].between(99, 101)]\n\n",
        "\n\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\n\nresult = df[~(99 <= df['closing_price'] <= 101)]\n\n",
        "\ndf1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\ndf1 = df1.merge(df[[\"item\", \"otherstuff\"]], on=\"item\")\nresult = df1\n",
        "\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\n\n# Solution:\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n\nresult = df\n",
        "\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\n\n# Solution\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n\nresult = df\n",
        "\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\nexample_df = pd.DataFrame(data={'SOURCE_NAME': strs})\n\ndef f(df=example_df):\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n    return df['SOURCE_NAME']\n\nresult = f()\n",
        "\ndf['Column_x'].fillna(df['Column_x'].quantile(0.25), inplace=True)\ndf['Column_x'].fillna(df['Column_x'].quantile(0.75), inplace=True)\n",
        "\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].median(), inplace= True)\n\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df['Column_x'].median()\n\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df['Column_x'].mean()\n\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df['Column_x'].mode()[0]\n\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df['Column_x'].fillna(0, inplace= True)\n\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df['Column_x'].fillna(0.5, inplace= True)\n\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = df['Column_x'].fillna(1, inplace= True)\n",
        "\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,1,1,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# Fill all NaN values with 0\ndf['Column_x'].fillna(0, inplace=True)\n\n# Count the number of 0 and 1 values in the column\ncount_0 = df['Column_x'].value_counts().get(0, 0)\ncount_1 = df['Column_x'].value_counts().get(1, 0)\n\n# Fill remaining NaN values with 1 or 0 based on the count\nif count_0 > count_1:\n    df['Column_x'].fillna(1, inplace=True)\nelse:\n    df['Column_x'].fillna(0, inplace=True)\n\nresult = df\n",
        "\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\n\n# [Missing]\n\nresult = pd.DataFrame(list(zip(a.values.tolist(), b.values.tolist())), columns=['one', 'two'])\n\n",
        "\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nc = pd.DataFrame(np.array([[9, 10],[11, 12]]), columns=['one', 'two'])\n\n# Solution\n\na_b_c = pd.concat([a, b, c], axis=1)\na_b_c = a_b_c.values.tolist()\nresult = pd.DataFrame(a_b_c, columns=['one', 'two', 'three'])\n\n",
        "\na_b = pd.DataFrame([[(1, 5), (2, 6)],[(3, 7), (4, 8)],[(np.nan,9),(np.nan,10)]], columns=['one', 'two'])\na_b = a_b.apply(lambda x: x.fillna(value=np.nan) if len(x) < len(b) else x, axis=1)\nresult = pd.concat([a, b], axis=1).apply(lambda x: tuple(x), axis=1).apply(pd.Series)\nresult = pd.concat([result, a_b], axis=1)\n",
        "\n\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\ngroups = df.groupby([pd.cut(df.views, bins), 'username'])\nresult = groups.size().unstack().fillna(0)\n\n",
        "\n\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\n# Solution\n\ngroups = df.groupby(pd.cut(df.views, bins))\nresult = groups.username.count()\n\n",
        "\n\ndf = pd.DataFrame({'username': ['tom', 'tom', 'tom', 'tom', 'jack', 'jack', 'jack', 'jack'],\n                   'post_id': [10, 8, 7, 6, 5, 4, 3, 2],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\n# Create a new column 'bin' based on the views column\ndf['bin'] = pd.cut(df.views, bins)\n\n# Group the data by bin and username and count the number of views\nresult = df.groupby(['bin', 'username']).views.count().unstack()\n\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\nresult = pd.DataFrame({'text': ['abc, def, ghi, jkl']})\n\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\nresult = pd.DataFrame({'text': ['-'.join(df['text'].tolist())]})\n\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\n# Merge the rows into a single row with a single string\nresult = df.agg(lambda x: ', '.join(x), axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\nresult = df['text'].str.cat(sep=', ')\n\n",
        "\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\n# Merge the rows into a single row\nresult = pd.DataFrame({'text': [df['text'].str.cat(sep='-')]})\n\n",
        "\nresult = pd.concat([df1, df2], axis=0, sort=False)\nresult = result.fillna({'city': 'sh', 'district': 'hp'})\n",
        "\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n# Concatenate the dfs based on id\nresult = pd.concat([df1, df2], axis=0)\n\n# Fill missing city and district values with NaN\nresult['city'].fillna('NaN', inplace=True)\nresult['district'].fillna('NaN', inplace=True)\n\n# Sort the rows by id and date\nresult.sort_values(['id', 'date'], inplace=True)\n\n# Convert date to desired format\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\n\n",
        "\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n# Concatenate the dfs based on id\nresult = pd.concat([df1, df2], axis=0)\n\n# Fill missing values with NaN\nresult = result.fillna(value=pd.np.nan)\n\n# Sort the rows by id and date\nresult = result.sort_values(by=['id', 'date'])\n\n# Fill missing city and district values with values from df1\nresult['city'] = result['city'].fillna(result['city'].mode()[0])\nresult['district'] = result['district'].fillna(result['district'].mode()[0])\n\n# Fill missing values with NaN\nresult = result.fillna(value=pd.np.nan)\n\n# Print the result\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_c', '_d'))\nresult['B'] = result['B_d'].fillna(result['B_c'])\nresult = result.drop(columns=['B_c', 'B_d'])\n",
        "\nresult = pd.merge(C, D, how='left', on='A')\nresult['B'] = result['B_y']\nresult = result.drop(columns=['B_y'])\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_c', '_d'))\nresult['dulplicated'] = result.apply(lambda x: True if x['B_c'] != x['B_d'] else False, axis=1)\nresult = result[['A', 'B_c', 'B_d', 'dulplicated']]\nresult.columns = ['A', 'B', 'B_d', 'dulplicated']\nresult = result.sort_values(by=['A'])\nresult = result.reset_index(drop=True)\n",
        "\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nresult = df.groupby('user').agg(lambda x: x.tolist())\n",
        "\n\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nresult = df.groupby('user')[['time', 'amount']].apply(list).apply(lambda x: sorted(x, key=lambda y: y[0]))\n",
        "\n\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nresult = df.groupby('user').agg(lambda x: x.tolist())\n\nresult = result.reset_index()\nresult['amount-time-tuple'] = result.apply(lambda x: sorted(zip(x['amount'], x['time']), reverse=True), axis=1)\nresult = result.drop(['amount', 'time'], axis=1)\nresult = result.rename(columns={'amount-time-tuple': 'transactions'})\n\n",
        "\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\n# Solution\n\ndf_concatenated = pd.DataFrame(columns=range(4), index=series.index)\nfor i, arr in series.iteritems():\n    df_concatenated.loc[i] = arr\n\nresult = df_concatenated\n",
        "\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\ndf = pd.DataFrame(series.tolist(), index=series.index)\n\nresult = df\n",
        "\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\nresult = [col for col in df.columns if s in col and 'spike' not in col]\n\n",
        "\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\nresult = df.filter(like=s, axis=1).columns\n\n",
        "\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\n# Find all columns that contain the string 'spike'\ncols = [col for col in df.columns if re.search(s, col)]\n\n# Rename the columns with a number suffix\nnew_cols = []\nfor i, col in enumerate(cols):\n    new_col = col.replace(s, f'spike{i+1}')\n    new_cols.append(new_col)\ndf.columns = new_cols\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\nresult = pd.DataFrame(columns=['code_0', 'code_1', 'code_2'])\n\nfor i, row in df.iterrows():\n    codes = row['codes']\n    if len(codes) == 1:\n        result.loc[i] = [codes[0], float('nan'), float('nan')]\n    elif len(codes) == 2:\n        result.loc[i] = [codes[0], codes[1], float('nan')]\n    elif len(codes) == 3:\n        result.loc[i] = [codes[0], codes[1], codes[2]]\n    else:\n        result.loc[i] = [codes[0], codes[1], codes[2]]\n\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\n# Solution\n\nresult = df['codes'].apply(pd.Series).fillna(value=pd.np.nan)\n\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99234, 99233, 99233]]})\n\n# Splitting the lists into columns\nresult = pd.DataFrame(df['codes'].apply(pd.Series).stack(), columns=['code_1', 'code_2', 'code_3']).reset_index(level=1, drop=True)\n\n# Filling missing values with NaNs\nresult = result.fillna(value=pd.np.nan)\n\n",
        "\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nids = df.loc[0:1, 'col1'].values.tolist()\nresult = []\nfor i in ids:\n    result += ast.literal_eval(i)\n\n",
        "\nfrom ast import literal_eval\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3],[4,5]]))\n\n# [Missing]\n# Convert each list value in the column to string and concatenate them into one string\nresult = ','.join(str(reverse(df.loc[0:index, 'col1'].values.tolist())) for index in range(len(df)))\n\n",
        "\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\n# Concatenate lists in column into one string\nresult = df['col1'].apply(lambda x: ','.join(map(str, x)))\n\n",
        "\nresult = df.groupby(pd.Grouper(key='Time', freq='2min')).mean()\n",
        "\npan = df.set_index('Time')\npan = pan.resample('3T').mean()\npan = pan.reset_index()\npan['Time'] = pan['Time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n\nresult = pan.groupby('Time')['Value'].sum().reset_index()\nresult['Time'] = pd.to_datetime(result['Time'])\nresult = result.set_index('Time')\nresult = result.resample('3T').mean()\nresult = result.reset_index()\nresult['Time'] = result['Time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\n# [Missing]\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\n# Convert 'TIME' column to datetime format\ndf['TIME'] = pd.to_datetime(df['TIME'])\n\n# Assign a rank to each time for each ID\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n\n# Format 'TIME' column to display date and time in desired format\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n                    'b': [1,2,3,1,2,3,1,2,3],\n                    'c': range(9)}).set_index(['a', 'b'])\nfilt = pd.Series({1:True, 2:False, 3:True})\n\n# Solution\n\nresult = df.loc[filt.index[filt], :]\n\n",
        "\nresult = df[filt]\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\nresult = df.iloc[0].ne(df.iloc[8]).index[df.iloc[0].ne(df.iloc[8])]\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\nresult = df.iloc[0].eq(df.iloc[8]).index[df.iloc[0].eq(df.iloc[8])].tolist()\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\nresult = df.iloc[0].equals(df.iloc[8])\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\nresult = []\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if not np.isnan(df.iloc[i,j]) and not np.isnan(df.iloc[8,j]):\n            if df.iloc[i,j] != df.iloc[8,j]:\n                result.append((df.iloc[i,j], df.iloc[8,j]))\n\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n",
        "\n\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n\n# Concatenate the rows using the concat() function\nresult = pd.concat(df, axis=0)\n\n# Print the result\n",
        "\nresult = df.iloc[[0,1,2]]\n",
        "\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, .03), (.21, .18),(pd.NA, .18)],\n                  columns=['dogs', 'cats'])\n\n# Solution:\ndf['dogs'] = df['dogs'].round(2)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\n\n# Solution\n\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n\nresult = df\n",
        "\nresult = df[list_of_my_columns].sum(axis=1)\n",
        "\nresult = df[list_of_my_columns].mean(axis=1)\n",
        "\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\n# Solution\nresult = df[list_of_my_columns].mean(axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_index(level='time')\n\n",
        "\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_values(by=['VIM', 'time'], ascending=[True, True])\n\n",
        "\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\nsp = sp[(sp.index < hd1_from) | (sp.index > hd1_till)]\n",
        "\n# Solution code with the missing line(s) of code\n\n\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\n\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\n# Delete particular days '2020-02-17' and '2020-02-18' from the 'Date' column\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\ndf = df[(df.index < hd1_from) | (df.index > hd1_till)]\n\n# Add a new column 'Day of the week'\ndf['Day of the week'] = df.index.day_name()\n\n# Print the result\n",
        "\nresult = corr[(corr > 0.3) & (corr != 1)]\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\n\nresult = corr.where(lambda x: x > 0.3)\n\n",
        "\n\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\n\n# Solution\ndf.columns[-1] = 'Test'\n\n",
        "\n\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\n\ndf.columns[0] = 'Test'\n\n",
        "\n\ndf = pd.DataFrame({'bit1': [0, 1, 1],\n                   'bit2': [0, 1, 0],\n                   'bit3': [1, 0, 1],\n                   'bit4': [1, 0, 1],\n                   'bit5': [0, 1, 1]})\n\n# Create a new dataframe with the frequent values and their count\nresult = df.groupby(df.columns.tolist()).size().reset_index(name='freq_count')\n\n# Add a new column with the most frequent value in each row\nresult['frequent'] = df.apply(lambda x: x.value_counts().index[0], axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4]})\n\nfrequent_values = {}\n\nfor i in range(len(df)):\n    row = df.iloc[i]\n    for j in range(len(row)):\n        value = row[j]\n        if value not in frequent_values:\n            frequent_values[value] = 1\n        else:\n            frequent_values[value] += 1\n\ndf['frequent'] = [max(frequent_values[value] for value in row) for row in df.values]\ndf['freq_count'] = [frequent_values[value] for row in df.values for value in row]\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4],\n                   'bit6': [3, 0, 5]})\n\n# Find the frequent values in each row\nresult = df.groupby(df.columns.tolist()[:-2]).size().reset_index(name='freq_count')\nresult['frequent'] = result[0].apply(lambda x: list(df.loc[df.groupby(df.columns.tolist()[:-2]).groups[x]].values[0][:-2]))\n\n",
        "\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\ngroupedFrame = df.groupby([\"id1\",\"id2\"])\naggrFrame = groupedFrame.agg({\"foo\":np.mean, \"bar\":np.mean})\n\nresult = pd.concat([groupedFrame[\"foo\"].mean(), groupedFrame[\"bar\"].mean()], axis=1)\nresult.columns = [\"foo\", \"bar\"]\n\n",
        "\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\n# [Missing]\n\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nresult = result.fillna(0)\nresult = result.astype({'foo': 'float64', 'bar': 'float64'})\n\n",
        "\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\nresult = pd.merge(df_a, df_b, on='EntityNum', suffixes=('_foo', '_bar'))\nresult = result.drop(columns=['b_col_bar'])\n\n",
        "\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\n# Merge the two dataframes on the 'EntityNum' column\nresult = pd.merge(df_a, df_b, on='EntityNum', how='inner')\n\n# Drop the 'a_col' column from the result dataframe\nresult = result.drop(columns=['a_col'])\n\n"
    ],
    "Numpy": [
        "\na = np.array([[1,2],[3,4]])\n\n# To get the dimensions of an array, we can use the shape attribute.\nresult = a.shape\n\n",
        "\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\n\n# Remove nan values from x\nx = x[~np.isnan(x)]\n\n",
        "\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\n\n# Replace nan values with np.inf\nx = np.nan_to_num(x, nan=np.inf)\n\n",
        "\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\n\n# Remove nan values\nx = x[~np.isnan(x)]\n\n# Convert to list of lists\nresult = x.tolist()\n\n",
        "\na = np.array([1, 0, 3])\n\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a] = 1\n\n",
        "\na = np.array([1, 0, 3])\n\nb = np.zeros((len(a), a.max()+1))\nb[np.arange(len(a)), a] = 1\n\n",
        "\na = np.array([-1, 0, 3])\n\n# One-hot encoding of a\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a] = 1\n\n",
        "\n\na = np.array([1.5, -0.4, 1.3])\n\nnum_classes = np.max(a) + 1\nb = np.eye(num_classes)[a.astype(int)]\n\n",
        "\n\na = np.array([[1,0,3], [2,4,1]])\n\n# create a 2D identity matrix with the same number of rows as a and the same number of columns as the maximum value in a plus one\nmax_val = np.max(a)\nb = np.eye(a.shape[0], max_val + 1, dtype=int)[a]\n\n",
        "\n\na = np.array([1,2,3,4,5])\np = 25\n\nresult = np.percentile(a, p)\n\n",
        "\nA = np.array([1,2,3,4,5,6])\nncol = 2\n\n# Filling out the missing part of the code\nB = A.reshape(-1, ncol)\n\n",
        "\nA = np.array([1,2,3,4,5,6])\nnrow = 3\n\n# Solution\nB = A.reshape(nrow,-1)\n\n",
        "\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\n\n# Reshape A into a 2D array with ncol columns\nB = A.reshape(-1, ncol)\n\n",
        "\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\n\n# Reshape A into a 2D array with ncol columns\nnrows = int(np.ceil(len(A) / ncol))\nB = A[:nrows*ncol].reshape(nrows, ncol)\n\n",
        "\na = np.array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\nshift = 3\n\n# Fill in the missing line(s) of code here\nresult = np.roll(a, shift)\n\n",
        "\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = 3\n\n# Fill in the missing line(s) of code here\nresult = np.roll(a, shift, axis=1)\n\n",
        "\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = [-2, 3]\n\n# Fill in the missing line(s) of code here\nresult = np.roll(a, shift, axis=1)\n\n",
        "\n\nnp.random.seed(42)\nr = np.random.randint(3, size=(100, 2000)) - 1\n\n# [Missing]\n\n",
        "\na = np.array([[10,50,30],[60,20,40]])\n\n# Get the raveled index of the largest value in a\nresult = np.argmax(a)\n\n",
        "\na = np.array([[10,50,30],[60,20,40]])\n\n# To get the raveled index of the smallest value in a multi-dimensional NumPy array, we can use the argmin() method.\n# The argmin() method returns the indices of the minimum values along the specified axis.\n# If axis is not specified, it returns the indices of the minimum values in the flattened array.\nresult = a.argmin()\n\n",
        "\na = np.array([[10,50,30],[60,20,40]])\n\nresult = np.unravel_index(a.argmax(), a.shape, order='F')\n\n",
        "\na = np.array([[10,50,30],[60,20,40]])\n\n# Get the indices of the largest value in a\nindices = np.unravel_index(a.argmax(), a.shape)\n\n# Get the unraveled index of the largest value in a\nresult = indices\n\n",
        "\nexample_a = np.array([[10,50,30],[60,20,40]])\ndef f(a = example_a):\n    result = np.unravel_index(np.argmax(a), a.shape)\n    return result\n",
        "\na = np.array([[10,50,30],[60,20,40]])\n\n# Get the indices of the second largest value in a\nresult = np.unravel_index(np.argsort(a.ravel())[-2], a.shape)\n\n",
        "\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\n\n# [Missing]\n\n",
        "\n\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\n\n# Find the indices of the rows that contain NaN\nnan_indices = np.where(np.isnan(a))\n\n# Delete the rows that contain NaN\na = np.delete(a, nan_indices, axis=0)\n\n",
        "\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\nresult = np.array(a)\n\n",
        "\n\na = np.array([[10, 20, 30, 40, 50],\n       [ 6,  7,  8,  9, 10]])\n\npermutation = [0, 4, 1, 3, 2]\n\n# Apply the permutation to the columns of the array\na = a[:, permutation]\n\n",
        "\na = np.array([[[10, 20],\n        [30, 40]],\n       [[6,  7],\n        [8,  9]],\n\t[[10, 11],\n\t [12, 13]]])\npermutation = [1, 0, 2]\n\n# Apply the permutation on the matrices\nresult = np.transpose(a, axes=permutation)\n\n",
        "\na = np.array([[1, 2], [3, 0]])\n\n# Find the index of the minimum element\nresult = np.unravel_index(a.argmin(), a.shape)\n\n",
        "\na = np.array([[1, 2], [3, 0]])\n\n# Find the indices of the maximum element in the array\nmax_index = np.argmax(a)\n\n# Extract the row and column indices from the max_index\nrow_index = max_index // a.shape[1]\ncol_index = max_index % a.shape[1]\n\n# Print the result\n",
        "\na = np.array([[1, 0], [0, 2]])\n\n# Find the index of the minimum element in each row and column\nrow_index = np.argmin(a, axis=1)\ncol_index = np.argmin(a, axis=0)\n\n# Combine the row and column indices into a single array\nresult = np.stack((row_index, col_index), axis=1)\n\n",
        "\ndegree = 90\n\n# [Missing]\n\nresult = np.degrees(np.arcsin(np.sin(np.radians(degree))))\n\n",
        "\ndegree = 90\n\n# [Missing]\nresult = np.cos(np.deg2rad(degree))\n\n",
        "\n\nnumber = np.random.randint(0, 360)\n\nif np.sin(np.deg2rad(number)) > 0.5:\n    result = 0\nelse:\n    result = 1\n\n",
        "\n\nvalue = 1.0\n\nresult = np.degrees(np.arctan(value))\n\n",
        "\n\nA = np.array([1,2,3,4,5])\nlength = 8\n\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0))\n\n",
        "\n\nA = np.array([1,2,3,4,5])\nlength = 8\n\nresult = np.pad(A, (0, length - len(A)), mode='constant', constant_values=0)\n\n",
        "\na = np.arange(4).reshape(2, 2)\npower = 5\n\n# Solution:\nresult = a ** power\n\n",
        "\nresult = np.power(a, 2)\n",
        "\n\nnumerator = 98\ndenominator = 42\n\nresult = fractions.Fraction(numerator, denominator).limit_denominator()\n\n",
        "\n\ndef f(numerator = 98, denominator = 42):\n    result = np.gcd(numerator, denominator)\n    return (numerator // result, denominator // result)\n",
        "\n\nnumerator = 98\ndenominator = 42\n\nresult = np.gcd(numerator, denominator)\nif result == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // result, denominator // result)\n\n",
        "\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\n\nresult = (a + b + c) / 3\n\n",
        "\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\n\nresult = np.maximum(np.maximum(a, b), c)\n\n",
        "\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n\n",
        "\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\n\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\ndiagonal = np.diag_indices_from(a)\nresult = a[diagonal[0], diagonal[1]+1]\n\n",
        "\nX = np.random.randint(2, 10, (5, 6))\n\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n\n",
        "\nX = np.random.randint(2, 10, (5, 6))\n\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n\n",
        "\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i][j])\n    return result\n",
        "\nX = np.random.randint(2, 10, (5, 6))\n\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\n\n",
        "\nmystr = \"100110\"\n\nresult = np.fromstring(mystr, dtype=int, sep='')\n\n",
        "\na = np.random.rand(8, 5)\ncol = 2\nmultiply_number = 5.2\n\n# multiply the col-th column of a by multiply_number\na[:, col] = a[:, col] * multiply_number\n\n# calculate the cumulative sum of the numbers in that column\nresult = np.cumsum(a[:, col])\n\n",
        "\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\n\n# multiply the row-th row of the array by the multiply_number\na[row] *= multiply_number\n\n# calculate the cumulative sum of the numbers in the row\nresult = np.cumsum(a[row])\n\n",
        "\na = np.random.rand(8, 5)\nrow = 2\ndivide_number = 5.2\n\n# divide the row-th row of the array by the divide_number\na[row] /= divide_number\n\n# calculate the multiplication of the numbers in the row\nresult = np.prod(a[row])\n\n",
        "\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\n\n# Find the null space of the matrix\nnull_space = np.linalg.null_space(a)\n\n# Convert the null space to a list of vectors\nvectors = [list(v) for v in null_space]\n\n# Remove the zero vectors\nvectors = [v for v in vectors if v != [0]*len(v)]\n\n# Sort the vectors by their length in descending order\nvectors.sort(key=lambda v: len(v), reverse=True)\n\n# Select the first vector as the first element of the result\nresult = [vectors[0]]\n\n# Add the remaining vectors to the result\nfor v in vectors[1:]:\n    # Check if the vector is already in the result\n    if v not in result:\n        # Check if the vector is linearly independent from the vectors in the result\n        if all(np.dot(v, w) == 0 for w in result):\n            # Add the vector to the result\n            result.append(v)\n\n",
        "\na = np.random.rand(np.random.randint(5, 10), np.random.randint(6, 10))\n\nresult = a.shape[0]\n\n",
        "\n\n# Generate sample 1 and sample 2\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# Calculate mean and standard deviation of sample 1 and sample 2\nmean_a = np.mean(a)\nmean_b = np.mean(b)\nstd_a = np.std(a, ddof=1)\nstd_b = np.std(b, ddof=1)\n\n# Calculate t-statistic and p-value\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n\n# Print p-value\n",
        "\n\n# Generate sample data\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# Remove NaN values\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n\n# Calculate weighted two-tailed t-test\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n\n",
        "\n\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\n\n# Calculate the weighted mean\nwmean = (anobs*amean + bnobs*bmean)/(anobs + bnobs)\n\n# Calculate the weighted variance\nwvar = ((anobs-1)*avar + (bnobs-1)*bvar + (anobs+bnobs-2)*wmean**2)/(anobs+bnobs-2)\n\n# Calculate the weighted standard deviation\nwstd = np.sqrt(wvar)\n\n# Calculate the t-value\nt_value = (amean - bmean) / (wstd * np.sqrt(1/anobs + 1/bnobs))\n\n# Calculate the p-value\np_value = 2*(1 - scipy.stats.t.cdf(np.abs(t_value), anobs + bnobs - 2))\n\n",
        "\nA=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\n# Remove elements from A that are in B\noutput = []\nfor i in range(len(A)):\n    if not any(np.array_equal(A[i], b) for b in B):\n        output.append(A[i])\n\n",
        "\nA=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\n# elements in A first, elements in B then. in original order.\n# output = array([[1,1,2], [1,1,3], [0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0]])\n\n# create sets from A and B\nsetA = set(tuple(x) for x in A)\nsetB = set(tuple(x) for x in B)\n\n# find symmetric difference between A and B\noutput = np.array(list(setA.symmetric_difference(setB)))\n\n",
        "\n\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Sort b along axis 0 using a as the sorting key\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n\n",
        "\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Sort b along the first axis of a\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n\n",
        "\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Sort b along axis 0 in decreasing order based on values in a\nsort_indices = np.argsort(a, axis=0)[::-1]\nc = b[sort_indices]\n\n",
        "\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Sort b by the values of a\nresult = b[np.argsort(np.sum(a, axis=(1, 2)))]\n\n",
        "\na = np.arange(12).reshape(3, 4)\n\n# Delete the 3rd column\na = np.delete(a, 2, axis=1)\n\n",
        "\na = np.arange(12).reshape(3, 4)\n\n# Delete the 3rd row\na = np.delete(a, 2, axis=0)\n\n",
        "\na = np.arange(12).reshape(3, 4)\n\n# Delete the 1st and 3rd column\na = np.delete(a, [0, 2], axis=1)\n\n",
        "\n\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\ndel_col = [1, 2, 4, 5]\n\n# create a boolean mask to ignore out-of-bound indices\nmask = np.ones(arr.shape[1], dtype=bool)\nmask[del_col] = False\n\n# create a new array with the selected columns\nresult = arr[:, mask]\n\n",
        "\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\n\na_l = a.tolist()\na_l.insert(pos, element)\na = np.asarray(a_l)\n\n",
        "\n\na = np.array([[1,2],[3,4]])\n\npos = 1\nelement = [3,5]\n\n# Insert the new row at the specified position\na = np.vstack((a[:pos], element, a[pos:]))\n\n",
        "\nexample_a = np.asarray([1,2,3,4])\ndef f(a = example_a, pos=2, element = 66):\n    a_l = a.tolist()\n    a_l.insert(pos, element)\n    a = np.asarray(a_l)\n    return a\n",
        "\na = np.array([[1,2],[3,4]])\npos = [1, 2]\nelement = np.array([[3, 5], [6, 6]])\n\nfor i in range(len(pos)):\n    a = np.insert(a, pos[i], element[i], axis=0)\n\n",
        "\npairs = [(2, 3), (3, 4), (4, 5)]\narray_of_arrays = np.array([np.arange(a*b).reshape(a,b) for (a, b) in pairs])\n\n# Solution\n\nresult = np.array([np.copy(arr) for arr in array_of_arrays])\n\n",
        "\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\n\nresult = np.all(np.all(a == a[0], axis=1))\n\n",
        "\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\n\nresult = np.all(a == a[:,0].reshape(-1, 1), axis=0)\n\n",
        "\nexample_a = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\ndef f(a = example_a):\n    result = np.all([np.array_equal(a[0], a[i]) for i in range(1,len(a))])\n    return result\n",
        "\nfrom scipy.integrate import trapz, dblquad\n\n# Define the function to integrate\ndef f(x, y):\n    return np.cos(x)**4 + np.sin(y)**2\n\n# Define the sample points\nx = np.linspace(0, np.pi, 100)\ny = np.linspace(0, np.pi, 100)\n\n# Compute the trapezoidal rule approximation of the integral\nresult = trapz(trapz(f(x, y), x), y)\n\n# Multiply by the area of the rectangle to get the integral over the whole rectangle\nresult *= (x[-1] - x[0]) * (y[-1] - y[0])\n\n",
        "\nfrom scipy.integrate import trapz, dblquad\n\nexample_x = np.linspace(0, 1, 20)\nexample_y = np.linspace(0, 1, 30)\n\ndef f(x, y):\n    return np.cos(x)**4 + np.sin(y)**2\n\n# Approximate the integral using trapezoidal rule\nintegral_approx = trapz(trapz(f(example_x, y), y), x)\n\n# Multiply the approximation by the function values at the sample points\nintegral_exact = np.sum(f(example_x, example_y) * (example_x[1] - example_x[0]) * (example_y[1] - example_y[0]))\n\n",
        "\n\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n\n# sort the grades in ascending order\ngrades_sorted = np.sort(grades)\n\n# calculate the ecdf of the sorted grades\necdf_sorted = ecdf(grades_sorted)\n\n# interpolate the ecdf to get the ecdf of the original grades\nresult = np.interp(grades, grades_sorted, ecdf_sorted)\n\n",
        "\n\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\neval = np.array([88, 87, 62])\n\n# apply ECDF function to eval array\nresult = ecdf(grades)(eval)\n\n",
        "\n\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\nthreshold = 0.5\n\n# find the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\necdf_grades = ecdf(grades)\n\nlow = 0\nhigh = len(grades)\nwhile ecdf_grades[high-1] >= threshold:\n    high -= 1\n\nwhile ecdf_grades[low] < threshold:\n    low += 1\n\n",
        "\n\none_ratio = 0.9\nsize = 1000\n\n# Generate random array of size N with some ratio between 0 and 1\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n\n",
        "\n\na = torch.ones(5)\n\na_np = a.numpy()\n\n",
        "\n\na = np.ones(5)\n\n# Convert numpy array to pytorch tensor\na_pt = torch.from_numpy(a)\n\n",
        "\n\na = tf.ones([2,3,4])\n\na_np = a.numpy()\n\n",
        "\n\na = np.ones([2,3,4])\n\na_tf = tf.constant(a)\n\n",
        "\na = np.array([4, 1, 0, 8, 5, 2])\n\nresult = np.argsort(a)[::-1]\n\n",
        "\na = np.array([4, 1, 0, 8, 5, 2])\n\n# Solution\nresult = np.argsort(a)\n\n",
        "\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\n\n# Sort the array in descending order\na_sorted = np.sort(a)[::-1]\n\n# Get the indexes of the N biggest elements in decreasing order\nresult = np.argsort(a)[-N:][::-1]\n\n",
        "\nA = np.arange(16).reshape(4, 4)\nn = 5\n\nresult = np.linalg.matrix_power(A, n)\n\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        patch = a[i:i+2, j:j+2]\n        if not (patch == a[i,j]).all():\n            result.append(patch)\n\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(len(a)-1):\n    for j in range(len(a[0])-1):\n        result.append([[a[i][j], a[i][j+1]], [a[i+1][j], a[i+1][j+1]]])\n\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(0, len(a), 2):\n    for j in range(0, len(a[0]), 2):\n        patch = []\n        for k in range(i, i+2):\n            for l in range(j, j+2):\n                patch.append(a[k][l])\n        result.append(patch)\n\n",
        "\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\n# Solution\n\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        if patch.shape == (patch_size, patch_size):\n            result.append(patch)\n\n",
        "\na = np.array([[[ 0,  1,  2],\n        [ 6,  7,  8]],    \n       [[ 3,  4,  5],\n        [ 9, 10, 11]], \n       [[12, 13, 14],\n        [18, 19, 20]],    \n       [[15, 16, 17],\n        [21, 22, 23]]])\nh = 4\nw = 6\n\n# Solution:\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i][j] = a[i//2][j//3][i%2*3+j%3]\n\n",
        "\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\n# Solution\n\nresult = np.array([a[i:i+patch_size, j:j+patch_size] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])\n\n",
        "\nresult = a[:, low-1:high]\n",
        "\nresult = a[low:high+1]\n",
        "\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 10\n\n# Solution\nresult = a[:, low-1:high]\n\n",
        "\n\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\n\n# Convert string to array\na = np.fromstring(string, dtype=float, sep=' ')\n\n# Reshape array to 2D\na = a.reshape(2,2)\n\n",
        "\n\nmin = 1\nmax = np.e\nn = 10000\n\n# Generate log-uniform samples using numpy.random.uniform\nresult = np.random.uniform(np.log(min), np.log(max), n)\n\n# Convert log-uniform samples to linear scale\nresult = np.exp(result)\n\n",
        "\n\nmin = 0\nmax = 1\nn = 10000\n\n# Generate log-uniform distribution using numpy.random.uniform\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n\n",
        "\ndef f(min=1, max=np.e, n=10000):\n    result = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n    return result\n",
        "\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\n\nB = pd.Series(np.zeros(len(A)))\nB[0] = a*A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n\n",
        "\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nc = 4\n\nB = pd.Series(np.zeros(len(A)))\nB[0] = a*A[0]\nB[1] = a*A[1]+b*B[0]\nfor t in range(2,len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n\n",
        "\n\n# initialize an empty numpy array with shape (0,)\nresult = np.empty((0,))\n\n",
        "\n\n# Initialize an empty numpy array with shape (3,0)\nresult = np.empty((3,0))\n\n",
        "\nlinearInd = np.ravel_multi_index(index, dims, order='F')\n",
        "\n\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\n\nlinear_index = index[0] + index[1] * dims[1] + index[2] * np.prod(dims[1:])\n\n",
        "\n\nindex = ['x', 'y']\ncolumns = ['a','b','c']\n\nvalues = np.zeros((2,3), dtype=[('a', 'i4'), ('b', 'f4'), ('c', 'f4')])\n\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n\n",
        "\n\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,1,1,2,2,1])\n\n# Solution\nresult = np.bincount(accmap, weights=a, minlength=3)\n\n",
        "\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\n\nresult = np.zeros(3)\n\nfor i in range(len(a)):\n    if index[i] == 0:\n        result[0] = max(result[0], a[i])\n    elif index[i] == 1:\n        result[1] = max(result[1], a[i])\n    else:\n        result[2] = max(result[2], a[i])\n\n",
        "\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,-1,-1,2,2,1])\n\nresult = np.zeros(3)\nfor i in range(len(accmap)):\n    if accmap[i] >= 0:\n        result[accmap[i]] += a[i]\n    else:\n        result[accmap[i]+1] += a[i]\n\n",
        "\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\n\nresult = []\nfor i in range(len(index)):\n    if index[i] == -1:\n        result.append(float('inf'))\n    else:\n        result.append(a[index[i]])\n\nresult = np.array(result)\n\n",
        "\nx = [[2, 2, 2],\n     [2, 2, 2],\n     [2, 2, 2]]\ny = [[3, 3, 3],\n     [3, 3, 3],\n     [3, 3, 1]]\n\nz = np.zeros_like(x)\n\nfor i in range(len(x)):\n    for j in range(len(x[0])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n\n",
        "\n\nprobabilit = [0.333, 0.334, 0.333]\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\n\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n\n",
        "\na = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\n\n# pad the array with zeros\npadded_a = np.pad(a, ((0, 0), (0, 0)), 'constant', constant_values=0)\n\n# slice the padded array\nresult = padded_a[low_index:high_index, low_index:high_index]\n\n",
        "\nx = np.array([-2, -1.4, -1.1, 0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2])\n\nresult = np.delete(x, np.where(x < 0))\n\n",
        "\nx = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\n\nresult = np.delete(x, np.where(np.real(x) != 0))\n\n",
        "\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\n\n# calculate the number of bins\nnum_bins = len(data) // bin_size\n\n# create an empty list to hold the binned data\nbin_data = []\n\n# loop through the data and add each bin to the list\nfor i in range(num_bins):\n    bin_data.append(data[i*bin_size:(i+1)*bin_size])\n\n# calculate the mean of each bin and store in a list\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n\n",
        "\n\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\n\n# create a 2D array of the data with shape (n_bins, bin_size)\nn_bins = len(data) // bin_size\nbin_data = data[:n_bins*bin_size].reshape((n_bins, bin_size))\n\n# calculate the maximum of each bin\nbin_data_max = np.max(bin_data, axis=1)\n\n",
        "\n\ndata = np.array([[4, 2, 5, 6, 7],\n                 [5, 4, 3, 5, 7]])\nbin_size = 3\n\n# calculate the number of bins\nnum_bins = int(np.ceil(data.shape[1] / bin_size))\n\n# create an empty array to store the binned data\nbin_data = np.zeros((data.shape[0], num_bins, bin_size))\n\n# fill the bin_data array with the binned data\nfor i in range(num_bins):\n    start = i * bin_size\n    end = start + bin_size\n    bin_data[:, i, :] = data[:, start:end]\n\n# calculate the mean of each bin\nbin_data_mean = np.mean(bin_data, axis=2)\n\n",
        "\n\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\n\n# calculate the number of bins\nnum_bins = len(data) // bin_size\n\n# calculate the last bin size\nlast_bin_size = len(data) % bin_size\n\n# create an empty list to store the binned data\nbin_data = []\n\n# loop through the data and bin it into equal partitions\nfor i in range(num_bins):\n    start_index = (num_bins - i - 1) * bin_size\n    end_index = start_index + bin_size\n    bin_data.append(data[start_index:end_index])\n\n# add the last bin if it is not empty\nif last_bin_size > 0:\n    bin_data.append(data[-last_bin_size:])\n\n# calculate the mean of each bin\nbin_data_mean = [np.mean(bin_data[i]) for i in range(len(bin_data))]\n\n",
        "\n\ndata = np.array([[4, 2, 5, 6, 7],\n                 [5, 4, 3, 5, 7]])\nbin_size = 3\n\n# calculate the number of bins\nnum_bins = len(data[0]) // bin_size\n\n# create an empty array to store the binned data\nbin_data = np.zeros((len(data), num_bins))\n\n# loop through each row of the data and bin it\nfor i in range(len(data)):\n    for j in range(num_bins):\n        bin_data[i][j] = np.mean(data[i][-bin_size*(j+1):-bin_size*j])\n\n# calculate the mean of each bin\nbin_data_mean = np.mean(bin_data, axis=0)\n\n",
        "\n\ndata = np.array([[4, 2, 5, 6, 7],\n                 [5, 4, 3, 5, 7]])\nbin_size = 3\n\n# calculate the number of bins in each row\nnum_bins = (data.shape[1] + bin_size - 1) // bin_size\n\n# calculate the start and end indices of each bin\nbin_indices = np.arange(bin_size, data.shape[1] + bin_size, bin_size)\n\n# create an empty array to hold the binned data\nbin_data = np.empty((data.shape[0], num_bins), dtype=object)\n\n# iterate over each row of the data\nfor i in range(data.shape[0]):\n    # iterate over each bin of the row\n    for j in range(num_bins):\n        # calculate the start and end indices of the bin\n        start = bin_indices[j] - bin_size\n        end = bin_indices[j]\n        # if the bin is within the bounds of the row, calculate the mean\n        if start >= 0:\n            bin_data[i, j] = np.mean(data[i, start:end])\n        # otherwise, discard the first few elements of the row\n        else:\n            bin_data[i, j] = np.mean(data[i, :end])\n\n# calculate the mean of each bin\nbin_data_mean = np.mean(bin_data, axis=0)\n\n",
        "\n\ndef smoothclamp(x, x_min=0, x_max=1):\n    if x < x_min:\n        return x_min\n    elif x > x_max:\n        return x_max\n    else:\n        return 3*x**2 - 2*x**3\n",
        "\ndef smoothclamp(x, N=5, x_min=0, x_max=1):\n    # Clamp the input value between the minimum and maximum values\n    x = np.clip(x, x_min, x_max)\n    \n    # Calculate the smoothstep value\n    t = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    t = t**N\n    \n    # Return the result\n    return (3*t**2 - 2*t**3) * (x_max - x_min) + x_min\n",
        "\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\n\nresult = np.correlate(a, b, mode='same', method='fft')\n\n",
        "\n\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\n\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('colu'))\n\nresult = df.to_numpy().reshape(4, 15, 5)\n\n",
        "\nresult = df.values.reshape(15, 4, 5)\n",
        "\n\na = np.array([1, 2, 3, 4, 5])\nm = 8\n\n# Solution\nresult = np.array([np.unpackbits(np.uint8(num)) for num in a], dtype=np.uint8).reshape(-1, m)\n\n",
        "\na = np.array([1, 2, 3, 4, 5])\nm = 6\n\n# Convert each integer to binary numpy array of length m\nresult = np.unpackbits(np.array([np.uint8(num) for num in a], dtype=np.uint8).view(np.uint8))[:, -m:]\n\n",
        "\n\ndef convert_to_binary_array(num, m):\n    binary_array = np.unpackbits(np.uint8(num))\n    binary_array = binary_array[-m:]\n    return binary_array\n\na = np.array([1, 2, 3, 4, 5])\nm = 6\n\nbinary_array = np.apply_along_axis(convert_to_binary_array, axis=1, arr=a, m=m)\nresult = np.sum(binary_array, axis=0) % 2\n\n",
        "\n\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\n# Calculate the sample standard deviation\nstd = np.std(a, ddof=1)\n\n# Sort the array in ascending order\na_sorted = np.sort(a)\n\n# Find the index of the element that is 3 standard deviations away from the mean\nindex = np.where(a_sorted > 3*std)[0][0]\n\n# Find the start and end of the 3rd standard deviation interval\nstart = a_sorted[index] - 3*std\nend = a_sorted[index] + 3*std\n\n# Print the result as a tuple\nresult = (start, end)\n\n",
        "\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\n# Calculate the mean of the array\nmean = np.mean(a)\n\n# Calculate the variance of the array\nvariance = np.var(a)\n\n# Calculate the standard deviation of the array\nstd_dev = np.std(a)\n\n# Calculate the 2nd standard deviation interval\nstart = mean - 2*std_dev\nend = mean + 2*std_dev\n\n# Create a tuple containing the start and end of the 2nd standard deviation interval\nresult = (start, end)\n\n",
        "\n\ndef f(a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])):\n    # Calculate the mean of the array\n    mean = np.mean(a)\n    # Calculate the standard deviation of the array\n    std_dev = np.std(a)\n    # Calculate the 3rd standard deviation\n    third_std_dev = std_dev * 3\n    # Calculate the start and end of the 3rd standard deviation interval\n    start = mean - third_std_dev\n    end = mean + third_std_dev\n    # Return the start and end of the 3rd standard deviation interval as a tuple\n    return (start, end)\n\n# Test the function with the example array\n",
        "\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\n# Calculate the 2nd standard deviation for the array\nstd_dev = np.std(a)\n\n# Calculate the mean of the array\nmean = np.mean(a)\n\n# Calculate the 2nd standard deviation interval\nstd_dev_interval = (mean - 2*std_dev, mean + 2*std_dev)\n\n# Detect outliers of 2nd standard deviation interval from array x\nresult = np.logical_or(a < std_dev_interval[0], a > std_dev_interval[1])\n\n",
        "\n\nDataArray = np.array(data)\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\npercentile = 5\nprob = np.percentile(masked_data, percentile)\n\n",
        "\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = 0\nzero_cols = 0\n\n# Zero out rows and columns corresponding to the given index\na = np.delete(a, zero_rows, axis=0)\na = np.delete(a, zero_cols, axis=1)\n\n",
        "\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = [1, 3]\nzero_cols = [1, 2]\n\n# Create boolean mask array\nmask = np.ones_like(a, dtype=bool)\nmask[zero_rows, :] = False\nmask[:, zero_cols] = False\n\n# Zero out the corresponding rows and columns\na[mask] = 0\n\n",
        "\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\n\n# Zero out the second row and the first column\na[1, :] = 0\na[:, 0] = 0\n\n",
        "\na = np.array([[0, 1], [2, 1], [4, 8]])\n\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(len(a)), np.argmax(a, axis=1)] = True\n\n",
        "\na = np.array([[0, 1], [2, 1], [4, 8]])\n\nmask = np.min(a, axis=1) == a\n\n",
        "\n\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n\n# Calculate the correlation coefficient and p-value\ncorr, pval = np.corrcoef(post, distance)\n\n# Print the correlation coefficient and p-value\n",
        "\nX = np.random.randint(2, 10, (5, 6))\n\nresult = np.array([np.dot(X[:, i], X[:, i].T) for i in range(X.shape[1])])\n\n",
        "\nY = np.array([[[81, 63, 63],\n        [63, 49, 49],\n        [63, 49, 49]],\n\n       [[ 4, 12,  8],\n        [12, 36, 24],\n        [ 8, 24, 16]],\n\n       [[25, 35, 25],\n        [35, 49, 35],\n        [25, 35, 25]],\n\n       [[25, 30, 10],\n        [30, 36, 12],\n        [10, 12,  4]]])\n\n# [Missing]\n\nX = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        X[i] += Y[i][j]\n\n",
        "\na = np.array([9, 2, 7, 0])\nnumber = 0\n\n# Solution\nis_contained = (number in a)\n\n",
        "\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\n# Create a boolean array that indicates which elements of A are present in B\nmask = np.in1d(A, B)\n\n# Create a new array C that contains only the elements of A that are not present in B\nC = A[~mask]\n\n",
        "\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\nC = np.intersect1d(A,B)\n\n",
        "\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,4,8])\n\nbins = np.digitize(B, A)\nmask = (bins == 1) | (bins == 2)\nC = A[mask]\nC = np.unique(C)\n\n",
        "\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\n\n# Reverse the order of the ranks\nresult = rankdata(a, method='max').astype(int)\n\n",
        "\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\n\n# Reverse the order of the ranks\nresult = np.arange(len(a))[::-1]\n\n",
        "\nfrom scipy.stats import rankdata\nexample_a = [1,2,3,4,3,2,3,4]\ndef f(a = example_a):\n    # [Missing]\n    # We can use the `rankdata` function from `scipy.stats` to get the ranks of the values in the list.\n    # We can then use the `argsort` function to get the indices of the sorted values in descending order.\n    # Finally, we can use this index list to get the ranks in the desired order.\n    ranks = rankdata(a)\n    sorted_indices = np.argsort(-ranks)\n    result = np.empty_like(ranks)\n    result[sorted_indices] = ranks\n    # [Missing]\n    # We can return the result array.\n    return result\n",
        "\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, 1, -2],\n                 [ -1,  0, 1],\n                 [ -2,  1,  0]])\n\n# Solution\ndists = np.stack((x_dists, y_dists), axis=-1)\n\n",
        "\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\n# Solution\ndists = np.dstack((x_dists, y_dists))\n\n",
        "\n\na = np.random.rand(5, 5, 5)\nsecond = [1, 2]\nthird = [3, 4]\n\nresult = a[:, second, third]\n\n",
        "\n\narr = np.zeros((20,)*4)\narr = arr.reshape((20,10,10,2))\n\n",
        "\nfrom numpy import linalg as LA\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\n# Normalize each row with L1 Norm\nl1 = np.sum(np.abs(X), axis=1)\nresult = X / l1.reshape(-1, 1)\n\n",
        "\nfrom numpy import linalg as LA\n\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\nnorms = LA.norm(X, axis=1)\nresult = X / norms[:, np.newaxis]\n\n",
        "\nfrom numpy import linalg as LA\n\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\n# Normalize each row with L\u221e Norm\nnorms = np.apply_along_axis(LA.norm, 1, X, ord=np.inf)\nnormalized = X / norms[:, np.newaxis]\n\n# Print the normalized matrix\n",
        "\n\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nchoices = ['XX']\n\nconditions = [df['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\n\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\n\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n\n# Calculate distance between all points using pdist\ndistance_matrix = squareform(pdist(a))\n\n# Print the distance matrix\n",
        "\n\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n\n# Calculate distance between all points\nresult = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        result[i][j] = np.linalg.norm(a[i]-a[j])\n        result[j][i] = result[i][j]\n\n",
        "\n\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n\n# Calculate distance between each point\nresult = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        result[i][j] = np.linalg.norm(a[i]-a[j])\n        result[j][i] = result[i][j]\n\n",
        "\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\n",
        "\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA)\n",
        "\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=float)\nAVG = np.mean(NA, axis=0)\n",
        "\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\n\n# Remove adjacent duplicate non-zero value and all the zero value\nresult = np.trim_zeros(np.diff(a))\n\n",
        "\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\n\n# Remove adjacent duplicate non-zero values and all the zero values\nresult = np.concatenate([a[1:], a[1:] != a[:-1]], axis=0)\n\n",
        "\n\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\n# Create a list of tuples with the values of each array\ndata = list(zip(lat.flatten(), lon.flatten(), val.flatten()))\n\n# Create a dataframe with the list of tuples\ndf = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\n\n# Set the index to the row number\ndf.index = np.arange(len(df))\n\n",
        "\n\ndef f(lat = example_lat, lon = example_lon, val = example_val):\n    # Create a list of tuples with the values of lat, lon, and val\n    data = [(lat[i][j], lon[i][j], val[i][j]) for i in range(len(lat)) for j in range(len(lat[i]))]\n    # Create a dataframe with the list of tuples\n    df = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\n    return df\n",
        "\n\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\n# Create a list of tuples with the coordinates and values\ncoordinates_values = list(zip(lat.flatten(), lon.flatten(), val.flatten()))\n\n# Create a dataframe with the coordinates and values\ndf = pd.DataFrame(coordinates_values, columns=['lat', 'lon', 'val'])\n\n# Add a column with the maximum value of each row\ndf['maximum'] = df.groupby(['lat', 'lon'])['val'].transform('max')\n\n",
        "\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n\n# Add padding to the edges of the array\na = np.pad(a, ((size[0]//2, size[0]//2), (size[1]//2, size[1]//2)), mode='edge')\n\n# Get the sliding window view\nwindow_view = np.lib.stride_tricks.sliding_window_view(a, size)\n\n# Extract the sub-arrays that correspond to each window\nresult = []\nfor i in range(window_view.shape[0]):\n    for j in range(window_view.shape[1]):\n        sub_array = window_view[i, j]\n        # Remove any padding from the edges of the sub-array\n        sub_array = np.trim_zeros(sub_array, trim='fb')\n        result.append(sub_array)\n\n",
        "\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n\n# Add padding to handle edge effects\npad_width = ((size[0] - 1) // 2, (size[0] - 1) // 2), ((size[1] - 1) // 2, (size[1] - 1) // 2)\na_padded = np.pad(a, pad_width, mode='edge')\n\n# Create the sliding window view\nresult = np.lib.stride_tricks.sliding_window_view(a_padded, size)\n\n# Slice the padded array to remove padding\nresult = result[:, :, size[0] // 2:-(size[0] // 2), size[1] // 2:-(size[1] // 2)]\n\n",
        "\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\n\n# Compute the mean of the array of complex numbers\nresult = np.mean(a)\n\n",
        "\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    # compute the mean of the array of complex numbers\n    result = np.mean(a)\n    return result\n",
        "\nZ = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\n\n# Solution:\nresult = Z[..., -1:]\n\n",
        "\na = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\n\n# [Missing]\n\n",
        "\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ 57, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  78, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ 57, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, 727]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ 66, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\n\n# [Missing]\n\n",
        "\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ np.nan, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  np.nan, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, np.nan]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ np.nan, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\n\n# [Missing]\n\nresult = c in CNTS\n\n",
        "\nfrom scipy import interpolate as intp\na = np.arange(0, 4, 1).reshape(2, 2)\na = a.repeat(2, axis=0).repeat(2, axis=1)\nx_new = np.linspace(0, 2, 4)\ny_new = np.linspace(0, 2, 4)\n\n# Interpolate the values linearly using scipy.interp2d\nf = intp.interp2d(np.arange(a.shape[0]), np.arange(a.shape[1]), a, kind='linear')\nresult = f(x_new, y_new)\n\n",
        "\n\ndata = {'D':[2015,2015,2015,2015,2016,2016,2016,2017,2017,2017], 'Q':np.arange(10)}\ndf = pd.DataFrame(data)\n\n# Generate cumulative sum by D\ndf['Q_cum'] = df.groupby('D')['Q'].apply(lambda x: x.cumsum())\n\n# Print updated dataframe\n",
        "\na = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])\nU, i, V = np.linalg.svd(a,full_matrices=True)\n\n# Create a diagonal matrix with the values of the i matrix\ni_matrix = np.diag(i)\n\n# Multiply i_matrix with V to get the full diagonal matrix\ni_matrix_full = i_matrix * V\n\n",
        "\na = np.array([[1,0,2,3],[0,5,3,4],[2,3,2,10],[3,4, 10, 7]])\n\nnp.fill_diagonal(a, 0)\n\n",
        "\n\nstart = dateutil.parser.parse(\"23-FEB-2015 23:09:19.445506\")\nend = dateutil.parser.parse(\"24-FEB-2015 01:09:22.404973\")\nn = 10**4\n\n# Create a linearly spaced array of floats between start and end epochs\nepoch_range = np.linspace(start, end, n)\n\n# Convert the floats to datetime objects\nseries = pd.DatetimeIndex(epoch_range)\n\n",
        "\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\n\n# Solution\nresult = -1\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result = i\n        break\n\n",
        "\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\n\nresult = np.where(np.logical_and(x == a, y == b))[0]\n\n",
        "\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\n\n# Perform linear regression using least squares method\nA = np.vstack([x, np.ones(len(x))]).T\nm, c = np.linalg.lstsq(A, y, rcond=None)[0]\n\n# Get coefficients in the order of highest order to lowest order\nresult = [m, c, 0]\n\n",
        "\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\ndegree = 3\n\nA = np.zeros((len(x), degree+1))\nfor i in range(degree+1):\n    A[:,i] = x ** i\n\nresult = np.linalg.lstsq(A, y, rcond=None)[0]\n\n",
        "\na = np.arange(4)\ndf = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\n\ndf = df.apply(lambda x: x-a, axis=1)\n\n",
        "\nA = np.random.rand(5, 6, 3)\nB = np.random.rand(3, 3)\n\nresult = np.einsum('ijk,jl->ilk', A, B)\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[-1, 2], [-0.5, 6]])\n\n# Reshape the array into a 1D array\na_1d = a.reshape(-1, 1)\n\n# Create a MinMaxScaler object and fit and transform the array\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a_1d)\n\n# Reshape the result back into a 2D array\nresult = result.reshape(a.shape)\n\n",
        "\n# Solution\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Create a scaler object\nscaler = MinMaxScaler()\n\n# Reshape the array to a 2D array\narr_2d = arr.reshape(-1, 1)\n\n# Fit the scaler on the 2D array\nscaler.fit(arr_2d)\n\n# Transform the array using the scaler\nresult = scaler.transform(arr_2d)\n\n# Reshape the result back to the original shape\nresult = result.reshape(arr.shape)\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[[1, 0.5, -2], [-0.5,1, 6], [1,1,1]], [[-2, -3, 1], [-0.5, 10, 6], [1,1,1]]])\n\nresult = []\nfor matrix in a:\n    scaler = MinMaxScaler()\n    result.append(scaler.fit_transform(matrix))\n\nresult = np.array(result)\n\n",
        "\narr = (np.random.rand(100, 50)-0.5) * 50\n\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n\n",
        "\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\n\nfor i in range(len(n1)):\n    arr_temp = arr.copy()\n    mask = arr_temp < n1[i]\n    mask2 = arr_temp >= n2[i]\n    mask3 = mask ^ mask2\n    arr[mask] = 0\n    arr[mask3] = arr[mask3] + 5\n    arr[~mask2] = 30\n\n",
        "\n\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\n\n# Use decimal module to perform calculations with higher precision\ns1_dec = [decimal.Decimal(str(x)) for x in s1]\ns2_dec = [decimal.Decimal(str(x)) for x in s2]\nresult = sum(abs(x - y) for x, y in zip(s1_dec, s2_dec))\n\n",
        "\n\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\ns1 = np.append(s1, np.nan)\ns2 = np.append(s2, np.nan)\n\n# Use decimal module to perform summation and product\ns1_dec = [decimal.Decimal(str(x)) for x in s1]\ns2_dec = [decimal.Decimal(str(x)) for x in s2]\nresult = len([x for x, y in zip(s1_dec, s2_dec) if x != y])\n\n",
        "\n\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\n\nresult = True\nfor i in range(len(a)):\n    if i == 0:\n        continue\n    if not np.array_equal(a[0], a[i]):\n        result = False\n        break\n\n",
        "\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\n\nresult = np.all([np.isnan(arr).all() for arr in a])\n\n",
        "\na = np.ones((41, 13))\nshape = (93, 13)\n\nresult = np.pad(a, pad_width=(0, 36), mode='constant', constant_values=0)\n\n",
        "\n\na = np.ones((41, 12))\nshape = (93, 13)\n\nresult = np.pad(a, pad_width=(0, 93-41, 0, 13-12), mode='constant', constant_values=0)\n\n",
        "\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\n\n# Solution\n\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n\n",
        "\n\nexample_arr = np.ones((41, 13))\n\ndef f(arr = example_arr, shape=(93,13)):\n    result = np.pad(arr, pad_width=((0, 2), (0, 3)), mode='constant', constant_values=0)\n    return result\n",
        "\n\na = np.ones((41, 12))\nshape = (93, 13)\n\narray_shape = np.shape(a)\nlargest_shape = shape\n\nrows_to_pad = (largest_shape[0] - array_shape[0]) // 2\ncols_to_pad = (largest_shape[1] - array_shape[1]) // 2\n\nresult = np.pad(a, ((rows_to_pad, rows_to_pad), (cols_to_pad, cols_to_pad)), mode='constant', constant_values=0)\n\n",
        "\na = np.arange(12)\n\n# Reshape the numpy array to a (4,3) matrix\na = a.reshape(4,3)\n\n",
        "\na = np.array( \n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\nb = np.array( \n    [[0, 1, 1],\n    [1, 0, 1],\n    [1, 1, 0]]\n)\n\n# select the elements in a according to b\n# to achieve this result:\ndesired = np.array(\n  [[ 0,  3,  5],\n   [ 7,  8, 11],\n   [13, 15, 16]]\n)\n\n# create an empty array with the desired dimensions\nresult = np.zeros(desired.shape)\n\n# iterate over the indices in b\nfor i in range(b.shape[0]):\n    for j in range(b.shape[1]):\n        # if the corresponding element in b is 1,\n        # select the corresponding element in a and\n        # assign it to the corresponding element in result\n        if b[i][j] == 1:\n            result[i][j] = a[i][j][0]\n        else:\n            result[i][j] = a[i][j][1]\n\n",
        "\na = np.array( # dims: 3x3x2\n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\nb = np.array( # dims: 3x3\n    [[1, 1, 1],\n    [1, 1, 1],\n    [1, 1, 1]]\n)\n\n# select the elements in a according to b\n# to achieve this result:\ndesired = np.array(\n  [[ 1,  3,  5],\n   [ 7,  9, 11],\n   [13, 15, 17]]\n)\n\n# [Missing]\n\nresult = np.take_along_axis(a, b[..., None], axis=2).squeeze(axis=2)\n\n",
        "\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\n\n# select the elements in a according to b\nresult = a[np.arange(b.shape[0])[:, np.newaxis], np.arange(b.shape[1]), b]\n\n",
        "\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\n\n# select and sum the elements in a according to b\n# to achieve this result:\ndesired = 85\n\n# create an empty array to store the result\nresult = np.zeros(b.shape)\n\n# loop through each index in b\nfor i in range(b.shape[0]):\n    for j in range(b.shape[1]):\n        # get the corresponding index in a\n        index = b[i,j]\n        # sum the corresponding elements in a\n        result[i,j] = np.sum(a[i,j,:index+1])\n\n# check if the result matches the desired output\nif np.array_equal(result, desired):\n    print(\"The solution is correct!\")\nelse:\n    print(\"The solution is incorrect.\")\n",
        "\n\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\n\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n    [1, 0, 3]]\n)\n\n# use advanced indexing to extract un-indexed elements of a\nresult = a[np.arange(len(b)), b[:,0], b[:,1]]\n\n# sum the extracted elements\nresult = np.sum(result)\n\n",
        "\ndata = {'a': [1, 2, 3, 4, 5], 'b': [2, -6, 0, -4, 100]}\ndf = pd.DataFrame(data)\n\nresult = df.loc[(df['a'] > 1) & (df['a'] <= 4), 'b'].fillna(np.nan)\n\n",
        "\n\nim = np.array([[0,0,0,0,0,0],\n               [0,0,1,1,1,0],\n               [0,1,1,0,1,0],\n               [0,0,0,1,1,0],\n               [0,0,0,0,0,0]])\n\n# Create a mask of all non-zero elements in the first row\nmask = im[0] != 0\n\n# Apply the mask to all rows and columns to create the desired output\nresult = im[mask][:,mask]\n\n",
        "\n\nA = np.array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]])\n\n# Truncate the array to find the bounding box of nonzero data\nnonzero_rows = np.any(A, axis=1)\nnonzero_cols = np.any(A, axis=0)\nmin_row, max_row = np.where(nonzero_rows)[0][[0, -1]]\nmin_col, max_col = np.where(nonzero_cols)[0][[0, -1]]\n\n# Slice the array to get the bounding box of nonzero data\nresult = A[min_row:max_row+1, min_col:max_col+1]\n\n",
        "\n\nim = np.array([[1,1,1,1,1,5],\n               [1,0,0,1,2,0],\n               [2,1,0,0,1,0],\n               [1,0,0,7,1,0],\n               [1,0,0,0,0,0]])\n\n# Create a mask to filter out the peripheral non-zeros\nmask = np.array([[1,1,1,1,1,1],\n                 [1,1,1,1,1,1],\n                 [1,1,1,1,1,1],\n                 [1,1,1,1,1,1],\n                 [1,1,1,1,1,1]])\n\n# Apply the mask to the image\nresult = im * mask\n\n# Remove any rows/columns that are completely black\nresult = result[np.any(result, axis=1)]\nresult = result[:, np.any(result, axis=0)]\n\n",
        "\n\nim = np.array([[0,0,0,0,0,0],\n               [0,0,5,1,2,0],\n               [0,1,8,0,1,0],\n               [0,0,0,7,1,0],\n               [0,0,0,0,0,0]])\n\n# create a mask to filter out the black pixels\nmask = np.array([[1,1,1,1,1,1],\n                 [1,1,1,1,1,1],\n                 [1,1,0,0,0,1],\n                 [1,1,0,0,0,1],\n                 [1,1,1,1,1,1]])\n\n# apply the mask to the image\nresult = im[mask.astype(bool)]\n\n# reshape the result to remove the extra dimension\nresult = result.reshape(result.shape[0], -1)\n\n"
    ],
    "Tensorflow": [
        "\n\nx = tf.Variable(0)\n\n# assign the value of x to 1\nx.assign(1)\n\nresult = x\n",
        "\n\nx = tf.Variable(0)\n\nx.assign(114514)\n\nresult = x.numpy().astype(int)\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\n\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\n\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\n\nresult = tf.one_hot(labels, depth=10, on_value=1, off_value=0, axis=-1)\n\n",
        "\n\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    num_classes = 10\n    result = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n    return result\n",
        "\n\nlabels = [0, 6, 5, 4, 2]\n\nnum_classes = 10\n\n# Create a one-hot tensor with the correct shape\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\n",
        "\n\n\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\n\ndef my_map_func(i):\n  return [[i, i+1, i+2]]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n\nresult = [item for sublist in result for item in sublist]\n\n",
        "\ntf.compat.v1.disable_eager_execution()\n\nexample_input = [10, 20, 30]\ndef f(input=example_input):\n    result = tf.data.Dataset.from_tensor_slices(input)\n    result = result.flat_map(lambda x: tf.data.Dataset.from_tensor_slices([x, x+1, x+2]))\n    return result\n\nds = f()\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\n",
        "\n\nlengths = [4, 3, 5, 2]\n\nmax_length = 8\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\nresult = tf.pad(mask, [[0, max_length - tf.shape(mask)[0]], [0, 0]])\n\n",
        "\n\nlengths = [4, 3, 5, 2]\n\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\nresult = tf.pad(mask, [[0, 0], [1, 1]], constant_values=1)\n\n",
        "\n\nlengths = [4, 3, 5, 2]\n\nmax_length = max(lengths)\n\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\n\nresult = tf.pad(mask, [[0, 8-max_length], [0, 0]])\n\n",
        "\n\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    max_length = tf.reduce_max(lengths)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n    result = tf.pad(mask, [[0, 8-max_length], [0, 0]])\n    return result\n",
        "\n\nlengths = [4, 3, 5, 2]\n\nmax_length = max(lengths)\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\nresult = tf.pad(mask, [[0, 8-max_length], [0, 0]])\n\n",
        "\n\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\n\nresult = tf.stack([tf.tile(a, [len(b)]), tf.repeat(b, len(a))], axis=1)\n\n",
        "\n\nexample_a = tf.constant([1,2,3])\nexample_b = tf.constant([4,5,6,7])\n\ndef f(a=example_a,b=example_b):\n    result = tf.stack(tf.meshgrid(a,b), axis=-1)\n    return result\n\n",
        "\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\nresult = tf.expand_dims(a, axis=-2)\n",
        "\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\n\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\n\nresult = tf.reduce_sum(A, axis=1)\n\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\n\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\n\nresult = tf.math.reciprocal(A)\n",
        "\n\na = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nb = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\n\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n\n",
        "\n\na = tf.constant([\n  [1,1,1],\n  [0,1,1]\n])\nb = tf.constant([\n  [0,0,1],\n  [1,1,1]\n])\n\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n\n",
        "\n\nexample_a = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nexample_b = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\ndef f(A=example_a,B=example_b):\n    # Calculate the L2 distance d(A,B) element-wise\n    result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n    return result\n",
        "\n\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\n\nm = tf.gather_nd(x, tf.stack([tf.range(2), z], axis=1))\n\n",
        "\nm = tf.gather_nd(x, tf.stack([row, col], axis=1))\n",
        "\n\nexample_x = [[1,2,3],[4,5,6]]\nexample_y = [0,1]\nexample_z = [1,2]\nexample_x = tf.constant(example_x)\nexample_y = tf.constant(example_y)\nexample_z = tf.constant(example_z)\ndef f(x=example_x,y=example_y,z=example_z):\n    m = tf.gather_nd(x, tf.stack([y,z], axis=-1))\n    return m\n\nresult = f()\n",
        "\n\nnp.random.seed(10)\nA = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\nB = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\n\n# [Missing]\n\nresult = tf.tensordot(A, B, axes=[[2], [2]])\n\n",
        "\nC = tf.tensordot(A, B, axes=[[2], [2]])\n",
        "\n\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\n\nresult = tf.strings.unicode_decode(x, 'UTF-8')\n\n",
        "\n\nexample_x=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\ndef f(x=example_x):\n    result = tf.strings.unicode_decode(x, \"UTF-8\")\n    return result\n",
        "```python\n# Solution\nx_nonzero = tf.reduce_sum(tf.cast(tf.not_equal(x, 0), tf.float32), axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x)[-1]])\nx_nonzero = tf.cast(tf.not_equal(x_nonzero, 0), tf.float32)\nx_nonzero = tf.reduce_sum(x_nonzero, axis=-2)\nx_nonzero = tf.expand_dims(x_nonzero, axis=-1)\nx_nonzero = tf.tile(x_nonzero, [1, 1, tf.shape(x",
        "\n\nx = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nx = tf.convert_to_tensor(x, dtype=tf.float32)\n\n# Calculate variance of second to last dimension of X (the features) but only the non-zero entries\nresult = tf.math.reduce_variance(tf.boolean_mask(x, tf.math.not_equal(x, 0), axis=-2), axis=-2)\n\n",
        "\n\nexample_x = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nexample_x = tf.convert_to_tensor(example_x, dtype=tf.float32)\n\ndef f(x=example_x):\n    mask = tf.reduce_any(tf.not_equal(x, 0), axis=-2)\n    num_non_zero = tf.reduce_sum(tf.cast(mask, tf.float32), axis=-1)\n    avg = tf.reduce_sum(tf.boolean_mask(x, mask), axis=-2) / num_non_zero[..., tf.newaxis]\n    return avg\n\nresult = f()\n",
        "\n\ntry:\n    Session = tf.Session\nexcept AttributeError:\n    Session = tf.compat.v1.Session\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nwith tf.Session() as sess:\n   result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmax(a, axis=1)\n\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\n# Get the index of the highest value in each column\nresult = tf.argmax(a, axis=1)\n\n",
        "\n\nexample_a = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\ndef f(a=example_a):\n    # convert the scores tensor so that each row simply contains the index of the highest value in each row\n    result = tf.argmax(a, axis=1)\n    return result\n\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmin(a, axis=1)\n\n",
        "\n# Save the model in \"export/1\"\ntf.saved_model.save(model, \"export/1\")\n",
        "\n\nseed_x = 10\ntf.random.set_seed(seed_x)\n\n# generate 10 random integers between 1 and 4\nresult = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25]).sample(10)\n\n",
        "\n\nseed_x = 10\n\n# set the random seed\ntf.random.set_seed(seed_x)\n\n# generate 114 random integers as a tensor in TensorFlow\nresult = tfp.distributions.Categorical(probs=[0.1, 0.2, 0.3, 0.4]).sample(114)\n\n",
        "\n\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25]).sample(10)\n    return result\n",
        "\n\n# output the version of tensorflow into variable 'result'\nresult = tf.__version__\n\n"
    ],
    "Scipy": [
        "\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\n\n# Fit y = Alogx + B using polyfit()\ncoefficients = np.polyfit(np.log(x), y, 1)\n\n# Extract A and B from the coefficients\nA, B = coefficients[0], coefficients[1]\n\n# Print the result\nresult = np.array([A, B])\n",
        "\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\n\n# Fit y = A + Blogx using polyfit()\ncoefficients = np.polyfit(np.log(x), y, 1)\n\n# Extract A and B from the coefficients\nA = coefficients[0]\nB = coefficients[1]\n\n# Create a numpy array of [A, B]\nresult = np.array([A, B])\n\n",
        "\n\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\n\ndef func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\npopt, pcov = scipy.optimize.curve_fit(func, x, y, p0)\n\nresult = popt\n\n",
        "\nfrom scipy import stats\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\n\n# [Missing]\n\ntest_stat, p_value = stats.ks_2samp(x, y)\n\n",
        "\nfrom scipy.stats import kstest\n\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\n\n# Perform the two-sample KS test\ntest_stat, p_value = kstest(x, y)\n\n# Check if we can reject the null hypothesis that the two distributions are identical\nif p_value <= alpha:\n    result = True\nelse:\n    result = False\n\n",
        "\nfrom math import *\n\ninitial_guess = [-1, 0, -3]\n\ndef f(x):\n    return ((x[0]+x[1]-x[2])-2)**2 + ((3*x[0]-x[1]-x[2]))**2 + sin(x[1]) + cos(x[1]) + 4\n\nresult = optimize.minimize(f, initial_guess)\n\n",
        "\n\nz_scores = np.array([-3, -2, 0, 2, 2.5])\n\np_values = 1 - scipy.stats.norm.cdf(z_scores)\n\n",
        "\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\n\n# Calculate the p-values from the Z-scores\np_values = scipy.stats.norm.cdf(z_scores)\n\n",
        "\n\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\n\nz_scores = []\n\nfor p in p_values:\n    z_score = scipy.stats.norm.ppf(p)\n    z_scores.append(z_score)\n\n",
        "\nfrom scipy import stats\n\nstddev = 2.0785\nmu = 1.744\nx = 25\n\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n\n",
        "\nfrom scipy import stats\n\nstddev = 2.0785\nmu = 1.744\ntotal = 37\n\ndist = stats.lognorm(s=stddev, scale=mu, loc=0, a=total)\n\nexpected_value, variance, skewness, kurtosis = dist.stats('mvsk')\nmedian = dist.median()\n\n",
        "\nresult = sa.dot(sb)\n",
        "\nfrom scipy import sparse\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n    result = sA.dot(sB)\n    return result\n",
        "\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30]])\n\n# Interpolate using scipy.interpolate.LinearNDInterpolator\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n\n",
        "\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\nresult = scipy.interpolate.griddata(points, V, request, method='linear')\n\n",
        "\nfrom scipy import misc\nfrom scipy.ndimage import rotate\ndata_orig = misc.face()\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nangle = np.random.randint(1, 360)\n\n# Calculate rotation matrix\ntheta = np.deg2rad(angle)\nc, s = np.cos(theta), np.sin(theta)\nR = np.array(((c,-s), (s, c)))\n\n# Rotate image\ndata_rot = rotate(data_orig, angle, reshape=False)\n\n# Calculate translation matrix\nx1,y1 = np.array([x0,y0]) - np.array([data_orig.shape[0]/2, data_orig.shape[1]/2])\nx2,y2 = np.dot(R, np.array([x1,y1]))\nxrot, yrot = np.array([data_rot.shape[0]/2, data_rot.shape[1]/2]) + np.array([x2,y2])\n\n",
        "\nfrom scipy.sparse import csr_matrix\n\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\n\nresult = M.diagonal()\n\n",
        "\nfrom scipy import stats\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n\n# Kolmogorov-Smirnov test\nresult = stats.kstest(times, \"uniform\")\n\n",
        "\nfrom scipy import stats\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nexample_rate = 1.0\nexample_T = 100.0\nexample_times = poisson_simul(example_rate, example_T)\ndef f(times = example_times, rate = example_rate, T = example_T):\n    # Kolmogorov-Smirnov test for uniformity\n    result = stats.kstest(times, \"uniform\")\n    return result\n",
        "\n# [Missing]\nresult = stats.kstest(times, \"uniform\")\n",
        "\nfrom scipy import sparse\n\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\n# Concatenate c1 and c2 horizontally\nFeature = sparse.hstack([c1, c2])\n\n# Convert Feature to csr_matrix\nFeature = sparse.csr_matrix(Feature)\n\n",
        "\nfrom scipy import sparse\n\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\nFeature = sparse.hstack([c1, c2])\n\n",
        "\nfrom scipy import sparse\n\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\n# Concatenate c1 and c2 in vertical direction\nFeature = sparse.vstack([c1, c2])\n\n",
        "\n\n# create six points in 2d space; the first three belong to set \"A\" and the\n# second three belong to set \"B\"\nx = [1, 2, 3, 1.8, 1.9, 3.4]\ny = [2, 3, 1, 2.6, 3.4, 0.4]\npoints1 = np.array([(x, y) for x in x for y in y])\n\n# initialize points2 randomly\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\n# K-means clustering\ndef kmeans(points, k, max_iter=100):\n    # initialize k centroids randomly\n    centroids = points[np.random.choice(points.shape[0], k, replace=False)]\n    for i in range(max_iter):\n        # assign points to nearest centroid\n        distances = scipy.spatial.distance.cdist(points, centroids)\n        labels = np.argmin(distances, axis=1)\n        # update centroids as mean of points assigned to each cluster\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(new_centroids == centroids):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\n# run K-means clustering on points1 and points2\nlabels1, centroids1 = kmeans(points1, 2)\nlabels2, centroids2 = kmeans(points2, 2)\n\n# assign points from points2 to points1 based on centroid distances\ndistances = scipy.spatial.distance.cdist(centroids1, centroids2)\nresult = np.argmin(distances, axis=1)\n\n",
        "\n\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\n# use Kuhn-Munkres algorithm to find the matching\ncost_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(cost_matrix)\n\n# convert the matching to a list of indices\nresult = [col_ind[i] for i in range(N) if row_ind[i] == i]\n\n",
        "\nfrom scipy import sparse\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n\n# Remove diagonal elements from b\nb.setdiag(0)\n\n",
        "\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Create a marker image with unique integer values for each region\nmarkers = np.zeros_like(img)\nmarkers[img > threshold] = 1\nmarkers[img <= threshold] = 2\n\n# Apply watershed algorithm to the marker image\nlabels = ndimage.watershed_ift(img, markers)\n\n# Count the number of regions with value > threshold\nresult = (labels == 2).sum()\n\n",
        "\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Count the number of regions with value below threshold\nlabels, num_labels = ndimage.label(img < threshold)\nresult = np.bincount(labels.ravel())\n\n",
        "\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nexample_img /= img.max()\ndef f(img = example_img):\n    threshold = 0.75\n    labels, num_labels = ndimage.label(img >= threshold)\n    return num_labels\n",
        "\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Create a binary mask of the array where the values are greater than or equal to the threshold\nmask = img >= threshold\n\n# Label the regions of the binary mask\nlabels, num_labels = ndimage.label(mask)\n\n# Calculate the center of mass of each region and the distance between the center of mass and the top left corner\ncom = ndimage.center_of_mass(img, labels=labels, index=range(1, num_labels+1))\ndist = np.sqrt(np.sum((com - [0, 0])**2, axis=1))\n\n# Output the distances as a list\nresult = dist.tolist()\n",
        "\nfrom scipy.sparse import lil_matrix\nfrom scipy import sparse\n\nM= sparse.random(10, 10, density=0.1, format='lil')\n\n# set diagonal elements to 1\nM.setdiag(1)\n\n# set corresponding elements in transpose to 1\nM = M + M.T\n\n",
        "\nfrom scipy.sparse import lil_matrix\nexample_sA = sparse.random(10, 10, density=0.1, format='lil')\ndef f(sA = example_sA):\n    # make the matrix symmetric\n    for i in range(sA.shape[0]):\n        for j in range(i):\n            sA[j, i] = sA[i, j]\n    return sA\n",
        "\n\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\n\n# Remove single cells that are surrounded by other single cells\nsquare = scipy.ndimage.binary_erosion(square)\nsquare = scipy.ndimage.binary_dilation(square)\n\n",
        "\n\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = np.random.randint(1, 255, size = (12, 12))\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = np.random.randint(1, 255, size = (20,))\n\n# Label all connected regions and calculate their sizes\nlabels, num_labels = scipy.ndimage.label(square)\nsizes = scipy.ndimage.sum(square, labels, range(1, num_labels+1))\n\n# Remove all regions with a size of 1\nfor i in range(1, num_labels+1):\n    if sizes[i-1] == 1:\n        square[labels == i] = 0\n\n",
        "\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\nmean = col.mean()\nstandard_deviation = col.std()\n\n",
        "\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Get the non-zero values of the sparse vector\ndata = col.data\n\n# Get the max and min values of the non-zero values\nMax = np.max(data)\nMin = np.min(data)\n\n",
        "\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Convert sparse vector to dense vector\ndense_col = col.toarray()\n\n# Get median and mode values of dense vector\nMedian = np.median(dense_col)\nMode = np.bincount(dense_col.astype(int)).argmax()\n\n",
        "\ndef fourier(x, *args):\n    a = args[:degree]\n    return np.sum([a[i] * np.cos(i * np.pi / tau * x) for i in range(degree)], axis=0)\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1] * degree)\n",
        "\n\n# Sample study area array\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\n# Calculate pairwise Euclidean distances between all regions\nresult = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean')\n\n# Convert distances measured in cells back to metres by multiplying by the raster resolution\nresolution = 10 # (assuming raster resolution is 10m x 10m)\nresult = result * resolution\n\n",
        "\n\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\n# Calculate pairwise Manhattan distances between all regions\nresult = scipy.spatial.distance.cdist(example_array, example_array, 'cityblock')\n\n",
        "\n# Calculate pairwise Euclidean distances between all regions\nresult = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean')\n",
        "\nresult = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    y_int = interpolate.splev(x_val, tck, der = 0)\n    result[i, :] = y_int\n",
        "\n\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\nx3 = [34.0, 35.0, 39.0, 40.0, 43.0, 43.0, 44.0, 45.0]\nx4 = [34.0, 34.8, 34.8, 35.4, 37.2, 37.8, 41.2, 42.8]\n\n# Concatenate the datasets into one 2-d array\nX = np.vstack((x1, x2, x3, x4))\n\n# Calculate the Anderson-Darling test statistic and critical values\nstatistic, critical_values, significance_level = ss.anderson_ksamp(X)\n\n",
        "\n\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\n\n# concatenate the two datasets into one array\nx = np.concatenate((x1, x2))\n\n# calculate the Anderson-Darling test statistic and p-value\nresult = ss.anderson_ksamp([x1, x2])\n\n# interpret the result\nif result.statistic < result.critical_values[4]:\n    print(\"The two datasets are drawn from the same population at the 5% significance level.\")\nelse:\n    print(\"The two datasets are not drawn from the same population at the 5% significance level.\")\n",
        "\n\ndef tau(x):\n    y = np.array(A['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\n\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau(x))\n\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\n\n# Solution:\n\nresult = len(sa.data) == 0 and len(sa.indices) == 0 and len(sa.indptr) == 1\n\n",
        "\nfrom scipy.sparse import lil_matrix\n\ndef is_lil_matrix_only_zeroes(my_lil_matrix):\n    return(len(my_lil_matrix.nonzero()[0]) == 0)\n\nsa = lil_matrix((10, 10))\n\n# [Missing]\n\n",
        "\nfrom scipy.linalg import block_diag\nnp.random.seed(10)\na = np.random.rand(100,2,2)\n\nresult = block_diag(*a)\n\n",
        "\nfrom scipy import stats\nnp.random.seed(10)\npre_course_scores = np.random.randn(10)\nduring_course_scores = np.random.randn(10)\n\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n\n",
        "\nfrom scipy import stats\n\nexample_pre_course_scores = np.random.randn(10)\nexample_during_course_scores = np.random.randn(10)\n\ndef f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n    ranksums_result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = ranksums_result.pvalue\n    return p_value\n\n",
        "\n\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\n\nn = len(a)\nmean = np.mean(a)\nstd = np.std(a, ddof=1)\n\n# Calculate kurtosis\nkurtosis_result = (n / ((n-1)*(n-2)*(n-3))) * np.sum((a - mean)**4) / (std**4)\n\n",
        "\n\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\n\nn = len(a)\nmean = np.mean(a)\n\n# Calculate the sum of squares of deviations from the mean\nsum_of_squares = np.sum((a - mean)**2)\n\n# Calculate the sum of cubes of deviations from the mean\nsum_of_cubes = np.sum((a - mean)**3)\n\n# Calculate the sum of fourth powers of deviations from the mean\nsum_of_fourths = np.sum((a - mean)**4)\n\n# Calculate the kurtosis\nkurtosis = (n*(n-1)*(n-2)*(n-3)) / ((n-1)*(n-2)*(n-3)) * (sum_of_cubes / (sum_of_squares**1.5))\n\n",
        "\n\ns = np.linspace(-1, 1, 50)\nt = np.linspace(-2, 0, 50)\nx, y = np.ogrid[-1:1:10j,-2:0:10j]\nz = (x + y)*np.exp(-6.0 * (x * x + y * y))\n\nf = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n\nresult = f(s, t)\n\n",
        "\n\nexample_s = np.linspace(-1, 1, 50)\nexample_t = np.linspace(-2, 0, 50)\n\ndef f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    return scipy.interpolate.interp2d(s, t, z, kind='cubic')\n\ninterpolated_f = f()\n\nresult = interpolated_f(example_s, example_t)\n",
        "\n\n# Finding the voronoi tessallation for the given points\nvor = scipy.spatial.Voronoi(points)\n\n# Finding the regions occupied by the extra points\nresult = []\nfor point in extraPoints:\n    region = vor.point_region(point)\n    result.append(region)\n\n# Converting the result to a numpy array\nresult = np.array(result)\n",
        "\n\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n\n# Count how many extra points are in each Voronoi cell\nresult = np.zeros(len(vor.regions), dtype=int)\nfor i, region in enumerate(vor.regions):\n    if -1 not in region:\n        # Check which extra points are in this Voronoi cell\n        for point in extraPoints:\n            if scipy.spatial.distance.euclidean(point, vor.points[vor.regions[i][0]]) < vor.radius[i]:\n                result[i] += 1\n\n",
        "\n\nnp.random.seed(10)\nmax_vector_size = 1000\nvectors = [np.random.randint(100,size=900),np.random.randint(100,size=max_vector_size),np.random.randint(100,size=950)]\n\n# Create a sparse matrix from the numpy vectors\nsparse_matrix = sparse.csr_matrix(vectors)\n\n# Pad zeros to the end of the vectors which are smaller than the maximum size\npadded_sparse_matrix = pad_sparse_matrix(sparse_matrix, max_vector_size)\n\n",
        "\n\na = np.random.binomial(n=1, p=1/2, size=(9, 9))\nb = nd.median_filter(a, 3, origin=1.0)\n\n",
        "\nrow_vector = M.getrow(row)\nresult = row_vector.toarray()[0][column]\n",
        "\nfrom scipy.sparse import csr_matrix\n\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = [2, 1]\ncolumn = [3, 0]\n\nresult = M[row, column]\n\n",
        "\n\narray = np.random.randint(0, 9, size=(10, 10, 10))\nx = np.linspace(0, 10, 10)\nx_new = np.linspace(0, 10, 100)\n\nnew_array = np.apply_along_axis(\n    lambda x: scipy.interpolate.interp1d(x, array[:, :, 0], kind='cubic')(x_new),\n    axis=2,\n    arr=array\n)\n\n",
        "\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\nx = 2.5\nu = 1\no2 = 3\n\n# Definition of the mathematical function:\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\n\n",
        "\n# [Missing]\nprob = NormalDistro(u,o2,x)\n",
        "\n\nN = 8\n\n# Create DCT matrix using scipy.fftpack.dctn function\nresult = sf.dctn(N, norm='ortho')\n\n",
        "\nfrom scipy.sparse import diags\n\nv1 = [3*i**2 +(i/2) for i in range(1, 6)]\nv2 = [-(6*i**2 - 1) for i in range(1, 6)]\nv3 = [3*i**2 -(i/2) for i in range(1, 6)]\nmatrix = np.array([v1, v2, v3])\n\nTridiagonal_1 = diags(matrix, [-1,0,1], (5, 5)).toarray()\n\n",
        "\n\nN = 3\np = 0.5\n\n# Create a 2D array of zeros with shape (N+1, N+1)\nM = np.zeros((N+1, N+1))\n\n# Fill in the values of the matrix using the binomial distribution\nfor i in range(N+1):\n    for j in range(i+1):\n        M[i,j] = scipy.stats.binom.pmf(j, i, p)\n\n",
        "\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\n# Perform row-zscore calculation using SCIPY\nresult = stats.zscore(df, axis=1)\n\n",
        "\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\n# Perform column-zscore calculation using SCIPY\nresult = pd.DataFrame(stats.zscore(df), columns=df.columns, index=df.index)\n\n",
        "\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\n# Calculate row-wise zscores using scipy\nzscores = stats.zscore(df)\n\n# Create a new dataframe with the original data and the zscores\nresult = pd.concat([df, pd.DataFrame(zscores, columns=df.columns)], axis=1)\n\n# Rename the columns to include the word \"zscore\"\nresult.columns = ['data', 'zscore']\n\n# Add a prefix to the index to indicate that it is a zscore\nresult.index = ['zscore_' + i for i in result.index]\n\n# Print the result\n",
        "\n# Calculate z-scores for each column\nz_scores = stats.zscore(df)\n\n# Create a new dataframe with the z-scores\nresult = pd.DataFrame(z_scores, columns=df.columns, index=df.index)\n\n# Round the z-scores to 3 decimal places\nresult = result.round(3)\n\n# Add a new column with the original data\nresult['data'] = df\n\n# Rename the columns\nresult.columns = ['zscore', 'data']\n\n# Print the result\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\nfrom scipy.spatial import distance\nshape = (6, 6)\n\n# Create a 2D array of zeros with the given shape\nmid = np.zeros(shape + (2,))\n\n# Fill the array with the coordinates of each pixel\nfor i in range(shape[0]):\n    for j in range(shape[1]):\n        mid[i, j] = (i, j)\n\n# Compute the distance from each pixel to the center\nresult = distance.cdist(mid, mid)\n",
        "\nfrom scipy.spatial import distance\nshape = (6, 6)\n\n# Create a 2D array of zeros with the same shape as the image\nmid = np.zeros(shape + (2,))\n\n# Compute the Manhattan distance from center point to every point in the image\nresult = distance.cdist(mid, np.indices(shape).reshape((shape[0]*shape[1], 2)))\n\n",
        "\nfrom scipy.spatial import distance\n\ndef f(shape = (6, 6)):\n    mid = np.array([[[i, j] for j in range(shape[1])] for i in range(shape[0])])\n    result = distance.cdist(mid, mid, metric='euclidean')\n    return result\n",
        "\n\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\n\nresult = skimage.transform.resize(x, shape, order=1)\n\n",
        "\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\n\ndef func(x, a):\n    return np.dot(a, x**2)\n\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model) **2\n\ndef main():\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = minimize(residual, fit_params, args=(a, y))\n    print(out)\n\nif __name__ == '__main__':\n    main()\n",
        "\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\n\ndef func(x, a):\n    return np.dot(a, x ** 2)\n\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x, a)\n    return (y - model) ** 2\n\ndef main():\n    fit_params = scipy.optimize.least_squares(residual, x0, args=(a, y), bounds=(0, x_lower_bounds))\n    print(fit_params.x)\n\nif __name__ == '__main__':\n    main()\n",
        "\n\nN0 = 10\ntime_span = [-0.1, 0.1]\n\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\n\nresult = sol.y\n",
        "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi - t + np.sin(2*np.pi - t)\n",
        "\n\nN0 = 10\ntime_span = [-0.1, 0.1]\n\ndef input_func(t):\n    return -np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,], fun_args=(input_func,))\n\nresult = sol.y\n",
        "\n# [Missing]\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n\nresult = sparse.vstack((sa, sb))\n\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n\nresult = sparse.vstack((sa, sb))\n\n",
        "\nc = 5\nlow = 0\nhigh = 1\n\n# [Missing]\n\nI = []\nfor n in range(len(c)):\n    # equation\n    eqn = lambda x: 2*x*c[n]\n    # integrate \n    result,error = scipy.integrate.quad(eqn,low,high)\n    I.append(result)\nI = np.array(I)\n\n",
        "\ndef f(c=5, low=0, high=1):\n    # equation\n    eqn = lambda x: 2*x*c\n    # integrate\n    result,error = scipy.integrate.quad(eqn, low, high)\n    return result\n",
        "\nfrom scipy import sparse\n\n# Generate a sparse matrix V with 10 rows and 10 columns, with 5% density\nV = sparse.random(10, 10, density = 0.05, format = 'dok', random_state = 42)\n\n# Define a scalar value x\nx = 99\n\n# Create a copy of V to avoid modifying it\nV_copy = V.copy()\n\n# Iterate over non-zero elements in V and add x to them\nfor i, j in zip(V_copy.row, V_copy.col):\n    if V_copy[i, j] != 0:\n        V_copy[i, j] += x\n\n# Replace V with the updated copy\nV = V_copy\n\n# Print the updated sparse matrix V\n",
        "\nfrom scipy import sparse\n\n# Create a sparse matrix with random values\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\n\n# Define a scalar value\nx = 100\n\n# Multiply each non-zero value in the matrix by x\nV.data *= x\n\n# Create a new coo_matrix object with the resulting values and the same dimensions as the original matrix\nresult = sparse.coo_matrix((V.data, (V.row, V.col)), shape = V.shape)\n\n# Print the resulting matrix\n",
        "\nfrom scipy import sparse\n\n# create a sparse matrix V with 10% non-zero elements\nV = sparse.random(10, 10, density = 0.1, format = 'coo', random_state = 42)\n\n# create scalars x and y\nx = 100\ny = 99\n\n# multiply each non-zero element of V by x and add the result to V\nV.data *= x\nV = V.tocsr()\nV.sum_duplicates()\n\n# add y to each non-zero element of V\nV.data += y\n\n# print the modified matrix V\n",
        "\nfrom scipy import sparse\n\nsa = sparse.random(10, 10, density = 0.3, format = 'csc', random_state = 42)\n\n#iterate through columns\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    #get the column length\n    Len = math.sqrt(sum(List))\n    #normalize the column\n    sa[:,Col] = sparse.csc_matrix(np.array(Column)/Len)\n\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.3, format = 'csr', random_state = 42)\n\n#iterate through columns\nfor Col in xrange(sa.shape[1]):\n   Column = sa[:,Col].data\n   List = [x**2 for x in Column]\n   #get the column length\n   Len = math.sqrt(sum(List))\n   #normalize the column\n   sa[:,Col] = sparse.csr_matrix(np.array([x/Len for x in Column]))\n\n",
        "\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n\n# Convert decimal matrix to binary matrix\nb = np.array([[bin(x)[2:].zfill(8) for x in row] for row in a])\n\n# Transpose binary matrix to get desired output\nc = b.transpose()\n\n",
        "\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n\n# Convert decimal matrix to binary matrix\nb = np.zeros(a.shape)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        if a[i][j] % 2 == 0:\n            b[i][j] = 0\n        else:\n            b[i][j] = 1\n\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Calculate distance matrix between data and centroids\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\n\n# Cluster data using Ward linkage\nZ = scipy.cluster.hierarchy.linkage(dist_matrix, method='ward')\n\n# Cut tree to get cluster assignments for each data point\nresult = scipy.cluster.hierarchy.cut_tree(Z, n_clusters=5)\n\n# Find the index of the closest element to each centroid\nclosest_indices = np.argmin(dist_matrix, axis=1)\n\n# Extract the closest element for each cluster\nclosest_elements = data[closest_indices]\n\n# Print the closest element for each cluster\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Calculate distance matrix between data and centroids\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\n\n# Cluster data using Ward's linkage\nZ = scipy.cluster.hierarchy.linkage(dist_matrix, method='ward')\n\n# Cut tree to get cluster assignments for each data point\nassignments = scipy.cluster.hierarchy.cut_tree(Z, n_clusters=5)\n\n# Extract closest-to-centroid elements for each cluster\nresult = []\nfor i in range(5):\n    # Find index of closest centroid for each data point in cluster i\n    closest_centroid_index = np.argmin(dist_matrix[assignments == i, i])\n    # Extract data point with closest centroid\n    closest_data_point = data[assignments == i][closest_centroid_index]\n    result.append(closest_data_point)\n\n",
        "\n\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nk = 3\n\n# Calculate distance between each data point and its centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n\n# Sort distances to find the k-th closest for each cluster\nresult = []\nfor i in range(len(centroids)):\n    sorted_indices = np.argsort(distances[i])\n    result.append(sorted_indices[k-1])\n\n",
        "\nfrom scipy.optimize import fsolve\n\ndef eqn(a, x, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\n\n# [Missing]\n\nresult = fsolve(eqn, x0=0.5, args=(xdata, bdata))\n\n",
        "\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\n\nresult = []\nfor i in range(len(xdata)):\n    b = fsolve(eqn, x0=adata[i], args = (xdata[i],))\n    result.append([xdata[i], b[0]])\n\nresult = np.array(result)\nresult = result[np.argsort(result[:,0])]\n\n",
        "\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n\n# [Missing]\n\nresult = stats.kstest(sample_data, 'bekkers', args=(estimated_a, estimated_m, estimated_d))\n\n",
        "\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n\n# [Missing]\n\nresult = stats.kstest(sample_data, 'norm', args=(estimated_a, estimated_m, estimated_d))\n",
        "\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(lambda x: integrate.trapz(x['A'], x.index))\n",
        "\n\nx = np.array([(2,2), (1,2), (2,3), (3,2), (2,1)])\ny = np.array([5,7,8,10,3])\neval = np.array([(2.7, 2.3)])\n\n# Using griddata function from scipy.interpolate\nresult = scipy.interpolate.griddata(x, y, eval)\n\n",
        "\n\n# create the data frame\na = pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\n\n# define the likelihood function\ndef likelihood(weights):\n    n = len(a)\n    x = a.iloc[:,0].value_counts().values\n    return np.prod(weights**x) * np.exp(np.dot(weights, x))\n\n# define the objective function\ndef objective(weights):\n    return -likelihood(weights)\n\n# define the initial guess for the weights\nweights = np.ones(a.iloc[:,0].nunique()) / a.iloc[:,0].nunique()\n\n# find the MLE\nresult = sciopt.minimize(objective, weights, method='BFGS')\n\n# print the weights\n",
        "\nresult = sciopt.fminbound(e, pmin, pmax, args=(x,y))\n",
        "\nfrom scipy import signal\n\narr = np.array([-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,])\nn = 2\n\n# Finding the indices of relative extrema\nresult = signal.argrelmax(arr, order=n)[0]\n\n",
        "\nfrom scipy import signal\narr = np.array([[-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,], [3, 0, 0, 1, 2, 4]])\nn = 2\n\n# Finding relative extrema indices\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if i == 0:\n            if j == 0:\n                if arr[i][j] <= arr[i][j+1] and arr[i][j] <= arr[i+1][j]:\n                    result.append([i,j])\n            elif j == arr.shape[1]-1:\n                if arr[i][j] <= arr[i][j-1] and arr[i][j] <= arr[i+1][j]:\n                    result.append([i,j])\n            else:\n                if arr[i][j] <= arr[i][j+1] and arr[i][j] <= arr[i][j-1] and arr[i][j] <= arr[i+1][j]:\n                    result.append([i,j])\n        elif i == arr.shape[0]-1:\n            if j == 0:\n                if arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i][j+1]:\n                    result.append([i,j])\n            elif j == arr.shape[1]-1:\n                if arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i][j-1]:\n                    result.append([i,j])\n            else:\n                if arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i][j+1] and arr[i][j] <= arr[i][j-1]:\n                    result.append([i,j])\n        else:\n            if j == 0:\n                if arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i+1][j] and arr[i][j] <= arr[i][j+1]:\n                    result.append([i,j])\n            elif j == arr.shape[1]-1:\n                if arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i+1][j] and arr[i][j] <= arr[i][j-1]:\n                    result.append([i,j])\n            else:\n                if arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i+1][j] and arr[i][j] <= arr[i][j+1] and arr[i][j] <= arr[i][j-1]:\n                    result.append([i,j])\n\n",
        "\ndf = df[(np.abs(stats.zscore(df[['NUM1', 'NUM2', 'NUM3']])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\n\n# Convert data to Pandas DataFrame\ndata1 = pd.DataFrame(data)\n\n",
        "\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\n\n# Create a dictionary from the Bunch object\ndata_dict = dict(zip(data.keys(), data.values()))\n\n# Convert the dictionary to a dataframe\ndata1 = pd.DataFrame(data_dict)\n\n",
        "\nfrom sklearn.datasets import load_boston\ndata = load_boston()\n\n# [Missing]\n\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\n\n",
        "\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\n\ndef solve(data):\n    # Convert data from Bunch object to DataFrame\n    df = pd.DataFrame(data=data['data'], columns=data['feature_names'])\n    # Add target variable to DataFrame\n    df['target'] = data['target']\n    return df\n\ndata1 = solve(data)\n",
        "\n\ndf = load_data()\n\n# Create a new dataframe with one column for each unique element in the original column\ndf_out = pd.get_dummies(df['Col3'], prefix='Col3')\n\n# Add the original columns to the new dataframe\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'Col1': ['C', 'A', 'B'],\n                   'Col2': [33, 2.5, 42],\n                   'Col3': [['Apple', 'Orange', 'Banana'], ['Apple', 'Grape'], ['Banana']]})\n\n# one-hot-encode the list of names\ndf_encoded = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\n\n# merge the encoded columns with the original dataframe\ndf_out = pd.concat([df, df_encoded], axis=1)\n\n",
        "\ndf = load_data()\n\n# One-hot-encode the last column\ndf_out = pd.get_dummies(df, columns=['Col4'])\n\n",
        "\ndf = load_data()\n\n# One-hot-encode the last column\ndf_out = pd.get_dummies(df.iloc[:, -1], prefix=df.columns[-1])\n\n# Merge the one-hot-encoded columns with the original dataframe\ndf_out = pd.concat([df.iloc[:, :-1], df_out], axis=1)\n\n",
        "\n\n# Load the data\ndf = load_data()\n\n# Get the last column name\nlast_col = df.columns[-1]\n\n# Get the unique elements in the last column\nunique_elements = list(set(df[last_col].sum()))\n\n# Create a new dataframe with one-hot-encoded columns\ndf_out = pd.get_dummies(df, columns=[last_col])\n\n# Rename the one-hot-encoded columns with the unique elements\ndf_out.columns = df_out.columns.str.replace(last_col + \"_\", \"\")\ndf_out.columns = [last_col + \"_\" + str(elem) for elem in unique_elements]\n\n# Output the new dataframe\n",
        "\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Fit model:\nsvmmodel=suppmach.LinearSVC()\ncalibrated_svm = CalibratedClassifierCV(svmmodel, cv=5)\ncalibrated_svm.fit(X, y)\n\n# Predict probabilities:\nproba = calibrated_svm.predict_proba(x_test)[:, 1]\n",
        "\nfrom sklearn.calibration import CalibratedClassifierCV\n\nmodel = svm.LinearSVC()\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\n\npredicted_test = calibrated_model.predict(x_predict)\npredicted_test_scores = calibrated_model.decision_function(x_predict)\n\nproba = 1 / (1 + np.exp(-predicted_test_scores))\n",
        "\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\n\n# [Missing]\n\ndf = pd.concat([df_origin, pd.DataFrame(transform_output.todense())], axis=1)\n\n",
        "\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\n\n# [Missing]\n\ndf = pd.DataFrame(transform_output.todense(), columns=df_origin.columns)\ndf = pd.concat([df_origin, df], axis=1)\n\n",
        "\nfrom scipy.sparse import csr_matrix\n\ndf_origin, transform_output = load_data()\n\ndef solve(df, transform_output):\n    # Convert transformed output to dense numpy array\n    transform_output = transform_output.toarray()\n    \n    # Concatenate transformed output with original dataframe\n    result = pd.concat([df, pd.DataFrame(transform_output)], axis=1)\n    \n    return result\n\ndf = solve(df_origin, transform_output)\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n\n# Solution\nsteps = clf.named_steps()\ndel steps['poly']\nclf = Pipeline(steps.items())\n\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())]\nclf = Pipeline(estimators)\n\n# Solution:\n\n# To delete a step, we can use the `del` keyword.\ndel clf.steps[1]\n\n# To insert a step, we can use the `insert` method.\nclf.steps.insert(1, ('new_pca', PCA()))\n\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n\n# [Missing]\n\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n\n# [Missing]\n\nclf.steps.insert(1, ('poly2', PolynomialFeatures()))\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())]\nclf = Pipeline(estimators)\n\n# [Missing]\n\n",
        "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n\n# [Missing]\n\n",
        "\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\ndef load_data():\n    # Load data here\n    # Return gridsearch, testX, testY, trainX, trainY\n    pass\n\ngridsearch, testX, testY, trainX, trainY = load_data()\n\n# [Missing]\n\ngridsearch.fit(trainX, trainY, early_stopping_rounds=42, eval_metric=\"mae\", eval_set=[(testX, testY)])\n\nb = gridsearch.score(trainX, trainY)\nc = gridsearch.predict(trainX)\n",
        "\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\ndef load_data():\n    # load data here\n    # return gridsearch, testX, testY, trainX, trainY\n    pass\n\ngridsearch, testX, testY, trainX, trainY = load_data()\n\n# [Missing]\n\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n\ngridsearch.fit(trainX, trainY, **fit_params)\n\nb = gridsearch.score(trainX, trainY)\nc = gridsearch.predict(trainX)\n",
        "\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\n\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict_proba(X_test)\n    proba.append(y_pred)\n\nproba = np.concatenate(proba, axis=0)\n",
        "\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\n\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict_proba(X_test)[:, 1]\n    proba.append(y_pred)\n\nproba = np.concatenate(proba)\n",
        "\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_df = pd.DataFrame({'t': [1, 2, 3, 4, 5], 'f1': [1, 2, 3, 4, 5], 'f2': [1, 2, 3, 4, 5]})\n\nscaler = StandardScaler()\nscaler.fit(train_df['t'].values.reshape(-1, 1))\ntrain_df['t'] = scaler.transform(train_df['t'].values.reshape(-1, 1))\n\n# [Missing]\n\ninversed = scaler.inverse_transform(predicted_t)\n",
        "\n# [Missing]\ninversed = scaler.inverse_transform(predicted)\n",
        "\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Create a list of models\nmodels = [LinearRegression()]\n\n# Create a sample dataset\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\n\n# Loop through each model and calculate the cross-validation score\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5)\n    model_name = model.__class__.__name__\n    print(f'Name model: {model_name} , Mean score: {scores.mean()}')\n",
        "\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n\n# Solution\nmodel_name = model.__class__.__name__\n\n",
        "\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\n\n# create some data\nX = np.random.rand(100, 5)\ny = np.random.randint(2, size=100)\n\n# create a list of models\nmodels = [LinearSVC(C=0.1, random_state=0),\n          LinearSVC(C=1, random_state=0),\n          LinearSVC(C=10, random_state=0)]\n\n# create a dataframe to store the results\nresults = pd.DataFrame(columns=['Model', 'Mean Score'])\n\n# loop over the models and calculate their cross-validation scores\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5)\n    # [Missing]\n    # add the model name and mean score to the results dataframe\n    results = results.append({'Model': model_name, 'Mean Score': scores.mean()}, ignore_index=True)\n\n# print the results dataframe\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\n\ndata = load_data()\n\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\n\n# [Missing]\n\n",
        "\n# [Missing]\n\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nfrom sklearn.metrics import accuracy_score\n\n# [Missing]\n\nclf = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef preprocess(s):\n    return s.upper()\n\n# [Missing]\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n",
        "\nfrom sklearn import preprocessing\n\ndata = pd.read_csv(\"lala.csv\", delimiter=\",\")\n\n# [Missing]\n\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n\n",
        "\nfrom sklearn import preprocessing\n\n# Load data\ndata = load_data()\n\n# Convert DataFrame to numpy array\ndata_array = data.values\n\n# Apply preprocessing.scale\nscaled_array = preprocessing.scale(data_array)\n\n# Convert numpy array back to DataFrame\ndf_out = pd.DataFrame(scaled_array, columns=data.columns, index=data.index)\n\n",
        "\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"model\", RidgeClassifier(random_state=24))\n])\ngrid = GridSearchCV(pipe, param_grid={\"model__alpha\": [2e-4, 3e-3, 4e-2, 5e-1]}, cv=7)\ngrid.fit(X, y)\n\n# [Missing]\n\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# [Missing]\n\ncolumn_names = X.columns[model.get_support()]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# [Missing]\n\ncolumn_names = X.columns[model.get_support()]\n\n",
        "\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# [Missing]\n\ncolumn_names = X.columns[model.get_support()]\n",
        "\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\n\n# Fit the k-means model\nkm.fit(X)\n\n# Find the 50 samples closest to the p^th center\nclosest_50_samples = km.kneighbors(km.cluster_centers_[p], n_neighbors=50, return_distance=False)\n\n",
        "\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\n\n# Fit the data to KMeans\nkm.fit(X)\n\n# Get the index of the p^th center\np_center_index = km.cluster_centers_.argsort()[p-1]\n\n# Get the 50 samples closest to the p^th center\nclosest_50_samples = X[km.labels_ == p_center_index][:50]\n\n",
        "\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import euclidean_distances\n\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\nkm.fit(X)\n\n# Find the index of the p^th center\np_index = np.where(km.labels_ == p)[0][0]\n\n# Calculate the distance between each sample and the p^th center\ndistances = euclidean_distances(X, km.cluster_centers_[p_index])\n\n# Sort the distances and return the indices of the 100 samples with the smallest distances\nclosest_100_samples = np.argsort(distances)[:100]\n\n",
        "\ndef get_samples(p, X, km):\n    km.fit(X)\n    center = km.cluster_centers_[p]\n    distances = np.linalg.norm(X - center, axis=1)\n    indices = np.argsort(distances)[:50]\n    samples = X[indices]\n    return samples\n",
        "\n# One hot encoding for categorical variable\nX_train = pd.get_dummies(X_train)\n\n# Merge back with original data\nX_train = pd.concat([X_train, pd.DataFrame(X_train[0])], axis=1)\nX_train.drop(0, axis=1, inplace=True)\n\n# Model\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n",
        "\nfrom sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# load data in the example\nX_train, y_train = load_data()\n\n# one-hot encoding for categorical variables\nX_train = pd.get_dummies(X_train, columns=[0])\n\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import load_boston\n\n# Load the Boston housing dataset\nX, y = load_boston(return_X_y=True)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVR model with a gaussian kernel\nsvr = SVR(kernel='rbf')\n\n# Fit the model to the training data\nsvr.fit(X_train, y_train)\n\n# Predict the test data\ny_pred = svr.predict(X_test)\n\n# Print the mean squared error (MSE)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\n# define the model\nmodel = SVR(kernel='rbf')\n\n# define the hyperparameters to tune\nparam_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n\n# define the grid search\ngrid_search = GridSearchCV(model, param_grid, cv=5)\n\n# fit the grid search\ngrid_search.fit(X, y)\n\n# get the best hyperparameters\nbest_params = grid_search.best_params_\n\n# define the model with the best hyperparameters\nmodel = SVR(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'])\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\n\n# Generate sample data\nX, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n\n# Create SVR model with polynomial kernel\nmodel = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Predict the output for a new input\nnew_input = np.array([[50]])\npredict = model.predict(new_input)\n\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\nsvr_poly = SVR(kernel='poly', C=1e3, gamma=0.1)\nsvr_poly.fit(X_poly, y)\n\nX_test = np.array([[1, 2], [3, 4], [5, 6]])\nX_test_poly = poly.transform(X_test)\npredict = svr_poly.predict(X_test_poly)\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef load_data():\n    # Load data from file or database\n    queries = ['query1', 'query2', 'query3']\n    documents = ['document1', 'document2', 'document3', 'document4', 'document5']\n    return queries, documents\n\ndef get_term_frequency_inverse_data_frequency(documents):\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(documents)\n    return matrix\n\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    query_matrix = tfidf.transform([query])\n    cosine_similarities = np.dot(tfidf.transform(documents), query_matrix.T).toarray()\n    return cosine_similarities\n\nqueries, documents = load_data()\ntfidf = TfidfVectorizer()\ntfidf.fit_transform(documents)\n\ncosine_similarities_of_queries = []\nfor query in queries:\n    cosine_similarities = get_tf_idf_query_similarity(documents, query)\n    max_similarity = np.max(cosine_similarities)\n    max_index = np.argmax(cosine_similarities)\n    most_similar_document = documents[max_index]\n    cosine_similarities_of_queries.append((query, most_similar_document, max_similarity))\n\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef load_data():\n    # Load data from file or database\n    queries = [\"query1\", \"query2\", \"query3\"]\n    documents = [\"document1\", \"document2\", \"document3\", \"document4\", \"document5\"]\n    return queries, documents\n\ndef get_term_frequency_inverse_data_frequency(documents):\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(documents)\n    return matrix\n\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = np.dot(tfidf, query_tfidf.T).toarray()\n    return cosine_similarities\n\nqueries, documents = load_data()\ntfidf = get_term_frequency_inverse_data_frequency(documents)\n\n# [Missing]\n\ncosine_similarities_of_queries = []\nfor query in queries:\n    cosine_similarities = get_tf_idf_query_similarity(documents, query)\n    cosine_similarities_of_queries.append(cosine_similarities)\n\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\n\ndef get_term_frequency_inverse_data_frequency(documents):\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(documents)\n    return matrix\n\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = np.dot(tfidf, query_tfidf.T).toarray()\n    return cosine_similarities\n\ndef solve(queries, documents):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    cosine_similarities_of_queries = []\n    for query in queries:\n        query_tfidf = tfidf.transform([query])\n        cosine_similarities = np.dot(tfidf, query_tfidf.T).toarray()\n        cosine_similarities_of_queries.append(cosine_similarities)\n    return cosine_similarities_of_queries\n\ncosine_similarities_of_queries = solve(queries, documents)\n",
        "\nfeatures = load_data()\n\n# Convert the features to a 2D-array\nnew_features = np.array(features)\n\n",
        "\nnew_f = np.array(f).transpose()\n",
        "\nfeatures = load_data()\n\n# Convert the features to a 2D-array\nnew_features = np.array(features)\n\n",
        "\nfeatures = load_data()\ndef solve(features):\n    # Convert the features to a 2D-array\n    new_features = np.array(features)\n    # Transpose the array to get the samples as rows and features as columns\n    new_features = new_features.T\n    return new_features\nnew_features = solve(features)\n",
        "\n\nfeatures = load_data()\n\n# Convert the list of features to a 2D-array\nnew_features = np.array(features)\n\n",
        "\n\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# [Missing]\n\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\ncluster_labels = model.fit_predict(data_matrix)\n\n",
        "\n\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# [Missing]\n\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = model.fit_predict(data_matrix)\n\n",
        "\n\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# [Missing]\n\n# Create a linkage matrix\nZ = linkage(simM, 'ward')\n\n# Plot dendrogram\nplt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()\n\n# Create an AgglomerativeClustering object with 2 clusters\nagglom = sklearn.cluster.AgglomerativeClustering(n_clusters=2, linkage='ward')\n\n# Fit the model to the data\nagglom.fit(simM)\n\n# Get the cluster labels\ncluster_labels = agglom.labels_\n\n",
        "\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndata_matrix = [[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]]\n\n# perform hierarchical clustering on the data\nZ = linkage(data_matrix, 'single')\n\n# find the number of clusters (2) using fcluster\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n\n",
        "\n\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Perform hierarchical clustering\nZ = scipy.cluster.hierarchy.linkage(data_matrix, method='ward')\n\n# Determine number of clusters\nT = scipy.cluster.hierarchy.fcluster(Z, 2, criterion='maxclust')\n\n# Assign cluster labels\ncluster_labels = pd.Series(T, index=data_matrix.index)\n\n",
        "\n\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Perform hierarchical clustering on the similarity matrix\nZ = scipy.cluster.hierarchy.linkage(simM, method='average')\n\n# Determine the number of clusters based on the dendrogram\nplt.figure(figsize=(10, 7))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\nscipy.cluster.hierarchy.dendrogram(Z, labels=['fruit1', 'fruit2', 'fruit3'])\n\n# Find the optimal number of clusters using the elbow method\nfrom sklearn.cluster import KMeans\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    km.fit(simM)\n    distortions.append(km.inertia_)\nplt.plot(range(1, 11), distortions, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()\n\n# Determine the optimal number of clusters\nk = 2\n\n# Perform the clustering\nkmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\nkmeans.fit(simM)\n\n# Assign cluster labels to the similarity matrix\ncluster_labels = kmeans.labels_\n",
        "\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n\n",
        "\nfrom sklearn.preprocessing import StandardScaler\n\ndata = load_data()\nassert type(data) == np.ndarray\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\n\ntransformer = PowerTransformer(method='box-cox')\nbox_cox_data = transformer.fit_transform(data)\n",
        "\nfrom scipy.stats import boxcox\n\ndata = load_data()\nassert type(data) == np.ndarray\n\n# Apply Box-Cox transformation to the data\nbox_cox_data, _ = boxcox(data)\n\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\n\ndata = load_data()\nassert type(data) == np.ndarray\n\n# [Missing]\n\n# Create a PowerTransformer object with method=\"yeo-johnson\"\npt = PowerTransformer(method=\"yeo-johnson\")\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\n\n# Load data\ndata = load_data()\n\n# Check if data is a numpy array\nassert type(data) == np.ndarray\n\n# Create PowerTransformer object\npt = PowerTransformer(method='yeo-johnson')\n\n# Fit and transform data\nyeo_johnson_data = pt.fit_transform(data)\n\n# Print transformed data\n",
        "\nfrom sklearn.feature_extraction.text import CountVectorizer\ntext = load_data()\n\nvectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 1), max_features=10000, strip_accents='unicode',\n                             stop_words='english', preprocessor=lambda x: x.replace(\"!\", \"\").replace(\"?\", \"\").replace('\"', \"\").replace(\"'\", \"\"))\n\ntransformed_text = vectorizer.fit_transform(text)\n\n",
        "\nfrom sklearn.model_selection import train_test_split\n\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n\n# Splitting the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndata = load_data()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n\n# Print the split data\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.5, random_state=42)\n\n# Split the training and testing sets into x and y variables\nx_train = x_train.values\ny_train = y_train.values\nx_test = x_test.values\ny_test = y_test.values\n",
        "\nfrom sklearn.model_selection import train_test_split\n\ndef solve(data):\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n",
        "\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndf = pd.read_csv(\"generate_csv/all_data_device.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\n# generate another list\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n# Centroid values\ncentroids = kmeans.cluster_centers_\n#print(centroids)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(X[:, 0], X[:, 1], c=labels)\nax.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='#050505', s=1000)\nplt.title('K Mean Classification')\nplt.show()\n",
        "\nfrom sklearn.cluster import KMeans\n\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n\n# [Missing]\n\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_indices = featureSelector.get_support(indices=True)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import Lasso\n\ncorpus, y = load_data()\nassert type(corpus) == list\nassert type(y) == list\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X, y)\n\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[lasso.coef_ != 0]\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_indices = featureSelector.get_support(indices=True)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n\n# Sort the feature names in alphabetical order\nfeature_names = sorted(feature_names)\n\n# Reorder the columns of the X matrix according to the sorted feature names\nX = pd.DataFrame(X.toarray(), columns=feature_names)\n\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n\n# Sort the feature names in alphabetical order\nfeature_names = sorted(feature_names)\n\n# Reorder the feature matrix X according to the sorted feature names\nX = pd.DataFrame(X.toarray(), columns=feature_names)\n\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n\n# [Missing]\n\nX = pd.DataFrame(X.toarray(), columns=feature_names)\nX = X.reindex(sorted(X.columns), axis=1)\nX = X.values\n\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n\n# [Missing]\n\n# print(feature_names)\n# print(X.toarray())\n\n# print(X)\n",
        "\nfrom sklearn.linear_model import LinearRegression\n\ndf1 = load_data()\n\nslopes = []\n\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    slopes.append(slope.coef_[0])\n\n",
        "\nfrom sklearn.linear_model import LinearRegression\n\ndf1 = pd.read_csv('data.csv')\n\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('titanic.csv')\n\n# [Missing]\n\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('data.csv')\n\n# [Missing]\n\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv('titanic.csv')\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\nfrom sklearn import linear_model\nfrom sklearn.datasets import load_boston\n\n# Load the Boston housing dataset\nboston = load_boston()\n\n# Split the data into training and testing sets\nX_train, y_train = boston.data[:400], boston.target[:400]\nX_test, y_test = boston.data[400:], boston.target[400:]\n\n# Create an Elastic Net regression instance\nElasticNet = linear_model.ElasticNet()\n\n# Fit the data to the Elastic Net regression instance\nElasticNet.fit(X_train, y_train)\n\n# Calculate the R^2 score for the training set\ntraining_set_score = ElasticNet.score(X_train, y_train)\n\n# Calculate the R^2 score for the test set\ntest_set_score = ElasticNet.score(X_test, y_test)\n\n# Print the R^2 scores\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\n# Reshape the numpy array to a 2D array\nnp_array = np.reshape(np_array, (np_array.shape[0]*np_array.shape[1], 1))\n\n# Create a MinMaxScaler object and fit and transform the numpy array\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n\n# Reshape the transformed numpy array back to the original shape\ntransformed = np.reshape(transformed, (np_array.shape[0], np_array.shape[1]))\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\n# Reshape the numpy array into a 1D array\nnp_array_1d = np_array.reshape(-1, 1)\n\n# Create a MinMaxScaler object and fit and transform the data\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array_1d)\n\n# Reshape the transformed array back into a 3D array\ntransformed = transformed.reshape(3, 3)\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\n# Reshape the numpy array into a 1D array\nnp_array = np_array.reshape(-1, 1)\n\n# Create a MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Fit and transform the numpy array\ntransformed = scaler.fit_transform(np_array)\n\n# Reshape the transformed array back into a 2D array\ntransformed = transformed.reshape(2, 2)\n\n",
        "\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n\npredict = clf.predict(b)\n",
        "\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create sample data\nX = [['asdf', '1'], ['asdf', '0']]\ny = ['2', '3']\n\n# Convert string data to numerical data using one-hot encoding\ndf = pd.DataFrame(X, columns=['feature1', 'feature2'])\ndf = pd.get_dummies(df)\nX = df.values\n\n# Train the model\nclf = DecisionTreeClassifier()\nclf.fit(X, y)\n",
        "\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# create sample data\nX = [['asdf', '1'], ['asdf', '0']]\ny = ['2', '3']\n\n# encode string input\nle = LabelEncoder()\nle.fit(X)\nnew_X = le.transform(X)\n\n# train classifier\nclf = DecisionTreeClassifier()\nclf.fit(new_X, y)\n",
        "\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# create sample data\nX = [['dsa', '2'], ['sato', '3']]\ny = ['4', '5']\n\n# encode string data\nle = LabelEncoder()\nnew_X = [[le.fit_transform(x[0]), le.fit_transform(x[1])] for x in X]\n\n# train model\nclf = DecisionTreeClassifier()\nclf.fit(new_X, y)\n",
        "\n# Reshaping the data to fit the model\nX = dataframe.iloc[-1:].values.reshape(-1, 1)\ny = dataframe.iloc[:,-1]\n\nlogReg = LogisticRegression()\n\nlogReg.fit(X, y)\n\n# Predicting the output for the given input\npredict = logReg.predict(X)\n",
        "\nX = dataframe.iloc[-1:].astype(float)\n",
        "\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n\n# Split the data to train and test set in a way that the test set have to be newer than the train set\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, random_state=42)\n\n# Sort the dataframes by date\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\n\n",
        "\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n\ntrain_dataframe = train_dataframe.sort_values(by=['date'])\ntest_dataframe = test_dataframe.sort_values(by=['date'])\n",
        "\nfrom sklearn.model_selection import train_test_split\n\ndef solve(features_dataframe):\n    # Sort the data by date\n    features_dataframe = features_dataframe.sort_values(by=['date'])\n\n    # Split the data to train and test set\n    train_size = 0.8\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, random_state=42)\n\n    # Sort the train and test set by date\n    train_dataframe = train_dataframe.sort_values(by=['date'])\n    test_dataframe = test_dataframe.sort_values(by=['date'])\n\n    return train_dataframe, test_dataframe\n\n# Load the data\nfeatures_dataframe = load_data()\n\n# Solve the problem\ntrain_dataframe, test_dataframe = solve(features_dataframe)\n\n# Print the results\n",
        "\n# [Missing]\n\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].apply(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\n",
        "\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x.values.reshape(-1,1)))\n",
        "\ncount = CountVectorizer(lowercase = False)\n\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase = False)\n\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n\n",
        "\nfrom sklearn.model_selection import GridSearchCV\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\n\n# Get the full results of GridSearchCV as a pandas dataframe\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n\n# Print the full results\n",
        "\nfrom sklearn.model_selection import GridSearchCV\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\n\n# [Missing]\n\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\n",
        "\n\n# Save the model in the file named \"sklearn_model\"\njoblib.dump(fitted_model, 'sklearn_model.pkl')\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndf = pd.DataFrame({'items': ['1fgg', '2hhj', '3jkl'], 'description': ['abcd ty', 'abc r', 'r df']})\n\ntfidf = TfidfVectorizer()\n\n# Tokenize each description\ndf['description'] = df['description'].apply(lambda x: x.split())\n\n# Transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# Calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = 1 - sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix)\n\n"
    ],
    "Pytorch": [
        "\n\n# Define the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Train the model\nfor epoch in range(10):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        output = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n        \n        # Change the learning rate dynamically\n        if epoch == 5:\n            optimizer.set_lr(0.001)\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\n\n# create an optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n\n# train the model\nfor epoch in range(10):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        output = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n        # [Missing] change the learning rate to 0.0005\n        optimizer.set_lr(0.0005)\n",
        "\n\n# load data\ntrain_data = load_data()\n\n# define optimizer\noptim = torch.optim.SGD(model.parameters(), lr=0.005)\n\n# define scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=10, verbose=True)\n\n# train model\nfor epoch in range(100):\n    # train\n    train_loss = train_model(model, optim, train_data)\n    \n    # update scheduler\n    scheduler.step(train_loss)\n    \n    # update learning rate\n    for param_group in optim.param_groups:\n        param_group['lr'] *= 0.5\n",
        "\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n\n# Load input data\ninput_Tensor = load_data()\n\n# Load pre-trained word2vec embedding\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n# Get embedding weights from gensim\nembedding_weights = np.zeros((len(word2vec.wv.vocab), word2vec.vector_size))\nfor i, word in enumerate(word2vec.wv.vocab):\n    embedding_weights[i] = word2vec.wv[word]\n\n# Convert embedding weights to PyTorch tensor\nembedding_weights_tensor = torch.from_numpy(embedding_weights).float()\n\n# Create embedding layer\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights_tensor)\n\n# Embed input data using embedding layer\nembedded_input = embedding_layer(input_Tensor)\n\n",
        "\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\ninput_Tensor = load_data()\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\ndef get_embedded_input(input_Tensor):\n    # [Missing]\n    # Convert input_Tensor to a list of sentences\n    sentences = []\n    for i in range(len(input_Tensor)):\n        sentence = []\n        for j in range(len(input_Tensor[i])):\n            sentence.append(input_Tensor[i][j])\n        sentences.append(sentence)\n    # Get the embedding weights from gensim\n    embedding_weights = np.zeros((len(word2vec.wv.vocab), word2vec.vector_size))\n    for i, word in enumerate(word2vec.wv.vocab):\n        embedding_weights[i] = word2vec.wv[word]\n    # Create a PyTorch embedding layer with the embedding weights\n    embedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_weights))\n    # Embed the input data using the embedding layer\n    embedded_input = embedding_layer(torch.LongTensor(sentences))\n    return embedded_input\n\nembedded_input = get_embedded_input(input_Tensor)\n",
        "\nx = torch.rand(4,4)\n\n# Convert torch tensor to numpy array\nx_np = x.numpy()\n\n# Convert numpy array to pandas dataframe\npx = pd.DataFrame(x_np)\n\n",
        "\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n",
        "\nx = torch.rand(6,6)\n\n# Convert torch tensor to numpy array\nx_np = x.numpy()\n\n# Convert numpy array to pandas dataframe\npx = pd.DataFrame(x_np)\n\n",
        "\n\nC = torch.LongTensor([[1, 3], [4, 6]])\n# 1 3\n# 4 6\n\nA_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\n\n# Solution:\nC = B[:, A_log.bool()] # Convert A_log to boolean tensor and use it as a logical index\n",
        "\n\nA_logical = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical] # Throws error\n\n",
        "\nC = B[:, A_log] # Throws error\n",
        "\n\nC = torch.LongTensor([[1, 3], [4, 6]])\n# 1 3\n# 4 6\n\nA_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\n\n# Solution:\nC = B[:, A_log.bool()] # Convert the logical index to a boolean tensor and use it for slicing\n\n",
        "\n\nA_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\n",
        "\n\nA_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\n\n",
        "\n\nidx, B = load_data()\n\n# [Missing]\nC = torch.index_select(B, dim=1, index=idx)\n\n",
        "\n\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\n\nx_tensor = torch.from_numpy(x_array)\n\n",
        "\nx_array = load_data()\n\n# Convert numpy array of dtype=object to torch tensor\nx_tensor = torch.from_numpy(np.array(x_array, dtype=np.float64))\n\n",
        "\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\n\ndef Convert(a):\n    t = torch.tensor(a, dtype=torch.float16)\n    return t\n\nx_tensor = Convert(x_array)\n",
        "\n\n# Load data\nlens = [3, 5, 4]\n\n# Convert to tensor\nlens = torch.LongTensor(lens)\n\n# Create mask\nmax_len = lens.max().item()\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n\n",
        "\n\nlens = [1, 9, 3, 5]\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\n\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n\n",
        "\n\nlens = [3, 5, 4]\n\n# Solution\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n\n",
        "\n\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\n\nlens = [3, 5, 4]\nmask = get_mask(lens)\n",
        "\n\nTensor_2D = load_data()\n\n# Create diagonal matrix from 2D tensor\ndiag_ele = torch.diag(Tensor_2D)\n\n# Repeat diagonal matrix along batch dimension\nTensor_3D = diag_ele.unsqueeze(0).repeat(index_in_batch, 1, 1)\n\n",
        "\n\nTensor_2D = load_data()\n\ndef Convert(t):\n    batch_size = t.shape[0]\n    diag_ele = t.shape[1]\n    result = torch.zeros(batch_size, diag_ele, diag_ele)\n    for i in range(batch_size):\n        result[i] = torch.diag(t[i])\n    return result\n\nTensor_3D = Convert(Tensor_2D)\n",
        "\na, b = load_data()\n\n# [Missing]\nab = torch.cat((a, b), dim=0)\n\n",
        "\na, b = load_data()\n\nab = torch.cat((a,b),0)\n\n",
        "\na, b = load_data()\ndef solve(a, b):\n    # [Missing]\n    ab = torch.cat((a, b), dim=0)\n    # [Missing]\n    return ab\nab = solve(a, b)\n",
        "\na[ : , lengths : , : ]  = 0\n",
        "\na[ : , lengths : , : ]  = 2333\n",
        "\na[ : , : lengths , : ]  = 0\n",
        "\n\na = torch.rand((10, 1000, 23))\nlengths = torch.randint(1000, (10,))\n\n# create mask tensor\nmask = (torch.arange(a.shape[1]) < lengths.unsqueeze(1)).unsqueeze(2).expand_as(a)\n\n# set values before specified index to 2333\na[mask] = 2333\n\n",
        "\n\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n\n",
        "\n\nlist = [ torch.randn(3), torch.randn(3), torch.randn(3)]\nnew_tensors = torch.stack(list)\n\n",
        "\nlist_of_tensors = load_data()\ndef Convert(lt):\n    tt = torch.stack(lt)\n    return tt\ntensor_of_tensors = Convert(list_of_tensors)\n",
        "\n\nlist_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n\n",
        "\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\n\nresult = t[idx]\n\n",
        "\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\n\nresult = t[idx].view(-1)\n\n",
        "\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\n\nresult = t[idx]\n",
        "\n\ndef load_data():\n    ids = torch.tensor([[1],[0],[2],...])\n    x = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]],...])\n    return ids, x\n\nids, x = load_data()\n\n# [Missing]\n\nresult = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1, -1, x.shape[-1]))\n\n",
        "\n\ndef load_data():\n    ids = torch.tensor([[2],[1],[0],...])\n    x = torch.randn(30,3,114)\n    return ids, x\n\nids, x = load_data()\n\n# [Missing]\n\nresult = x.gather(1,ids.unsqueeze(-1).expand(-1,-1,x.shape[-1]))\n\n",
        "\n\ndef load_data():\n    ids = np.array([[0,1,0],[1,0,0],[0,0,1],...])\n    x = np.random.rand(70,3,2)\n    return ids, x\n\nids, x = load_data()\n\n# Solution\n\nresult = np.zeros((70,2))\nfor i in range(70):\n    max_index = np.argmax(ids[i])\n    result[i] = x[i][max_index]\n\n",
        "\nsoftmax_output = load_data()\n\n# [Missing]\n\ny = torch.argmax(softmax_output, dim=1)\n\n",
        "\nsoftmax_output = load_data()\n\n# Create a tensor with the index of the maximum value in each row\ny = torch.argmax(softmax_output, dim=1).view(-1, 1)\n\n",
        "\nsoftmax_output = load_data()\n\n# [Missing]\n\ny = torch.argmin(softmax_output, dim=1)\n\n",
        "\nsoftmax_output = load_data()\ndef solve(softmax_output):\n    y = torch.argmax(softmax_output, dim=1)\n    return y\ny = solve(softmax_output)\n",
        "\nsoftmax_output = load_data()\ndef solve(softmax_output):\n    # convert softmax_output to a tensor\n    softmax_output = torch.tensor(softmax_output)\n    # get the index of the minimum value along the last dimension\n    y = torch.argmin(softmax_output, dim=-1)\n    # convert y to a tensor of type Long\n    y = y.long()\n    return y\ny = solve(softmax_output)\n",
        "\nloss = F.cross_entropy(input=images, target=labels, weight=None, size_average=True, ignore_index=-100, reduce=True)\n",
        "\n\nA, B = load_data()\n\n# Count the number of equal elements in the two tensors\ncnt_equal = np.count_nonzero(A == B)\n\n",
        "\nA, B = load_data()\n\n# Count the number of equal elements in A and B\ncnt_equal = np.count_nonzero(A == B)\n\n",
        "\n\nA, B = load_data()\n\n# Count the number of elements that are not equal\ncnt_not_equal = np.count_nonzero(A != B)\n\n",
        "\nA, B = load_data()\ndef Count(A, B):\n    cnt_equal = np.sum(A == B)\n    return cnt_equal\ncnt_equal = Count(A, B)\n",
        "\nA, B = load_data()\n\n# Solution\ncnt_equal = np.count_nonzero(A[-x:] == B[-x:])\n\n",
        "\n\nA = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\nB = np.array([[1, 2, 3, 4, 6], [7, 8, 9, 10, 11]])\n\nx = 3\n\ncnt_not_equal = 0\nfor i in range(A.shape[0]):\n    for j in range(A.shape[1]-x, A.shape[1]):\n        if A[i][j] != B[i][j]:\n            cnt_not_equal += 1\n\n",
        "\na = load_data()\nassert a.shape == (1, 3, 10, 40, 1)\nchunk_dim = 10\n\n# Split the tensor into smaller tensors with a step of 1\ntensors_31 = torch.chunk(a, 31, dim=3)\n\n# Edit the last tensor to have a size of 10\ntensors_31[-1] = tensors_31[-1][:, :, :, :10, :]\n\n# Print the shapes of the tensors\nfor tensor in tensors_31:\n    print(tensor.shape)\n",
        "\na = load_data()\nassert a.shape == (1, 3, 10, 40, 1)\nchunk_dim = 10\n\n# Split the tensor into smaller tensors with a step of 1 along the third dimension\ntensors_31 = []\nfor i in range(0, 40, 10):\n    tensor = a[:, :, i:i+chunk_dim, :, :]\n    tensors_31.append(tensor)\n\n# Concatenate the tensors along the fourth dimension\na_split = torch.cat(tensors_31, dim=3)\n\n# Check the shape of the resulting tensor\nassert a_split.shape == (1, 3, 10, 31, 1)\n\n# Print the resulting tensor\n",
        "\nmask, clean_input_spectrogram, output= load_data()\n\noutput[mask==1] = clean_input_spectrogram[mask==1]\n\n",
        "\nmask, clean_input_spectrogram, output= load_data()\n\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n\n",
        "\n\nx, y = load_data()\n\n# [Missing]\n\nsigned_min = torch.where(torch.abs(x) < torch.abs(y), x, y)\nsigned_min = torch.where(torch.abs(signed_min) == torch.abs(x), x, signed_min)\nsigned_min = torch.where(torch.abs(signed_min) == torch.abs(y), y, signed_min)\nsigned_min = torch.where(torch.sign(x) == torch.sign(y), signed_min, -signed_min)\n\n",
        "\nx, y = load_data()\n\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.where(torch.abs(x) == max, sign_x, torch.where(torch.abs(y) == max, sign_y, 0))\n\n",
        "\nx, y = load_data()\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = torch.where(torch.abs(x) < torch.abs(y), x, y)\n    signed_min = torch.where(torch.abs(signed_min) == min, signed_min * sign_x, signed_min * sign_y)\n    return signed_min\nsigned_min = solve(x, y)\n",
        "\nMyNet = torch.nn.Sequential(torch.nn.Linear(4, 15),\n                            torch.nn.Sigmoid(),\n                            torch.nn.Linear(15, 3),\n                            )\nMyNet.load_state_dict(torch.load(\"my_model.pt\"))\ninput = load_data()\nassert type(input) == torch.Tensor\n\n# [Missing]\n\nconfidence_score = torch.nn.functional.softmax(output, dim=1)[:,1]\n\n",
        "\na, b = load_data()\n\n# Merge the two tensors by side-by-side\nresult = torch.cat((a, b), dim=1)\n\n# Calculate the average of the last column of 'a' and the first column of 'b'\navg = (a[:, -1] + b[:, 0]) / 2\n\n# Insert the average column into the result tensor\nresult = torch.cat((result, avg.unsqueeze(1)), dim=1)\n\n",
        "\na, b = load_data()\ndef solve(a, b):\n    # concatenate tensors along the columns\n    c = torch.cat((a, b), dim=1)\n    # create a mask to select the overlapping elements\n    mask = torch.ones_like(c)\n    mask[:, -b.shape[1]:] = 0\n    # calculate the average of the overlapping elements\n    c[:, -b.shape[1]:] = (a[:, -1:] + b[:, :1]) / 2\n    # multiply the mask with the concatenated tensor\n    result = c * mask\n    return result\nresult = solve(a, b)\n",
        "\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[0., 0., 0.,0.]])\nr = torch.cat([t,new], dim=0)  # stacking along the first dimension\n",
        "\nt = torch.arange(4).reshape(1,2,2).float()\nnew=torch.tensor([[0., 0., 0.,0.]])\nr = torch.cat([t,new], dim=0)  # adding new tensor to the top of the existing tensor\n",
        "\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[-1, -1, -1, -1,]])\nr = torch.cat([t,new], dim=0)  # stack along the first dimension\n",
        "\n\nhid_dim = 32\ndata = torch.randn(10, 2, 3, hid_dim)\ndata = data.view(10, 2 * 3, hid_dim)\nW = torch.randn(hid_dim)\n\n# Multiply data with weights\nresult = torch.bmm(data.view(10, 2, 3, 1, hid_dim), W.view(1, 1, 1, hid_dim, 1)).squeeze()\n\n# Reshape the result to match the shape of the original data\nresult = result.view(10, 2, 3)\n\n"
    ]
}