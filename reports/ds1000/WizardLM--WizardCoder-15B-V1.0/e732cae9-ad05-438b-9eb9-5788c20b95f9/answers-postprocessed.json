{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\nresult = df.iloc[List].reset_index(drop=True)\n",
        "\n# Create a new column 'others' for Qu1 column\ndf['others'] = df['Qu1'].apply(lambda x: 'other' if x not in ['cheese', 'potato', 'banana'] else x)\n\n# Replace Qu1 values according to value_counts()\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df[df['Qu1'] == x]) >= 2 else x)\n\n# Replace Qu2 values according to value_counts()\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df[df['Qu2'] == x]) >= 2 else x)\n\n# Replace Qu3 values according to value_counts()\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df[df['Qu3'] == x]) >= 2 else x)\n",
        "\n# Create a new column 'others' for Qu1, Qu2, Qu3\ndf['others'] = df.apply(lambda x: 'other' if pd.value_counts(x) < 3 else x, axis=1)\n",
        "\n    # Create a new column 'others' for Qu1 column\n    df['others'] = df['Qu1'].apply(lambda x: 'other' if x not in ['cheese', 'potato', 'banana'] else x)\n    # Replace Qu1 values with 'others' where value count great or equal 2\n    df['Qu1'] = df['Qu1'].apply(lambda x: 'others' if pd.value_counts(df[df['Qu1'] == x]) >= 2 else x)\n    # Replace Qu2 values with 'others' where value count great or equal 2\n    df['Qu2'] = df['Qu2'].apply(lambda x: 'others' if pd.value_counts(df[df['Qu2'] == x]) >= 2 else x)\n    # Replace Qu3 values with 'others' where value count great or equal 2\n    df['Qu3'] = df['Qu3'].apply(lambda x: 'others' if pd.value_counts(df[df['Qu3'] == x]) >= 2 else x)\n    ",
        "\n# Create a new column 'others' for Qu1\ndf['others'] = df['Qu1'].apply(lambda x: 'other' if x not in ['cheese', 'potato', 'banana', 'apple', 'egg'] else x)\n\n# Create a new column 'others' for Qu2 and Qu3\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'others' if x not in ['banana', 'apple', 'sausage'] else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'others' if x not in ['cheese', 'potato', 'sausage'] else x)\n",
        "\n# Create a new column 'others' in Qu1 dataframe\ndf['others'] = df['Qu1'].apply(lambda x: 'other' if x in ['potato', 'banana', 'apple', 'egg'] else x)\n\n# Replace values in Qu1 dataframe according to value_counts() when value count great or equal 3\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'cheese' if pd.value_counts(df['Qu1']) >= 3 else x)\n\n# Replace values in Qu2 and Qu3 dataframe according to value_counts() when value count great or equal 2\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df['Qu2']) >= 2 else x)\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df['Qu3']) >= 2 else x)\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first', inplace=False)\nresult = result[result['drop_if_dup'] == 'No']\n",
        "\nresult = df.drop_duplicates(subset='url', keep='last', inplace=False)\nresult = result[result['keep_if_dup'] == 'Yes']\n",
        "\n\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\n\nresult = {}\nfor name, group in df.groupby('name'):\n    result[name] = {}\n    for v1, group1 in group.groupby('v1'):\n        result[name][v1] = {}\n        for v2, v3 in zip(group1['v2'], group1['v3']):\n            result[name][v1][v2] = v3\n\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    ",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n# Extract the key value pairs from the message column\ndf['message'] = df['message'].str.replace('[', '').str.replace(']', '').str.split(',', expand=True)\ndf[['job', 'money', 'wife']] = df['message'].str.split(': ', expand=True)\ndf['wife'] = df['wife'].str.replace('none', '')\ndf['money'] = df['money'].str.replace('none', '')\ndf['job'] = df['job'].str.replace('none', '')\ndf['group'] = df['message'].str.extract(r'group: (.*)', expand=False)\ndf['kids'] = df['message'].str.extract(r'kids: (.*)', expand=False)\ndf = df.drop(columns=['message'])\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10\n",
        "\nresult = df.loc[~df['product'].isin(products), 'score'] *= 10\n",
        "\nfor product_group in products:\n    mask = (df['product'].isin(product_group))\n    df.loc[mask, 'score'] = df.loc[mask, 'score'] * 10\n",
        "\n# Min-Max Normalize scores corresponding to products 1069104 and 1069105\ndf.loc[df['product'].isin(products), 'score'] = (df.loc[df['product'].isin(products), 'score'] - df.loc[df['product'].isin(products), 'score'].min()) / (df.loc[df['product'].isin(products), 'score'].max() - df.loc[df['product'].isin(products), 'score'].min())\n",
        "\n# Convert the binary columns into a single categorical column\ndf['category'] = df.apply(lambda x: ''.join(x.astype(int).astype(str)), axis=1)\n",
        "\n\ndf = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\n\n# Convert binary columns to boolean values\ndf = df.astype('bool')\n\n# Create categorical column using pd.get_dummies()\nresult = pd.get_dummies(df)\n\n# Print result\n",
        "\n# Convert binary columns to categorical column of lists\ndf['category'] = df.apply(lambda x: [col for col, val in x.items() if val == 1], axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.to_period(\"M\")\n",
        "\ndf['Date'] = df['Date'].dt.to_period(\"M\")\n",
        "\ndf = df[(df['Date'] >= '2019-01-17') & (df['Date'] <= '2019-02-20')]\ndf['Date'] = df['Date'].dt.to_period(\"M\")\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%d-%b-%Y'))\ndf['Day'] = df['Date'].apply(lambda x: x.split('-')[0])\ndf['Month'] = df['Date'].apply(lambda x: x.split('-')[1])\ndf['Year'] = df['Date'].apply(lambda x: x.split('-')[2])\ndf['Day'] = df['Day'].astype(int)\ndf['Month'] = df['Month'].apply(lambda x: x.title())\ndf['Year'] = df['Year'].astype(int)\ndf['Day'] = df['Day'].apply(lambda x: str(x).zfill(2))\ndf['Date'] = df['Day'] + '-' + df['Month'] + '-' + df['Year']\ndf['Day'] = df['Date'].apply(lambda x: pd.to_datetime(x).strftime('%A'))\ndf = df[['Date', 'Day']]\n",
        "\n# Shift the first row of the first column down 1 row\ndf.iloc[0,0] = df.iloc[1,0]\n# Shift the last row of the first column to the first row, first column\ndf.iloc[0,0] = df.iloc[-1,0]\n",
        "\n# Shift the last row of the first column up 1 row\ndf.iloc[-1,0] = df.iloc[-2,0]\n# Shift the first row of the first column to the last row, first column\ndf.iloc[0,0] = df.iloc[-1,0]\n",
        "\n# Shift the first row of the first column down 1 row\ndf.iloc[0,0] = df.iloc[1,0]\n# Shift the last row of the first column up 1 row\ndf.iloc[-1,0] = df.iloc[-2,0]\n# Shift the last row of the second column up 1 row\ndf.iloc[-1,1] = df.iloc[-2,1]\n# Shift the first row of the second column down 1 row\ndf.iloc[0,1] = df.iloc[1,1]\n",
        "\ndf.iloc[0] = df.iloc[1]\ndf.iloc[-1] = df.iloc[0]\n",
        "\ndf.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX'}, inplace=True)\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.rename(columns=lambda x: 'X' + x if x[-1] != 'X' else x, inplace=True)\n",
        "\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\n\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", [[Missing]]})\n\n",
        "\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\n\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", [[Missing]]})\n\n",
        "\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val42':[1,1,4,5,1] })\n\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"mean\"})\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].mean(axis=0)\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].sum(axis=0)\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list,column_list].sum(axis=0)\nresult = result.drop(result.idxmax())\n\n",
        "\nresult = df.apply(lambda x: x.value_counts(), axis=0)\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = ''\nfor col in df.columns:\n    if df[col].nunique() > 1:\n        result += f'---- {col} ---\\n'\n        result += str(df[col].value_counts()) + '\\n'\n",
        "\nresult = df.head().combine_first(df.iloc[[0]])\n",
        "\nresult = df.head().combine_first(df.iloc[[0]])\n",
        "\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+[np.nan]*len(x[x.isnull()]))[:len(x)],1)\n",
        "\nresult = df.apply(lambda x : pd.Series(np.where(x.isnull(),x.mean(),x)),axis=1)\n",
        "\nresult = df.apply(lambda x : pd.concat([x[x.isnull()],x[x.notnull()]],axis=1).values.tolist(),0)\n",
        "\nresult = df.loc[df['value'] < thresh].sum()\nresult.name = 'X'\nresult = result.to_frame()\n",
        "\nresult = df.loc[df['value'] >= thresh].groupby(level=0).mean().reset_index()\nresult.loc[result['value'] < thresh, 'value'] = df.loc[df['value'] < thresh]['value'].mean()\n",
        "\n# Create a new dataframe with only the rows that are not in the given section\ndf_not_in_section = df[(df.index < section_left) | (df.index > section_right)]\n\n# Calculate the average value of the rows that are not in the given section\navg_value = df_not_in_section['value'].mean()\n\n# Create a new row with the average value and label it as 'X'\nnew_row = pd.DataFrame({'value':[avg_value]}, index=['X'])\n\n# Concatenate the new row with the original dataframe\nresult = pd.concat([df_not_in_section, new_row])\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    result[f\"inv_{col}\"] = 1/df[col]\n",
        "\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A\": [e**1, e**2, e**3], \"exp_B\": [e**4, e**5, e**6]})\n\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    if df[col].sum() != 0:\n        result[f\"inv_{col}\"] = 1/df[col]\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    result[f\"sigmoid_{col}\"] = 1/(1+pd.np.exp(-df[col]))\n",
        "\nresult = df.idxmin().apply(lambda x: df.index[df[x] == df[x].min()][-1])\n",
        "\nresult = df.idxmin(axis=0).where(df.idxmin(axis=0) != df.idxmax(axis=0), df.idxmax(axis=0))\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['a']*len(pd.date_range(min_dt, max_dt)) + ['b']*len(pd.date_range(min_dt, max_dt)), 'val': [0]*len(pd.date_range(min_dt, max_dt))})\nresult = result.merge(df, on=['dt', 'user'], how='left')\nresult['val'] = result['val'].fillna(0)\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['abc']*len(pd.date_range(min_dt, max_dt)), 'val': [0]*len(pd.date_range(min_dt, max_dt))})\nresult = pd.merge(result, df, how='left', on=['dt', 'user'])\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['a']*len(pd.date_range(min_dt, max_dt)) + ['b']*len(pd.date_range(min_dt, max_dt)), 'val': [233]*len(pd.date_range(min_dt, max_dt))})\nresult = result.merge(df, on=['dt', 'user'], how='left')\nresult['val'] = result['val'].fillna(233)\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': df['user'].unique(), 'val': df['val'].max()})\nresult = result.merge(df, on=['dt', 'user'], how='left')\nresult = result.fillna(method='ffill')\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt), 'user': ['a']*len(pd.date_range(min_dt, max_dt)), 'val': [df[df['user'] == 'a']['val'].max()]*len(pd.date_range(min_dt, max_dt))})\n",
        "\n# Create a new column 'id' with unique IDs for each name\ndf['id'] = df['name'].astype('category').cat.codes + 1\n",
        "\n# Create a new dataframe with unique IDs for each a\nunique_ids = pd.DataFrame({'a': df['a'].unique(), 'id': range(1, len(df['a'].unique())+1)})\n\n# Merge the new dataframe with the original dataframe on a\nresult = pd.merge(df, unique_ids, on='a')\n\n# Rename the 'id' column to 'a'\nresult = result.rename(columns={'id': 'a'})\n",
        "\n    # Create a new column 'id' with unique IDs for each name\n    df['id'] = df['name'].astype('category').cat.codes\n    ",
        "\n# Create a new dataframe with unique IDs for each name and a\ndf_new = pd.DataFrame({'ID': df['name'].astype('category').cat.codes,\n                       'b': df['b'],\n                       'c': df['c']})\n",
        "\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n\n# pivot_table function to repartition the date columns into two columns date and value\nresult = pd.pivot_table(df, values=['01/12/15', '02/12/15'], index=['user'], columns=['someBool'], aggfunc='first')\n\n# rename the columns\nresult.columns = ['date', 'value']\n\n# concatenate the two columns into one column\nresult['date'] = result['date'].astype(str) + result['value'].astype(str)\nresult = result.drop('value', axis=1)\n\n",
        "\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n\n# pivot_table is a pandas function that can be used to rearrange the columns of a dataframe\n# pivot_table takes the values of the '01/12/15' and '02/12/15' columns and rearranges them into two new columns 'others' and 'value'\n# the 'others' column contains the values of the columns that are not '01/12/15' or '02/12/15'\n# the 'value' column contains the values of the '01/12/15' or '02/12/15' column\nresult = pd.pivot_table(df, values=['01/12/15', 'someBool'], index=['user'], columns=['02/12/15'], aggfunc='first')\n\n# the result dataframe has two new columns 'others' and 'value'\n# the 'others' column contains the values of the columns that are not '01/12/15' or '02/12/15'\n# the 'value' column contains the values of the '01/12/15' or '02/12/15' column\n",
        "\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, None],\n                   '02/12/15': [None, -100, 200],\n                   'someBool': [True, False, True]})\n\n# pivot_table is a pandas function that can be used to repartition the date columns into two columns date and value\nresult = pd.pivot_table(df, values=['01/12/15', '02/12/15'], index=['user'], columns=['someBool'], aggfunc='first')\n\n# The missing code is to add the date column to the result dataframe\nresult['date'] = ['01/12/15', '01/12/15', '02/12/15', '02/12/15']\n\n",
        "\nresult = df[df.c > 0.5][columns].values\n",
        "\nresult = df[df.c > 0.45][columns].values\n",
        "\n\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    return result.values\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    result['sum'] = result.apply(lambda x: x.sum(), axis=1)\n    return result\n",
        "\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs]\n    return result\n",
        "\nfrom datetime import timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 120\n\nfilter_dates = []\nfor index, row in df.iterrows():\n    if X == 0:\n        filter_dates.append(index)\n    else:\n        for i in range(1, X):\n            filter_dates.append((index.date() + timedelta(days=i)))\n\nresult = df[~df.index.isin(filter_dates)]\n\n",
        "\nfrom datetime import timedelta\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n\nfilter_dates = []\nfor index, row in df.iterrows():\n    if X == 1:\n        filter_dates.append(index)\n    else:\n        for i in range(1, X):\n            filter_dates.append((index.date() + timedelta(weeks=i)))\n\nresult = df[~df.index.isin(filter_dates)]\n\n",
        "\nresult = df.copy()\nresult['date'] = pd.to_datetime(result['date'], format='%m/%d/%y')\nresult = result.sort_values(by='date')\n\nobservation_time = 'M'\nobservation_period = X\n\nfilter_dates = []\nfor index, row in result.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(days=i)))\n    elif observation_time == 'M':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(months=i)))\n    else:\n        print('Invalid observation time')\n\nresult = result[~result.index.isin(filter_dates)]\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 3).agg({'col1': 'sum'})\n",
        "\nresult = df.groupby(df.index // 4).sum()\n",
        "\nresult = df.rolling(3).mean()\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 3):\n    if i+2 <= len(df):\n        result = result.append({'col1': df.iloc[i:i+3]['col1'].sum()}, ignore_index=True)\n    if i+1 <= len(df):\n        result = result.append({'col1': df.iloc[i:i+2]['col1'].mean()}, ignore_index=True)\n",
        "\nresult = pd.DataFrame()\nfor i in range(0, len(df), 3):\n    if i+2 >= len(df):\n        break\n    result = result.append({'col1': df.iloc[i:i+3].sum()['col1']}, ignore_index=True)\n    result = result.append({'col1': df.iloc[i:i+2].mean()['col1']}, ignore_index=True)\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Filling the zeros with the previous non-zero value using pandas\ndf = df.fillna(method='ffill')\n\nresult = df\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Filling the zeros with the posterior non-zero value using pandas\ndf = df.fillna(method='ffill')\n\nresult = df\n",
        "\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Fill zeros with the maximun between previous and posterior non-zero value using pandas\ndf = df.fillna(method='ffill').fillna(method='bfill')\n\nresult = df\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\ndf['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n",
        "\ndf['numer'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n",
        "\n    df['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\n    df['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\n    df['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n    ",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\D+)', expand=False)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\ndf['time_day'] *= df['number'].astype(int)\n",
        "\nresult = np.where([df1[column] != df2[column] for column in columns_check_list])\n",
        "\n\ncolumns_check_list = ['A','B','C','D','E','F']\n\ncheck = np.where([df1[column] == df2[column] for column in columns_check_list])\n\nresult = [True] * len(check[0])\n\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "```python\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_datetime64_any_dtype\nfrom pandas.api.types import is_numeric_dtype\nfrom pandas.api.types import is_categorical_dtype\nfrom pandas.api.types import is_object_dtype\nfrom pandas.api.types import is_bool_dtype\nfrom pandas.api.types import is_integer_dtype\nfrom pandas.api.types import is_float_dtype\nfrom pandas.api.types import is_datetime64_dtype\nfrom pandas.api.types import is_timedelta64_dtype\nfrom pandas.api.types import is_period_dtype\nfrom pandas.api.types import is_interval_dtype\nfrom pandas.api.types import is_sparse\nfrom pandas.api.types import is_list_like\nfrom pandas.api.types import is_dict_like\nfrom pandas.api.types import is_iterator\nfrom pandas.api.types import is_tuple\nfrom pandas.api.types import is_hashable\nfrom pandas.api.types import is_named_tuple\nfrom pandas.api.types import is_file_like\nfrom pandas.api.types import is_valid_dtype\nfrom pandas.api.types import pandas_dtype\nfrom pandas.api.types import infer_dtype\nfrom pandas.api.types import is_dtype_equal\nfrom pandas.api.types import is_dtype_constructible\nfrom pandas.api.types import is_datetimelike\nfrom pandas.api.types import is_datetimelike_v_numeric\nfrom pandas.api.types import is_datetimelike_v_object\nfrom pandas.api.types import is_numeric_v_object\nfrom pandas.api.types import is_datetime64_ns_dtype\nfrom pandas.api.types import is_datetime64_dtype\nfrom pandas.api.types import is_datetime64tz_dtype\nfrom pandas.api.types import is_timedelta64_dtype\nfrom pandas.api.types import is_timedelta64_ns_dtype\nfrom pandas.api.types import is_timedelta64tz_dtype\nfrom pandas.api.types import is_period_dtype\nfrom pandas.api.types import is_interval_dtype\nfrom pandas.api.types import is_extension_type\nfrom pandas.api.types import is_extension_array_dtype\nfrom pandas.api.types import is_extension_array\nfrom pandas.api.types import is_sparse\nfrom pandas.api.types import is_list_like\nfrom pandas.api.types import is_dict_like\nfrom pandas.api.types import is_iterator\nfrom pandas.api.types import is_tuple\nfrom pandas.api.types import is_hashable\nfrom pandas.api.types import is_named_tuple\nfrom pandas.api.types import is_file_like\nfrom pandas.api.types import is_valid_dtype\nfrom pandas.api.types import pandas_dtype\nfrom pandas.api.types import infer_dtype\nfrom pandas.api.types import is_dtype_equal\nfrom pandas.api.types import is_dtype_constructible\nfrom pandas.api.types import is_datetimelike\nfrom pandas.api.types import is_datetimelike_v_numeric\nfrom pandas.api.types import is_datetimelike_v_object\nfrom pandas.api.types import is_numeric_v_object\nfrom pandas.api.types import is_datetime64_ns_dtype\nfrom pandas.api.types import is_datetime64_dtype\nfrom pandas.api.types import is_datetime64tz_dtype\nfrom pandas.api.types import is_timedelta64_dtype\nfrom pandas.api.types import is_timedelta64_ns_dtype\nfrom pandas.api.types import is_timedelta64tz_dtype\nfrom pandas.api.types import is_period_dtype\nfrom pandas.api.types import is_interval_dtype\nfrom pandas.api.types import is_extension_type\nfrom pandas.api.types import is_extension_array_dtype\nfrom pandas.api.types import is_extension_array\nfrom pandas.api.types import is_sparse\nfrom pandas.api.types import is_list_like\nfrom pandas.api.types import is_dict_like\nfrom pandas.api.types import is_iterator\nfrom pandas.api.types import is_tuple\nfrom pandas.api.types import is_hashable\nfrom pandas.api.types import is_named_tuple\nfrom",
        "\ndef f(df):\n    df.index = pd.MultiIndex.from_tuples([(pd.to_datetime(date), id) for date, id in df.index], names=['date', 'id'])\n    df = df.swaplevel(0, 1)\n    return df\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'],\n             var_name='year', value_name='value')\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'],\n             var_name='year', value_name='var1')\ndf = df.sort_values(['year'], ascending=False)\n",
        "\nresult = df[df.apply(lambda x: abs(x['Value_B']) < 1 and abs(x['Value_C']) < 1 and abs(x['Value_D']) < 1, axis=1)]\n",
        "\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\nresult = df[abs(df[['Value_B', 'Value_C', 'Value_D']]).max(axis=1) > 1]\n\n",
        "\ndf = df.loc[abs(df[['Value_B', 'Value_C', 'Value_D']]).max(axis=1) > 1]\ndf.columns = [col.replace('Value_', '') for col in df.columns]\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n",
        "\ndf['A'] = df['A'].str.replace('&LT', '<')\n",
        "\n    df['A'] = df['A'].str.replace('&AMP;', '&')\n    ",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['A'] = df['A'].str.replace('&LT;', '<')\ndf['A'] = df['A'].str.replace('&GT;', '>')\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\ndf['first_name'] = df['name'].apply(lambda x: x.split()[0] if validate_single_space_name(x) else x)\ndf['last_name'] = df['name'].apply(lambda x: x.split()[1] if validate_single_space_name(x) else None)\n\nresult = df[['first_name', 'last_name']]\n",
        "\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\ndf['1_name'] = df['name'].apply(lambda x: x.split()[0] if validate_single_space_name(x) else x)\ndf['2_name'] = df['name'].apply(lambda x: x.split()[1] if validate_single_space_name(x) else '')\n\nresult = df[['1_name', '2_name']]\n",
        "\ndf['name'] = df['name'].apply(lambda x: validate_single_space_name(x))\ndf = df[df['name'].notna()]\ndf['first_name'] = df['name'].apply(lambda x: x.split()[0])\ndf['middle_name'] = df['name'].apply(lambda x: x.split()[1] if len(x.split()) > 2 else None)\ndf['last_name'] = df['name'].apply(lambda x: x.split()[-1])\ndf = df[['first_name', 'middle_name', 'last_name']]\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\nresult = pd.merge_asof(df1, df2, on='Timestamp')\n",
        "\ndf['state'] = df[['col1', 'col2', 'col3']].apply(lambda x: x.max() if x.min() <= 50 else x['col1'], axis=1)\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] > 50 and x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'], axis=1)\n",
        "\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n\nerrors = []\nfor index, row in df.iterrows():\n    if not isinstance(row[\"Field1\"], int):\n        errors.append(row[\"Field1\"])\n\nresult = errors\n",
        "\ninteger_list = []\nfor index, row in df.iterrows():\n    if row[\"Field1\"].isnumeric():\n        integer_list.append(int(row[\"Field1\"]))\n",
        "\n\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    result = []\n    for index, row in df.iterrows():\n        if not isinstance(row[\"Field1\"], int):\n            result.append(row[\"Field1\"])\n    return result\n",
        "\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Compute the row total for each category\nrow_total = df.groupby('cat').sum()\n\n# Compute the percentage of each value for each category\nresult = df.apply(lambda x: x / row_total.loc[x.name] * 100, axis=1)\n\n# Print the result\n",
        "\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Compute the total of each column\ntotal = df.groupby('cat').sum()\n\n# Compute the percentage of each value\nresult = df.div(total, axis=0)\n\n# Output the result\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\ndf = df.drop(test)\n",
        "\n    result = df.loc[test]\n    ",
        "\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\ndef get_nearest_neighbour(df):\n    df['euclidean_distance'] = df.groupby('time')['x'].apply(lambda x: pd.np.sqrt((x - x.shift())**2 + (df['y'] - df['y'].shift())**2))\n    df['nearest_neighbour'] = df['euclidean_distance'].idxmin(axis=1)\n    return df[['car', 'nearest_neighbour', 'euclidean_distance']]\n\nresult = get_nearest_neighbour(df)\n",
        "\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Calculate pairwise distances between cars\ndf['distance'] = df.groupby('car')['x'].transform(lambda x: x.diff().abs().sum())\n\n# Get the farmost neighbour for each car\ndf2 = df.groupby(['time', 'car'])['distance'].agg(['max', 'idxmax']).reset_index()\ndf2.columns = ['time', 'car', 'farmost_neighbour', 'euclidean_distance']\n\n# Calculate the average distance for each time point\nresult = df2.groupby('time')['euclidean_distance'].mean().reset_index()\nresult.columns = ['time', 'average_distance']\n\n",
        "\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nresult = df\n",
        "\n# Select 20% of rows using df.sample(n) and set random_state=0\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set the Quantity column of the selected rows to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the selected rows\nselected_indexes = sample_df.index\n\n# Create a new DataFrame with the selected rows and the original DataFrame\nresult = pd.concat([df, sample_df])\n\n# Drop the selected rows from the result DataFrame\nresult = result.drop(selected_indexes)\n",
        "\n# Select 20% of rows using df.sample(n) and set random_state=0\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the ProductId column of these rows to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nresult = df.loc[sample_df.index]\n",
        "\n# Select 20% of rows for each user using df.sample(n) and set random_state=0\nsampled_rows = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# Set Quantity of selected rows to zero\nsampled_rows['Quantity'] = 0\n\n# Keep the indexes of the selected rows\nselected_indexes = sampled_rows.index\n\n# Create a new DataFrame with the selected rows\nresult = df.loc[selected_indexes]\n",
        "\n\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\n\nresult = duplicate\n",
        "\n\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\n\nresult = duplicate\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.index.values\n    ",
        "\n\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\n\nresult = duplicate[['val', 'col1', 'col2', '3col', 'index_original']]\n\n",
        "\n\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index.values\n\nresult = duplicate\n\n",
        "\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\nresult = df.groupby(['Sp','Mt'])['count'].agg(['max']).reset_index()\nresult = df[df['count']==result['max']]\n\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\nresult = df.groupby(['Sp','Mt'])['count'].agg(['min']).reset_index()\nresult = df[df['count']==result['min']]\n\n",
        "\n\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n\nresult = df.groupby(['Sp','Value']).apply(lambda x: x.loc[x['count'].idxmax()])\n\n",
        "\nresult=df.query(\"Category in @filter_list\")\n",
        "\nresult=df.query(\"Category not in filter_list\")\n",
        "\nresult = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                ('A', 'B', 'F'),\n                                ('A', 'C', 'G'),\n                                ('A', 'C', 'H'),\n                                ('A', 'D', 'I'),\n                                ('A', 'D', 'J')])\n",
        "\n# Create a list of tuples where each tuple contains the column levels to melt\nvalue_vars = [(col1, col2, col3) for col1 in df.columns.levels[0] for col2 in df.columns.levels[1] for col3 in df.columns.levels[2]]\n\n# Melt the DataFrame using the list of tuples as value_vars\nresult = pd.melt(df, id_vars=df.columns.names, value_vars=value_vars)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum())\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].transform(pd.Series.cumsum)\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n",
        "\ndf['cummax'] = df.groupby('id')['val'].transform(pd.Series.cummax)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.where(x >= 0, 0).cumsum())\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('r')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('l')['v'].apply(np.sum, skipna=False)\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            ct = pd.crosstab(df[col1], df[col2])\n            if ct.shape[0] == 1 and ct.shape[1] == 1:\n                result.append(f'{col1} {col2} one-to-one')\n            elif ct.shape[0] == 1 and ct.shape[1] > 1:\n                result.append(f'{col1} {col2} one-to-many')\n            elif ct.shape[0] > 1 and ct.shape[1] == 1:\n                result.append(f'{col1} {col2} many-to-one')\n            else:\n                result.append(f'{col1} {col2} many-to-many')\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            merged = pd.merge(df[[col1]], df[[col2]], how='inner')\n            if len(merged) == len(df):\n                result.append(f'{col1} {col2} one-2-many')\n            elif len(merged) == 0:\n                result.append(f'{col1} {col2} many-2-many')\n            else:\n                result.append(f'{col1} {col2} many-2-one')\n",
        "\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor i in df.columns:\n    for j in df.columns:\n        if i == j:\n            result.loc[i, j] = 'one-to-one'\n        elif df[i].nunique() == df[j].nunique():\n            result.loc[i, j] = 'one-to-many'\n        elif df[i].nunique() == df.shape[0]:\n            result.loc[i, j] = 'many-to-one'\n        else:\n            result.loc[i, j] = 'many-to-many'\n",
        "\nresult = pd.crosstab(df.columns, df.columns)\n",
        "\n# remove the dupes that don't have an bank account\ndfiban_uniq = dfiban_uniq[dfiban_uniq['bank'].notna()]\n",
        "\n\n# Set the locale to the user's default setting\nlocale.setlocale(locale.LC_ALL, '')\n\n# Create a sample DataFrame\ndf = pd.DataFrame({'Revenue': ['2,144.78', '2,036.62', '1,916.60', '1,809.40', '1,711.97', '6,667.22', '5,373.59', '4,071.00', '3,050.20', '-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0'],\n                   'Other, Net': ['-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0', '-0.8', '-1.12', '1.32', '-0.05', '-0.34', '-1.37', '-1.9', '-1.48', '0.1', '41.98', '35', '-11.66', '27.09', '-3.44', '14.13', '-18.69', '-4.87', '-5.7']})\n\n# Convert the Revenue column to float using pd.to_numeric\ndf['Revenue'] = pd.to_numeric(df['Revenue'].str.replace(',', ''), errors='coerce')\n\n# Print the resulting DataFrame\n",
        "\nresult = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0)).mean()['Survived']\n",
        "\nresult = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0)).mean()['SibSp']\n",
        "\nresult = df.groupby((df['SibSp'] == 1) & (df['Parch'] == 1), as_index=False).mean()['Survived']\nresult = result.append(df.groupby((df['SibSp'] == 0) & (df['Parch'] == 0), as_index=False).mean()['Survived'])\nresult = result.append(df.groupby((df['SibSp'] == 0) & (df['Parch'] == 1), as_index=False).mean()['Survived'])\nresult = result.append(df.groupby((df['SibSp'] == 1) & (df['Parch'] == 0), as_index=False).mean()['Survived'])\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(['A']))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values(['A']))\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\n\nl = [('A', '1', 'a'),  ('A', '1', 'b'), ('A', '2', 'a'), ('A', '2', 'b'), ('B', '1','a'),  ('B', '1','b')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\n\n# Create a MultiIndex object with the desired column headers\ncols = pd.MultiIndex.from_tuples([('Caps', 'Middle', 'Lower'), ('A', '1', 'a'), ('A', '1', 'b'), ('A', '2', 'a'), ('A', '2', 'b'), ('B', '1', 'a'), ('B', '1', 'b')])\n\n# Assign the MultiIndex object to the columns of the DataFrame\ndf.columns = cols\n\n# Output the resulting DataFrame\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\ndf.columns = df.columns.set_levels(['Caps', 'Middle', 'Lower'], level=0)\ndf.columns = df.columns.set_levels(['A', 'B'], level=1)\ndf.columns = df.columns.set_levels(['a', 'b'], level=2)\ndf.columns = df.columns.set_names(['Caps', 'Middle', 'Lower', 'index'])\n",
        "\n\nnp.random.seed(123)\nbirds = np.random.choice(['African Swallow', 'Dead Parrot', 'Exploding Penguin'], size=int(5e4))\nsomeTuple = np.unique(birds, return_counts=True)\n\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n\n",
        "\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('a')['b'].apply(stdMeann))\n\n",
        "\n\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\n\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('b')['a'].apply(stdMeann))\n\n",
        "\n# Calculate softmax and min-max normalization for each group\nresult = df.groupby('a').agg({'b': ['softmax', 'min-max']})\n",
        "\nresult = df.loc[(df['A'] == 1) & (df['B'] == 1) | (df['C'] == 0) & (df['D'] == 1), ['A', 'B', 'D']]\n",
        "\nresult = df.loc[(df['A']+df['B']+df['C']+df['D']==0),['A','B','D']]\n",
        "\nresult = df[df.max(axis=1) == 2]\nresult = result.drop(result.columns[result.max() == 0], axis=1)\n",
        "\nresult = df.where(df != 2, 0)\n",
        "\n\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n          index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n\nresult = s.sort_values(ascending=True)\n\n",
        "\ndf = s.to_frame().reset_index().sort_values(['index', 1], ascending=[True, False])\n",
        "\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n\n",
        "\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n\n",
        "\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\nresult = df.groupby(['Sp','Mt'])['count'].agg(['max']).reset_index()\nresult = df[df['count']==result['max']]\n\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].agg(max).reset_index()\nresult = df.merge(result, on=['Sp','Mt','count'], how='inner')\n",
        "\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\nresult = df.groupby(['Sp','Mt'])['count'].agg(['min']).reset_index()\nresult = df[df['count']==result['min']]\n\n",
        "\n\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n\nresult = df.groupby(['Sp','Value']).apply(lambda x: x.loc[x['count'].idxmax()])\n\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Member'])\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Member'])\n",
        "\n    result = df.fillna(df['Member']).copy()\n    for key, value in dict.items():\n        result.loc[result['Member'] == key, 'Date'] = value\n    ",
        "\ndf['Date'] = df['Date'].fillna(df['Member'].apply(lambda x: dict.get(x, '17/8/1926')))\n",
        "\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\ndf1.reset_index(inplace=True)\ndf1.rename(columns={'count': 'Count_m'}, inplace=True)\ndf1['Count_y'] = df1['year']\ndf1['Count_d'] = df1['Count_m']\ndf1['Date'] = pd.to_datetime(df1['year'].astype(str) + '-' + df1['month'].astype(str) + '-01')\ndf1 = df1[['Date', 'Val', 'Count_d', 'Count_m', 'Count_y']]\n",
        "\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\ndf1['Count_Val'] = df1.Val.map(df.groupby(['Date','Val']).size())\ndf1['Count_m'] = df1.month.map(df.groupby(['Date','month']).size())\ndf1['Count_y'] = df1.year.map(df.groupby(['Date','year']).size())\ndf1 = df1.reset_index()\ndf1 = df1.rename(columns={'Date': 'Date_m'})\ndf1['Date'] = pd.to_datetime(df1['Date_m'], format='%Y-%m')\ndf1 = df1.drop(columns=['Date_m'])\n",
        "\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\ndf1['Count_Val'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'Val': 'count'})\ndf1['Count_d'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'Date': 'count'})\ndf1['Count_m'] = df1.index.get_level_values('month').map(df1.groupby('month').size())\ndf1['Count_y'] = df1.index.get_level_values('year').map(df1.groupby('year').size())\ndf1['Count_w'] = df['Date'].dt.dayofweek.map(df1.groupby('Date').size())\n",
        "\n# create a new dataframe with the zero and non-zero values for each column for each date\nresult1 = df.groupby('Date').apply(lambda x: x.eq(0).sum())\nresult2 = df.groupby('Date').apply(lambda x: x.ne(0).sum())\n",
        "\n\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n\n# result1: even\nresult1 = df.groupby('Date')[['B', 'C']].apply(lambda x: (x % 2 == 0).astype(int))\n\n# result2: odd\nresult2 = df.groupby('Date')[['B', 'C']].apply(lambda x: (x % 2 != 0).astype(int))\n\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n",
        "\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.sum, np.mean])\n",
        "\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n    'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n    'B' : ['A', 'B', 'C'] * 4,\n    'D' : np.random.randn(12),\n    'E' : np.random.randn(12)\n})\n\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.max, np.min])\n",
        "\n\ndf = dd.read_csv('file.csv')\n\n# Split the 'var2' column into multiple rows using str.split with expand=True\ndf['var2'] = df['var2'].str.split(',', expand=True)\n\n# Concatenate the 'var2' column with the original dataframe\nresult = dd.concat([df.drop('var2', axis=1), df['var2'].stack().str.strip().str.split(',', expand=True).stack()], axis=1)\n\n# Rename the columns\nresult.columns = ['id', 'var1', 'var2']\n\n# Print the result\n",
        "\n\ndf = dd.read_csv('file.csv')\n\n# Split the string in the 'var2' column into multiple rows\ndf['var2'] = df['var2'].str.split(',', expand=True)\n\n# Stack the resulting dataframe vertically\nresult = df.stack().reset_index()\n\n# Rename the columns\nresult.columns = ['var1', 'var2', 'value']\n\n# Set the index\nresult = result.set_index(['var1', 'var2'])\n\n# Print the result\n",
        "\n\ndf = dd.read_csv('file.csv')\n\n# Split the string in the 'var2' column into multiple rows\ndf['var2'] = df['var2'].str.split('-', expand=True)\n\n# Stack the resulting dataframe vertically\nresult = df.stack().reset_index()\n\n# Rename the columns\nresult.columns = ['var1', 'var2', 'value']\n\n# Set the index\nresult = result.set_index(['var1', 'var2'])\n\n# Print the result\n",
        "\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\n",
        "\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nresult = df\n",
        "\ndf['fips'], df['row'] = df['row'].str.split(' ', 1).str\n",
        "\ndf[['fips', 'row']] = df.row.str.split(expand=True)\n",
        "\ndf[['fips', 'medi', 'row']] = df.row.str.split(expand=True)\n",
        "\ndf = df.apply(lambda x: x.where(x != 0).dropna().cumsum() / x.where(x != 0).dropna().cumcount(), axis=1)\n",
        "\ndf = df.apply(lambda x: x.where(x != 0).dropna(), axis=1)\ndf = df.apply(lambda x: x.expanding().mean(), axis=1)\n",
        "\n    result = df.apply(lambda x: x.where(x != 0).mean(), axis=1)\n    ",
        "\ndf = df.apply(lambda x: x.where(x != 0).dropna(), axis=1)\ndf = df.apply(lambda x: x.expanding().mean(), axis=1)\n",
        "\ndf['Label'] = 0\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 1)\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\ndf.iloc[0, df.columns.get_loc('label')] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['Duration'] = df.departure_time.shift(-1) - df.arrival_time\n",
        "\ndf['Duration'] = df.departure_time.shift(-1) - df.arrival_time\n",
        "\nfrom datetime import datetime\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\n# Convert datetime64[ns] to datetime format\ndf['arrival_time'] = df['arrival_time'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S') if x != '0' else x)\ndf['departure_time'] = df['departure_time'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n\n# Calculate time difference in seconds\ndf['Duration'] = df.departure_time.diff().dt.total_seconds()\n\n# Convert datetime format to string format\ndf['arrival_time'] = df['arrival_time'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S') if type(x) == datetime else x)\ndf['departure_time'] = df['departure_time'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\n\n# Select the first row and the second row\nresult = df.iloc[[0,1]]\n\n# Rename the columns\nresult.columns = ['id', 'arrival_time', 'departure_time', 'Duration']\n\n",
        "\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'] == 'one'].shape[0])\n\n",
        "\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'] == 'two'].shape[0])\n\n",
        "\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'gee', 'two', 'three', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'].str.endswith('e')].shape[0])\n\n",
        "\nmin_result = df.index.min()\nmax_result = df.index.max()\n",
        "\nmode_result = df.mode(axis=0).iloc[0][0]\nmedian_result = df.median(axis=0)\n",
        "\n\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\n\nresult = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]\n\n",
        "\nresult = df[~(99 <= df['closing_price'] <= 101)]\n",
        "\ndf1 = df.groupby([\"item\", \"otherstuff\"], as_index=False)[\"diff\"].min()\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n",
        "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n    ",
        "\ndf['Column_x'].fillna(df['Column_x'].quantile(0.25), inplace=True)\ndf['Column_x'].fillna(df['Column_x'].quantile(0.75), inplace=True)\n",
        "\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace= True)\n",
        "\ndf['Column_x'].fillna(0, inplace=True)\ndf['Column_x'].fillna(1, inplace=True)\n",
        "\na_b = pd.concat([a, b], axis=1)\na_b = a_b.apply(lambda x: tuple(x), axis=1)\nresult = pd.DataFrame(a_b.values.tolist(), columns=['one', 'two'])\n",
        "\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nc = pd.DataFrame(np.array([[9, 10],[11, 12]]), columns=['one', 'two'])\n\nresult = pd.DataFrame(columns=['one', 'two'])\n\nfor i in range(len(a)):\n    for j in range(len(b)):\n        row = []\n        for k in range(len(c)):\n            row.append((a.iloc[i]['one'], b.iloc[j]['one'], c.iloc[k]['one']))\n        result.loc[len(result)] = row\n\n",
        "\na_b = pd.DataFrame([[(1, 5), (2, 6)],[(3, 7), (4, 8)],[(np.nan,9),(np.nan,10)]], columns=['one', 'two'])\n",
        "\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack().fillna(0)\n",
        "\ngroups = df.groupby(pd.cut(df.views, bins))\nresult = groups.username.count()\n",
        "\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack().fillna(0)\n",
        "\nresult = pd.DataFrame({'text': ['abc, def, ghi, jkl']})\n",
        "\nresult = pd.DataFrame({'text': ['-'.join(df['text'].tolist())]})\n",
        "\nresult = pd.DataFrame({'text': ['jkl, ghi, def, abc']})\n",
        "\nresult = pd.Series(df['text'].str.cat(sep=\", \"))\n",
        "\nresult = df.iloc[0].str.cat(df.iloc[1:].str[::-1], sep='-')\n",
        "\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\nresult = pd.concat([df1, df2], axis=0)\nresult = result.fillna({'city': 'sh', 'district': 'hp'})\n\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(['id', 'date'])\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby('id').fillna(method='ffill')\n",
        "\n\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(columns=['B_x', 'B_y'])\n\n",
        "\n\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n\nresult = pd.merge(C, D, how='outer', on='A')\nresult['B'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(columns=['B_x', 'B_y'])\n\n",
        "\n\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['duplicated'] = result.apply(lambda x: True if x['A_x'] == x['A_y'] else False, axis=1)\nresult = result[['A_x', 'B_x', 'B_y', 'duplicated']]\nresult.columns = ['A', 'B', 'B_y', 'duplicated']\n",
        "\nresult = df.groupby('user').agg(lambda x: x.tolist())\n",
        "\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nresult = df.groupby('user').agg(lambda x: x.tolist())\n\nresult['amount-time-tuple'] = result.apply(lambda x: sorted(zip(x['time'], x['amount'])), axis=1)\n\nresult.drop(['time', 'amount'], axis=1, inplace=True)\n\n",
        "\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nresult = df.groupby('user').agg(lambda x: x.tolist())\n\nresult = result.reset_index()\n\nresult['amount-time-tuple'] = result.apply(lambda x: sorted(zip(x['amount'], x['time']), reverse=True), axis=1)\n\nresult = result.drop(['amount', 'time'], axis=1)\n\nresult = result.rename(columns={'amount-time-tuple': 'transactions'})\n\n",
        "\ndf_concatenated = pd.DataFrame(columns=range(4))\nfor i in range(len(series)):\n    df_concatenated = pd.concat([df_concatenated, pd.DataFrame(series.iloc[i], columns=range(4))], ignore_index=True)\n",
        "\ndf_concatenated = pd.DataFrame(columns=['name'] + list(range(series.shape[1])), index=series.index)\nfor i, (name, arr) in enumerate(series.iteritems()):\n    df_concatenated.loc[name] = [name] + list(arr)\n",
        "\nresult = []\nfor col in df.columns:\n    if s in col and not col == s:\n        result.append(col)\n",
        "\nresult = df.filter(like=s, axis=1)\n",
        "\n# Create a new dataframe with only the columns that contain the string 'spike'\nresult = df[[col for col in df.columns if s in col]]\n\n# Rename the columns with the string 'spike' to start with 'spike1', 'spike2', etc.\nresult.columns = [f'spike{i+1}' for i in range(len(result.columns))]\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\nresult = df['codes'].apply(pd.Series).fillna(value=pd.np.nan)\n\n",
        "\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\nresult = df['codes'].apply(pd.Series).fillna(value=pd.np.nan)\n\n",
        "\nresult = pd.DataFrame(columns=['code_1', 'code_2', 'code_3'])\nfor i in range(len(df)):\n    codes = df.loc[i, 'codes']\n    if len(codes) == 1:\n        result.loc[i, 'code_1'] = codes[0]\n    elif len(codes) == 2:\n        result.loc[i, 'code_1'] = codes[0]\n        result.loc[i, 'code_2'] = codes[1]\n    elif len(codes) == 3:\n        result.loc[i, 'code_1'] = codes[0]\n        result.loc[i, 'code_2'] = codes[1]\n        result.loc[i, 'code_3'] = codes[2]\n    else:\n        result.loc[i, 'code_1'] = codes[0]\n        result.loc[i, 'code_2'] = codes[1]\n        result.loc[i, 'code_3'] = codes[2]\n",
        "\nfrom ast import literal_eval\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nids = df.loc[0:1, 'col1'].values.tolist()\nresult = []\nfor i in ids:\n    result.extend(i)\n\n",
        "\nresult = reverse_list_str_df_col(df, 'col1')\n",
        "\nresult = ','.join(str(x) for sublist in df['col1'].apply(lambda x: [str(i) for i in x]).tolist() for x in sublist)\n",
        "\npan = df.set_index('Time')\npan = pan.resample('2T').mean()\nresult = pan.reset_index()\n",
        "\npan = df.set_index('Time')\npan = pan.resample('3T').mean()\npan = pan.reset_index()\npan = pan.groupby(pd.Grouper(key='Time', freq='3T')).sum()\npan = pan.interpolate(method='linear')\nresult = pan[['Time', 'Value']]\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\nresult = df[filt]\n",
        "\nresult = df[filt]\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n\nresult = df.apply(lambda x: all(equalp(x[i], x[j]) for j in range(len(x))) for i in range(len(df.columns)))\nresult = df.columns[result.values.astype(bool)]\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n\nresult = []\nfor i in range(2):\n    row = df.iloc[i]\n    same_cols = []\n    for j in range(i+1, df.shape[0]):\n        if all(equalp(row[col], df.iloc[j][col]) for col in df.columns):\n            same_cols.append(j)\n    result.append(same_cols)\n\nresult = pd.Index([df.columns[i] for i in range(df.shape[1]) if all(result[0][j] == i for j in range(len(result[0])))])\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n\nresult = []\nfor i in range(2):\n    row1 = df.iloc[i]\n    row2 = df.iloc[8]\n    diff = []\n    for j in range(10):\n        if not equalp(row1[j], row2[j]):\n            diff.append(df.columns[j])\n    result.append(diff)\n\n",
        "\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n\nresult = []\nfor i in range(df.shape[0]):\n    for j in range(i+1, df.shape[0]):\n        if not equalp(df.iloc[i], df.iloc[j]):\n            result.append([(df.iloc[i][k], df.iloc[j][k]) for k in range(df.shape[1]) if not equalp(df.iloc[i][k], df.iloc[j][k])])\n\n",
        "\nts = df.set_index('Date')['Value'].to_series()\n",
        "\nresult = df.iloc[0]\n",
        "\nresult = df.iloc[0]\n",
        "\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n",
        "\ndf['dogs'] = df['dogs'].fillna(0).round(2)\ndf['cats'] = df['cats'].fillna(0).round(2)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_index(level='time')\n\n",
        "\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_values(by='VIM', ascending=True)\n\n",
        "\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\ndf = df[(df.index < hd1_from) | (df.index > hd1_till)]\n",
        "\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\ndf = df[(df.index < hd1_from) | (df.index > hd1_till)]\n",
        "\nresult = corr[corr > 0.3]\n",
        "\nresult = corr.apply(lambda x: x[x > 0.3])\n",
        "\ndf.columns[-1] = 'Test'\n",
        "\ndf.columns[0] = 'Test'\n",
        "\ngrouped = df.groupby(['bit1', 'bit2', 'bit3', 'bit4', 'bit5']).size().reset_index(name='freq_count')\nresult = df.merge(grouped, on=['bit1', 'bit2', 'bit3', 'bit4', 'bit5'], how='left')\nresult['frequent'] = (result['freq_count'] == 1).astype(int)\n",
        "\n# Group the rows by all columns except the frequent and freq_count columns\ngrouped = df.groupby(df.columns.difference(['frequent', 'freq_count']))\n\n# Find the frequent value in each row and count the number of occurrences of this value\nfrequent_values = grouped.agg({'bit1': 'max', 'bit2': 'max', 'bit3': 'max', 'bit4': 'max', 'bit5': 'max'})\nfrequent_counts = grouped.agg({'bit1': 'count', 'bit2': 'count', 'bit3': 'count', 'bit4': 'count', 'bit5': 'count'})\n\n# Assign the frequent value to the frequent column and count the number of occurrences of this value in the row to the freq_count column\nresult = pd.concat([frequent_values, frequent_counts], axis=1)\nresult.columns = ['frequent', 'freq_count']\n",
        "\n\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4],\n                   'bit6': [3, 0, 5]})\n\n# Find the frequent value in each row\ngrouped = df.groupby(['bit1', 'bit2', 'bit3', 'bit4', 'bit5']).size().reset_index(name='freq_count')\ngrouped['frequent'] = grouped.apply(lambda x: list(x[0:5])[::-1], axis=1)\n\n# Merge the frequent value and count columns with the original dataframe\nresult = pd.merge(df, grouped[['frequent', 'freq_count']], left_index=True, right_index=True)\n\n# Print the result\n",
        "\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\ngroupedFrame = df.groupby([\"id1\",\"id2\"])\naggrFrame = groupedFrame.aggregate(np.mean)\n\nresult = pd.concat([aggrFrame, groupedFrame[\"bar\"].mean().rename(\"bar\")], axis=1)\n\n",
        "\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\ngroupedFrame = df.groupby([\"id1\",\"id2\"])\naggrFrame = groupedFrame.aggregate(np.mean)\n\nresult = aggrFrame.fillna(0)\n\n",
        "\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(columns=['b_col'])\n\n",
        "\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(columns=['a_col'])\n\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nresult = np.nan_to_num(x)\nresult = result.tolist()\n",
        "\na = np.array([1, 0, 3])\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a] = 1\n",
        "\na = np.array([1, 0, 3])\nb = np.zeros((len(a), a.max()+1))\nb[np.arange(len(a)), a] = 1\n",
        "\na = np.array([-1, 0, 3])\nb = np.zeros((len(a), max(a)+1))\nb[np.arange(len(a)), a] = 1\n",
        "\na = np.array([1.5, -0.4, 1.3])\nb = np.zeros((len(a), len(np.unique(a))))\nfor i, val in enumerate(np.unique(a)):\n    b[a == val, i] = 1\n",
        "\na = np.array([[1,0,3], [2,4,1]])\nb = np.zeros((a.shape[0], a.max()+1))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i,a[i,j]] = 1\n",
        "\nresult = np.percentile(a, p)\n",
        "\nA = np.array([1,2,3,4,5,6])\nncol = 2\nB = np.reshape(A, (-1, ncol))\n",
        "\nA = np.array([1,2,3,4,5,6])\nnrow = 3\nB = np.reshape(A, (nrow, -1))\n",
        "\nB = A.reshape(-1, ncol)\n",
        "\nB = A.reshape(-1, ncol)\n",
        "\nresult = np.roll(a, shift)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.empty_like(a)\nfor i in range(a.shape[0]):\n    result[i] = np.roll(a[i], shift[i])\n",
        "\n# Generate the same random array using the seed\nnp.random.seed(42)\nr_old = np.random.randint(3, size=(100, 2000)) - 1\n\n# Generate the same random array using the seed\nnp.random.seed(42)\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.argmin(a, axis=None)\n",
        "\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape, order='F')\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    result = np.unravel_index(np.argmax(a), a.shape)\n    ",
        "\nresult = np.unravel_index(np.argsort(a.flatten())[-2], a.shape)\n",
        "\nz = np.any(np.isnan(a), axis=0)\na = np.delete(a, np.where(z), axis=1)\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = np.array([[10, 20, 30, 40, 50],\n       [ 6,  7,  8,  9, 10]])\npermutation = [0, 4, 1, 3, 2]\n\n# Transpose the array to get the columns in the new order\na_new = np.transpose(a)[permutation]\n\n# Transpose the array back to get the columns in the original order\na_new = np.transpose(a_new)\n\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nresult = np.argmin(a, axis=1)\nresult = np.stack([np.arange(a.shape[0]), result], axis=1)\n",
        "\nresult = np.sin(np.deg2rad(degree))\n",
        "\nresult = np.cos(np.radians(degree))\n",
        "\n\nnumber = np.random.randint(0, 360)\n\nif np.sin(np.deg2rad(number)) > 0:\n    result = 0\nelse:\n    result = 1\n\n",
        "\nresult = np.arcsin(value) * 180 / np.pi\n",
        "\n\nA = np.array([1,2,3,4,5])\nlength = 8\n\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0))\n\n",
        "\n\nA = np.array([1,2,3,4,5])\nlength = 8\n\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0))\n\n",
        "\na **= power\n",
        "\nexample_a = np.arange(4).reshape(2, 2)\ndef f(a = example_a, power = 5):\n    result = a**power\n    return result\n",
        "\nresult = np.gcd(numerator, denominator)\nnumerator //= result\ndenominator //= result\n",
        "\nresult = np.gcd(numerator, denominator)\nnumerator //= result\ndenominator //= result\n",
        "\nresult = np.gcd(numerator, denominator)\nif result == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // result, denominator // result)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(np.maximum(a, b), c)\n",
        "\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n\ndiagonal = np.diag_indices(5)\nresult = a[diagonal[::-1]]\n\n",
        "\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\ndiagonal = np.diag_indices(5, k=-1)\nresult = a[diagonal]\n",
        "\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n",
        "\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i][j])\n    return result\n",
        "\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\n",
        "\nresult = np.fromstring(mystr, dtype=int, sep='')\n",
        "\nresult = np.multiply(a[:, col], multiply_number).cumsum()\n",
        "\nrow_array = a[row, :]\nrow_array = row_array * multiply_number\nresult = np.cumsum(row_array)\n",
        "\nrow_array = a[row]\nrow_array = row_array / divide_number\nresult = np.prod(row_array)\n",
        "\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\nrank = np.linalg.matrix_rank(a)\nresult = a[0:rank, :]\n",
        "\nresult = a.shape[0]\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind_from_stats(amean, avar, anobs, bmean, bvar, bnobs, equal_var=False)\n",
        "\noutput = []\nfor i in range(len(A)):\n    if not any(np.array_equal(A[i], b) for b in B):\n        output.append(A[i])\n",
        "\n# create a set of elements in A\nset_A = set(tuple(row) for row in A)\n# create a set of elements in B\nset_B = set(tuple(row) for row in B)\n# find the symmetric difference of the two sets\noutput = np.asarray(list(set_A.symmetric_difference(set_B)))\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)[::-1]\nc = b[sort_indices]\n",
        "\n# Sort b by the sum of a\nindex_array = np.argsort(np.sum(a, axis=(1, 2)))\nresult = b[index_array]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = a[:, [1, 3]]\n",
        "\nresult = np.delete(a, del_col, axis=1)\n",
        "\na_l = a.tolist()\na_l.insert(pos, element)\na = np.asarray(a_l)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\nexample_a = np.asarray([1,2,3,4])\ndef f(a = example_a, pos=2, element = 66):\n    a_l = a.tolist()\n    a_l.insert(pos, element)\n    a = np.asarray(a_l)\n    return a\n",
        "\nfor i in range(len(pos)):\n    a = np.insert(a, pos[i], element[i], axis=0)\n",
        "\nresult = np.array([np.copy(array) for array in array_of_arrays])\n",
        "\nresult = np.all(np.all(a == a[0], axis=1))\n",
        "\nresult = np.all(np.apply_along_axis(lambda x: np.array_equal(x, a[0]), 0, a))\n",
        "\n    result = np.all(np.all(a == a[0], axis=1))\n    ",
        "\nfrom scipy.integrate import dblquad\n\nx = np.linspace(0, 1, 20)\ny = np.linspace(0, 1, 30)\n\ndef integrand(x, y):\n    return np.cos(x)**4 + np.sin(y)**2\n\nresult, error = dblquad(integrand, 0, 1, lambda x: 0, lambda x: 1)\n\n",
        "\n    result = np.zeros((len(x), len(y)))\n    for i in range(len(x)):\n        for j in range(len(y)):\n            result[i][j] = (np.cos(x[i])**4 + np.sin(y[j])**2)\n    ",
        "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\n",
        "\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\neval = np.array([88, 87, 62])\n\n# normalize grades to sum to 1\ngrades = grades / np.sum(grades)\n\n# generate ECDF function\ndef ecdf(x):\n    return np.cumsum(x)\n\n# apply ECDF function to eval array\nresult = ecdf(grades)[eval]\n\n",
        "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\ndef longest_interval(x, threshold):\n  ecdf_x = ecdf(x)\n  low = 0\n  high = len(x)\n  while high - low > 1:\n    mid = (low + high) // 2\n    if ecdf_x[mid] < threshold:\n      low = mid\n    else:\n      high = mid\n  return low, high\n\nlow, high = longest_interval(grades, threshold)\n",
        "\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\na_np = a.numpy()\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[-N:][::-1]\n",
        "\nA = np.arange(16).reshape(4, 4)\nn = 5\nresult = np.linalg.matrix_power(A, n)\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        patch = a[i:i+2, j:j+2]\n        if not np.array_equal(patch[0], patch[1]):\n            result.append(patch)\n\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(a.shape[0] - 1):\n    for j in range(a.shape[1] - 1):\n        result.append(a[i:i+2, j:j+2])\n\n",
        "\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        patch = a[i:i+2, j:j+2]\n        if not np.array_equal(patch[0], patch[1]):\n            result.append(patch)\n\n",
        "\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        if patch.shape == (patch_size, patch_size):\n            result.append(patch)\n\n",
        "\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i][j] = a[i//2][j//3][i%2*3+j%3]\n",
        "\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        if patch.shape == (patch_size, patch_size):\n            result.append(patch)\n\n",
        "\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 5\nresult = a[:, low-1:high]\n",
        "\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 0\nhigh = 2\nresult = a[low:high+1,:]\n",
        "\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 10\nresult = a[:, low-1:high]\n",
        "\na = np.fromstring(string, dtype=float, sep=' ')\n",
        "\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\nresult = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n",
        "\n    result = np.exp(np.random.uniform(np.log(min), np.log(max), n))\n    ",
        "\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nB = pd.Series(np.zeros(len(A)))\nB[0] = a*A[0]\nfor t in range(1,len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a*A[0]\nB[1] = a*A[1]+b*B[0]\nfor t in range(2,len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nlinear_index = np.ravel_multi_index(index, dims, order='F') - 1\n",
        "\nresult = np.ravel_multi_index(index, dims, order='C')\n",
        "\nindex = ['x', 'y']\ncolumns = ['a','b','c']\nvalues = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nresult = np.bincount(accmap, weights=a, minlength=3)\n",
        "\nresult = np.zeros(len(np.unique(index)))\nfor i in np.unique(index):\n    result[i] = np.max(a[index==i])\n",
        "\nresult = np.zeros(3, dtype=int)\nfor i in range(len(accmap)):\n    if accmap[i] >= 0:\n        result[accmap[i]] += a[i]\n    else:\n        result[accmap[i]+1] += a[i]\n",
        "\nresult = np.zeros(len(index))\nfor i in range(len(index)):\n    if index[i] >= 0:\n        result[i] = a[index[i]]\n    else:\n        result[i] = np.min(a[:index[i]])\n",
        "\nx = [[2, 2, 2],\n     [2, 2, 2],\n     [2, 2, 2]]\ny = [[3, 3, 3],\n     [3, 3, 3],\n     [3, 3, 1]]\nz = np.zeros_like(x)\nfor i in range(len(x)):\n    for j in range(len(x[0])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n",
        "\nprobabilit = [0.333, 0.334, 0.333]\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\nresult = np.pad(a, ((0, 0), (0, 0), (low_index, -high_index)), mode='constant', constant_values=0)\n",
        "\nresult = np.delete(x, np.where(x < 0))\n",
        "\nresult = np.delete(x, np.where(np.real(x) != 0))\n",
        "\n# Use numpy to bin the data into equal partitions of bin_size\nbin_data = np.array_split(data, len(data)//bin_size)\n\n# Calculate the mean of each bin\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\n# Use numpy to bin the data into equal partitions of bin_size\nbin_data = np.array_split(data, len(data) // bin_size)\n\n# Calculate the maximum of each bin\nbin_data_max = [np.max(bin) for bin in bin_data]\n",
        "\nbin_data = []\nfor i in range(data.shape[0]):\n    bin_data.append([])\n    for j in range(0, data.shape[1], bin_size):\n        bin_data[i].append(tuple(data[i, j:j+bin_size]))\nbin_data_mean = [np.mean(data[i, j:j+bin_size], axis=0) for i in range(data.shape[0]) for j in range(0, data.shape[1], bin_size)]\nbin_data_mean = np.reshape(bin_data_mean, (data.shape[0], -1))\n",
        "\n# Calculate the number of bins\nnum_bins = int(np.ceil(len(data) / bin_size))\n\n# Initialize the binned data and mean arrays\nbin_data = np.zeros((num_bins, bin_size))\nbin_data_mean = np.zeros(num_bins)\n\n# Loop through the data and bin it\nfor i in range(num_bins):\n    start_idx = len(data) - (i+1)*bin_size\n    end_idx = len(data) - i*bin_size\n    bin_data[i] = data[start_idx:end_idx]\n    bin_data_mean[i] = np.mean(bin_data[i])\n",
        "\nbin_data = []\nfor i in range(data.shape[0] // bin_size):\n    bin_data.append(data[i*bin_size:i*bin_size+bin_size])\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\n# Create an empty list to store the binned data\nbin_data = []\n\n# Loop through each row of the data\nfor row in data:\n    # Calculate the number of bins needed to fit all the data\n    num_bins = len(row) // bin_size\n    # If the last bin is not the same size as the rest, discard it\n    if len(row) % bin_size != 0:\n        num_bins -= 1\n    # Create an empty list to store the binned data for this row\n    row_bin_data = []\n    # Loop through each bin of the row\n    for i in range(num_bins):\n        # Calculate the start and end indices of the bin\n        start_idx = i * bin_size\n        end_idx = start_idx + bin_size\n        # If the bin is not aligned to the end of the row, discard the first few elements\n        if i == 0 and start_idx != 0:\n            start_idx = 0\n        # If the bin is not aligned to the end of the row, discard the last few elements\n        if i == num_bins - 1 and end_idx != len(row):\n            end_idx = len(row)\n        # Calculate the mean of the bin\n        bin_mean = np.mean(row[start_idx:end_idx])\n        # Append the bin mean to the row bin data list\n        row_bin_data.append(bin_mean)\n    # Append the row bin data list to the bin data list\n    bin_data.append(row_bin_data)\n",
        "\nx = 0.25\nx_min = 0\nx_max = 1\n\ndef smoothclamp(x):\n    x = np.clip(x, x_min, x_max)\n    x = 3*x**2 - 2*x**3\n    return x\n\nresult = smoothclamp(x)\n",
        "\n\ndef smoothstep(x, N=5):\n    \"\"\"\n    Returns the N-order smoothstep function evaluated at x.\n    \"\"\"\n    if N == 0:\n        return np.clip(x, 0, 1)\n    elif N == 1:\n        return np.where(x < 0, 0, np.where(x > 1, 0, x))\n    elif N == 2:\n        return np.where(x < 0, 0, np.where(x < 1, x**2, np.where(x < 2, 2*x**3 - 3*x**2 + 1, 0)))\n    else:\n        raise ValueError(\"N must be 0, 1, or 2.\")\n\ndef smoothclamp(x, min_val=0, max_val=1, N=5):\n    \"\"\"\n    Returns the value of x after applying the N-order smoothstep function to clamp it between min_val and max_val.\n    \"\"\"\n    return np.clip(smoothstep(x, N=N), min_val, max_val)\n\nx = 0.25\nx_min = 0\nx_max = 1\nN = 5\nresult = smoothclamp(x, N=N)\n",
        "\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\nresult = np.correlate(a, b, mode='same', method='direct')\n",
        "\nresult = df.to_numpy().reshape(4,15,5)\n",
        "\nresult = df.values.reshape((15, 4, 5))\n",
        "\nresult = np.zeros((a.shape[0], m), dtype=np.uint8)\nfor i in range(a.shape[0]):\n    result[i] = np.unpackbits(np.uint8(a[i]), bitorder='little')[-m:]\n",
        "\nresult = np.zeros((a.shape[0], m), dtype=np.uint8)\nfor i in range(a.shape[0]):\n    num = a[i]\n    if num >= 0:\n        bits = np.unpackbits(np.uint8(num))\n        result[i, -len(bits):] = bits\n    else:\n        bits = np.unpackbits(np.uint8(2**m + num))\n        result[i, -len(bits):] = bits[-len(bits):]\n",
        "\nresult = np.zeros((a.shape[0], m))\nfor i in range(a.shape[0]):\n    binary = np.unpackbits(np.uint8(a[i]))\n    result[i, :binary.shape[0]] = binary\n",
        "\n# Calculate the mean of the array\nmean = np.mean(a)\n# Calculate the standard deviation of the array\nstd = np.std(a)\n# Calculate the 3rd standard deviation interval\nstart = mean - 3*std\nend = mean + 3*std\n# Create a tuple containing the start and end of the 3rd standard deviation interval\nresult = (start, end)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nresult = (mean - 2*std, mean + 2*std)\n",
        "\n    mean = np.mean(a)\n    std = np.std(a)\n    third_std = std * 3\n    start = mean - third_std\n    end = mean + third_std\n    result = (start, end)\n    ",
        "\n# Calculate the mean and standard deviation of the array\nmean = np.mean(a)\nstd = np.std(a)\n# Calculate the 2nd standard deviation\nsecond_std = std * 2\n# Calculate the lower and upper bounds of the 2nd standard deviation interval\nlower_bound = mean - second_std\nupper_bound = mean + second_std\n# Create a bool array to store the outliers\nresult = np.zeros(a.shape, dtype=bool)\n# Loop through the array and mark the outliers as True\nfor i in range(len(a)):\n    if a[i] < lower_bound or a[i] > upper_bound:\n        result[i] = True\n",
        "\n\nDataArray = np.array(data)\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\npercentile = 5\nprob = np.percentile(masked_data, percentile)\n",
        "\na = np.delete(a, zero_rows, axis=0)\na = np.delete(a, zero_cols, axis=1)\n",
        "\na[zero_rows,:] = 0\na[:,zero_cols] = 0\n",
        "\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\na[1, :] = 0\na[:, 0] = 0\n",
        "\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(len(a)), np.argmax(a, axis=1)] = True\n",
        "\nmask = np.all(a == np.min(a, axis=1, keepdims=True), axis=1)\n",
        "\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n\n# Calculate the mean of each list\npost_mean = np.mean(post)\ndistance_mean = np.mean(distance)\n\n# Calculate the standard deviation of each list\npost_std = np.std(post)\ndistance_std = np.std(distance)\n\n# Calculate the numerator and denominator of the formula\nnumerator = 0\ndenominator = 0\nfor i in range(len(post)):\n    numerator += (post[i] - post_mean) * (distance[i] - distance_mean)\n    denominator += (post[i] - post_mean) ** 2\n\n# Calculate the Pearson correlation coefficient\nresult = numerator / denominator\n\n# Print the result\n",
        "\nresult = np.array([np.dot(X[:, i].reshape(-1, 1), X[:, i].reshape(1, -1)) for i in range(X.shape[1])])\n",
        "\nX = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        X[i] += Y[i][j].dot(Y[i][j].T)\n",
        "\nis_contained = (number in a)\n",
        "\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.isin(A, B, invert=True)\nC = A[C]\n",
        "\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = A[np.isin(A, B)]\n",
        "\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,4,8])\n\nbins = np.array([1,2,3,4,5,6,7,8])\nindices = np.digitize(A, bins)\nunique_indices = np.unique(indices)\nC = A[unique_indices]\n\n",
        "\nresult = np.argsort(rankdata(a))[::-1]\n",
        "\nresult = np.argsort(rankdata(a, method='min'), kind='mergesort')[::-1]\n",
        "\n    # Reverse the order of the ranks\n    result = np.argsort(rankdata(a))\n    ",
        "\ndists = np.stack((x_dists, y_dists), axis=-1)\n",
        "\ndists = np.stack((x_dists, y_dists), axis=-1)\n",
        "\nresult = a[:, second, third]\n",
        "\n\narr = np.zeros((20,)*4)\narr = arr.reshape((20,10,10,2))\n\n",
        "\nl1 = np.sum(np.abs(X), axis=1)\nresult = X / l1.reshape(-1, 1)\n",
        "\nnorms = LA.norm(X, axis=1)\nresult = X / norms[:, np.newaxis]\n",
        "\nnorms = np.apply_along_axis(LA.norm, 1, X, ord=np.inf)\nresult = X / norms[:, np.newaxis]\n",
        "\nconditions = [df['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\n\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n\n# Calculate distance matrix using pdist\ndist_matrix = squareform(pdist(a))\n\n# Print distance matrix\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\n\nresult = squareform(pdist(a))\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\n\nresult = squareform(pdist(a))\n",
        "\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA)\n",
        "\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA)\n",
        "\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=float)\nAVG = np.mean(NA, axis=0)\n",
        "\nresult = np.unique(a[a != 0])\n",
        "\nresult = np.delete(a, np.where(np.diff(a) == 0)[0]+1)\nresult = np.delete(result, np.where(result == 0)[0])\n",
        "\ndf = pd.DataFrame({'lat': np.concatenate(lat).ravel(),\n                   'lon': np.concatenate(lon).ravel(),\n                   'val': np.concatenate(val).ravel()})\n",
        "\n    df = pd.DataFrame({'lat': np.concatenate(lat), 'lon': np.concatenate(lon), 'val': np.concatenate(val)})\n    ",
        "\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\n# Create a list of tuples with the lat, lon, and val values\ndata = []\nfor i in range(len(lat)):\n    for j in range(len(lat[i])):\n        data.append((lat[i][j], lon[i][j], val[i][j]))\n\n# Create a dataframe with the lat, lon, and val values\ndf = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\n\n# Add a column with the maximum value of each row\ndf['maximum'] = df.apply(lambda row: max(row['lat'], row['lon'], row['val']), axis=1)\n\n# Print the dataframe\n",
        "\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n\n# Add padding to handle edge effects\npad_width = ((size[0] - 1) // 2, (size[0] - 1) // 2), ((size[1] - 1) // 2, (size[1] - 1) // 2)\na_pad = np.pad(a, pad_width, mode='edge')\n\n# Create the sliding window view\nresult = np.lib.stride_tricks.sliding_window_view(a_pad, size)\n\n# Remove padding from the result\nresult = result[pad_width[0][0]:-pad_width[0][1], pad_width[1][0]:-pad_width[1][1], :]\n\n",
        "\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n\n# Add padding to handle edge effects\npad_width = ((size[0] - 1) // 2, (size[0] - 1) // 2), ((size[1] - 1) // 2, (size[1] - 1) // 2)\na_pad = np.pad(a, pad_width, mode='edge')\n\n# Create the sliding window view\nresult = np.lib.stride_tricks.sliding_window_view(a_pad, size)\n\n# Remove padding from the result\nresult = result[pad_width[0][0]:-pad_width[0][1], pad_width[1][0]:-pad_width[1][1], :]\n\n",
        "\nresult = np.mean(a, where=np.isfinite(a))\n",
        "\nresult = np.mean(a)\n",
        "\nresult = Z[..., -1:]\n",
        "\nresult = a[[Missing]]\n",
        "\nresult = False\nfor cnt in CNTS:\n    if np.array_equal(c, cnt):\n        result = True\n        break\n",
        "\nresult = False\nfor cnt in CNTS:\n    if np.array_equal(c, cnt):\n        result = True\n        break\n",
        "\nf = intp.interp2d(np.arange(a.shape[0]), np.arange(a.shape[1]), a, kind='linear')\nresult = f(x_new, y_new)\n",
        "\ndata = {'D':[2015,2015,2015,2015,2016,2016,2016,2017,2017,2017], 'Q':np.arange(10)}\nname= 'Q_cum'\ndf = pd.DataFrame(data)\ndf['Q_cum'] = np.where(df['D'] == df['D'].shift(), df['Q'].shift(), 0).cumsum()\n",
        "\na = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])\nU, i, V = np.linalg.svd(a,full_matrices=True)\ni = np.diag(i)\n",
        "\na[np.triu_indices(a.shape[0], k=1)] = 0\n",
        "\ndelta = (end - start) / n\nresult = pd.DatetimeIndex([start + i * delta for i in range(n)])\n",
        "\nresult = -1\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result = i\n        break\n",
        "\nresult = np.where(np.logical_and(x == a, y == b))[0]\n",
        "\nA = np.vstack([x, np.ones(len(x))]).T\nresult = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "\nA = np.zeros((len(x), degree+1))\nfor i in range(degree+1):\n    A[:, i] = x ** i\n\nresult = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "\ndf.apply(lambda x: x-a, axis=1)\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[-1, 2], [-0.5, 6]])\n\n# Reshape the array into a 1D array\na_1d = a.reshape(-1, 1)\n\n# Create a MinMaxScaler object and fit it to the 1D array\nscaler = MinMaxScaler()\nscaler.fit(a_1d)\n\n# Transform the 1D array using the fitted scaler\nresult = scaler.transform(a_1d)\n\n# Reshape the result back into a 2D array\nresult = result.reshape(a.shape)\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\narr = np.array([[1.0,2.0,3.0],[0.1, 5.1, 100.1],[0.01, 20.1, 1000.1]])\n\nscaler = MinMaxScaler()\n\n# Fit the scaler to the data\nscaler.fit(arr)\n\n# Transform the data using the scaler\nresult = scaler.transform(arr)\n\n# Print the result\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[[1, 0.5, -2], [-0.5,1, 6], [1,1,1]], [[-2, -3, 1], [-0.5, 10, 6], [1,1,1]]])\n\nscaler = MinMaxScaler(axis=None)\nresult = scaler.fit_transform(a)\n\n",
        "\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] += 5\narr[~mask2] = 30\n",
        "\nfor i in range(arr.shape[0]):\n    arr_temp = arr.copy()\n    mask = arr_temp < n1[i]\n    mask2 = arr_temp >= n2[i]\n    mask3 = mask ^ mask2\n    arr[mask] = 0\n    arr[mask3] = arr[mask3] + 5\n    arr[~mask2] = 30\n",
        "\nresult = sum(decimal.Decimal(str(x)) for x in s1) - sum(decimal.Decimal(str(x)) for x in s2)\n",
        "\nresult = np.count_nonzero(np.logical_not(np.isclose(s1, s2, equal_nan=True)))\n",
        "\n\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\n\nresult = True\nfor i in range(len(a)):\n    for j in range(i+1, len(a)):\n        if not np.array_equal(a[i], a[j]):\n            result = False\n            break\n    if not result:\n        break\n\n",
        "\nresult = all(np.isnan(arr).all() for arr in a)\n",
        "\n\na = np.ones((41, 13))\nshape = (93, 13)\n\nresult = np.pad(a, pad_width=((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant', constant_values=0)\n\n",
        "\n\na = np.ones((41, 12))\nshape = (93, 13)\n\nresult = np.pad(a, pad_width=((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant', constant_values=0)\n\n",
        "\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\n\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n\n",
        "\n    result = np.pad(arr, pad_width=((0, shape[0]-arr.shape[0]), (0, shape[1]-arr.shape[1])), mode='constant', constant_values=0)\n    ",
        "\na = np.ones((41, 12))\nshape = (93, 13)\n\n# Get the maximum shape\nmax_shape = np.max(np.vstack((np.shape(a), shape)), axis=0)\n\n# Pad the array with zeros\nresult = np.pad(a, [(0, max_shape[0]-np.shape(a)[0]), (0, max_shape[1]-np.shape(a)[1])], mode='constant', constant_values=0)\n\n",
        "\na.reshape(a.shape[0]//3, 3)\n",
        "\nresult = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        if b[i][j] == 1:\n            result[i][j] = a[i][j][0]\n        else:\n            result[i][j] = a[i][j][1]\n",
        "\nresult = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        if b[i][j] == 1:\n            result[i][j] = a[i][j][0]\n        else:\n            result[i][j] = a[i][j][1]\n",
        "\nresult = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        result[i][j] = a[i][j][b[i][j]]\n",
        "\nresult = np.zeros(b.shape)\nfor i in range(b.shape[0]):\n    for j in range(b.shape[1]):\n        result[i,j] = np.sum(a[i,j,b[i,j]:b[i,j]+1])\n",
        "\nresult = np.sum(a[b==0], axis=(0,1))\n",
        "\nresult = np.where((df['a'] > 1) & (df['a'] <= 4), df['b'], np.nan)\n",
        "\n# Create a mask to filter out the peripheral zeros\nmask = np.array([[1,1,1,1,1,1],\n                 [1,1,0,0,0,1],\n                 [1,0,0,0,0,1],\n                 [1,0,0,0,0,1],\n                 [1,1,1,1,1,1]])\n\n# Apply the mask to the image to filter out the peripheral zeros\nresult = im * mask\n",
        "\n# Find the bounding box of the nonzero data in the array\nnonzero_rows = np.any(A, axis=1)\nnonzero_cols = np.any(A, axis=0)\nmin_row, max_row = np.where(nonzero_rows)[0][[0, -1]]\nmin_col, max_col = np.where(nonzero_cols)[0][[0, -1]]\n\n# Truncate the array to the bounding box\nresult = A[min_row:max_row+1, min_col:max_col+1]\n",
        "\n# Create a mask that is True for all non-zero elements in each row and column\nmask = np.logical_or.reduce([np.all(im != 0, axis=0), np.all(im != 0, axis=1)])\n# Apply the mask to the image to remove peripheral non-zeros\nresult = im[mask]\n",
        "\n# Create a mask to filter out the black pixels\nmask = (im != 0)\n\n# Apply the mask to the image to filter out the black pixels\nfiltered_im = im[mask]\n\n# Reshape the filtered image to create a 2D array\nresult = filtered_im.reshape((-1, im.shape[1]))\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nnum_classes = 10\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n\n\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n",
        "\n    result = tf.data.Dataset.from_tensor_slices(input)\n    result = result.flat_map(lambda x: tf.data.Dataset.from_tensor_slices([x, x+1, x+2]))\n    ",
        "\n\nlengths = [4, 3, 5, 2]\n\n# Create a tensor of zeros with shape (max_length, batch_size)\nmax_length = max(lengths)\nbatch_size = len(lengths)\nmask = tf.zeros((max_length, batch_size))\n\n# Create a tensor of ones with shape (batch_size, 1)\nones = tf.ones((batch_size, 1))\n\n# Create a tensor of zeros with shape (batch_size, 1)\nzeros = tf.zeros((batch_size, 1))\n\n# Concatenate the ones and zeros tensors along the second dimension\nmask = tf.concat([ones, mask], axis=1)\n\n# Transpose the mask tensor to get the desired output\nmask = tf.transpose(mask)\n\n# Pad the mask tensor with zeros to the desired length\nmask = tf.pad(mask, [[0, 0], [1, max_length - tf.shape(mask)[1] + 1]])\n\n# Set the values of the padded zeros to 1\nmask = tf.where(mask == 0, tf.ones_like(mask), tf.zeros_like(mask))\n\n",
        "\n\nlengths = [4, 3, 5, 2]\n\n# Create a tensor of zeros with shape (max_length, batch_size)\nmax_length = max(lengths)\nbatch_size = len(lengths)\nmask = tf.zeros((max_length, batch_size))\n\n# Create a tensor of indices with shape (batch_size)\nindices = tf.range(batch_size)\n\n# Create a tensor of lengths with shape (batch_size)\nlengths_tensor = tf.constant(lengths)\n\n# Create a tensor of padded lengths with shape (batch_size)\npadded_lengths = tf.maximum(lengths_tensor, max_length)\n\n# Create a tensor of padded indices with shape (max_length)\npadded_indices = tf.concat([indices, indices[-1:] + 1], axis=0)[:max_length]\n\n# Create a tensor of padded indices with shape (max_length, batch_size)\npadded_indices_tensor = tf.expand_dims(padded_indices, axis=-1)\n\n# Create a tensor of padded mask with shape (max_length, batch_size)\npadded_mask = tf.scatter_nd(padded_indices_tensor, tf.ones_like(padded_indices), shape=(max_length, batch_size))\n\n# Set the padded mask to 0 where the length is less than the max length\npadded_mask *= tf.expand_dims(tf.sequence_mask(padded_lengths, maxlen=max_length), axis=-1)\n\n# Set the padded mask to 0 where the length is equal to the max length\npadded_mask *= tf.expand_dims(tf.sequence_mask(lengths_tensor, maxlen=max_length), axis=-1)\n\nresult = padded_mask\n",
        "\n\nlengths = [4, 3, 5, 2]\nmax_length = 8\n\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\nresult = tf.pad(mask, [[0, max_length - tf.shape(mask)[0]], [0, 0]])\n\n",
        "\n\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    max_length = max(lengths)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n    result = tf.pad(mask, [[0, 8-max_length], [0, 0]])\n    return result\n",
        "\n\nlengths = [4, 3, 5, 2]\nmax_length = 8\n\nmask = tf.sequence_mask(lengths, maxlen=max_length, dtype=tf.float32)\nresult = tf.pad(mask, [[0, max_length-tf.shape(mask)[0]], [0, 0]])\n\n",
        "\n\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\n\nresult = tf.stack([tf.tile(a, [len(b)]), tf.repeat(b, len(a))], axis=1)\n\n",
        "\n    result = tf.stack(tf.meshgrid(a,b), axis=-1)\n    ",
        "\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\nresult = tf.expand_dims(a, axis=-2)\n",
        "\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.reciprocal(A)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=0)\n",
        "\n    result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n    ",
        "\n\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\nm = tf.gather_nd(x, tf.stack([tf.expand_dims(y, axis=-1), tf.expand_dims(z, axis=-1)], axis=-1))\n\n",
        "\nm = tf.gather_nd(x, tf.stack([row, col], axis=1))\n",
        "\n    m = tf.gather_nd(x, tf.stack([y,z], axis=1))\n    ",
        "\nC = tf.tensordot(A, B, axes=[[2], [2]])\n",
        "\nC = tf.tensordot(A, B, axes=[[2], [2]])\n",
        "\n\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\n\nresult = tf.strings.unicode_decode(x, \"UTF-8\")\n\n",
        "\n    result = tf.strings.unicode_decode(x, \"UTF-8\")\n    ",
        "\n# Create a boolean mask for non-zero entries\nmask = tf.not_equal(x, tf.constant(0, dtype=tf.float32))\n# Create a tensor of ones with the same shape as the mask\nones = tf.ones_like(mask, dtype=tf.float32)\n# Multiply the mask with the tensor of ones to get a tensor of ones for non-zero entries\nmask_ones = tf.multiply(mask, ones)\n# Get the number of non-zero entries for each sample in the batch\nnum_nonzero = tf.reduce_sum(mask_ones, axis=-2)\n# Divide the tensor of non-zero entries by the number of non-zero entries to get the average\nresult = tf.divide(tf.reduce_sum(tf.multiply(x, mask), axis=-2), num_nonzero)\n",
        "\n# Calculate the mean of the non-zero entries of the second to last dimension of X\nmean = tf.reduce_mean(tf.boolean_mask(x, tf.not_equal(x, 0), axis=-2), axis=-2)\n# Calculate the variance of the non-zero entries of the second to last dimension of X\nvariance = tf.reduce_mean(tf.math.squared_difference(tf.boolean_mask(x, tf.not_equal(x, 0), axis=-2), mean), axis=-2)\n",
        "\n    non_zero_mask = tf.math.not_equal(x, tf.constant(0, dtype=tf.float32))\n    non_zero_count = tf.reduce_sum(tf.cast(non_zero_mask, tf.float32), axis=[-2, -1])\n    non_zero_x = tf.boolean_mask(x, non_zero_mask)\n    result = tf.reduce_sum(non_zero_x, axis=-2) / non_zero_count[..., tf.newaxis]\n    ",
        "\n\ntry:\n    Session = tf.Session\nexcept AttributeError:\n    Session = tf.compat.v1.Session\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nwith tf.Session() as sess:\n   result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmax(a, axis=1)\n\n",
        "\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmax(a, axis=1)\n\n",
        "\n    result = tf.argmax(a, axis=1)\n    ",
        "\nresult = tf.argmin(a, axis=1)\n",
        "\n# Save the model in \"export/1\"\ntf.saved_model.save(model, \"export/1\")\n",
        "\nresult = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25]).sample(10)\n",
        "\nresult = tfp.distributions.Categorical(probs=[0.2, 0.3, 0.2, 0.3]).sample(114)\n",
        "\n\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25]).sample(10)\n    return result\n",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\ncoefficients = np.polyfit(np.log(x), y, 1)\nresult = coefficients[::-1]\n",
        "\ncoefficients = np.polyfit(np.log(x), y, 1)\nresult = coefficients[::-1]\n",
        "\n\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\n\ndef func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\npopt, pcov = scipy.optimize.curve_fit(func, x, y, p0=p0)\n\nresult = popt\n\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\nresult = stats.ks_2samp(x, y)[1] < alpha\n",
        "\nfrom math import *\n\ninitial_guess = [-1, 0, -3]\n\ndef f(x):\n    return ((x[0]+x[1]-x[2])-2)**2 + ((3*x[0]-x[1]-x[2]))**2 + sin(x[1]) + cos(x[1]) + 4\n\nresult = optimize.minimize(f, initial_guess)\n\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)\n",
        "\n\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\nz_scores = []\n\nfor p in p_values:\n    z_score = scipy.stats.norm.ppf(0.05) * np.sqrt(p * (1 - p))\n    z_scores.append(z_score)\n\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nresult = sa.dot(sb)\n",
        "\n    result = sA.dot(sB)\n    ",
        "\n# Interpolation using scipy.interpolate.LinearNDInterpolator\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\n# Interpolate the data using scipy.interpolate.LinearNDInterpolator\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\n# Calculate the rotation matrix\ntheta = np.deg2rad(angle)\nc, s = np.cos(theta), np.sin(theta)\nR = np.array(((c,-s), (s, c)))\n# Calculate the translation matrix\nx_center, y_center = data_orig.shape[0]/2, data_orig.shape[1]/2\nx_shift, y_shift = x_center - x0, y_center - y0\nT = np.array(((1, 0, x_shift), (0, 1, y_shift), (0, 0, 1)))\n# Calculate the new coordinates\nxy = np.array((x0, y0, 1))\nxy_rot = np.dot(np.dot(R, T), xy)\nxrot, yrot = xy_rot[0], xy_rot[1]\n",
        "\nresult = np.array(M.diagonal()).reshape(-1, 1)\n",
        "\nresult = stats.kstest(times, \"uniform\")\n",
        "\n    _, p = stats.kstest(times, 'uniform')\n    ",
        "\nfrom scipy import stats\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n\n# Kolmogorov-Smirnov test\nresult = stats.kstest(times, 'uniform')\n\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.hstack([c1, c2])\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.vstack([c1, c2])\n",
        "\n\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\ndef distance(p1, p2):\n    return np.sqrt(np.sum((p1-p2)**2))\n\ndef kmeans(points, k):\n    centroids = points[np.random.choice(points.shape[0], k, replace=False)]\n    while True:\n        distances = scipy.spatial.distance.cdist(points, centroids)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels==i].mean(axis=0) for i in range(k)])\n        if np.all(new_centroids == centroids):\n            return labels, new_centroids\n        centroids = new_centroids\n\nlabels, centroids = kmeans(points1, 2)\nresult = [np.argmin(distance(points1[i], centroids[labels[i]])) for i in range(N)]\n\n",
        "\n\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\n# Use the Hungarian algorithm to find the optimal assignment of points\ncost_matrix = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(cost_matrix)\n\n# Convert the indices to a list of point indices\nresult = [col_ind[i] for i in range(N) if row_ind[i] == i]\n\n",
        "\nb.setdiag(0)\n",
        "\nmarkers = np.zeros_like(img)\nmarkers[img > threshold] = 1\nmarkers[img <= threshold] = 2\nlabels = ndimage.label(markers)[0]\nresult = len(np.unique(labels)) - 1\n",
        "\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Count the number of regions of cells which value below a given threshold\nlabels, num_labels = ndimage.label(img < threshold)\nresult = num_labels\n\n",
        "\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nexample_img /= img.max()\ndef f(img = example_img):\n    threshold = 0.75\n    markers = np.zeros_like(img)\n    markers[img > threshold] = 1\n    markers[img <= threshold] = 2\n    labels = ndimage.label(markers)[0]\n    regions = ndimage.find_objects(labels)\n    result = len(regions)\n    return result\n",
        "\nregions = np.zeros_like(img)\nregions[img > threshold] = 1\n",
        "\nM.setdiag(M.diagonal())\n",
        "\nfrom scipy.sparse import lil_matrix\n\ndef make_symmetric(sA):\n    # set diagonal elements to upper triangle values\n    sA.setdiag(sA.diagonal())\n    # set lower triangle elements to upper triangle values\n    sA = sA + sA.T\n    # remove duplicate entries\n    sA = sA.tocsr()\n    return sA\n\nexample_sA = lil_matrix((10, 10))\nexample_sA[0, 1] = 1\nexample_sA[1, 0] = 1\nexample_sA[2, 3] = 2\nexample_sA[3, 2] = 2\nexample_sA[4, 5] = 3\nexample_sA[5, 4] = 3\nexample_sA = make_symmetric(example_sA)\n",
        "\n\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\n\n# Fill in the missing code here\n# Use scipy.ndimage.binary_erosion and scipy.ndimage.binary_dilation to remove single cells\n# and then use scipy.ndimage.label to separate the remaining blobs\n# Finally, use scipy.ndimage.sum to count the number of pixels in each blob and remove the small ones\n\n# Remove single cells\neroded = scipy.ndimage.binary_erosion(square)\ndilated = scipy.ndimage.binary_dilation(eroded)\n# Remove small blobs\nlabels, num_labels = scipy.ndimage.label(dilated)\nsizes = scipy.ndimage.sum(square, labels, range(num_labels + 1))\nmask = sizes > 100\nfiltered_labels = np.arange(1, num_labels + 1)[mask]\nfiltered_image = np.isin(labels, filtered_labels)\n\nsquare = filtered_image.astype(int)\n\n",
        "\n# Create a mask of all 1s\nmask = np.ones((3, 3))\n\n# Erode the mask to remove single cells\neroded_mask = scipy.ndimage.binary_erosion(mask, iterations=1)\n\n# Dilate the mask to include single cells\ndilated_mask = scipy.ndimage.binary_dilation(mask, iterations=1)\n\n# Create a mask of all 0s\nbackground_mask = np.zeros_like(square)\n\n# Set the background mask to 1 where the dilated mask is 1\nbackground_mask[dilated_mask == 1] = 1\n\n# Set the square to 0 where the background mask is 1\nsquare[background_mask == 1] = 0\n",
        "\nmean = np.mean(col.data)\nstandard_deviation = np.std(col.data)\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\ndata = col.data\nMedian = np.median(data)\nMode = np.bincount(data).argmax()\n",
        "\ndef fourier(x, *args):\n    a = args[:degree]\n    return np.sum([a[i] * np.cos(i * np.pi / tau * x) for i in range(degree)], axis=0)\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=np.ones(degree))\n",
        "\n\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\n# Calculate pairwise Euclidean distances between all regions\nresult = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean')\n\n# Convert distances from cells to metres\nresolution = 1  # Assuming 1 metre per cell\nresult *= resolution\n\n# Plot array\nplt.imshow(example_array, cmap=\"spectral\", interpolation='nearest')\n",
        "\n\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\n# Calculate pairwise Manhattan distances between all regions\nresult = scipy.spatial.distance.cdist(example_array, example_array, 'cityblock')\n\n",
        "\n    # Create a list of unique IDs for each patch/region\n    unique_ids = np.unique(example_array)\n    # Create a dictionary to store the pairwise distances between each pair of patches/regions\n    pairwise_distances = {}\n    # Loop through each pair of patches/regions and calculate their Euclidean distance\n    for i in range(len(unique_ids)):\n        for j in range(i+1, len(unique_ids)):\n            # Get the indices of the current pair of patches/regions\n            id1 = unique_ids[i]\n            id2 = unique_ids[j]\n            indices1 = np.where(example_array == id1)\n            indices2 = np.where(example_array == id2)\n            # Calculate the Euclidean distance between the two patches/regions\n            distance = scipy.spatial.distance.euclidean(indices1, indices2)\n            # Store the distance in the dictionary\n            pairwise_distances[(id1, id2)] = distance\n    # Convert the dictionary to a numpy array\n    result = np.zeros((len(unique_ids), len(unique_ids)))\n    for i in range(len(unique_ids)):\n        for j in range(i+1, len(unique_ids)):\n            id1 = unique_ids[i]\n            id2 = unique_ids[j]\n            result[i, j] = pairwise_distances[(id1, id2)]\n            result[j, i] = pairwise_distances[(id1, id2)]\n    ",
        "\nfrom scipy import interpolate\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85.36, 93.24],\n              [61.48, 69.31, 75.36, 81.87, 89.35],\n              [57.61, 65.75, 71.7, 79.1, 86.13],\n              [55.12, 63.34, 69.32, 77.29, 83.88],\n              [54.58, 62.54, 68.7, 76.72, 82.92],\n              [56.58, 63.87, 70.3, 77.69, 83.53],\n              [61.67, 67.79, 74.41, 80.43, 85.86],\n              [70.08, 74.62, 80.93, 85.06, 89.84]])\nx_val = np.linspace(-1, 1, 100)\nresult = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    y_int = interpolate.splev(x_val, tck, der = 0)\n    result[i] = y_int\n",
        "\n\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\nx3 = [34.0, 35.0, 39.0, 40.0, 43.0, 43.0, 44.0, 45.0]\nx4 = [34.0, 34.8, 34.8, 35.4, 37.2, 37.8, 41.2, 42.8]\n\n# Concatenate the datasets into a single 2-dimensional array\nX = np.vstack((x1, x2, x3, x4)).T\n\n# Calculate the Anderson-Darling test statistic and critical values\nstatistic, critical_values, significance_level = ss.anderson_ksamp(X)\n\n# Print the results\n",
        "\n\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\n\n# concatenate the two arrays\nx = np.concatenate((x1, x2))\n\n# calculate the anderson-ksamp test\nresult = ss.anderson_ksamp([x1, x2])\n\n# interpret the result\nif result.statistic < result.critical_values[2]:\n    print(\"The two datasets are drawn from the same population at the 5% significance level.\")\nelse:\n    print(\"The two datasets are not drawn from the same population at the 5% significance level.\")\n",
        "\n\ndef tau1(x):\n    y = np.array(A['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\n\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\n\n",
        "\nif sa.nnz == 0:\n    result = True\nelse:\n    result = False\n",
        "\nresult = len(sa.data) == 0 and len(sa.rows) == 0\n",
        "\nfrom scipy.linalg import block_diag\nnp.random.seed(10)\na = np.random.rand(100,2,2)\nresult = block_diag(*a)\n",
        "\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\n    _, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n    ",
        "\n# Calculate the mean and standard deviation of the array\nmean = np.mean(a)\nstd = np.std(a, ddof=1)\n\n# Calculate the kurtosis using the formula given in mathworld\nkurtosis_result = (np.sum((a - mean)**4) / (len(a) - 1)) / (std**4)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, bias=False)\n",
        "\nf = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = f(s, t)\n",
        "\n\nexample_s = np.linspace(-1, 1, 50)\nexample_t = np.linspace(-2, 0, 50)\n\ndef f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    f = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n    result = f(example_s, example_t)\n    return result\n\nresult = f()\n",
        "\n# Create a list of indices for each voronoi cell\nindices = [[] for i in range(len(vor.regions))]\n\n# Loop through each extra point and find its corresponding voronoi cell\nfor point in extraPoints:\n    point_region = vor.point_region(point)\n    indices[point_region].append(point)\n\n# Count the number of extra points in each voronoi cell\nresult = np.zeros(len(vor.regions))\nfor i in range(len(vor.regions)):\n    result[i] = len(indices[i])\n",
        "\n# Initialize an empty list to store the indices of the regions occupied by each extra point\nresult = []\n\n# Loop through each extra point\nfor point in extraPoints:\n    # Find the index of the region that the point is in\n    regionIndex = vor.point_region[vor.regions[vor.point_region == -1][0]].tolist().index(point)\n    # Append the index to the result list\n    result.append(regionIndex)\n\n# Convert the result list to a numpy array\nresult = np.array(result)\n",
        "\n# Stack the vectors vertically\nstacked_vectors = np.vstack(vectors)\n\n# Pad zeros to the end of elements for the vectors which are smaller than the maximum size\npadded_vectors = np.pad(stacked_vectors, ((0,0),(0,max_vector_size-stacked_vectors.shape[1])), 'constant', constant_values=0)\n\n# Convert the resulting 2D array to a sparse matrix\nresult = sparse.csr_matrix(padded_vectors)\n",
        "\nb = scipy.ndimage.shift(a, [1, 0], order=0)\n",
        "\nrow_vec = M.getrow(row)\nresult = row_vec.toarray()[0][column]\n",
        "\nresult = []\nfor i in range(len(row)):\n    result.append(M.getrow(row[i]).toarray()[0][column[i]])\n",
        "\nnew_array = np.zeros((10, 10, 100))\nfor i in range(10):\n    for j in range(10):\n        for k in range(100):\n            f = scipy.interpolate.interp1d(x, array[i, j, :], kind='cubic')\n            new_array[i, j, k] = f(x_new[k])\n",
        "\nprob = scipy.integrate.quad(NDfx, -abs((x-u)/o2), abs((x-u)/o2))[0]\n",
        "\nprob = NormalDistro(u,o2,x)\n",
        "\n\nN = 8\nresult = sf.dctn(np.eye(N), norm='ortho')\n",
        "\noffset = [-1,0,1]\nTridiagonal_1 = sparse.diags(matrix, offset, shape=(5,5)).toarray()\n",
        "\nresult = scipy.stats.binom.pmf(np.arange(N+1), N, p)\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\nz_scores = stats.zscore(df)\nresult = pd.concat([df, pd.DataFrame(z_scores, columns=df.columns)], axis=1)\nresult.columns = ['data', 'zscore']\n",
        "\nz_scores = stats.zscore(df)\nresult = pd.DataFrame(df)\nresult['zscore'] = z_scores\nresult = result.round(3)\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([[3, 3], [3, 4], [4, 3], [4, 4]])\nresult = distance.cdist(mid, np.indices(shape).reshape(2, -1).T)\n",
        "\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([[[3, 3], [3, 3]], [[3, 3], [3, 3]]])\nresult = distance.cdist(np.indices(shape).reshape(shape + (2,)), mid, 'cityblock')\n",
        "\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    mid = np.array([[[i,j] for j in range(shape[1])] for i in range(shape[0])])\n    result = distance.cdist(mid, mid, 'euclidean')\n    return result\n",
        "\n\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\n\nresult = skimage.transform.resize(x, shape, order=1)\n\n",
        "\ndef func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model) **2\ndef main():\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = minimize(residual, fit_params, args=(a, y))\n    print(out)\nif __name__ == '__main__':\n    main()\n",
        "\ndef func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model)**2\ndef main():\n    fit_params = Parameters()\n    fit_params.add('x', value=x0, min=x_lower_bounds)\n    out = minimize(residual, fit_params, args=(a, y))\n    print(out)\nif __name__ == '__main__':\n    main()\n",
        "\nN0 = 10\ntime_span = [-0.1, 0.1]\n\ndef dN1_dt_simple(t, N1, t_input):\n    return -100 * N1 + np.sin(t_input * t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,], t_eval=np.linspace(time_span[0], time_span[1], 100))\nresult = sol.y\n",
        "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi - t + np.sin(2*np.pi)\n",
        "\n\nN0 = 10\ntime_span = [-0.1, 0.1]\n\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n",
        "\nfrom scipy.optimize import minimize\n\ndef function(x):\n    return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\n\nI=np.array((20,50,50,80))\nx0=I\n\ncons=[]\nsteadystate={'type':'eq', 'fun': lambda x: x.sum()-I.sum() }\ncons.append(steadystate)\n\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n    cons.append({'type':'ineq', 'fun': lambda x: -x[t]})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nI = []\nfor n in range(len(c)):\n    # equation\n    eqn = lambda x: 2*x*c[n]\n    # integrate \n    result,error = scipy.integrate.quad(eqn,low,high)\n    I.append(result)\nI = np.array(I)\n",
        "\n    def integrand(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(integrand, low, high)\n    ",
        "\nV = V + x\n",
        "\nV.data += x\n",
        "\nB = V.copy()\nB.data[B.data != 0] += x\nB += y\n",
        "\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    sa[:,Col] = (1/Len)*Column\n",
        "\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    sa[:,Col] = (1/Len)*Column\n",
        "\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n\n# Convert the decimal matrix to binary matrix\nb = sparse.csr_matrix(a)\n\n# Print the binary matrix\n",
        "\n# Convert binary matrix to undirected graph\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Cluster the data using Ward's linkage\nZ = scipy.spatial.distance.cdist(data, centroids, metric='euclidean')\nZ = np.square(Z)\nZ = 1 / Z\nZ = Z / np.sum(Z)\nlinkage_matrix = np.array([[0, 1, 2, 3, 4], [1, 0, 2, 3, 4], [2, 2, 0, 3, 4], [3, 3, 3, 0, 4], [4, 4, 4, 4, 0]])\nresult = scipy.cluster.hierarchy.cut_tree(linkage_matrix, Z)\n\n# Find the index of the closest element to each cluster's centroid\nclosest_to_centroid = np.zeros(5)\nfor i in range(5):\n    centroid = np.mean(data[result == i], axis=0)\n    distances = scipy.spatial.distance.cdist(data, centroid.reshape(1, -1), metric='euclidean')\n    closest_to_centroid[i] = np.argmin(distances)\n\n# Print the result\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Cluster the data using Ward's linkage\nZ = scipy.spatial.distance.cdist(data, centroids, metric='euclidean')\nZ = np.square(Z)\nZ = 1 / Z\nZ = Z / np.sum(Z)\nlinkage_matrix = np.array([[0, 1, 2, 3, 4], [1, 0, 2, 3, 4], [2, 2, 0, 3, 4], [3, 3, 3, 0, 4], [4, 4, 4, 4, 0]])\nresult = scipy.cluster.hierarchy.cut_tree(linkage_matrix, Z)\n\n# Extract the closest point to each cluster\nclosest_points = []\nfor i in range(5):\n    cluster_points = data[result == i]\n    centroid = centroids[i]\n    distances = scipy.spatial.distance.cdist(cluster_points, [centroid], metric='euclidean')\n    closest_points.append(cluster_points[np.argmin(distances)])\n\n",
        "\n\n# Generate random data and centroids\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\n# Set k\nk = 3\n\n# Calculate distance matrix\ndistance_matrix = scipy.spatial.distance.cdist(data, centroids)\n\n# Calculate linkage matrix\nlinkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix)\n\n# Calculate cluster memberships\ncluster_memberships = scipy.cluster.hierarchy.cut_tree(linkage_matrix, n_clusters=5)\n\n# Extract k-th closest element for each cluster\nresult = []\nfor i in range(5):\n    # Get indices of data points in cluster i\n    cluster_indices = np.where(cluster_memberships[:, 0] == i)[0]\n    # Calculate distances to centroid\n    distances = scipy.spatial.distance.cdist(data[cluster_indices], centroids[i].reshape(1, -1))\n    # Sort distances and get indices of k-th closest element\n    k_closest_indices = np.argsort(distances, axis=0)[:k]\n    # Get index of k-th closest element in original data\n    k_closest_original_indices = cluster_indices[k_closest_indices]\n    result.append(k_closest_original_indices)\n\n",
        "\nfrom scipy.optimize import fsolve\n\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\n\ndef new_eqn(a, x, b):\n    return eqn(x, a, b)\n\nresult = fsolve(new_eqn, x0=0.5, args=(xdata, bdata))\n",
        "\nresult = []\nfor i in range(len(xdata)):\n    bdata = fsolve(eqn, x0=0.5, args=(adata[i], xdata[i]))\n    result.append([bdata, adata[i]])\nresult = sorted(result, key=lambda x: x[1])\n",
        "\nkstest_result = stats.kstest(sample_data, 'bekkers', args=(estimated_a, estimated_m, estimated_d))\n",
        "\nkstest_result = stats.kstest(sample_data, 'bekkers', args=(estimated_a, estimated_m, estimated_d))\nresult = kstest_result[1] < 0.05\n",
        "\nfrom scipy import integrate\nstring = '''\nTime                      A\n2017-12-18-19:54:40   -50187.0\n2017-12-18-19:54:45   -60890.5\n2017-12-18-19:54:50   -28258.5\n2017-12-18-19:54:55    -8151.0\n2017-12-18-19:55:00    -9108.5\n2017-12-18-19:55:05   -12047.0\n2017-12-18-19:55:10   -19418.0\n2017-12-18-19:55:15   -50686.0\n2017-12-18-19:55:20   -57159.0\n2017-12-18-19:55:25   -42847.0\n'''\ndf = pd.read_csv(io.StringIO(string), sep = '\\s+')\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(lambda x: integrate.trapz(x['A'], x.index.to_numpy()))\n",
        "\nresult = scipy.interpolate.griddata(x, y, eval)\n",
        "\ndef neg_log_likelihood(params, data):\n    n_categories = len(params)\n    freq = np.array(data.value_counts())\n    weights = params\n    prob = np.zeros(n_categories)\n    for i in range(n_categories):\n        prob[i] = freq[i] * np.log(weights[i])\n    return -np.sum(prob)\n\ndef neg_log_likelihood_grad(params, data):\n    n_categories = len(params)\n    freq = np.array(data.value_counts())\n    weights = params\n    prob = np.zeros(n_categories)\n    for i in range(n_categories):\n        prob[i] = freq[i] * np.log(weights[i])\n    grad = np.zeros(n_categories)\n    for i in range(n_categories):\n        grad[i] = freq[i] / weights[i]\n    return -grad\n\ndata = a['A1']\nn_categories = len(data.value_counts())\ninit_params = np.ones(n_categories) / n_categories\nresult = sciopt.minimize(neg_log_likelihood, init_params, args=(data,), method='BFGS', jac=neg_log_likelihood_grad)\nweights = result.x\n",
        "\npopt = sciopt.fmin_l_bfgs_b(e, pmin, args=(x,y), bounds=[(pmin[0],pmax[0]),(pmin[1],pmax[1])])\n",
        "\nresult = []\nfor i in range(len(arr)):\n    if i < n or i >= len(arr) - n:\n        continue\n    if arr[i] <= max(arr[i-n:i+1]) and arr[i] <= max(arr[i:i+n+1]):\n        result.append(i)\n",
        "\nfrom scipy import signal\narr = np.array([[-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,], [3, 0, 0, 1, 2, 4]])\nn = 2\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if (i == 0 or arr[i-1][j] <= arr[i][j]) and (i == arr.shape[0]-1 or arr[i+1][j] <= arr[i][j]) and (j == 0 or arr[i][j-1] <= arr[i][j]) and (j == arr.shape[1]-1 or arr[i][j+1] <= arr[i][j]):\n            result.append([i,j])\n",
        "\ndf = df[(np.abs(stats.zscore(df[['NUM1', 'NUM2', 'NUM3']])) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'])\n",
        "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'])\n",
        "\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col4'])\n",
        "\ndf_out = pd.get_dummies(df.iloc[:, -1], prefix=df.columns[-1], prefix_sep='_')\n",
        "\n\ndf = pd.DataFrame({'Col1': ['C', 'A', 'B'],\n                   'Col2': [33, 2.5, 42],\n                   'Col3': [['Apple', 'Orange', 'Banana'],\n                            ['Apple', 'Grape'],\n                            ['Banana']]})\n\n# One-hot-encode the last column\ndf_out = pd.get_dummies(df, columns=['Col3'])\n\n# Rename the columns to have the same name as the original column\ndf_out.columns = ['Col1', 'Col2'] + list(df['Col3'].explode().unique())\n\n",
        "\n# Use CalibratedClassifierCV to obtain probability estimates\ncalibrated_svm = CalibratedClassifierCV(svmmodel, cv=5)\ncalibrated_svm.fit(X, y)\nproba = calibrated_svm.predict_proba(x_test)[:, 1]\n",
        "\nfrom sklearn.calibration import CalibratedClassifierCV\nmodel = svm.LinearSVC()\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\ndf = pd.concat([df_origin, pd.DataFrame(transform_output.todense())], axis=1)\n",
        "\ndf = pd.DataFrame(transform_output.todense())\ndf = pd.concat([df_origin, df], axis=1)\n",
        "\n    # Convert the transform_output to a dense numpy array\n    transform_output = transform_output.toarray()\n    # Create a new dataframe with the transformed data\n    df_transformed = pd.DataFrame(transform_output, columns=df.columns)\n    # Concatenate the original dataframe with the transformed dataframe\n    result = pd.concat([df, df_transformed], axis=1)\n    ",
        "\nsteps = clf.named_steps()\ndel steps['poly']\n",
        "\n# Insert a step before 'dim_svm'\nclf.steps.insert(1, ('new_pca', PCA()))\n# Delete the step 'sVm_233'\ndel clf.steps[2]\n",
        "\n# Insert a step before 'svm' step\nclf.steps.insert(1, ('poly2', PolynomialFeatures()))\n# Delete the 'poly' step\ndel clf.steps[1]\n",
        "\nsteps = clf.named_steps()\ndel steps['poly']\n",
        "\n# Insert a step before 'dim_svm'\nclf.steps.insert(1, ('new_pca', PCA()))\n# Delete the step 'sVm_233'\ndel clf.steps[2]\n",
        "\n# Insert a step before 'svm' step\nclf.steps.insert(1, ('poly2', PolynomialFeatures()))\n# Delete the 'poly' step\ndel clf.steps[1]\n",
        "\nmodel = xgb.XGBRegressor()\nparamGrid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01, 0.001]}\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=False, fit_params=fit_params)\ngridsearch.fit(trainX, trainY)\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\ngridsearch.fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba, axis=0)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba, axis=0)\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    # inverse the StandardScaler to get back the real time\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\n\ndata = load_data()\n\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\n\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\n\ndata = load_data()\n\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\n\ntf_idf_out = pipe.fit_transform(data.test)\n",
        "\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\ndata, target = load_data()\n\npipe = Pipeline(steps=[\n    ('select', SelectKBest(k=2)),\n    ('clf', LogisticRegression())]\n)\n\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\n\nbc = BaggingClassifier(dt, n_estimators=20, max_samples=0.5, max_features=0.5)\n\nclf = GridSearchCV(bc, param_grid=param_grid, cv=5)\nclf.fit(X_train, y_train)\n\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\n",
        "\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n",
        "\nfrom sklearn import preprocessing\n\ndata = load_data()\n\n# Convert DataFrame to numpy array\ndata_array = data.values\n\n# Apply preprocessing.scale to numpy array\ndata_scaled = preprocessing.scale(data_array)\n\n# Convert numpy array back to DataFrame\ndf_out = pd.DataFrame(data_scaled, columns=data.columns, index=data.index)\n\n# [Missing]\n",
        "\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\ncolumn_names = X.columns[model.get_support()]\n",
        "\ncolumn_names = X.columns[model.get_support()]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nclosest_50_samples = km.fit_predict(X) == p\nclosest_50_samples = X[closest_50_samples]\nclosest_50_samples = pd.DataFrame(closest_50_samples).sample(50)\n",
        "\nclosest_50_samples = km.fit_predict(X) == p\nclosest_50_samples = X[closest_50_samples]\nclosest_50_samples = closest_50_samples[np.argsort(np.linalg.norm(closest_50_samples - km.cluster_centers_[p], axis=1))[:50]]\n",
        "\nclosest_100_samples = km.fit_predict(X) == p\n",
        "\n    closest_50_samples = []\n    for i in range(X.shape[0]):\n        dist = np.linalg.norm(X[i] - km.cluster_centers_[p], axis=1)\n        closest_50_samples.append(X[np.argsort(dist)[:50]])\n    closest_50_samples = np.concatenate(closest_50_samples)\n    ",
        "\n# One hot encoding\nX_train = pd.get_dummies(X_train)\n# Merge back with original training data\nX_train = pd.concat([X_train, pd.DataFrame(X_train[0])], axis=1)\nX_train.drop(columns=[0], inplace=True)\n",
        "\n# Fit the GradientBoostingClassifier with the one-hot encoded data\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\n\n# Generate some sample data\nX, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n\n# Create an SVR model with a gaussian kernel\nmodel = SVR(kernel='rbf')\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Predict the output for a new input\nX_test = np.array([[50]])\ny_pred = model.predict(X_test)\n\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\n# define the model\nmodel = SVR(kernel='rbf')\n\n# define the hyperparameters to tune\nparam_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n\n# define the grid search\ngrid_search = GridSearchCV(model, param_grid, cv=5)\n\n# fit the grid search\ngrid_search.fit(X, y)\n\n# get the best hyperparameters\nbest_params = grid_search.best_params_\n\n# define the model with the best hyperparameters\nmodel = SVR(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'])\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\n\n# Generate sample data\nX, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n\n# Create SVR model with polynomial kernel\nmodel = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Predict the output for a new input\nX_test = np.array([[50]])\ny_pred = model.predict(X_test)\n\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# create polynomial features\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n# create pipeline\nmodel = make_pipeline(poly, SVR())\n\n# fit, then predict X\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\n# Create a matrix of cosine similarities between the queries and the documents\ncosine_similarities_of_queries = np.zeros((len(queries), len(documents)))\nfor i, query in enumerate(queries):\n    query_tfidf = tfidf.transform([query])\n    for j, document in enumerate(documents):\n        document_tfidf = tfidf.transform([document])\n        cosine_similarities_of_queries[i][j] = np.dot(query_tfidf.toarray(), document_tfidf.toarray().T)[0][0]\n",
        "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef load_data():\n    # Load data here\n    return queries, documents\n\ndef get_term_frequency_inverse_data_frequency(documents):\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(documents)\n    return matrix\n\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities_of_queries = np.dot(tfidf, query_tfidf.T).toarray()\n    return cosine_similarities_of_queries\n\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ntfidf = TfidfVectorizer()\ntfidf.fit_transform(documents)\ncosine_similarities_of_queries = get_tf_idf_query_similarity(documents, query)\n",
        "\n    # Create a matrix of cosine similarities between the queries and the documents\n    cosine_similarities_of_queries = np.zeros((len(queries), len(documents)))\n    for i, query in enumerate(queries):\n        query_tfidf = tfidf.transform([query])\n        for j, document in enumerate(documents):\n            document_tfidf = tfidf.transform([document])\n            cosine_similarities_of_queries[i][j] = np.dot(query_tfidf.toarray(), document_tfidf.toarray().T)[0][0]\n    ",
        "\nnew_features = pd.DataFrame(np.zeros((len(features), max(len(f) for f in features))), columns=[f for f in features[0]])\nfor i, f in enumerate(features):\n    new_features.iloc[i, :len(f)] = f\n",
        "\nnew_f = pd.DataFrame(np.zeros((len(f), max(len(x) for x in f))), columns=[f'f{i}' for i in range(max(len(x) for x in f))])\nfor i, sample in enumerate(f):\n    for j, feature in enumerate(sample):\n        new_f.iloc[i, j] = 1\n",
        "\nnew_features = pd.DataFrame(np.zeros((len(features), max(len(f) for f in features))), columns=[f for f in features[0]])\nfor i, f in enumerate(features):\n    new_features.iloc[i, :len(f)] = f\n",
        "\n    # Convert the features to a 2D-array\n    new_features = np.array(features)\n    ",
        "\nnew_features = pd.DataFrame(np.zeros((len(features), max(len(f) for f in features))), columns=list(set().union(*features)))\nfor i, f in enumerate(features):\n    new_features.iloc[i, :len(f)] = 1\n",
        "\n# Using sklearn.cluster AgglomerativeClustering\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\ncluster_labels = model.fit_predict(data_matrix)\n",
        "\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n",
        "\n# Create a distance matrix using 1-d array of simM\ndistM = 1 - simM\n\n# Perform hierarchical clustering using AgglomerativeClustering\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\ncluster_labels = model.fit_predict(distM)\n",
        "\nfrom scipy.cluster.hierarchy import linkage, fcluster\nZ = linkage(data_matrix, 'ward')\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n",
        "\n# Perform hierarchical clustering using scipy.cluster.hierarchy\nZ = scipy.cluster.hierarchy.linkage(data_matrix, method='ward')\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, 2, criterion='maxclust')\n",
        "\n\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\nZ = scipy.cluster.hierarchy.linkage(simM, method='ward')\n\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, 2, criterion='maxclust')\n\n",
        "\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n\nscaler = StandardScaler()\nrobust_scaler = RobustScaler()\npower_transformer = PowerTransformer()\n\ncentered_scaled_data = scaler.fit_transform(data)\ncentered_scaled_data = robust_scaler.fit_transform(centered_scaled_data)\ncentered_scaled_data = power_transformer.fit_transform(centered_scaled_data)\n",
        "\nfrom sklearn.preprocessing import StandardScaler\n\ndata = load_data()\nassert type(data) == np.ndarray\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n\n",
        "\n# Create a PowerTransformer object with method='box-cox'\npt = PowerTransformer(method='box-cox')\n\n# Fit and transform the data\nbox_cox_data = pt.fit_transform(data)\n",
        "\nbox_cox_data, _ = boxcox(data)\n",
        "\n# Create a PowerTransformer object with method='yeo-johnson'\npt = PowerTransformer(method='yeo-johnson')\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\n# Create a PowerTransformer object with method='yeo-johnson'\npt = PowerTransformer(method='yeo-johnson')\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntext = load_data()\n\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+' + f\"[{string.punctuation}]\" + r'\\b')\ntransformed_text = vectorizer.fit_transform(text)\n\n",
        "\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n",
        "\nfrom sklearn.model_selection import train_test_split\n\ndata = load_data()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n\n",
        "\nfrom sklearn.model_selection import train_test_split\n\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n\n# Splitting the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.3, random_state=42)\n\n# Splitting the training and testing sets into x and y\nx_train = x_train.values\ny_train = y_train.values\nx_test = x_test.values\ny_test = y_test.values\n\n",
        "\n\ndef solve(data):\n    # Split the data into training and testing sets (80/20)\n    np.random.seed(42)\n    msk = np.random.rand(len(data)) < 0.8\n    train = data[msk]\n    test = data[~msk]\n    \n    # Split the training and testing sets into x and y\n    x_train = train.iloc[:, :-1]\n    y_train = train.iloc[:, -1]\n    x_test = test.iloc[:, :-1]\n    y_test = test.iloc[:, -1]\n    \n    return x_train, y_train, x_test, y_test\n\ndataset = pd.read_csv('example.csv', header=None, sep=',')\nx_train, y_train, x_test, y_test = solve(dataset)\n",
        "\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndf = pd.read_csv(\"generate_csv/all_data_device.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\n# generate another list\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n# Centroid values\ncentroids = kmeans.cluster_centers_\n#print(centroids)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(X[:, 0], X[:, 1], c=labels)\nax.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='#050505', s=1000)\nplt.title('K Mean Classification')\nplt.show()\n",
        "\n# Reshape the labels array to match the shape of the mse values array\nlabels = labels.reshape(len(f1), 1)\n# Concatenate the mse values array and the labels array\nX = np.concatenate((f1.reshape(len(f1), 1), labels), axis=1)\n# Get the mean mse values for each cluster\nmean_mse = X.groupby(X[:, 2]).mean()\n# Print the mean mse values for each cluster\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_indices = featureSelector.get_support(indices=True)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfeatureSelector = SelectKBest(f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_classif, k=1000)\nX_new = featureSelector.fit_transform(X, y)\nselected_feature_indices = featureSelector.get_support(indices=True)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n",
        "\nX = pd.DataFrame(X, columns=feature_names)\nX = X.reindex(sorted(X.columns), axis=1)\n",
        "\nX = pd.DataFrame(X, columns=feature_names)\nX = X.reindex(sorted(X.columns), axis=1)\n",
        "\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\n\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] #removes NaN values for each column to apply sklearn function\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) # either this or the next line\n    m = slope.coef_[0]\n    slopes.append(m)\n\n",
        "\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\ndf = load_data()\ndef Transform(df):\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\ntransformed_df = Transform(df)\n",
        "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\n# Reshape the numpy array into a 1D array\nnp_array = np.reshape(np_array, (np_array.shape[0] * np_array.shape[1],))\n\n# Create a MinMaxScaler object and fit and transform the data\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n\n# Reshape the transformed data back into a 2D array\ntransformed = np.reshape(transformed, (np_array.shape[0], np_array.shape[1]))\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\n# Reshape the numpy array into a 1D array\nnp_array_1d = np_array.reshape(-1, 1)\n\n# Create a MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Fit and transform the numpy array\ntransformed = scaler.fit_transform(np_array_1d)\n\n# Reshape the transformed array back into a 3D array\ntransformed = transformed.reshape(3, 3)\n\n",
        "\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = load_data()\n\ndef Transform(a):\n    scaler = MinMaxScaler(feature_range=(0,1))\n    new_a = scaler.fit_transform(a)\n    return new_a\n\ntransformed = Transform(np_array)\n",
        "\npredict = clf.predict([close_buy1, m5, m10, ma20])\n",
        "\nnew_X = np.array(X, dtype=object)\n",
        "\nnew_X = np.array(X, dtype=object)\n",
        "\nnew_X = np.array(X, dtype=object)\n",
        "\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataframe.iloc[:,:-1], dataframe.iloc[:,-1], test_size=0.2, random_state=42)\n\n# Create a logistic regression model\nlogReg = LogisticRegression()\n\n# Train the model on the training set\nlogReg.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = logReg.predict(X_test)\n\n# Print the accuracy score of the model\nfrom sklearn.metrics import accuracy_score\n",
        "\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n",
        "\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n",
        "\n    train_size = 0.8\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\n    train_dataframe = train_dataframe.sort_values(by='date')\n    test_dataframe = test_dataframe.sort_values(by='date')\n    ",
        "\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].apply(lambda x: scaler.fit_transform(x.values.reshape(-1,1)))\n",
        "\nmyData[['new_A2', 'new_A3']] = scaler.fit_transform(myData[['A2', 'A3']])\n",
        "\nfrom sklearn.feature_extraction.text import CountVectorizer\nwords = \"Hello @friend, this is a good day. #good.\"\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\n# Sort the feature names in alphabetical order\nfeature_names = sorted(feature_names)\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results[['params', 'mean_fit_time', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\nfull_results = full_results.sort_values(by='mean_fit_time')\n",
        "\n!pip install joblib\n",
        "\ncosine_similarity_matrix = 1 - sklearn.metrics.pairwise.cosine_similarity(tfidf.fit_transform(df['description']), tfidf.transform(df['description']))\n"
    ],
    "Pytorch": [
        "\n\n# Define the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Train the model\nfor epoch in range(10):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        output = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n        \n        # Change the learning rate dynamically\n        if epoch == 5:\n            optimizer.set_lr(0.001)\n",
        "\n\ndef load_data():\n    # Load data\n    # ...\n    # Define optimizer\n    optim = torch.optim.SGD(model.parameters(), lr=0.01)\n    return optim\n\ndef train_model(model, optim, train_loader, val_loader, num_epochs):\n    # Train model\n    for epoch in range(num_epochs):\n        # Train\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            # Forward pass\n            output = model(data)\n            loss = criterion(output, target)\n            # Backward and optimize\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n            # Adjust learning rate\n            if loss > prev_loss:\n                optim.lr = optim.lr * 0.1\n                print(f\"Adjusting learning rate to {optim.lr}\")\n            prev_loss = loss\n        # Validate\n        model.eval()\n        with torch.no_grad():\n            val_loss = 0\n            for data, target in val_loader:\n                output = model(data)\n                val_loss += criterion(output, target).item()\n            val_loss /= len(val_loader)\n        print(f\"Epoch {epoch+1}: Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\")\n\n# Load data\ntrain_loader, val_loader, test_loader = load_data()\n\n# Define model\nmodel = Net()\n\n# Define criterion and optimizer\ncriterion = nn.CrossEntropyLoss()\noptim = load_data()\n\n# Train model\ntrain_model(model, optim, train_loader, val_loader, num_epochs)\n",
        "\n\n# Define the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Train the model\nfor epoch in range(10):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        output = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n    # Change the learning rate dynamically\n    if epoch == 5:\n        optimizer.set_lr(0.001)\n",
        "\n\ndef load_data():\n    # Load data\n    # ...\n    # Define optimizer\n    optim = torch.optim.SGD(model.parameters(), lr=0.005)\n    return optim\n\ndef train_model(model, optim, train_loader, val_loader, num_epochs):\n    # Train model\n    for epoch in range(num_epochs):\n        # Train\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            # Forward pass\n            output = model(data)\n            loss = criterion(output, target)\n            # Backward and optimize\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n            # Adjust learning rate\n            if loss > prev_loss:\n                optim.lr = optim.lr * 0.1\n                print(f\"Adjusting learning rate to {optim.lr}\")\n            prev_loss = loss\n        # Validate\n        model.eval()\n        with torch.no_grad():\n            val_loss = 0\n            for data, target in val_loader:\n                output = model(data)\n                val_loss += criterion(output, target).item()\n            val_loss /= len(val_loader)\n        print(f\"Epoch {epoch+1}: Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n\n# Load data\ntrain_loader, val_loader, test_loader = load_data()\n\n# Define model\nmodel = Net()\n\n# Define criterion and optimizer\ncriterion = nn.CrossEntropyLoss()\noptim = load_data()\n\n# Train model\ntrain_model(model, optim, train_loader, val_loader, num_epochs=10)\n",
        "\nembedding_weights = torch.from_numpy(np.array(word2vec.wv.vectors))\nembedding_layer = nn.Embedding.from_pretrained(embedding_weights)\n",
        "\n    embedded_input = torch.from_numpy(np.array([word2vec[word] for word in input_Tensor]))\n    ",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\n\nA_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\n\nC = B[A_log, :] # This works\n\n",
        "\nA_logical = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical] # Throws error\n",
        "\n\nA_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\n\nC = B[:, A_log.bool()] # This works\n\n",
        "\n\nA_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\n\nC = B[A_log, :] # This works\n\n",
        "\n    C = B[:, A_log] # Throws error\n    ",
        "\n\nA_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\n\nC = B[:, A_log.bool()] # This works\n\n",
        "\nC = torch.index_select(B, dim=1, index=idx)\n",
        "\nx_array = np.array([\n   [0.5, 1.0, 2.0],\n   [4.0, 6.0, 8.0]\n], dtype=object)\nx_tensor = torch.tensor(x_array)\n",
        "\nx_tensor = torch.tensor(x_array, dtype=torch.double)\n",
        "\n    a = np.array(a, dtype=np.float32)\n    t = torch.from_numpy(a)\n    ",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nlens = load_data()\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\nmask = get_mask(lens)\n",
        "\ndiag_ele = Tensor_2D.diag()\nTensor_3D = torch.repeat_interleave(torch.diag(diag_ele), Tensor_2D.shape[0], dim=0)\n",
        "\nTensor_3D = torch.diag_embed(Tensor_2D, offset=0, dim1=-2, dim2=-1)\n",
        "\nab = torch.cat((a,b),0)\n",
        "\nab = torch.cat((a,b),0)\n",
        "\n    ab = torch.cat((a, b), 0)\n    ",
        "\na[ : , lengths : , : ]  = 0\n",
        "\na[ : , lengths : , : ]  = 2333\n",
        "\na[ : , : lengths , : ]  = 0\n",
        "\na[ : , : lengths , : ]  = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n    ",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx].flatten()\n",
        "\nresult = t[idx].flatten()\n",
        "\nresult = t[idx, torch.arange(t.shape[1])]\n",
        "\nresult = x.gather(1,ids.unsqueeze(-1).expand(-1,-1,x.shape[-1]))\n",
        "\nresult = x.gather(1,ids.unsqueeze(2).expand(-1,-1,x.shape[2]))\n",
        "\nresult = np.zeros((70,2))\nfor i in range(70):\n    idx = np.argmax(ids[i])\n    result[i] = x[i][idx]\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.zeros(softmax_output.shape[0], 1, dtype=torch.long)\nfor i in range(softmax_output.shape[0]):\n    y[i] = torch.argmin(softmax_output[i])\n",
        "\n    y = torch.argmax(softmax_output, dim=1)\n    ",
        "\n    y = torch.argmin(softmax_output, dim=1)\n    ",
        "\nloss = cross_entropy2d(images, labels)\n",
        "\n\nA, B = load_data()\n\n# Count the number of equal elements in the two tensors\ncnt_equal = np.count_nonzero(A == B)\n\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = np.count_nonzero(A != B)\n",
        "\nA, B = load_data()\ndef Count(A, B):\n    cnt_equal = 0\n    for i in range(len(A)):\n        if A[i] == B[i]:\n            cnt_equal += 1\n    return cnt_equal\ncnt_equal = Count(A, B)\n",
        "\ncnt_equal = np.count_nonzero(A[-x:] == B[-x:])\n",
        "\ncnt_not_equal = np.count_nonzero(np.not_equal(A[-x:], B[-x:]))\n",
        "\ntensors_31 = []\nfor i in range(0, 40, 10):\n    tensor = a[:, :, :, i:i+10, :]\n    tensors_31.append(tensor)\n",
        "\ntensors_31 = []\nfor i in range(0, 40, 10):\n    tensor = a[:, :, i:i+chunk_dim, :, :]\n    tensors_31.append(tensor)\n",
        "\noutput[mask==1] = clean_input_spectrogram[mask==1]\n",
        "\noutput[mask==0] = clean_input_spectrogram[mask==0]\n",
        "\nsigned_min = torch.min(torch.abs(x), torch.abs(y)) * torch.sign(x)\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.mul(torch.mul(sign_x, sign_y), max)\n",
        "\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = sign_x * min * sign_y\n    ",
        "\nconf, classes = torch.max(MyNet(input).reshape(1, 3), 1)\nconfidence_score = conf.item()\n",
        "\nresult = torch.cat((a[:, :2], (a[:, 2:].mean(dim=1).unsqueeze(1) + b[:, 1:2]), b[:, 2:]), dim=1)\n",
        "\n    # Create a new tensor with zeros\n    result = torch.zeros((a.shape[0], a.shape[1] + b.shape[1]))\n    # Copy the first tensor into the first part of the result tensor\n    result[:, :a.shape[1]] = a\n    # Calculate the average of the last column of 'a' and the first column of 'b'\n    avg = (a[:, -1] + b[:, 0]) / 2\n    # Copy the average into the last part of the result tensor\n    result[:, a.shape[1]:] = avg.reshape(-1, 1)\n    ",
        "\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[0., 0., 0.,0.]])\nr = torch.cat([t,new], dim=1)  # adding new row to the tensor\n",
        "\nt = torch.arange(4).reshape(1,2,2).float()\nnew=torch.tensor([[0., 0., 0.,0.]])\nr = torch.cat([t,new], dim=0)  # stacking vertically\n",
        "\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[-1, -1, -1, -1,]])\nr = torch.cat([t,new], dim=1)  # concatenate along the second dimension\n",
        "\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\n"
    ]
}