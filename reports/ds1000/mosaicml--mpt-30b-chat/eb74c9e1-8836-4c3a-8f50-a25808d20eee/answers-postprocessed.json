{
    "Pandas": [
        "\n# result = result.reset_index(drop=True)\n",
        "\ndf_shuffled = df.iloc[List]\n# [Missing Code]\nresult = df_shuffled.loc[df_shuffled['Type'] != df['Type']]\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\nvalue_counts_threshold = 2\n\nresult = df.copy()\n\nresult['Qu1'] = result['Qu1'].replace(result['Qu1'].value_counts(normalize=True).loc[lambda x: x >= value_counts_threshold].index, 'other')\nresult['Qu2'] = result['Qu2']\nresult['Qu3'] = result['Qu3'].replace(result['Qu3'].value_counts(normalize=True).loc[lambda x: x >= value_counts_threshold].index, 'other')\n\nprint(result)",
        "\n# result = df.apply(lambda x: x.value_counts().reset_index(name='counts')[x.value_counts() >= 3].index.tolist())\n# \n# value_counts_df = {}\n# for col in df.columns:\n#     value_counts_df[col] = df[col].value_counts().reset_index(name='counts')[df[col].value_counts() >= 3].index.tolist()\n# \n# result = df.replace(df.apply(lambda x: x.value_counts().reset_index(name='counts')[x.value_counts() >= 3].index.tolist()), value_counts_df)\n",
        "import pandas as pd\n\nexample_df = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\ndef f(df=example_df):\n    result = df.copy()\n    \n    # Qu1\n    qu1_counts = df['Qu1'].value_counts()\n    qu1_to_keep = qu1_counts[qu1_counts >= 2].index.to_list()\n    result.loc[result['Qu1'].isin(qu1_to_keep), 'Qu1'] = 'other'\n    \n    # Qu2\n    qu2_counts = df['Qu2'].value_counts()\n    qu2_to_keep = qu2_counts[qu2_counts >= 2].index.to_list()\n    result.loc[result['Qu2'].isin(qu2_to_keep), 'Qu2'] = 'other'\n    \n    # Qu3\n    qu3_counts = df['Qu3'].value_counts()\n    qu3_to_keep = qu3_counts[qu3_counts >= 2].index.to_list()\n    result.loc[result['Qu3'].isin(qu3_to_keep), 'Qu3'] = 'other'\n    \n    # Qu1\n    result.loc[result['Qu1'] not in qu1_to_keep, 'Qu1'] = 'other'\n    \n    # Qu2\n    result.loc[result['Qu2'] not in qu2_to_keep, 'Qu2'] = 'other'\n    \n    # Qu3\n    result.loc[result['Qu3'] not in qu3_to_keep, 'Qu3'] = 'other'\n    \n    # Qu1, Qu2, Qu3\n    result.loc[result['Qu1'] == 'egg', 'Qu1'] = 'other'\n    result.loc[result['Qu2'] == 'apple', 'Qu2'] = 'other'\n    result.loc[result['Qu3'] == 'apple', 'Qu3'] = 'other'\n    \n    return result\n\nf()",
        "import pandas as pd\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\nvalue_counts_threshold = 2\n\nresult = df.copy()\nresult['Qu1'] = result['Qu1'].replace(result['Qu1'].value_counts(normalize=True).loc[lambda x: x >= 1/3].index, 'other')\nresult['Qu2'] = result['Qu2'].replace(result['Qu2'].value_counts(normalize=True).loc[lambda x: x >= 1/3].index, 'other')\nresult['Qu3'] = result['Qu3'].replace(result['Qu3'].value_counts(normalize=True).loc[lambda x: x >= 1/3].index, 'other')\n\nprint(result)",
        "\nvalue_counts_qu1 = df['Qu1'].value_counts()\nvalue_counts_qu2 = df['Qu2'].value_counts()\nvalue_counts_qu3 = df['Qu3'].value_counts()\n\n# replace values in Qu1\ndf.loc[df['Qu1'].isin(value_counts_qu1[value_counts_qu1 < 3].index), 'Qu1'] = 'other'\ndf.loc[df['Qu1'] == 'apple', 'Qu1'] = 'apple'\n\n# replace values in Qu2 and Qu3\ndf.loc[df['Qu2'].isin(value_counts_qu2[value_counts_qu2 < 2].index), ['Qu2', 'Qu3']] = 'other'\n\n",
        "\n# result = df.drop_duplicates(subset='url', keep='first')\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'drop_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\nresult = df.loc[~df.duplicated(subset=['url'], keep=False)]\n\n# keep only rows where drop_if_dup is No\nresult = result[result['drop_if_dup'] == 'No']\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\nresult = df.sort_values(['url', 'keep_if_dup']).groupby(['url', 'keep_if_dup']).last().reset_index(drop=True)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\n\ndef dataframe_to_nested_dict(df):\n    result = {}\n    for index, row in df.iterrows():\n        name = row['name']\n        key = row['v1']\n        subkey = row['v2']\n        value = row['v3']\n        if name not in result:\n            result[name] = {}\n        if key not in result[name]:\n            result[name][key] = {}\n        result[name][key][subkey] = value\n    return result\n\nresult = dataframe_to_nested_dict(df)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\n# Remove the time zone info from the datetime column\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n\nresult = df\nprint(result)\n",
        "import pandas as pd\n\nexample_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\nexample_df['datetime'] = pd.to_datetime(example_df['datetime'])\ndef f(df=example_df):\n    result = df['datetime'].apply(lambda x: x.str[:-6])\n    return result\n```",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\n# Remove the time zone info from the datetime column\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n\nresult = df\nprint(result)\n",
        "\ndf['datetime'] = df['datetime'].apply(lambda x: x.replace('-06:00','').strftime('%Y-%m-%d %H:%M:%S'))\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\n\nresult = df.explode('message').apply(lambda x: x.str.split(', ')).apply(lambda x: pd.Series(x.tolist())).add_prefix('message.')\nresult = pd.concat([df, result], axis=1)\nresult = result.drop(columns=['message'])\nresult = result.replace({'none': None})\n\nprint(result)",
        "\nfor product in products:\n    df.loc[df['product'] == product, 'score'] = df.loc[df['product'] == product, 'score'] * 10\n    \n",
        "\nmask = df['product'].isin(products)\ndf.loc[mask, 'score'] = 10 * df.loc[mask, 'score']\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [[1069104, 1069105], [1066489, 1066491]]\n\nfor product_list in products:\n    product_min, product_max = product_list\n    for i, row in df.iterrows():\n        if row['product'] >= product_min and row['product'] <= product_max:\n            df.at[i, 'score'] *= 10\n\nresult = df\nprint(result)\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1069104, 1069105]\n\nmin_scores = df.loc[df['product'].isin(products), 'score'].min()\nmax_scores = df.loc[df['product'].isin(products), 'score'].max()\n\ndf.loc[df['product'].isin(products), 'score'] = (df.loc[df['product'].isin(products), 'score'] - min_scores) / (max_scores - min_scores)\n\nresult = df\nprint(result)",
        "\ncategory = df.apply(lambda row: ''.join(row[col] for col in df.columns if row[col] == 1), axis=1)\n",
        "\ncategory = df.apply(lambda row: ''.join([str(col) for col in row.values]), axis=1)\n",
        "\ncategory = []\nfor col in df.columns:\n    category.append(df[col].apply(lambda x: x if x else None).dropna())\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\n\ndf['Date'] = df['Date'].dt.strftime(\"%B-%Y\")\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\n\nresult = df.apply(lambda x: f\"{x['Date'].dt.month}-{x['Date'].dt.day}-{x['Date'].dt.year}\")\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\nList = ['2019-01-17', '2019-02-20']\n\n# convert the list of dates to datetime format\nList_datetime = [pd.to_datetime(d) for d in List]\n\n# get the month name and year and day in a simple way in the following format\nresult = df.loc[(df['Date'] >= List_datetime[0]) & (df['Date'] <= List_datetime[1]), ['Date','Day','Month','Year']]\nresult.columns = ['Date', 'Day', 'Month', 'Year']\nresult = result.apply(lambda x: x.strftime('%d-%m-%Y %A'))\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\nresult = df.loc[:, '#1'].shift(-1).ffill().reset_index(drop=True).rename(columns={'#1': 'shifted_#1'})\ndf.loc[0, '#1'] = result['shifted_#1']\ndf.loc[0, '#2'] = result['#2']\ndf.loc[5, '#1'] = result['shifted_#1']\ndf.loc[5, '#2'] = result['#2']\n\nprint(df)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\nresult = df.iloc[::-1].reset_index(drop=True)\nresult.loc[0, '#1'] = 72.4399\ndf.loc[-1, '#1'] = 11.6985\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the first row of the first column down 1 row\ndf.loc[df.index[1], '#1'] = df.loc['1980-01-01', '#1']\ndf.drop(df.index[0], inplace=True)\n\n# Shift the last row of the first column to the first row\ndf.loc['1980-01-01', '#1'] = df.loc['1980-01-05', '#1']\n\n# Shift the last row of the second column up 1 row\ndf.loc[df.index[-1], '#2'] = df.loc['1980-01-04', '#2']\ndf.drop(df.index[0], inplace=True)\n\n# Shift the first row of the second column to the last row\ndf.loc[df.index[-1], '#2'] = df.loc['1980-01-02', '#2']\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the first row of the first column down 1 row\ndf.loc[df.index[1], '#1'] = df.loc['1980-01-01', '#1']\ndf.shift(1, '#1')\n\n# Shift the last row of the first column to the first row, first column\ndf.loc['1980-01-01', '#1'] = df.loc['1980-01-05', '#1']\ndf.loc['1980-01-05', '#1'] = np.nan\ndf.dropna(subset=['#1'])\n\nresult = df\nprint(result)\n",
        "\nnew_columns = {col + 'X': col for col in df.columns}\n",
        "\ndf = df.add_prefix('X')\n",
        "\ndf.columns = ['XHeaderAX', 'XHeaderBX', 'XHeaderCX', 'XHeaderX']\n",
        "\n# result = df.groupby('group').agg({\"group_color\": \"first\", \"val\": \"mean\"})\n",
        "\n# result.columns = ['group_color'] + [f'val{i+1}' for i in range(len(df[df.columns.str.contains(\"val\")].columns))]\n",
        "\n# result = result.drop(['val1_2', 'val2_2', 'val1_3', 'val2_3', 'val1_4', 'val2_4'], axis=1)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.loc[row_list].mean(axis=1)[column_list]\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.iloc[row_list].sum(axis=1)[column_list]\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.iloc[row_list].sum(axis=1)[column_list]\nprint(result)\n",
        "import pandas as pd\n\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\nresult = df.apply(lambda x: pd.Series(x.value_counts().index, x.value_counts().values, name=x.name))\n\nprint(result)",
        "\nnull_counts = df.isna().sum()\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\nresult = ''\nfor col in df.columns:\n    counts = df[col].value_counts()\n    result += f'---- {col} ---n'\n    result += f'{counts.to_string(index=False)}n'\n\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1.883444,0.564835,0.144654]})\n\nresult = df.iloc[[0,1]].reset_index(drop=True).rename(columns={'Nanonose': 'Sample type', 'Unnamed: 1': 'Concentration'})\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1.883444,0.564835,0.144654]})\n\nresult = df.iloc[[0,1:]].reset_index(drop=True)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\n\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\n\n# [Missing Code]\n# result = pd.DataFrame(result)\n# result.loc[result.isnull().any(axis=1),:] = np.nan\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[3,1,2],[1,2,np.nan],[2,np.nan,np.nan]],columns=['0','1','2'])\n\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1)\n\n# [Missing Code]\n# result = pd.concat([df.loc[:,df.columns!=1].apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1),df.loc[:,1].apply(lambda x : (x[x.notnull()].values.tolist()),1)],axis=1)\n# \n# \n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\n\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n\n# [Missing Code]\n# result = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n# \n# # Create a new DataFrame with columns in reverse order\n# new_df = pd.DataFrame(columns=df.columns[::-1])\n# \n# # Iterate through rows and add non-null values to new DataFrame\n# for i in range(df.shape[0]):\n#     row = df.iloc[i]\n#     new_row = []\n#     for j in range(len(row)):\n#         if not np.isnan(row[j]):\n#             new_row.append(row[j])\n#     new_df.loc[i] = new_row\n# \n# # Reverse columns again\n# result = new_df[new_df.columns[::-1]]\n# \n# print(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n\nresult = df.groupby(level=0).filter(lambda x: x['value'] < thresh).sum().reset_index()\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n\nresult = df.groupby(level=0).apply(lambda x: x if x['value'] < thresh else x.mean())\n\nprint(result)\n",
        "\nsection_mask = (df['value'].between(section_left, section_right))\nmean_value = df.loc[section_mask].mean()['value']\n",
        "import pandas as pd\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nresult = pd.DataFrame({})\nfor col in df:\n    inv = 1 / df[col]\n    result[f\"inv_{col}\"] = inv\n\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nresult = df.copy()\n\nfor col in df.columns:\n    result[f\"exp_{col}\"] = np.exp(df[col])\n\nprint(result)",
        "import pandas as pd\n\ndf = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [4, 5, 6]})\n\nresult = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [4, 5, 6], \"inv_A\": [1/df['A'], 0, 1/df['A'].iloc[-1]], \"inv_B\": [1/df['B'], 1/df['B'].iloc[1], 1/df['B'].iloc[2]]})\n\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nresult = pd.DataFrame({})\nfor col in df.columns:\n    result[f'sigmoid_{col}'] = 1 / (1 + np.exp(-df[col]))\n\nprint(result)",
        "\nmax_mask = df.max(axis=1) == df.max(axis=1).shift(-1)\nmin_mask = df.min(axis=1) == df.min(axis=1).shift(1)\nresult = df.loc[max_mask & min_mask, :].idxmax()\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\n\n\nidx = pd.date_range('2017', periods=a.shape[0])\ndf = pd.DataFrame(a, index=idx, columns=list('abc'))\n\nresult = df.idxmin()\nresult = result.loc[result.eq(df.max(axis=1).idxmax()).dropna()]\n\nprint(result)\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = df.loc[df['dt'].dt.date >= min_dt.date, :] \\\n    .reindex(pd.date_range(min_dt, max_dt, freq='D')) \\\n    .fillna(0)\n\nresult['dt'] = result.dt.apply(lambda x: x.date())\n\nprint(result)",
        "import pandas as pd\n\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = df.assign(dt=pd.date_range(min_dt, max_dt, freq='D'))\nresult['val'] = 0\n\nprint(result)",
        "import pandas as pd\n\ndf = pd.DataFrame({'user': ['a', 'a', 'b', 'b'], 'dt': ['2016-01-01', '2016-01-02', '2016-01-05', '2016-01-06'], 'val': [1, 33, 2, 1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = df.reindex(pd.date_range(min_dt, max_dt, freq='D'))\nresult['val'] = 233\n\nprint(result)",
        "\nimport pandas as pd\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\nresult = df.assign(dt=lambda df: df.dt.add(pd.DateOffset(days=1))).reset_index(drop=True)\nresult = result.merge(df.loc[df['user'].idxmax(), :].rename(columns={'user': 'user_max', 'val': 'val_max'}), on=['user', 'dt'])\nresult = result.fillna(value={'user': 'user_max', 'val': 'val_max'})\n\nresult = result.assign(dt=lambda df: df.dt.dt.strftime('%Y-%m-%d')).reset_index(drop=True)\n\nprint(result)\n",
        "\n# result['val'] = result.groupby(['user', 'dt'])['val'].transform(max)\n",
        "\nunique_names = set(df['name'])\nname_map = {name: i+1 for i, name in enumerate(unique_names)}\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\nunique_ids = df['a'].unique()\n\nresult = df.assign(a=df['a'].map(unique_ids))\n\nprint(result)\n",
        "\n    unique_ids = df['name'].unique()\n    df['name'] = unique_ids\n",
        "\nresult = result.drop(['name_1'], axis=1)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n\ndf = df.pivot_table(index='user', columns='date', values='value', aggfunc=lambda x: ','.join(x))\ndf.columns = ['date', 'value']\ndf = df.reset_index()\n\nresult = df\nprint(result)\n",
        "\ndf = df.pivot_table(index='user', columns=['01/12/15', '02/12/15', 'someBool'], values=['01/12/15', '02/12/15', 'someBool'], \n                    fill_value=0, \n                    aggfunc={'01/12/15': 'first', '02/12/15': 'first', 'someBool': 'first'})\n",
        "\ndf = df.pivot_table(index='user', columns=['01/12/15', '02/12/15'], values=['value'], aggfunc=pd.Series.sum)\ndf.columns = ['date', 'value']\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['b','e']\n\nlocs = [df.columns.get_loc(_) for _ in columns]\nmask = df.c > 0.5\nresult = df.loc[mask, locs].values\n\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['a','b','e']\n\nlocs = [df.columns.get_loc(_) for _ in columns]\nmask = df.c > 0.45\nresult = df.iloc[mask].loc[locs]\n\nprint(result)\n",
        "import pandas as pd\ndef f(df, columns=['b', 'e']):\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs].values\n    return result\n```",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs].sum(axis=1).reset_index(drop=True)\n",
        "def f(df, columns=['b', 'e']):\n    result = df.loc[df.c > 0.5, columns]\n    return result\n```python\n\nNote that I changed the method from `.ix` to `.loc` because `.loc` is generally faster and more memory efficient. I also added the ability to specify the `columns` parameter to select only the desired columns.",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 120\n\nresult = df.copy()\nfilter_dates = []\nfor index, row in result.iterrows():\n    if X == 120:\n        for i in range(1, 121):\n            filter_dates.append((index.date() + timedelta(days=i)))\n    else:\n        for i in range(1, X+1):\n            filter_dates.append((index.date() + timedelta(days=i)))\n    \nresult = result[~result.index.isin(filter_dates)]\n\nprint(result)\n",
        "\nfilter_dates = []\nfor index, row in df.iterrows():\n    if X == 1:\n        filter_dates.append(index.date())\n    else:\n        for i in range(1, X):\n            filter_dates.append((index.date() + timedelta(weeks=i)))\n    \ndf = df[~df.index.isin(filter_dates)]\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n\nfilter_dates = []\nfor index, row in df.iterrows():\n    if X == 1:\n        filter_dates.append((index.date()))\n    else:\n        for i in range(1, X):\n            filter_dates.append((index.date() + timedelta(weeks=i)))\n\ndf = df[~df.index.isin(filter_dates)]\n\nresult = df.groupby(pd.Grouper(key='date', freq='W')).max()\n\nprint(result)\n",
        "\n# result['col1'] = result['col1'].apply(lambda x: x if x > 0 else 0.5)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\n\nresult = df.reset_index(drop=True).iloc[::3].reset_index(drop=True)\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1, 4]})\n\nresult = []\nfor i in range(0, len(df), 4):\n    row = df.iloc[i:i+4].sum()\n    result.append(row)\n\nprint(pd.concat(result))\n",
        "\nbinned_df = df.iloc[::3].reset_index(drop=True)\n# [Missing Code]\nresult = pd.concat([df.iloc[:3], binned_df], axis=0)\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\nresult = pd.DataFrame()\ncurrent_sum = 0\ncurrent_count = 0\ncurrent_avg = 0\nfor i in range(0, len(df), 3):\n    current_sum += df.iloc[i]\n    current_count += 1\n    \n    if i+2 < len(df):\n        current_avg = current_sum / current_count\n        result = result.append(pd.DataFrame({'col1':[current_avg]}))\n        current_sum = df.iloc[i+1]\n        current_count = 1\n        \nprint(result)",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\nresult = pd.DataFrame()\ncurrent_sum = 0\ncurrent_count = 0\ncurrent_avg = 0\ncurrent_index = 0\n\nfor i in range(len(df)):\n    if i % 3 == 0 and i != 0:\n        current_sum = 0\n        current_count = 0\n        current_index += 1\n        result.loc[current_index] = current_avg\n    elif i % 3 == 1:\n        current_sum += df.iloc[i]\n        current_count += 1\n    elif i % 3 == 2:\n        current_avg = current_sum / current_count\n        current_sum = 0\n        current_count = 0\n\nresult.loc[current_index + 1] = df.iloc[-1]\nresult.loc[current_index + 2] = df.iloc[-2]\n\nprint(result)\n",
        "\nresult = df.fillna(method='ffill')\n",
        "\nimport pandas as pd\n\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Fill zeros with posterior non-zero value\nresult = df.fillna(df.loc[df.index[df.neq(0).all(axis=1)]].iloc[:, 0])\n\nprint(result)\n",
        "\nresult = df.loc[df.neq(0).idxmax()+1]\n# [Missing Code]\ndf.loc[df.neq(0).idxmax()+1, 'A'] = result\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\n\ndf['number'] = df.duration.str.extract(r'\\d+')\ndf['time'] = df.duration.str.extract(r'([a-zA-Z]+)(\\d+)')\ndf['time_days'] = df.time.apply(lambda x: x[1] if x[0] in ['year', 'month', 'week', 'day'] else None)\ndf.loc[df.time_days is None, 'time_days'] = df.time.apply(lambda x: x[0] if x[1] is None else None)\ndf['time_days'] = df.time_days.fillna(df.time_days.astype('int').div(30).floor())\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\ndf['number'] = df.duration.str.extract(r'\\d+')\ndf['time'] = df.duration.str.extract(r'\\w+')\ndf['time_day'] = df.time.replace({'year': '365', 'month': '30', 'week': '7', 'day': '1'}, regex=True)\n\nresult = df\nprint(result)\n",
        "import pandas as pd\n\nexample_df = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\ndef f(df=example_df):\n    df['number'] = df.duration.str.extract(r'\\d+').astype(int)\n    df['time'] = df.duration.str.extract(r'\\w+').str.capitalize()\n    df['time_days'] = df.time.replace({'year': '365', 'month': '30', 'week': '7', 'day': '1'}, regex=True)\n    return df\nresult = f()\nprint(result)\n```",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\ndf['number'] = df.duration.str.extract(r'\\d+').astype(int)\ndf['time'] = df.duration.str.extract(r'\\w+').str.capitalize()\n\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number']\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\ncolumns_check_list = ['A','B','C','D','E','F']\n\nresult = np.where([df1[column] != df2[column] | \n                   column in columns_check_list \n                   for column in df1.columns], \n                  [True], \n                  [False])\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\n\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\n\ncolumns_check_list = ['A','B','C','D','E','F']\n\nresult = np.where([df1[column] == df2[column] for column in columns_check_list], True, False).all()\n\nprint(result)\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\n    df_dates = pd.to_datetime(df['date'])\n    df_x = df['x'].values\n    df_y = df['y'].values\n",
        "import pandas as pd\ndef f(df):\n    df = df.reset_index()\n    df = pd.to_datetime(df['date'], errors='coerce')\n    df = df.set_index(['id', df['date'].dt.strftime('%Y-%m-%d')])\n    df = df.reset_index(drop=True)\n    return df\ndf = pd.DataFrame({'x': [100, 90, 80], 'y': [7, 8, 9], 'id': ['abc', 'abc', 'abc'], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   'Value': [12, 1, 20, 0]})\n\ndf = pd.melt(df, id_vars=['Country', 'Variable'], \n             value_vars=['Value'], \n             var_name='year', \n             value_name='Var1')\n\ndf = df.dropna()\n\nresult = pd.pivot_table(df, index=['Country'], columns=['year'], values=['Var1'], aggfunc=pd.Series.sum).reset_index()\n\nresult.columns = ['year', 'Var1']\nresult.index.name = 'Country'\nresult.drop(['Variable'], axis=1, inplace=True)\n\nresult = pd.concat([result, df.groupby('Country')['Variable'].apply(lambda x: pd.Series(x, name='var2'))], axis=1)\n\nresult.columns.insert(1, 'var2')\n\nresult = result[['year', 'var1', 'var2']]\n\nresult\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   '2000': [12, 1, 20, 0],\n                   '2001': [15, 3, 23, 1],\n                   '2002': [18, 2, 25, 2],\n                   '2003': [17, 5, 29, 2],\n                   '2004': [23, 7, 31, 3],\n                   '2005': [29, 5, 32, 3]})\n\ndf = pd.melt(df, id_vars=['Country', 'Variable'], \n             value_vars=['2000', '2001', '2002', '2003', '2004', '2005'], \n             var_name='year', value_name='Var1')\n\n# sort by year in descending order\ndf = df.sort_values(by=['Country', 'year'], ascending=[True, False])\n\n# create a list of variables to melt\nvariables = ['var1', 'var2']\n\n# melt additional variables\nfor var in variables:\n    df = pd.concat([df, pd.melt(df.groupby('Country')[var].max().reset_index(), \n                                id_vars=['Country', 'year'], \n                                var_name=var, value_name=var)], axis=0)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\nresult = df[abs(df.filter(like='Value').sum(axis=1)) < 1]\n\nprint(result)\n",
        "\nmask = abs(df[df.columns.str.startswith('Value')]) > 1\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\nresult = df[abs(df.filter(like='Value').max(axis=1))>1]\nresult.columns = result.columns.str.replace('Value', '')\nprint(result)\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT;', '<', regex=True)\n",
        "\n    df = df.replace(result.loc[lambda x: x == 'Good &AMP; bad'], x.str.replace('&AMP;', '&'))\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &GT; bad'] * 5})\n\nresult = df.applymap(lambda x: x.replace('&AMP;', '&').replace('&LT;', '<').replace('&GT;', '>'))\n\nprint(result)\n",
        "\ndf = df.applymap(lambda x: x.replace('&AMP;', '&'))\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name.split(' ')[0]\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: x if not isinstance(x, str) else None)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name.split(' ')[0], name.split(' ')[1]\n    else:\n        return name, None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if not pd.notna(x.split(' ')[1]) else None)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\n\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name.split(' ')[0]\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: ' '.join(x.split(' ')[1:]))\ndf['middle_name'] = df['name'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n\nresult = df\nprint(result)\n",
        "\n# [Missing Code]\nresult = pd.concat([df1.loc[df1['Timestamp'].isin(df2['Timestamp'])], df2], axis=0)\n",
        "\nimport pandas as pd\n\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\n\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\n\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n\nresult = pd.merge_asof(df1, df2, on='Timestamp', tolerance=pd.Timedelta(minutes=1))\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2021-04-10 01:00:00', '2021-04-10 02:00:00', '2021-04-10 03:00:00', '2021-04-10 04:00:00', '2021-04-10 05:00:00'],\n                   'col1': [25, 25, 25, 50, 100],\n                   'col2': [50, 50, 100, 50, 100],\n                   'col3': [50, 50, 50, 100, 100]})\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\nresult = df.apply(lambda x: x['col1'] if (x['col2'] <= 50 and x['col3'] <= 50) else max(x['col1'], x['col2'], x['col3']), axis=1).rename('state')\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2021-04-10 01:00:00', '2021-04-10 02:00:00', '2021-04-10 03:00:00', '2021-04-10 04:00:00', '2021-04-10 05:00:00'],\n                   'col1': [25, 25, 25, 50, 100],\n                   'col2': [50, 50, 100, 50, 100],\n                   'col3': [50, 50, 50, 100, 100]})\n\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] > 50 and x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'], axis=1)\n\nresult = df\nprint(result)\n",
        "\nerrors = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], (int, float)):\n        errors.append(row['Field1'])\n",
        "\nresult = []\nfor index, row in df.iterrows():\n    if isinstance(row['Field1'], (int, float)):\n        result.append(row['Field1'])\n    else:\n        result.append(None)\n",
        "\nimport pandas as pd\n\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    result = []\n    for index, row in df.iterrows():\n        if isinstance(row['Field1'], (int, float)):\n            pass\n        else:\n            result.append(row['Field1'])\n    return result\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\nresult = df.apply(lambda x: x.div(x.sum(), axis=1).mul(100))\nresult.columns = ['val1', 'val2', 'val3', 'val4']\n\nresult = result.reset_index().rename(columns={'index': 'cat'})\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\nresult = df.div(df.sum(axis=1), axis=0)\n\n# Reset index and add '%' to values\nresult.index = range(len(df))\nresult.replace(1, 0, inplace=True)\nresult.add(1, inplace=True)\nresult.index = result.index.map(lambda x: f\"val{x}\")\nresult.index = result.index.str %=\"\"\n",
        "\nimport pandas as pd\nimport io\n\ndata = io.StringIO(\"\"\"\nrs  alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP7      A/T      0    7      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP15     C/A      0   15      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n\"\"\")\ndf = pd.read_csv(data, delim_whitespace=True).set_index('rs')\ntest = ['TP3', 'TP7', 'TP18']\nresult = df.loc[test]\nprint(result)\n",
        "\n# result = df.select(test)\n",
        "\ndf.drop(test, inplace=True)\n",
        "\n    result = []\n    for t in test:\n        if t in df.index:\n            result.append(df.loc[t])\n",
        "\nimport pandas as pd\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\ndf2 = df.groupby('time')['car'].apply(lambda x: x.sort()[0] if len(x) > 1 else x[0])\ndf2 = df2.reset_index()\ndf2['nearest_neighbour'] = df2['car'].shift(-1)\ndf2['nearest_neighbour'] = df2['nearest_neighbour'].fillna(df2['car'])\ndf2['euclidean_distance'] = df.apply(lambda x: np.sqrt((x['x'] - x['x'].shift(-1)) ** 2 + (x['y'] - x['y'].shift(-1)) ** 2), axis=1)\n\nresult = df2[['time', 'car', 'nearest_neighbour', 'euclidean_distance']]\n\nprint(result)\n",
        "import pandas as pd\nimport numpy as np\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\ndf2 = df.groupby('time')[['car']].apply(lambda x: x.sort(key=lambda y: np.sqrt((y['x']-x['x'])**2 + (y['y']-x['y'])**2)))\ndf2 = df2.reset_index()\n\ndf2['farmost_neighbour'] = df2['car'].shift(-1)\ndf2['farmost_neighbour'].iloc[0] = df2['car'].iloc[0]\ndf2['euclidean_distance'] = np.sqrt((df2['x']-df2['farmost_neighbour']['x'])**2 + (df2['y']-df2['farmost_neighbour']['y'])**2)\n\nresult = df2.groupby('time')['euclidean_distance'].mean().reset_index()\n\nresult.columns = ['time', 'average_distance']\n\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ncols = [df.columns[i] for i in range(len(df.columns))]\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \",\".join(row), axis=1)\n\nresult = df\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ncols = [df.columns[i] for i in range(len(df.columns))]\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \"-\".join(row), axis=1)\n\nresult = df\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.columns[i] for i in range(len(df.columns)) if df.columns[i] != 'users']\ndf[\"keywords_all\"] = '-'.join(df[cols].fillna('').values.tolist())\n\nresult = df\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.loc[:, col] for col in ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']]\ndf[\"keywords_all\"] = \"-\".join(cols)\n\nresult = df\nprint(result)",
        "\nmask = np.random.rand(len(df)) < 0.2\nsample_indices = np.where(mask)[0]\ndf.loc[sample_indices, 'Quantity'] = 0\nindex_to_keep = df.index.tolist()\nsample_index_to_remove = np.setdiff1d(np.arange(len(df)), sample_indices)\ndf.index = index_to_keep\ndf.drop(sample_index_to_remove, inplace=True)\n",
        "\nsample_rows = df.sample(n=int(0.2*len(df)), random_state=0)\ndf.loc[sample_rows.index, 'ProductId'] = 0\n",
        "import random\nimport numpy as np\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1, 1, 4, 7, 4, 2, 1, 1, 4, 7],\n                   'Quantity': [6, 1, 3, 2, 7, 2, 6, 1, 3, 2, 7, 2, 6, 1, 3]})\n\n# select 20% of rows for each user\nmask = np.random.choice([0, 1], size=df.shape[0], p=[1-0.2, 0.2], replace=False)\ndf.loc[mask, 'Quantity'] = 0\n\nresult = df\nprint(result)",
        "\nindex_original = duplicate.index.tolist()\n",
        "\nindex_original = duplicate.index.tolist()\n",
        "\nimport pandas as pd\n\nexample_df=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\ndef f(df=example_df):\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    df_no_duplicates = df.loc[~duplicate_bool]\n    df_duplicates = df.loc[duplicate_bool == True]\n    df_duplicates['index_original'] = df_duplicates.index\n    result = df_duplicates\n    return result\n",
        "\nindex_original = duplicate.index.tolist()\nindex_original.append(index_original[0])\n",
        "\nindex_original = duplicate['index'].values\nresult = pd.concat([duplicate.drop('index', axis=1), pd.DataFrame({'index_original': index_original})], axis=1)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\nresult = df.groupby(['Sp', 'Mt']).filter(lambda x: x['count'].eq(x['count'].max()))\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp':['MM2','MM2','MM4','MM4','MM4'],\n                   'Mt':['S4','S4','S2','S2','S2'],\n                   'Value':['bg','dgd','rd','cb','uyi'],\n                   'count':[10,1,2,8,8]})\n\nresult = df.groupby(['Sp','Mt']).filter(lambda x: x['count'].idxmax())\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\nresult = df.groupby(['Sp', 'Mt']).filter(lambda x: x['count'].eq(x['count'].min()))\nprint(result)",
        "import pandas as pd\n\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n\nresult = df.groupby(['Sp','Value']).filter(lambda x: x['count'].eq(x['count'].max()))\n\nprint(result)",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({\"Category\": ['Foo', 'Bar', 'Cho', 'Foo'], 'Index': [1, 2, 3, 4]})\nfilter_list = ['Foo', 'Bar']\n\nresult = df.query(f\"Category in {filter_list}\")\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({\"Category\": ['Foo', 'Bar', 'Cho', 'Foo'], 'Index': [1, 2, 3, 4]})\nfilter_list = ['Foo', 'Bar']\n\nresult = df.query(\"Category != @filter_list\")\n\nprint(result)\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n\nvalue_vars = [tuple(col) for col in zip(*df.columns)]\nresult = pd.melt(df, value_vars=value_vars)\n\nprint(result)",
        "import pandas as pd\n\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n\nvalue_vars = [(0,1,2), (3,4,5), (6,7,8)]\nresult = df.melt(id_vars=['index'], value_vars=value_vars)\nprint(result)",
        "\ndf['cumsum'] = df.groupby('id').apply(lambda x: x['val'].cumsum())\n",
        "\ncumsum = 0\nresult = df.copy()\nresult['cumsum'] = 0\nfor i, row in df.iterrows():\n    cumsum = result.loc[i, 'val']\n    result.loc[i, 'cumsum'] = cumsum\n",
        "\n# df['cumsum'] = df.groupby('id').cumsum(['val'])\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\n\ndf['cummax'] = df.groupby('id').apply(lambda x: x.val.cummax())\n\nprint(df)\nresult = df\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\n\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum())\ndf.loc[df['cumsum'] < 0, 'cumsum'] = 0\n\nprint(df)\nresult = df\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n    \nresult = df.groupby('l')['v'].apply(lambda x: np.nan if np.isnan(x.sum()) else x.sum())\n    \nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('r')['v'].transform(lambda x: x.fillna(x.mean()))\nprint(result)\n",
        "\nresult = df.groupby('l')['v'].apply(lambda x: np.nan if np.isnan(x).sum() else x.sum())\n",
        "import pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\nresult = []\nfor i in range(len(df.columns)-1):\n    for j in range(i+1, len(df.columns)):\n        if len(df[df[df.columns[i]] == df[df.columns[j]]].index) > 1:\n            result.append(f\"{df.columns[i]} {df.columns[j]} many-to-many\")\n        elif len(df[df[df.columns[i]] == df[df.columns[j]]].index) == 1:\n            result.append(f\"{df.columns[i]} {df.columns[j]} one-to-one\")\n        else:\n            result.append(f\"{df.columns[i]} {df.columns[j]} none\")\n\nprint(result)",
        "import pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\nresult = []\n\nfor i in range(len(df.columns)-1):\n    for j in range(i+1, len(df.columns)):\n        col1 = df.iloc[:, i].unique()\n        col2 = df.iloc[:, j].unique()\n        \n        if len(col1) == len(col2):\n            if len(set(col1).intersection(col2)) == 1:\n                relationship = 'one-2-one'\n            else:\n                relationship = 'many-2-many'\n        elif len(col1) > len(col2):\n            if len(set(col1).intersection(col2)) == 1:\n                relationship = 'one-2-many'\n            else:\n                relationship = 'many-2-one'\n        else:\n            if len(set(col1).intersection(col2)) == 1:\n                relationship = 'many-2-many'\n            else:\n                relationship = 'one-2-one'\n        \n        result.append(f'{df.columns[i]} {df.columns[j]} {relationship}')\n\nprint(result)\n```",
        "import pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\nresult = df.apply(lambda x: x.nunique() in [1, 2] and x.nunique(dropna=False) in [1, 2] and x.dtypes in ['int', 'float'] and x.nnull.sum() == 0, axis=1)\n\nresult = pd.DataFrame(result, columns=['Type of Relationship'])\n\ndf_out = pd.concat([df.drop(['Column1', 'Column2', 'Column3', 'Column4', 'Column5'], axis=1), result], axis=1)\n\ndf_out.columns = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Type of Relationship']\nprint(df_out)",
        "import pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\nresult = df.apply(lambda x: x.nunique() in [1, 2] and x.nunique(dropna=False) in [1, 2] and x.dtypes in ['int', 'float'] and x.dtypes not in ['object'] and x.axis[0] not in result.columns, axis=1) and df.drop(df[result].index).empty\n\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar'],\n                   'bank': [np.nan, 'abc', 'xyz']})\n\n# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# keep the records with a bank account\nresult = df.loc[df['bank'].notna()]\n\n# save unique records\nresult = result.loc[uniq_indx]\n\nprint(result)",
        "import pandas as pd\n\ns = pd.Series(['2,144.78', '2,036.62', '1,916.60', '1,809.40', '1,711.97', '6,667.22', '5,373.59', '4,071.00', '3,050.20', '-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0'],\n              index=['2016-10-31', '2016-07-31', '2016-04-30', '2016-01-31', '2015-10-31', '2016-01-31', '2015-01-31', '2014-01-31', '2013-01-31', '2016-09-30', '2016-06-30', '2016-03-31', '2015-12-31', '2015-09-30', '2015-12-31', '2014-12-31', '2013-12-31', '2012-12-31'])\n\nresult = pd.to_numeric(s.str.replace(',',''), errors='coerce')\nprint(result)",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Group the dataframe by the condition\n# (df['SibSp'] > 0) | (df['Parch'] > 0)\ngrouped = df.groupby((df['SibSp'] > 0) | (df['Parch'] > 0))\n\n# Calculate the mean of the 'Survived' column for each group\nmean_survived = grouped['Survived'].mean()\n\n# Reset the index of the result\nmean_survived.reset_index(inplace=True)\n\n# Rename the index level to 'New Group'\nmean_survived.index.name = 'New Group'\n\n# Create a new column 'Group' to indicate the group\nmean_survived['Group'] = 'Has Family'\n\n# Add a new row with the mean of 'No Family' group\nmean_survived.loc[len(mean_survived)] = {'Group': 'No Family', 'Survived': 1.0}\n\nresult = mean_survived\n\nprint(result)\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\nresult = pd.DataFrame()\n\nresult['Group'] = np.where((df['Survived'] > 0) | (df['Parch'] > 0), 'Has Family', 'No Family')\nresult['Mean'] = df.groupby('Group')['SibSp'].mean()\n\nprint(result)",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Define the groups\ngroups = [(df['SibSp'] == 0) & (df['Parch'] == 0),\n          (df['SibSp'] == 1) & (df['Parch'] == 0),\n          (df['SibSp'] == 0) & (df['Parch'] == 1),\n          (df['SibSp'] == 1) & (df['Parch'] == 1)]\n\n# Calculate the means for each group\nmeans = [df.loc[df[cond]['Survived'].mean()] for cond in groups]\n\n# Combine the means into a single dataframe\nresult = pd.concat(means)\n\nprint(result)\n",
        "\n# result = result.reset_index()\n",
        "\n# result = result.reset_index(level=0)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\nbirds = np.random.choice(['African Swallow', 'Dead Parrot', 'Exploding Penguin'], size=int(5e4))\nsomeTuple = np.unique(birds, return_counts=True)\n\nresult = pd.DataFrame({\n    'birdType': [bird[0] for bird in someTuple],\n    'birdCount': [bird[1] for bird in someTuple]\n})\n\nprint(result)\n",
        "import numpy as np\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\nresult = pd.Series(data=df.groupby('a')['b'].apply(lambda x: np.std(np.mean(x))))\n\nresult.name = 'mean_std'\n\nprint(result)",
        "\n# result = pd.Series(data=df.groupby('b')['a'].apply(stdMeann))\n",
        "\ngrouped = df.groupby('a')\ngrouped_b = grouped['b']\ngrouped_b_softmax = grouped_b.apply(softmax)\ngrouped_b_min_max = grouped_b.apply(min_max_normalization)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame([[1,1,0,1],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df.loc[df.sum(axis=1)==0].dropna(axis=0)\nresult = result.loc[result.sum(axis=1)==result.shape[1]-1].dropna(axis=1)\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame([[-1,-1,0,2],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df.loc[df.sum(axis=1) != 0].loc[df.sum(axis=0) != 0]\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df.loc[~df.eq(2).all(axis=1), ~df.eq(2).all(axis=0)]\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df.where(df<=2, 0)\n\nprint(result)\n",
        "\nimport pandas as pd\n\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n          index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n\nresult = s.sort_values(by=['index', 'value'], ascending=True)\n\n",
        "\nresult = s.sort_values([\"index\", \"1\"], ascending=True)\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\nresult = df[pd.to_numeric(df['A'], errors='coerce').notna()]\n\nprint(result)",
        "\n# result = df[df['A'].apply(lambda x: str(x)) == 's']\n",
        "\nresult = df.groupby(['Sp', 'Mt']).filter(lambda x: x['count'].eq(x['count'].max()))\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp':['MM2','MM2','MM4','MM4','MM4'],\n                   'Mt':['S4','S4','S2','S2','S2'],\n                   'Value':['bg','dgd','rd','cb','uyi'],\n                   'count':[10,1,2,8,8]})\n\nresult = df.groupby(['Sp','Mt']).filter(lambda x: x['count'].idxmax())\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\nresult = df.groupby(['Sp', 'Mt']).filter(lambda x: x['count'].eq(x['count'].min()))\n\nprint(result)",
        "\nresult = df.groupby(['Sp','Value']).filter(lambda x: x['count'].eq(x['count'].max()))\n",
        "\ndf['Date'] = df['Member'].map(dict)\n",
        "\ndf['Date'] = df['Member'].map(dict)\n",
        "import pandas as pd\n\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\ndef f(dict=example_dict, df=example_df):\n    result = df.join(pd.DataFrame(dict), on='Member', rsuffix='_old')\n    result['Date'] = result['Date_old']\n    result = result.dropna()\n    return result\n```",
        "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(df['Date'].dt.strftime('%d-%b-%Y'))\n",
        "\nimport pandas as pd\n\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date.year', 'Date.month']).size().rename('Count_d')\n\ndf1 = df.groupby(['Date.year', 'Date.month']).agg({'Count_d': 'sum'})\ndf1.columns = ['Count_m', 'Count_y']\n\nresult = df1\nprint(result)\n",
        "\nimport pandas as pd\n\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'], \n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').size()\n\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'Val': 'count'})\n\nresult = df1.reset_index()\nresult.columns = ['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_Val']\n\nprint(result)\n",
        "\nimport pandas as pd\n\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\ndf['Date'] = pd.to_datetime(df['Date'], format= '%m/%d/%y')\ndf = df.groupby(['Date.year', 'Date.month', 'Val']).size().reset_index(name='count')\ndf.drop(columns=['Date'], inplace=True)\ndf.rename(columns={'year': 'Count_y', 'month': 'Count_m'}, inplace=True)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n\nresult1 = pd.DataFrame(df.sum().reset_index().rename(columns={'index': 'Date'}))\nresult2 = pd.DataFrame(df.apply(lambda x: x.sum()).reset_index().rename(columns={'index': 'Date'}))\n\nprint(result1)\nprint(result2)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n\nresult1 = df.groupby('Date')['B'].apply(lambda x: x.even.sum()).reset_index(level=0, drop=True)\nresult1.name = 'even'\n\nresult2 = df.groupby('Date')['B'].apply(lambda x: x.odd.sum()).reset_index(level=0, drop=True)\nresult2.name = 'odd'\n\nprint(result1)\nprint(result2)\n",
        "import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n\nresult = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.sum)\nresult = pd.concat([result, pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)], axis=1)\nresult.columns = ['D_sum', 'E_mean']\nprint(result)",
        "\n# result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.mean, np.sum])\n",
        "\n# [Missing Code]\n",
        "import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n\nresult = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=lambda x: x[0] if x[0] > x[1] else x[1])\nprint(result)",
        "\nimport dask.dataframe as dd\nimport numpy as np\n\ndf = dd.from_pandas(pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2']), npartitions=4)\n\nresult = df.compute()\n\nsplit_vars = np.repeat(df['var2'].str.split(',').compute(), len(df), axis=0)\nsplit_vars.names = ['var2']\n\nresult = dd.concat([result, split_vars], axis=1)\n\nresult = result.reset_index()\n\nresult.columns = ['id', 'var1', 'var2']\n\nprint(result)\n",
        "\nresult = result.explode('var2').compute() # [Missing Code]\nresult = result.groupby('var1').apply(lambda x: x.str.split(',')).reset_index(level=0, drop=True)",
        "\nresult = result.explode('var2').compute() # [Missing Code]\nresult = result.groupby('var1').apply(lambda x: x.reset_index(drop=True)).reset_index()",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n",
        "\ndf = df.explode('row').reset_index()\ndf.columns = ['fips', 'row']\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\n",
        "\ndf = df.explode('row').reset_index(drop=True)\ndf = df.rename(columns={'row': 'medi'}).assign(fips='00' + df.index.str[:5])\n",
        "\ncumulative_average = df.loc[:, ~df.columns.isin([col for col in df.columns if '0' in col])].mean(axis=1, skipna=True)\n",
        "\ncumulative_avg = df.iloc[:-1].mean(axis=1).cumsum()\nnon_zero_index = df.iloc[:-1].dropna(axis=1).index\ncumulative_avg.loc[non_zero_index] = df.iloc[:-1].loc[non_zero_index].mean(axis=1).values\ncumulative_avg.loc[-1] = df.iloc[-1].mean(axis=1)\n",
        "\nimport pandas as pd\n\nexample_df = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\ndef f(df=example_df):\n    result = pd.DataFrame()\n    for col in df.columns:\n        cumulative_sum = df[df[col] != 0][col].cumsum()\n        cumulative_average = cumulative_sum / df[df[col] != 0][col].count()\n        result[col] = cumulative_average\n    return result\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\ncumulative_average = df.iloc[::-1].mean(skipna=True).reset_index(drop=True)\ncumulative_average.name = 'cumulative_average'\ndf = pd.concat([df, cumulative_average], axis=1)\ndf = df.sort_index(ascending=False)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07'],\n                   'Close': [1460, 1470, 1480, 1450]})\n\ndf['Label'] = 0\ndf.loc[0, 'Label'] = 1\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\n\nresult = df.assign(label=lambda df: df.Close.diff().apply(lambda x: 1 if x > 0 else 0 if x == 0 else -1).shift(-1).fillna(0))\nresult.loc[result.index[0], 'label'] = 1\nresult.dropna(inplace=True)\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\n\nresult = df.assign(label=lambda df: df.iloc[1:].apply(lambda row: row['Close'].shift().ge(row['Close']).astype(int), axis=1).ffill().astype(int).shift(-1).sub(df.iloc[0]['Close'].shift().sub(df.iloc[0]['Close').astype(int)).astype(int).ffill().astype(int).shift(-1)).iloc[:-1]\n\nresult = result.rename(columns={'label': 'label_diff'})\nresult = result.assign(label=lambda df: df.iloc[0].assign(label=1)).iloc[1:]\nresult = result.assign(DateTime=lambda df: df['DateTime'].dt.strftime('%d-%B-%Y'))\n\nprint(result)\n",
        "\ndf['Duration'] = df.departure_time.shift(-1) - df.arrival_time\ndf['Duration'] = df['Duration'].apply(lambda x: x.total_seconds() if not pd.isna(x) else x)\n",
        "\nimport pandas as pd\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\ndf['Duration'] = df.apply(lambda row: (row.departure_time - row.arrival_time).total_seconds(), axis=1)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\ndf['Duration'] = df['departure_time'].diff(df['arrival_time'])\ndf['arrival_time'] = df['arrival_time'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\n\nresult = df\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'] == 'one'].shape)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x[x['key2'] == 'two'].size)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'gee', 'two', 'three', 'two']})\n\nresult = df.groupby(['key1']).size().reset_index(name='count')\nmask = df['key2'].str.endswith('e')\nresult['count'] = result['count'][mask]\n\nprint(result)\n",
        "import pandas as pd\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\nmax_result = df.index.max()\nmin_result = df.index.min()\n\nprint(max_result,min_result)",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\nmode_result = df.mode(axis=0).index[0]\nmedian_result = df.median(axis=0).index[0]\n\nprint(mode_result,median_result)\n",
        "import pandas as pd\nimport numpy as np\n\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\n\nresult = df[(99 <= df['closing_price'] <= 101)]\n\nprint(result)",
        "\nresult = df[~((99 <= df['closing_price']) & (df['closing_price'] <= 101))]\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({\"item\": [1, 1, 1, 2, 2, 2, 2, 3, 3],\n                   \"diff\": [2, 1, 3, -1, 1, 4, -6, 0, 2],\n                   \"otherstuff\": [1, 2, 7, 0, 3, 9, 2, 0, 9]})\n\nresult = df.groupby(\"item\", as_index=False)[[\"diff\", \"otherstuff\"]].min()\nprint(result)\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').apply(lambda x: x[0] if len(x) == 1 else ''.join(x)).str.lstrip('0')\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').apply(lambda x: x[0] if len(x) > 1 else x[0])\n",
        "\nimport pandas as pd\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\nexample_df = pd.DataFrame(data={'SOURCE_NAME': strs})\ndef f(df=example_df):\n    result = []\n    for s in df['SOURCE_NAME']:\n        if '_' in s:\n            s_split = s.split('_')\n            last_index = len(s_split) - 1\n            s_last = s_split[last_index]\n            result.append(s_last)\n        else:\n            result.append(s)\n    df['SOURCE_NAME'] = result\n    return df\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# fill the first 50% of NaN values with '0' and the last 50% with '1'\nfill_values = np.concatenate((np.repeat(0, int(len(df) * 0.5 - len(df) / 2)), \n                              np.repeat(1, int(len(df) * 0.5 - len(df) / 2))))\ndf.loc[np.isnan(df['Column_x']), 'Column_x'] = fill_values\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# Fill NaN values with appropriate value based on percentage\nnan_count = len(df[df.isna()])\nthreshold = int(nan_count * 0.3)\n\ndf.loc[df.isna(), 'Column_x'] = np.select([df.index < threshold], ['0'],\n                                         [df.index >= threshold] & [df.index < (nan_count * 0.7)], ['0.5'],\n                                         default=['1'])\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,1,1,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\nfilled_df = df.fillna(value=df['Column_x'].value_counts(normalize=True).reset_index(name='value').iloc[:int(df['Column_x'].count()/2)].index)\n\nresult = filled_df\nprint(result)\n",
        "\n# result = pd.DataFrame()\n# for i in range(len(a)):\n#     result = result.append(pd.DataFrame(list(zip(a.iloc[i]['one'], b.iloc[i]['one']), list(zip(a.iloc[i]['two'], b.iloc[i]['two']))), columns=['one', 'two']))\n",
        "\nimport pandas as pd\nimport numpy as np\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nc = pd.DataFrame(np.array([[9, 10],[11, 12]]), columns=['one', 'two'])\n\nresult = pd.DataFrame([[(a['one'].iloc[i], b['one'].iloc[i], c['one'].iloc[i]), (a['two'].iloc[i], b['two'].iloc[i], c['two'].iloc[i])] for i in range(len(a))], columns=['one', 'two'])\n\nprint(result)\n",
        "\n# result = pd.DataFrame([], columns=['one', 'two'])\n# for i in range(min(a.shape[0], b.shape[0])):\n#     result = result.append(pd.DataFrame([[(a.iloc[i, 0], b.iloc[i, 0]), (a.iloc[i, 1], b.iloc[i, 1])], [np.nan, np.nan]], columns=['one', 'two']))\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\nresult = pd.DataFrame()\nfor i, bin_lower in enumerate(bins[:-1]):\n    bin_upper = bins[i+1]\n    grouped = df.groupby([pd.cut(df.views, [bin_lower, bin_upper], right=False)])\n    counts = grouped.username.count()\n    result = result.join(counts.reset_index(level=0).rename(columns={'username': f'views_{bin_lower}_{bin_upper}'}))\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\nresult = pd.DataFrame()\ngroups = df.groupby(['username'])\nfor name, group in groups:\n    group_bins = pd.cut(group.views, bins)\n    group_counts = group.groupby(group_bins).size().reset_index(name='counts')\n    result = pd.concat([result, group_counts], axis=0)\nresult.columns = ['views', 'username']\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'username': ['tom', 'tom', 'tom', 'tom', 'jack', 'jack', 'jack', 'jack'],\n                   'post_id': [10, 8, 7, 6, 5, 4, 3, 2],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\nresult = pd.DataFrame()\nfor bin_val, bin_range in zip(bins, bins[1:]+[float('inf')]):\n    group = df[df.views >= bin_val]\n    count = len(group)\n    result = result.append(pd.DataFrame({'views': [bin_val], 'username': [group.username.unique().tolist()], 'count': [count]}))\n\nresult.columns = ['views', 'username', 'count']\nprint(result)\n",
        "\ndf['text'] = result\n",
        "\nresult = '-'.join(df['text'])\n",
        "\nresult = ', '.join(df['text'])\n",
        "\n# result = df['text'].agg(' '.join)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\nresult = ''\nfor i, row in df.iterrows():\n    result += row['text']\n    if i != len(df) - 1:\n        result += '-'\n\nprint(result)\n",
        "import pandas as pd\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult['city'] = result['city'].fillna(result['city'].mode()[0])\nresult['district'] = result['district'].fillna(result['district'].mode()[0])\nprint(result)",
        "import pandas as pd\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(by=['id', 'date'])\nresult = result.groupby('id').apply(lambda x: x.sort_values(by='date'))\nresult = result.fillna({'city': 'NaN', 'district': 'NaN'})\nresult = result.astype({'date': 'str'})\nresult = result.replace({'date': lambda x: x.str.replace('/', '-')})\n\nprint(result)",
        "import pandas as pd\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(by=['id', 'date'])\nresult = result.groupby('id', as_index=False).apply(lambda x: x.sort_values(by='date'))\nresult = result.fillna({'city': 'NaN', 'district': 'NaN'})\nprint(result)",
        "\n# result.drop(columns=['B_y'], inplace=True)\n",
        "\n# result = result.drop(columns=['B_y'])\n",
        "\nimport pandas as pd\n\n\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n\nresult = pd.merge(C, D, how='outer', on='A')\nresult.loc[result['B_y'].notna(), 'B'] = result['B_y']\nresult.loc[result['B'].isna(), 'dulplicated'] = False\nresult.loc[result['A'].duplicated(keep=False), 'dulplicated'] = True\nresult.drop(columns=['B_x', 'B_y'], inplace=True)\n\nprint(result)\n",
        "\nresult = result.apply(lambda x: sorted(x, key=lambda y: y[1]))\n",
        "\n# result = df.groupby('user').agg(lambda x: x.tolist()))\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nresult = df.groupby('user').agg(lambda x: sorted(x.tolist(), key=lambda x: (x[1], x[0])))\n\n# print(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\nresult = pd.DataFrame(list(series.values.flatten()))\nresult.index = series.index\n\nresult = result.reshape(-1, 4)\nresult.columns = ['0', '1', '2', '3']\n\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\nresult = pd.DataFrame(list(series.values.flatten()), columns=['0', '1', '2', '3'])\n\nresult.index = series.index\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\nresult = []\nfor col in df.columns:\n    if s in col and col != s:\n        result.append(col)\nprint(result)\n",
        "\nimport pandas as pd\n\ndata = {'spike-2': [1, 2, 3], 'hey spke': [4, 5, 6], 'spiked-in': [7, 8, 9], 'no': [10, 11, 12]}\ndf = pd.DataFrame(data)\ns = 'spike'\nresult = df.columns[df.columns.str.contains(s, na=False) & df.columns.str.contains('-', na=False)]\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndata = {'spike-2': [1, 2, 3], 'hey spke': [4, 5, 6], 'spiked-in': [7, 8, 9], 'no': [10, 11, 12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\ndef find_spike_column(df, s):\n    result = []\n    for col in df.columns:\n        if s in col and col.split(s)[1] != '':\n            result.append(col)\n    return result\n\nresult = find_spike_column(df, s)\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\nresult = pd.DataFrame()\nfor i in range(len(df)):\n    temp_df = pd.DataFrame({col:df.iloc[i]['codes'][j] for j, col in enumerate([f'code_{j}' for j in range(len(df.iloc[i]['codes'])])}\n    result = pd.concat([result, temp_df], axis=0)\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\nresult = df.explode('codes').reset_index(drop=True)\n\nfor i in range(len(result)):\n    if len(result.iloc[i]['codes']) < 3:\n        result.iloc[i]['codes'] += ['NaN']*(3-len(result.iloc[i]['codes']))\n\nresult = result[['codes'] + ['code' + str(j+1) for j in range(3)]]\nresult.columns = ['code_1', 'code_2', 'code_3']\n\nprint(result)\n",
        "\nsplit_df = df['codes'].apply(lambda x: pd.Series(x)).stack().reset_index(level=1, drop=True)\nsplit_df.columns = ['code_' + str(i+1) for i in range(len(split_df.columns))]\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nresult = []\nfor index, row in df.iterrows():\n    ids = df.loc[index, 'col1'].values.tolist()\n    for id in ids:\n        result.append(id)\n\nprint(result)\n",
        "\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nresult = ', '.join(str(x) for sub_list in df['col1'] for x in sub_list)\n\nprint(result)\n",
        "\ndf_binned = df.groupby(pd.Grouper(key='Time', freq='2T')).mean()\n",
        "\ndf_binned = df.groupby(pd.Grouper(freq='3T', origin='2015-04-24 06:36:00')).sum()\n",
        "\ndf['RANK'] = df['RANK'].astype(int)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME_FORMATTED'] = df['TIME'].dt.strftime('%d-%B-%Y %H:%M:%S %A')\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n                    'b': [1,2,3,1,2,3,1,2,3],\n                    'c': range(9)}).set_index(['a', 'b'])\nfilt = pd.Series({1:True, 2:False, 3:True})\n\nresult = df.loc[~(df.index.get_level_values('a').isin([2])), :]\nresult.loc[filt, 'c'] = df.c[filt]\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n                    'b': [1,2,3,1,2,3,1,2,3],\n                    'c': range(9)}).set_index(['a', 'b'])\nfilt = pd.Series({1:True, 2:False, 3:True})\n\nresult = df.loc[~(df.index.get_level_values('a').isin([2]) & df.index.get_level_values('b').isin([2]))]\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n\nresult = []\nfor i in range(10):\n    col = df.columns[i]\n    row0 = df.iloc[0][col]\n    row8 = df.iloc[8][col]\n    if not equalp(row0, row8):\n        result.append(col)\n\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n\nresult = []\nfor i in range(len(df)):\n    same_cols = []\n    for j in range(len(df)):\n        if i != j and equalp(df.iloc[i], df.iloc[j]):\n            same_cols.append(df.columns[j])\n    result.append(same_cols)\n\nprint(result)\n",
        "import pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef find_different_columns(row1, row2):\n    result = []\n    for col in range(len(df.columns)):\n        if df.iloc[row1, col] != df.iloc[row2, col]:\n            result.append(df.columns[col])\n    return result\n\nrow0 = df.iloc[0]\nrow8 = df.iloc[8]\nresult = find_different_columns(row0, row8)\nprint(result)",
        "import pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\n\ndef find_different_pairs(df):\n    result = []\n    for i in range(len(df)):\n        for j in range(len(df)):\n            if j == i:\n                continue\n            if not np.isnan(df.iloc[i,j]):\n                if np.isnan(df.iloc[j,i]):\n                    result.append((df.iloc[i,j], df.iloc[j,i]))\n                else:\n                    result.append((df.iloc[i,j], np.isnan(df.iloc[j,i])))\n                    result.append((np.isnan(df.iloc[i,j]), df.iloc[j,i]))\n    return result\n\nresult = find_different_pairs(df)\nprint(result)\n```",
        "\nimport pandas as pd\n\ndates = ['2016-1-{}'.format(i) for i in range(1, 21)]\nvalues = [i for i in range(20)]\ndata = {'Date': dates, 'Value': values}\ndf = pd.DataFrame(data)\ndf['Date'] = pd.to_datetime(df['Date'])\n\nts = pd.Series(df['Value'], index=df['Date'])\n\nresult = ts\n\nprint(result)\n",
        "\n# result.columns = ['level_1', 'level_0', 'index']\n# result.columns = result.columns.droplevel(0)\n",
        "\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n\nresult = pd.concat([df.iloc[:,i].reset_index(drop=True) for i in range(df.shape[1])],axis=1)\nresult = pd.concat([result.iloc[:,i+1] for i in range(result.shape[1]-1)],axis=1)\nresult.columns = ['_'.join(col).replace('_','') for col in result.columns.values]\nresult = result.dropna()\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, .03), (.21, .18),(pd.NA, .18)],\n                  columns=['dogs', 'cats'])\n\ndf = df.applymap(lambda x: round(x, 2) if isinstance(x, float) else x)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\n\ndf[['dogs', 'cats']] = df[['dogs', 'cats']].applymap(lambda x: round(x, 2) if isinstance(x, float) else x)\n\nresult = df\nprint(result)\n",
        "\ndf['Sum'] = sum(df[list_of_my_columns])\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\navg_cols = [df[col] for col in list_of_my_columns]\navg = np.mean(avg_cols, axis=1)\ndf['Avg'] = avg\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort(level='time')\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_index(level=['treatment','dose'], ascending=[True,True], kind='mergesort')\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\ndf.index = df.index.remove(pd.to_datetime(hd1_from))\ndf.index = df.index.remove(pd.to_datetime(hd1_till))\n\nresult = df\n\nprint(result)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\n\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\n# Delete rows with dates '2020-02-17' and '2020-02-18'\nhd1_from = '2020-02-17 15:30:00'\nhd1_till = '2020-02-17 21:59:00'\ndf = df[(df.index < hd1_from) | (df.index > hd1_till)]\n\n# Get the day of the week for each date\ndf['Day of Week'] = df.index.day_name()\n\n# Display the result\nprint(df)\n",
        "\n# result = corr.filter(regex='.*', axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\n\nresult = corr.filter(regex='^(-?[0-9]+)d$', axis=1)\nresult = result[result > 0.3]\n\nprint(result)\n",
        "\n# result.columns[-1] = 'Test'\n",
        "\nresult = df.rename(columns={df.columns[0]: 'Test'}, inplace=True)\n",
        "\nfrequent = df.mode(axis=1)[0]\nfreq_count = df.apply(lambda x: x.mode(axis=1)[0].count(), axis=1)\n",
        "\nfrequent_values = {}\nfor _, row in df.iterrows():\n    frequent = row.mode()[0]\n    frequent_values[frequent] = df[df[frequent] == frequent].shape[0]\n",
        "\nfrequent_values = {}\nfor row in df.itertuples():\n    values = [col for col in row]\n    frequent_values[tuple(values)] = len(values)\n",
        "\nimport pandas as pd\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\n\n# fill missing values with 0\nresult = result.fillna(0)\n\n# add 'bar' column to result\nresult['bar'] = result['bar'].apply(lambda x: x if x is not None else 0)\n\nprint(result)\n",
        "\nres = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nresult = res.reset_index()\nresult.loc[result['bar'].isna(),'bar'] = 0\n",
        "\n# result = result.dropna()\n",
        "\n# result = result.dropna()\n"
    ],
    "Numpy": [
        "\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx[x == np.nan] = np.inf\n",
        "\nimport numpy as np\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nresult = [[elem for elem in row if not np.isnan(elem)] for row in x]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.zeros((len(a)+1, max(a)+1))\nb[np.arange(len(a)+1), a] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.zeros((len(a)+1, max(a)+1))\nfor i, j in enumerate(a):\n    b[i+1][j] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([-1, 0, 3])\nb = np.zeros((len(a)+1, len(np.unique(a))), dtype=np.uint8)\nb[np.argsort(a)+1] = np.eye(len(np.unique(a)))[np.argsort(np.unique(a))]\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1.5, -0.4, 1.3])\nb = np.eye(3)[np.argsort(a)]\nb[np.argsort(a)[::-1]] = 1 - b\n",
        "\nb[0,a[0,0]] = 1\nb[1,a[0,1]] = 1\nb[2,a[1,0]] = 1\nb[3,a[1,1]] = 1\nb[4,a[2,0]] = 1\nb[4,a[2,1]] = 1\n",
        "\nimport numpy as np\na = np.array([1,2,3,4,5])\np = 25\nresult = np.percentile(a, p)\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nncol = 2\nB = np.array([A[i:i+ncol] for i in range(0, len(A), ncol)])\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nnrow = 3\nB = np.reshape(A, (nrow, -1))\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (ncol, -1))\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (ncol, -1))\nprint(B)\n",
        "\nimport numpy as np\na = np.array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\nshift = 3\nresult = np.roll(a, -shift)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = 3\nresult = np.full((a.shape[0], a.shape[1], a.shape[2], shift), np.nan)\nresult[:, :, :, -shift:] = a[:, :, :, ::-1]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = [-2, 3]\nresult = np.zeros_like(a)\nfor i in range(a.shape[0]):\n    result[i, shift[0]:, shift[1]:] = a[i, :, :]\n    del result[i, :shift[0], :shift[1]]\nprint(result)\n",
        "\nseed_value = 12345\nnp.random.seed(seed_value)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a, axis=1).ravel()\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmin(a, axis=0)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n",
        "\nimport numpy as np\nexample_a = np.array([[10,50,30],[60,20,40]])\ndef f(a = example_a):\n    result = np.argmax(a, axis=0)\n    return result\n",
        "\nsecond_largest = np.max(a, axis=1)[:1] # find the second largest value along each row\nsecond_largest_index = np.where(a == second_largest)[0] # find the index of the second largest value\n",
        "\nimport numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\nz = any(np.isnan(a), axis=0)\ndelete(a, z, axis=1)\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.zeros((3,3))\n# [Missing Code]\nfor i in range(3):\n    for j in range(3):\n        result[i][j] = a[i][j]\n",
        "\nnew_order = np.argsort(permutation)\nnew_a = np.take(a, new_order, axis=1)\n",
        "\nimport numpy as np\na = np.array([[[10, 20],\n        [30, 40]],\n       [[6,  7],\n        [8,  9]],\n\t[[10, 11],\n\t [12, 13]]])\npermutation = [1, 0, 2]\nresult = np.apply_along_axis(lambda x: x[permutation], axis=1, arr=a)\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nrow, col = np.where(a == np.min(a))\nresult = (row[0], col[0])\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nrow, col = np.where(a == a.max())\nresult = (row[0], col[0])\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1, 0], [0, 2]])\nresult = np.unravel_index(a.argmin(), a.shape)\nprint(result)\n",
        "\nimport numpy as np\ndegree = 90\nresult = np.sin(np.deg2rad(degree))\nprint(result)\n",
        "\nimport numpy as np\ndegree = 90\nresult = np.cos(np.radians(degree))\nprint(result)\n",
        "\nimport numpy as np\nnumber = np.random.randint(0, 360)\nresult = 0\nif np.sin(np.radians(number)) > np.sin(number):\n    result = 1\nprint(result)\n",
        "\nimport numpy as np\nvalue = 1.0\nresult = np.rad2deg(np.arcsin(value))\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.concatenate([A, np.zeros(length - A.shape[0], dtype=A.dtype)])\n# result = np.pad(A, (0, length - A.shape[0]), 'constant', constant_values=0)\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.concatenate([A, np.zeros(length-A.shape[0], dtype=int)])\nresult = result.astype(int)\n",
        "\nimport numpy as np\na = np.arange(4).reshape(2, 2)\npower = 5\na_squared = a**power\nprint(a_squared)\n",
        "\nimport numpy as np\nexample_a = np.arange(4).reshape(2, 2)\ndef f(a = example_a, power = 5):\n    result = a\n    for i in range(power-1):\n        result = result * a\n",
        "\nresult = result[::-1]\n",
        "\nimport numpy as np\ndef f(numerator = 98, denominator = 42):\n    result = np.rint(numerator / denominator)\n    return result\n",
        "\n# result = numerator/denominator\n# if denominator == 0:\n#     result = (np.nan, np.nan)\n",
        "\nimport numpy as np\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = np.divide(a+b+c, 3)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = np.maximum(a, b)\nresult = np.maximum(result, c)\n",
        "\ndiagonal = np.diag_indices(5, k=1)\nresult = a[diagonal]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\nresult = np.diag_indices(a.shape[0])[::-1, :][:, ::-1]\na[result]\n",
        "\nresult = np.array([a[np.diag_indices(k, m=-1)] for k in range(1, len(a)+1)])\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\nresult = np.diag_indices(k=-1, m=a.shape[0])[0] + 1\nresult = np.resize(result, a.shape[1])\n\nprint(result)\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n",
        "\n    for i in range(X.shape[0]):\n        result.append(X[i])\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n",
        "\nimport numpy as np\nmystr = \"100110\"\nresult = np.fromstring(mystr, dtype=int, sep='').reshape(-1,1)\nprint(result)\n",
        "\n# result = \n",
        "\nrow_sum = np.sum(a[row])\ncumulative_sum = np.cumsum(np.array([row_sum]*a.shape[1]))\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\ndivide_number = 5.2\nresult = a[row] / divide_number\nresult = np.multiply.reduce(a[row])\nprint(result)\n",
        "\nb = np.linalg.qr(a)[0]\nresult = np.array_split(b, 3, axis=1)\n",
        "\nimport numpy as np\na = np.random.rand(np.random.randint(5, 10), np.random.randint(6, 10))\nrow_size = a.shape[0]\nresult = 0\nfor i in range(row_size):\n    result += a[i].shape[0]\nprint(result)\n",
        "\n# Create a new array with the product of the weights\nweights = np.array([1/n for n in a.shape[0]])\nweighted_a = a*weights\n# Reshape the arrays to have equal size\na_reshaped = weighted_a.reshape(-1,1)\nb_reshaped = b.reshape(-1,1)\n# Perform the two-tailed t-test\nttest_result = scipy.stats.ttest_ind(a_reshaped, b_reshaped)\n# Get the p-value\np_value = 2*ttest_result.pvalue\n",
        "\n# Create a mask to remove NaN values\nmask = np.isfinite(a) & np.isfinite(b)\na = a[mask]\nb = b[mask]\n# Calculate the pooled variance\npooled_var = ((a**2).sum() + (b**2).sum()) / (len(a) + len(b))\n# Calculate the pooled standard deviation\npooled_std_dev = np.sqrt(pooled_var)\n# Calculate the t-statistic\nt_statistic = (np.mean(a) - np.mean(b)) / pooled_std_dev\n# Calculate the degrees of freedom\ndegrees_of_freedom = len(a) + len(b) - 2\n# Calculate the p-value\np_value = 1 - scipy.stats.t.cdf(abs(t_statistic), degrees_of_freedom)\n",
        "\n# Calculate the pooled variance\npooled_var = ((anobs - 1) * avar + (bnobs - 1) * bvar) / anobs + bnobs\npooled_std = np.sqrt(pooled_var)\n# Calculate the t-statistic\nt_stat = (amean - bmean) / pooled_std\n# Calculate the degrees of freedom\ndf = anobs + bnobs - 2\n# Use the t-distribution to get the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(t_stat, df))\n",
        "\noutput = []\nfor row in A:\n    found = False\n    for b_row in B:\n        if np.all(row == b_row):\n            found = True\n            break\n    if not found:\n        output.append(row)\n",
        "\noutput = np.concatenate((A[A!=B], B[B!=A]))\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "import numpy as np\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\nsort_indices = np.argsort(a, axis=0)[:, :, np.newaxis]\nc = b[sort_indices]\n\nprint(\"Desired shape of b[sort_indices]: (3, 3, 3).\")\nprint(\"Actual shape of b[sort_indices]:\")\nprint(c.shape)",
        "\nimport numpy as np\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\nresult = np.argsort(b.sum(axis=0), axis=0)\nb = np.moveaxis(b, 0, result)\n\nprint(b)\n",
        "\na = a[:,:2]\n",
        "\na = a[:, :-1]  # remove last column of each row\n",
        "\na = a[:, 1:3]\n",
        "\n# result = np.delete(a, del_col, axis=1)\n# for i in range(len(del_col)):\n#     if del_col[i] >= a.shape[1]:\n#         del_col[i] = None\n# result = np.delete(a, del_col, axis=1)\n",
        "\nimport numpy as np\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\na = np.insert(a, pos, element)\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\n\npos = 1\nelement = [3,5]\n\na = np.insert(a, pos, element, axis=1)\n\nprint(a)\n",
        "\n    a_new = np.empty(len(a), dtype=a.dtype)\n    a_new[pos:]= a\n    a_new[pos] = element\n",
        "\nfor i in range(len(pos)):\n    a[pos[i]+1:]=a[pos[i]+1:] + element\n",
        "\nimport numpy as np\npairs = [(2, 3), (3, 4), (4, 5)]\narray_of_arrays = np.array([np.arange(a*b).reshape(a,b) for (a, b) in pairs])\n\nresult = np.array([np.copy(arr) for arr in array_of_arrays])\n\n# Alternatively, you could also use the deepcopy function from the copy module\n# result = [copy.deepcopy(arr) for arr in array_of_arrays]\n\nprint(result)\n",
        "\nresult = np.all(a == a[0])\n",
        "\nresult = np.all(a[:,0] == a[:,1:], axis=1)\n",
        "\nimport numpy as np\nexample_a = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\ndef f(a = example_a):\n    result = np.all([np.all(a[i] == a[0]) for i in range(1, len(a))])\n",
        "import numpy as np\nx = np.linspace(0, 1, 20)\ny = np.linspace(0, 1, 30)\nresult = np.zeros((20,30))\nfor i in range(20):\n for j in range(30):\n result[i,j] = (np.cos(x[i])**4 + np.sin(y[j])**2) * (x[i+1]-x[i])*(y[j+1]-y[j])/6\nprint(result)\n```",
        "\n    dx = x[1] - x[0]\n    dy = y[1] - y[0]\n    for i in range(len(x)-1):\n        for j in range(len(y)-1):\n            result += (np.power(x[i+1]-x[i],2)*np.power(y[j+1]-y[j],2)) / (24*dx*dy)\n",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\nresult = ecdf(grades)\nprint(result)\n",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\neval = np.array([88, 87, 62])\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\nresult = ecdf(eval)\n",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\nthreshold = 0.5\nlow, high = np.searchgate(np.cumsum(np.sort(grades) * np.arange(1, len(grades) + 1)) < (threshold * np.sum(grades)), np.sum(grades))\nprint(low, high)\n",
        "\nimport numpy as np\none_ratio = 0.9\nsize = 1000\nrandomLabel = np.random.randint(2, size=size)\nnums = []\nfor i in range(size):\n    if randomLabel[i] == 1:\n        nums.append(1)\n    else:\n        nums.append(0)\n    if i < int(size * one_ratio):\n        nums.append(1)\n    else:\n        nums.append(0)\nprint(nums)\n",
        "\n# [Missing Code]\nprint(a_np)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a, dtype=tf.float32)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = np.argsort(a)[::-1]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = np.empty(len(a)+1, dtype=int)\nresult[1:] = np.argsort(a)\nresult[0] = 0\nprint(result)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\nresult = np.argsort(a)[-N:]\nprint(result)\n",
        "\n# result = np.power(A, n)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\nresult = []\nfor i in range(0, len(a), 2):\n    row1 = a[i:i+2]\n    row2 = a[i+1:i+3]\n    result.append([row1, row2])\n# result.append([a[-2:], a[-1:]])\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\nresult = []\nfor i in range(0, len(a), 2):\n    for j in range(0, len(a[0]), 2):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\nfor i in range(0, len(a), 2):\n    row = []\n    for j in range(0, len(a[i]), 2):\n        col = []\n        for k in range(0, len(a[i][j]), 2):\n            col.append(a[i][j][k])\n            col.append(a[i][j][k+1])\n        row.append(col)\n    result.append(row)\n\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\nresult = []\nfor i in range(0, len(a), patch_size):\n    row = a[i:i+patch_size]\n    result.append(row)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[[ 0,  1,  2],\n        [ 6,  7,  8]],    \n       [[ 3,  4,  5],\n        [ 9, 10, 11]], \n       [[12, 13, 14],\n        [18, 19, 20]],    \n       [[15, 16, 17],\n        [21, 22, 23]]])\nh = 4\nw = 6\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i][j] = a[i][j][0]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\nresult = []\nfor i in range(0, len(a), patch_size):\n    row = a[i:i+patch_size]\n    result.append(row)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 5\nresult = a[:, low:high+1]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 0\nhigh = 2\nresult = a[low:high]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 10\nresult = a[:, low:high]\nprint(result)\n",
        "\nimport numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array([])\nfor line in string.split(\"\\n\"):\n    row = [float(x) for x in line.split(\" \")]\n    a = np.append(a, row, axis=0)\nprint(a)\n",
        "\nresult = np.power(np.random.uniform(min, max, n), np.random.uniform(1, np.log(max/min), n))\n",
        "\nresult = np.power(np.random.rand(n)+min, 1/n)\n",
        "import numpy as np\ndef f(min=1, max=np.e, n=10000):\n    result = np.power(np.random.uniform(min, max, n), np.random.uniform(1, np.log(max/min), n))\n    return result\n```",
        "import numpy as np\nimport pandas as pd\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nB = np.zeros_like(A)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n B[t] = a * A[t] + b * B[t-1]\nprint(B)",
        "\nimport numpy as np\nimport pandas as pd\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nc = 4\nB = np.empty_like(A)\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nB[2] = a * A[2] + b * B[1] + c * B[0]\nfor t in range(3, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\nprint(B)\n",
        "\nimport numpy as np\nresult = np.array([])\n",
        "\nimport numpy as np\nresult = np.zeros((3,0))\n",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = np.array(index)\nresult = result.reshape(-1, len(dims))\nresult = np.sum(np.array(dims[:-1]) * result, axis=1) + np.array(dims[-1])\nresult = result[0]\nprint(result)\n",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = np.numpy.prod(dims[:index[0]], dtype=int) * np.prod(dims[index[0]+1:], dtype=int) + np.prod(dims[:index[0]+1], dtype=int) * np.prod(dims[index[0]+1:][::-1], dtype=int)[::-1]\nprint(result)\n",
        "\nvalues = np.zeros((2,3), dtype='int32,float32')\n# [Missing Code]\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.zeros(3)\nidx = 0\nfor i in range(accmap.size):\n    if accmap[i] != accmap[idx]:\n        result[idx] = a[i]\n        idx += 1\n    else:\n        result[idx] += a[i]\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.zeros(3)\nfor i in range(len(index)):\n    if index[i] == index[i+1]:\n        result[i] = a[i+1]\n    else:\n        result[i] = max(a[i+1:index[i+2]+1])\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.zeros_like(a)\nfor i in range(len(accmap)):\n    result[accmap[i]:accmap[i+1]] += a[i]\n# print(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.empty(len(index),dtype=np.intp)\nfor i in range(len(index)):\n    result[i] = a[index[i]+1] if index[i]+1 >= 0 else a[-index[i]-1]\nprint(result)\n",
        "\nfor i in range(len(x)):\n    for j in range(len(x[i])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n",
        "import numpy as np\nprobabilit = [0.333, 0.334, 0.333]\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\nprint(result)",
        "\nimport numpy as np\na = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\nresult = np.zeros((high_index - low_index, high_index - low_index))\nresult[low_index:high_index, low_index:high_index] = a\n",
        "\nresult = np.array(x[x >= 0])\n",
        "\nresult = np.delete(x, [i for i in range(len(x)) if x[i] == -1.4])\n",
        "\n# bin_data = [(4,2),(5,6),(7,5),(4,3),(5,7)]\n# bin_data_mean = [3,5.5,6,3.5,6]\n# for a bin size of 3:\n# bin_data = [(4,2,5),(6,7,5),(4,3,5)]\n# bin_data_mean = [3.67,6,4]\n",
        "\nimport numpy as np\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\nbin_data_max = []\nfor i in range(0, len(data), bin_size):\n    bin_data = data[i:i+bin_size]\n    bin_data_max.append(bin_data.max())\nbin_data_max.append(data[-bin_size:])\nprint(bin_data_max)\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\n\nbin_data_mean = np.mean(np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)]), axis=1)\n\nprint(bin_data_mean)\n",
        "\nbin_data, bin_edges = np.histogram(data, bins=bin_size, range=(data[-1], data[0]), density=False)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\nbin_data = np.empty((data.shape[0]//bin_size+1,bin_size),dtype=data.dtype)\nbin_data_mean = np.empty_like(bin_data)\nfor i in range(data.shape[0]//bin_size+1):\n    bin_data[i] = np.mean(data[i*bin_size:(i+1)*bin_size,:],axis=0)\n    bin_data_mean[i] = np.mean(bin_data[i])\nprint(bin_data_mean)\n",
        "\nbin_data = []\nfor i in range(0, data.shape[0], bin_size):\n    row_bin_data = []\n    for j in range(bin_size):\n        start = i + j\n        end = min(start + bin_size, data.shape[1])\n        row_bin_data.append((data[i, start], data[i, end]))\n    bin_data.append(row_bin_data)\n",
        "\n    return (3*x**2 - 2*x**3) * (x_max - x_min) / (3*x**2 - x**3) + x_min if x < x_max else x_max if x < x_min else x\n",
        "\nt = (x - x_min) / (x_max - x_min)\nt_pow = t ** N\n",
        "\n# [Missing Code]\n",
        "\n# result = df.values.reshape(4, 15, 5)\n",
        "import numpy as np\nimport pandas as pd\n\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('colu'))\n\nresult = np.array(df).reshape(15, 4, 5)\n\nprint(result)",
        "\n# result = \n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = np.dstack(np.unpackbits(np.uint8(a/np.power(2,m-1))))\nprint(result)\n",
        "\nbinary_array = np.unpackbits(np.uint8(a.view(np.uint8)))\n",
        "\nmean, std = np.mean(a), np.std(a)\nmu, sigma = mean, std\nlower_bound = mu - 3 * sigma\nupper_bound = mu + 3 * sigma\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nsigma = std * 2\nlower_bound = mean - sigma\nupper_bound = mean + sigma\n",
        "\n    lower_bound = mean - third_std\n    upper_bound = mean + third_std\n",
        "\nmu = np.mean(a)\nsigma = np.std(a)\ntwo_sigma = 2*sigma\nresult = np.where((a < mu-two_sigma) | (a > mu+two_sigma), True, False)\n",
        "\nprob = np.percentile(masked_data.compressed(), percentile)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\nfor row in zero_rows:\n    a[row, :] = 0\nfor col in zero_cols:\n    a[:, col] = 0\n",
        "\na[1, :] = 0\na[:, 0] = 0\n",
        "import numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.zeros(a.shape)\nmask = np.where(a == np.amax(a, axis=1), True, mask)\nprint(mask)",
        "\n# mask[mask == a[np.argmin(a, axis=1)]] = True\n",
        "\npearson_corr = np.corrcoef(post, distance)[0,1]\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = np.zeros((X.shape[1], X.shape[1], X.shape[0]))\nfor i in range(X.shape[0]):\n    result[:,:,i] = X[:,i].dot(X[:,i].T)\n",
        "\nimport numpy as np\nY = np.array([[[81, 63, 63],\n        [63, 49, 49],\n        [63, 49, 49]],\n\n       [[ 4, 12,  8],\n        [12, 36, 24],\n        [ 8, 24, 16]],\n\n       [[25, 35, 25],\n        [35, 49, 35],\n        [25, 35, 25]],\n\n       [[25, 30, 10],\n        [30, 36, 12],\n        [10, 12,  4]]])\n\nM, N = Y.shape[1], Y.shape[0]\nX = np.zeros((N, M))\nfor i in range(N):\n    X[:, i] = np.linalg.norm(Y[i, :, :], axis=1)\n\nprint(X)\n",
        "\n# is_contained = len(a) == 1\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.copy(A)\nC[np.in1d(C, B, invert=True)] = np.nan\nprint(C)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,4,8])\nC = np.zeros(A.shape)\nC[np.logical_and(A >= B[0], A <= B[2])] = np.arange(2, 9) + 1\nprint(C)\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = np.flip(rankdata(a).astype(int))\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = np.empty_like(a)\nfor i in range(len(a)):\n    result[i] = len(set(a[:i])) + 1\n",
        "\n    ranked_a = rankdata(a)\n    # [Missing Code]\n    result = np.flip(ranked_a)\n",
        "\nmissing_code = dists.reshape(-1, 2)\n",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ndists = np.dstack((x_dists, y_dists))\n# Alternatively, you can use np.vstack if you want the x and y distances in separate columns\ndists = np.vstack((x_dists, y_dists[:, None]))\nprint(dists)\n",
        "\nimport numpy as np\na = np.random.rand(5, 5, 5)\nsecond = [1, 2]\nthird = [3, 4]\nresult = a[:][second][third].flatten()\nprint(result)\n",
        "\nimport numpy as np\n\narr_lengths = [20, 10, 10, 2]\narr = np.zeros((len(arr_lengths),) + arr_lengths)\n\nprint(arr)\n",
        "import numpy as np\nX = np.array([[1, 2, 3, 6],\n              [4, 5, 6, 5],\n              [1, 2, 5, 5],\n              [4, 5,10,25],\n              [5, 2,10,25]])\nl1 = np.apply_along_axis(lambda x: np.sqrt(np.sum(x**2)), 1, X)\nX_norm = X / l1.reshape(5,1)\nprint(X_norm)",
        "\nresult = LA.norm(X, axis=1)\n",
        "\nresult = LA.norm(X,ord=np.inf,axis=1)\n",
        "\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nconditions = [df['a'].str.contains(target), df['a'].str.contains('o'), df['a'].str.contains('r')]\nchoices = ['match', 'foo', 'bar']\ndf['result'] = np.select(conditions, choices, default=np.nan)\nprint(df)\n",
        "import numpy as np\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\nresult = np.zeros((len(a),len(a)))\nfor i in range(len(a)):\n for j in range(len(a)):\n if i!=j:\n result[i][j] = np.linalg.norm(a[i]-a[j])\n#print(result)",
        "\ndistances = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n for j in range(i+1, a.shape[0]):\n     distances[i,j] = np.linalg.norm(a[i]-a[j])\n     distances[j,i] = distances[i,j]\n",
        "\ndistances = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        distances[i,j] = np.linalg.norm(a[i] - a[j])\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\nresult = np.empty(len(a), dtype=np.int64)\nresult[0] = a[0]\nfor i in range(1, len(a)):\n    if a[i] != a[i-1]:\n        result[i] = a[i]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\nresult = np.concatenate([a[i:i+2][::2] for i in range(0, len(a), 2)])\nprint(result)\n",
        "\ndf = pd.DataFrame(list(zip(lat[0], lon[0], val[0])), columns=['lat', 'lon', 'val'])\nfor i in range(1, lat.shape[0]):\n    df = df.append(pd.DataFrame(list(zip(lat[i], lon[i], val[i])), columns=['lat', 'lon', 'val']))\n",
        "\nimport numpy as np\nimport pandas as pd\nexample_lat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nexample_lon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nexample_val=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\ndef f(lat = example_lat, lon = example_lon, val = example_val):\n    df = pd.DataFrame(np.dstack((lat, lon, val)), columns=['lat', 'lon', 'val'])\n    return df\n",
        "\ndf = pd.DataFrame(np.dstack((lat, lon, val)), columns=['lat', 'lon', 'val'])\n",
        "\nimport numpy as np\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\nresult = np.empty((4, 4, size[0], size[1]), dtype=a.dtype)\n# Missing Code\nresult[:, :, :, 0] = a[:, :size[1], :size[0], :]\nresult[:, :, :, 1] = a[:, :size[1], :size[0], :]\nresult[:, :, :, 2] = a[:, :size[1], :size[0], :]\nresult[:, :, :, 3] = a[:, :size[1], :size[0], :]\n# End of Missing Code\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\nresult = np.empty((4, 4, size[0], size[1]), dtype=a.dtype)\n# Missing Code\nresult[:, :, :, 0] = a[:, :size[1], :, np.newaxis]\nresult[:, :, :, 1] = a[:, size[1]:, :, np.newaxis]\nresult[:, :, :, 2] = a[:, :, :size[0], np.newaxis]\nresult[:, :, :, 3] = a[:, :, size[0]:, np.newaxis]\n# End of Missing Code\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\nresult = np.mean(a)\nprint(result)\n",
        "\nimport numpy as np\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    result = np.mean(a)\n    # Handle the complex infinity case\n    if np.isinf(result):\n        # The real part is infinity\n        result = np.inf\n        # The imaginary part is NaN\n        result += np.nan * 1j\n    # Return the result\n    return result\n",
        "\nresult = Z[tuple(slice(None) for _ in range(len(Z.shape) - 1))]\n",
        "\nresult = a[-1:]\n",
        "import numpy as np\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ 57, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  78, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ 57, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, 727]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ 66, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\n\nresult = True if c == CNTS[1] else False\n\nprint(result)",
        "\nimport numpy as np\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ np.nan, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  np.nan, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, np.nan]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ np.nan, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\nresult = any([c == cnt for cnt in CNTS])\nprint(result)\n",
        "\nf = intp.interp2d(x_new, y_new, a)\nresult = f(x_new, y_new)\n",
        "\nimport pandas as pd\nimport numpy as np\ndata = {'D':[2015,2015,2015,2015,2016,2016,2016,2017,2017,2017], 'Q':np.arange(10)}\nname= 'Q_cum'\ndf = pd.DataFrame(data)\ndf[name] = np.cumsum(df['Q'])\ndf[name].loc[df.groupby('D')['Q'].shift().neq(0).cumsum()] = 0\ndf.drop(columns=['Q'], inplace=True)\nprint(df)\n",
        "\ni_diag = i.diagonal()\ni_new = np.zeros((4,4))\ni_new[:,np.ix_([0,3],[0,3])] = i_diag\ni = i_new\n",
        "\na = np.where(np.diag(a) == 0, 0, a)\n",
        "\ntimedelta = pd.Timedelta(end - start)\nfrequency = timedelta / n\nindex = pd.date_range(start, end, freq=frequency)\nresult = pd.concat([start] + index[:-1] + [index[-1] + timedelta], axis=0)\n",
        "\nif result != -1:\n    result = np.where(y[result] == b)[0][0]",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nresult = []\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result.append(i)\nprint(result)\n",
        "\n# result = np.polyfit(x, y, 2)\n# a, b, c = result[0], result[1], result[2]\n# print(result)\n",
        "\nimport numpy as np\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\ndegree = 3\nresult = np.polyfit(x, y, degree)\nprint(result)\n",
        "\nsubtracted_values = np.subtract(df.values, a)\n",
        "\nimport numpy as np\nA = np.random.rand(5, 6, 3)\nB = np.random.rand(3, 3)\nresult = np.einsum('ijk,jl->ilk', A, B)\nprint(result)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n",
        "\nresult = MinMaxScaler().fit(arr).transform(arr)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\na = np.array([[[1, 0.5, -2], [-0.5,1, 6], [1,1,1]], [[-2, -3, 1], [-0.5, 10, 6], [1,1,1]]])\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\nprint(result)\n",
        "\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n",
        "\nmask = np.zeros_like(arr, dtype=bool)\nfor i in range(len(n1)):\n    mask[:,i] = arr[:,i] < n1[i]\nmask2 = np.zeros_like(arr, dtype=bool)\nfor i in range(len(n2)):\n    mask2[:,i] = arr[:,i] >= n2[i]\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[mask2] = 30\n",
        "\ntolerance = 1e-9\nresult = np.nonzero(np.abs(s1 - s2) > tolerance)[0].shape[0]\n",
        "\nresult = np.zeros(2)\nfor i in range(2):\n result[i] = np.count_nonzero(np.isnan(s1) | np.isnan(s2)) - np.count_nonzero(np.isnan(s1) & np.isnan(s2))\n",
        "\nimport numpy as np\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\nresult = np.all(a[0] == a[1:])\nprint(result)\n",
        "\nimport numpy as np\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\nresult = all(np.isnan(np.sum(arr)) for arr in a)\nprint(result)\n",
        "\nimport numpy as np\na = np.ones((41, 13))\nshape = (93, 13)\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nresult = np.zeros(shape)\nresult[np.ix_(*[slice(None) for _ in range(len(shape))])] = a\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\nresult = np.zeros(shape)\nresult[ : a.shape[0], : a.shape[1]] = a\nresult = np.pad(result, ((0, shape[0]-a.shape[0]),(0, shape[1]-a.shape[1])), constant_values=element)\nprint(result)\n",
        "\nimport numpy as np\nexample_arr = np.ones((41, 13))\ndef f(arr = example_arr, shape=(93,13)):\n    result = np.zeros(shape, dtype=arr.dtype)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n    return result\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\nimport numpy as np\na = np.arange(12)\na = a.reshape((4,3))\nprint(a)\n",
        "\nfor i in range(3):\n    for j in range(3):\n        if b[i,j] == 1:\n            result[i,j] = a[i,j,1]\n",
        "\nfor i in range(3):\n    for j in range(3):\n        if b[i, j] == 1:\n            result[i, j] = a[i, j, 0]\n        else:\n            result[i, j] = a[i, j, 1]\n",
        "\nfor i in range(3):\n    for j in range(3):\n        if b[i][j] != -1:\n            result[i][j] = a[i][j][b[i][j]]\n",
        "\nimport numpy as np\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\nresult = np.sum(a[:, :, b], axis=2)\nprint(result)\n",
        "\nimport numpy as np\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\nresult = np.sum(a[b[:,0], b[:,1], :], axis=1)\n",
        "\nresult = np.where(df['a'].gt(1) & df['a'].le(4), df['b'], np.nan)\n",
        "\nimport numpy as np\nim = np.array([[0,0,0,0,0,0],\n               [0,0,1,1,1,0],\n               [0,1,1,0,1,0],\n               [0,0,0,1,1,0],\n               [0,0,0,0,0,0]])\nresult = np.delete(im, [0, -1], axis=0)\nresult = np.delete(result, [0, -1], axis=1)\nprint(result)\n",
        "\ntruncated_A = A[:, :3]\n",
        "\nimport numpy as np\nim = np.array([[1,1,1,1,1,5],\n               [1,0,0,1,2,0],\n               [2,1,0,0,1,0],\n               [1,0,0,7,1,0],\n               [1,0,0,0,0,0]])\nresult = np.zeros_like(im)\nfor i in range(im.shape[0]):\n    for j in range(im.shape[1]):\n        if im[i][j] == 0:\n            continue\n        if i == 0 or i == im.shape[0]-1 or j == 0 or j == im.shape[1]-1:\n            result[i][j] = 1\nprint(result)\n",
        "\nimport numpy as np\nim = np.array([[0,0,0,0,0,0],\n               [0,0,5,1,2,0],\n               [0,1,8,0,1,0],\n               [0,0,0,7,1,0],\n               [0,0,0,0,0,0]])\nresult = np.zeros_like(im)\nfor i in range(im.shape[0]):\n    for j in range(im.shape[1]):\n        if im[i][j] != 0:\n            result[i][j] = im[i][j]\n"
    ],
    "Matplotlib": [
        "\n\nplt.scatter(x, y, label='x-y')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.show()\n\n",
        "\n# create a new figure with a larger dpi (dots per inch) for clearer display\nfig, ax = plt.subplots(figsize=(8, 6), dpi=100)\n\n# set the major tick locators and labels for both x and y axes\nax.xaxis.set_major_locator(plt.MultipleLocator(1))\nax.yaxis.set_major_locator(plt.MultipleLocator(1))\n\n# set the minor tick locators for the y axis only\nax.yaxis.set_minor_locator(plt.AutoMinorLocator(5))\n\n# turn on the minor ticks for the y axis only\nax.yaxis.set_minor_formatter(plt.NullFormatter())\n\n# turn on gridlines for both x and y axes\nax.grid(which='both', alpha=0.2)\n\n",
        "\n# create a new figure with larger figure size\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# set minor tick locations\nax.set_xticks(np.arange(0, 1.1, 0.1))\nax.set_yticks(np.arange(0, 1.1, 0.1))\n\n# turn on minor ticks\nax.set_xticks(np.arange(0, 1.1, 0.1), minor=True)\nax.set_yticks(np.arange(0, 1.1, 0.1), minor=True)\n\n# turn off major tick labels\nax.set_xticklabels([])\nax.set_yticklabels([])\n\n# turn on grid\nax.grid(which='minor', alpha=0.2)\n\n",
        "\nax = plt.gca()  # get current axes\nax.set_xticks(np.arange(0, 10.1, 1))  # set xticks\nax.set_xticklabels([str(i) for i in np.arange(1, 11)])  # set xticklabels\nax.xaxis.set_minor_locator(plt.AutoMinorLocator())  # turn on minor ticks on x axis only\nplt.show()\n",
        "\n\nx = np.arange(10)\n\n# draw a line (with random y) for each different line style\nstyles = ['-', '--', ':', '-.']\ncolors = ['r', 'g', 'b', 'y']\nrandom_ys = [np.random.randint(0, 10) for _ in range(4)]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\nfor i, style in enumerate(styles):\n    ax = axs[i // 2, i % 2]\n    ax.plot(x, np.random.randint(0, 10, size=10), color=colors[i], linestyle=style)\n    ax.scatter(x, random_ys[i], color=colors[i], marker='s', s=50, edgecolors='k')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title(f'Line Style: {style}, Color: {colors[i]}')\n\nplt.tight_layout()\nplt.show()\n",
        "\n\nx = np.arange(10)\n\n# draw a line (with random y) for each different line style\nstyles = ['-', '--', ':', '-.']\ncolors = ['r', 'g', 'b', 'y']\nmarkers = ['o', 's', 'p', '^']\nlabels = ['Line 1', 'Line 2', 'Line 3', 'Line 4']\n\nfig, axs = plt.subplots(2, 2)\nfig.set_size_inches(18, 12)\n\nfor i, style in enumerate(styles):\n    for j, color in enumerate(colors):\n        for k, marker in enumerate(markers):\n            ax = axs[i//2, i%2]\n            ax.plot(x, np.random.rand(len(x)), color=color, linestyle=style, marker=marker, label=labels[j*2+k])\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.legend()\n\nplt.tight_layout()\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.random.randn(10)\n\n# line plot x and y with a thin diamond marker\nplt.plot(x, y, '--', marker='d', markersize=5)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Line Plot with Thin Diamond Marker')\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.random.randn(10)\n\n# line plot x and y with a thick diamond marker\nplt.plot(x, y, 'k-', lw=2, marker='d', ms=10)\nplt.xlabel('X-axis label')\nplt.ylabel('Y-axis label')\nplt.title('Line plot with thick diamond marker')\nplt.show()\n",
        "\n\nsns.set_style(\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n\n# set the y axis limit to be 0 to 40\nylim = (0, 40)\nax.set_ylim(ylim)\n\n",
        "\n# create a new array with the highlighted range\nhighlight = np.array([2, 3, 4])\n# create a new array with the same length as x but all values set to NaN\nhighlight_mask = np.zeros(len(x))\nhighlight_mask[highlight] = np.nan\n# plot the original data with a different color\nplt.plot(x, color='blue')\n# plot the highlighted range with a different color and a different marker\nplt.scatter(highlight, np.nan, color='red', marker='s')\n",
        "\nx = np.linspace(0, 1, 100)\ny = 2 * x\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Full Line from (0,0) to (1,2)')\nplt.show()\n",
        "\nx = np.linspace(0, 1, 100)\ny = 2 - (x - 0.5) ** 2\n\n",
        "\n\ngenders = df[\"Gender\"].value_counts()\n\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df, kind=\"scatter\", height=4)\nseaborn.despine()\n\nplt.title(\"Height vs Weight by Gender\")\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Weight (kg)\")\n\nplt.legend(bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0,)\n\nplt.show()\n\n",
        "\n\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# draw a regular matplotlib style plot using seaborn\nsns.set(style=\"whitegrid\")\nsns.scatterplot(x=x, y=y)\n\n",
        "\n\nx = np.arange(10)\ny = np.sin(x)\n\n# draw a line plot of x vs y using seaborn and pandas\nsns.lineplot(x=x, y=y)\n\nplt.show()\n",
        "\n\nx = np.random.randn(10)\ny = np.random.randn(10)\n\n# in plt.plot(x, y), use a plus marker and give it a thickness of 7\nplt.plot(x, y, 'o', markersize=7)\n\nplt.show()\n",
        "\n# set the figure size to 8 inches by 6 inches\nplt.figure(figsize=(8, 6))\n# set the axis labels and tick labels to use font size 18\nplt.xlabel(\"x\", fontsize=18)\nplt.ylabel(\"y\", fontsize=18)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\n",
        "\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n\n# set legend title to xyz and set the title font to size 20\nplt.legend(title='xyz', fontsize=20)\nplt.title('Cosine Wave', fontsize=20)\n\n",
        "\n\nfor i, marker in enumerate(l.collection.get_markers()):\n    marker.set_facecolor('gray')\n\n",
        "\n# set the facecolor of the markers to black\nl.set_facecolor(\"black\")\n",
        "\n",
        "\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n\n# rotate the x axis labels clockwise by 45 degrees\nplt.xticks(rotation=45)\n\n",
        "\n# add a title to the plot\nplt.title(\"Trigonometric Function: cosine\")\n\n",
        "\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n\n# put a x axis ticklabels at 0, 2, 4...\nxticks = np.arange(0, 2*np.pi, np.pi)\nplt.xticks(xticks, ['0', '2', '4'])\n\nplt.legend()\nplt.show()\n",
        "\n\nx = np.random.randn(10)\ny = np.random.randn(10)\nsns.distplot(x, label=\"a\", color=\"0.25\")\nsns.distplot(y, label=\"b\", color=\"0.25\")\n\n# add legends\nleg = plt.legend()\nfor i, txt in enumerate(leg.get_texts()):\n    txt._txt = txt.get_text() + '\\n(' + str(i+1) + ')'\n\nplt.show()\n",
        "\n\n\nH = np.random.randn(10, 10)\n\n# color plot of the 2d array H\ncmap = plt.cm.jet\nplt.imshow(H, cmap=cmap, interpolation='nearest')\nplt.colorbar()\nplt.show()\n",
        "\n\nplt.imshow(H, cmap='gray')\nplt.show()\n\n",
        "\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n\n# set xlabel as \"X\"\n# put the x label at the right end of the x axis\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xlabel(\"X\", labelpad=20)\nax.set_xlabel(\"X\", fontdict={'weight': 'bold', 'size': 14, 'color': 'white'}, labelpad=20)\nplt.show()\n",
        "\n\ndf = sns.load_dataset(\"planets\")\ng = sns.boxplot(x=\"method\", y=\"orbital_period\", data=df)\n\n# rotate the x axis labels by 90 degrees\ng.set_xlabel('Method')\ng.set_ylabel('Orbital Period')\nplt.show()\n",
        "\n\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nplt.plot(x, y)\nmyTitle = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\n\n# fit a very long title myTitle into multiple lines\nplt.title(myTitle, fontsize=12, y=1.02)\n\n",
        "\n\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n\n# make the y axis go upside down\nplt.gca().invert_yaxis()\n\n",
        "\n# create a new axis on the right side of the current axis\nplt.axis('off')\n# create a new figure and axis for the second y-axis\nplt.subplot2grid((2,2), (0,1), colspan=1)\nplt.ylabel('Second Y-axis')\n# plot the same data on the second y-axis\nplt.plot(x, y, 'r-')\n",
        "\n# create a custom tick locator to only show ticks at -1 and 1\nytick_locator = plt.MultipleLocator(1)\nplt.gca().yaxis.set_major_locator(ytick_locator)\n\n",
        "\n\nfig, axs = plt.subplots(1, 3, figsize=(12, 4))\n\naxs[0].hist(x, bins=10)\naxs[0].set_title('x')\n\naxs[1].hist(y, bins=10)\naxs[1].set_title('y')\n\naxs[2].hist(z, bins=10)\naxs[2].set_title('z')\n\nplt.tight_layout()\n\n",
        "\nscatter_kwargs = {'s': 50, 'c': 'blue', 'edgecolors': 'black', 'facecolors': 'none'}\nplt.scatter(x, y, **scatter_kwargs)\n",
        "\n\n# set axis ticks to integers\nplt.xticks(np.arange(len(x)))\nplt.yticks(np.arange(len(y)))\n\n",
        "\n\ndata = {\n    \"reports\": [4, 24, 31, 2, 3],\n    \"coverage\": [35050800, 54899767, 57890789, 62890798, 70897871],\n}\ndf = pd.DataFrame(data)\nsns.factorplot(y=\"coverage\", x=\"reports\", kind=\"bar\", data=df, label=\"Total\")\n\n# set the tick label format to use plain numbers instead of scientific notation\nplt.yscale('linear')\n\n",
        "\n# create a new line object\nline, = ax.plot([0, 10], [0, 0], linestyle='--', color='r')\n# set the line properties\nline.set_dash([3, 3])\n",
        "\n\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# plot x vs y1 and x vs y2 in two subplots, sharing the x axis\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y1)\naxs[0].set_title('y1 = sin(x)')\naxs[1].plot(x, y2)\naxs[1].set_title('y2 = cos(x)')\n\n# set shared x-axis\naxs[0].set_xlabel('x')\naxs[1].set_xlabel('x')\nplt.tight_layout()\nplt.show()\n",
        "\n\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# create figure with two subplots\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# plot x vs y1 in first subplot\naxs[0].plot(x, y1)\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].tick_params(axis='both', which='both', length=0)\n\n# plot x vs y2 in second subplot\naxs[1].plot(x, y2)\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].tick_params(axis='both', which='both', length=0)\n\nplt.tight_layout()\nplt.show()\n",
        "\n# remove y axis label\nplt.ylabel(\"\")\n",
        "\n\nx = np.arange(10)\ny = np.sin(x)\ndf = pd.DataFrame({\"x\": x, \"y\": y})\nsns.lineplot(x=\"x\", y=\"y\", data=df)\n\n# remove x tick labels\nplt.xticks(rotation=90)\n\n",
        "\nplt.xticks([3, 4], ['A', 'B'])\nplt.grid(axis='x', alpha=0.5)\n",
        "\nplt.yticks([3, 4], ['Y3', 'Y4'])\nplt.hlines(3, 0, 10, color='gray', linestyles='--')\nplt.hlines(4, 0, 10, color='gray', linestyles='--')\n",
        "\n# show yticks and horizontal grid at y positions 3 and 4\nplt.yticks([3, 4], ['3', '4'], rotation=0)\nplt.hlines(3, 0, 10)\nplt.hlines(4, 0, 10)\n\n# show xticks and vertical grid at x positions 1 and 2\nplt.xticks([1, 2], ['1', '2'], rotation=45, ha='right')\nplt.vlines(1, 0, 10)\nplt.vlines(2, 0, 10)\n",
        "\nplt.show()\n",
        "\n",
        "\n\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), sharex='col', sharey='row')\naxes = axes.flatten()\n\nfor ax, ax_prev in zip(axes, axes[-1]):\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n    ax.set_position([ax_prev.get_position().x0, ax_prev.get_position().y0,\n                     ax_prev.get_position().width, ax_prev.get_position().height])\n\n# Copy the previous plot but adjust the subplot padding to have enough space to display axis labels\nfig.subplots_adjust(wspace=0.2, hspace=0.2)\n\nplt.show()\nplt.clf()\n\n",
        "\n# Give names to the lines in the above plot 'Y' and 'Z' and show them in a legend\n",
        "\n\ncolumn_labels = list(\"ABCD\")\nrow_labels = list(\"WXYZ\")\ndata = np.random.rand(4, 4)\nfig, ax = plt.subplots()\nheatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n\n# Move the x-axis of this heatmap to the top of the plot\nax.set_xticklabels(row_labels)\nax.set_yticklabels([]) # remove y-axis labels\nax.invert_yaxis() # flip x and y axes\n\n# Add colorbar\ncbar = plt.colorbar(heatmap, ax=ax)\ncbar.ax.set_ylabel('Data', rotation=-90, va=\"bottom\")\n\n# Add grid\nax.grid(which='both', axis='both', linestyle='-', color='lightgrey')\n\n# Add axis labels\nax.set_xlabel('Columns')\nax.set_ylabel('Rows')\n\n# Add legend\nlegend = ax.legend(bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0.)\n\n# Adjust spacing\nplt.tight_layout()\n\n# Add title\nplt.title('Heatmap')\n\n# Show plot\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x\nplt.plot(x, y)\n\n# Label the x-axis as \"X\"\nplt.xlabel(\"X\")\n\n# Set the space between the x-axis label and the x-axis to be 20\nplt.xticks(rotation=20)\n\n",
        "\n\nplt.plot(x, y)\nplt.xticks([])\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x\nplt.plot(x, y)\n\n# Move the y axis ticks to the right\nplt.gca().set_yticklabels(plt.gca().get_yticks() + 1)\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and label y axis \"Y\"\n# Show y axis ticks on the left and y axis label on the right\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.yticks(side='left')\nplt.ylim(0, 10)\n\n",
        "\n\njointplot = sns.jointplot(x='total_bill', y='tip', kind='reg', data=tips, color='green')\njointplot.plot_joint(sns.kdeplot, cmap='Blues', shade=True)\njointplot.ax_joint.set_axis_bgcolor('white')\n\n",
        "\n\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='green')\n\n",
        "\n\nsns.jointplot(x='total_bill', y='tip', kind='reg', data=tips)\n\n",
        "\n\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n\n# For data in df, make a bar plot of s1 and s1 and use celltype as the xlabel\n# Make the x-axis tick labels horizontal\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.bar(df[\"celltype\"], df[\"s1\"])\nax.set_xlabel(\"celltype\")\nax.set_xticklabels(df[\"celltype\"], rotation=90)\n\nfor i, txt in enumerate(ax.get_xticklabels()):\n    txt.set_horizontalalignment(\"center\")\n\nplt.show()\n",
        "\n\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n\n# For data in df, make a bar plot of s1 and s1 and use celltype as the xlabel\n# Make the x-axis tick labels rotate 45 degrees\n\nfig, ax = plt.subplots()\n\n# Make the bar chart\nax.bar(df[\"celltype\"], df[\"s1\"])\n\n# Rotate the x-axis labels (which are the row names) by 45 degrees\nax.set_xticklabels(df[\"celltype\"], rotation=45)\n\n# Make the y-axis label and tick labels disappear\nax.set_ylabel(\"\")\nax.set_xlabel(\"\")\n\n# Add a title\nax.set_title(\"s1 by celltype\")\n\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color='red')\nplt.xticks(color='red')\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and label the x axis as \"X\"\n# Make the line of the x axis red\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.axis('off')\nplt.gca().xaxis.set_color('red')\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# plot y over x with tick font size 10 and make the x tick labels vertical\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xticklabels(ax.get_xticks(), fontsize=10)\nax.set_yticklabels(ax.get_yticks(), fontsize=10, rotation=0)\nplt.xticks(rotation=90)\nplt.show()\n",
        "\n\nx_values = [0.22058956, 0.33088437, 2.20589566]\ny_values = [0 for _ in x_values]\ncolors = ['r', 'g', 'b']\nlinewidths = [1, 2, 3]\n\nfor i, (x, color, linewidth) in enumerate(zip(x_values, colors, linewidths)):\n    plt.plot([x, x], [0, 1], color=color, linewidth=linewidth, linestyle='-')\n\n",
        "\n\nxlabels = list(\"ABCD\")\nylabels = list(\"CDEF\")\nrand_mat = numpy.random.rand(4, 4)\n\n# Plot of heatmap with data in rand_mat and use xlabels for x-axis labels and ylabels as the y-axis labels\n# Make the x-axis tick labels appear on top of the heatmap and invert the order or the y-axis labels (C to F from top to bottom)\n\nfig, ax = plt.subplots()\n\n# Create a color map for the heatmap\ncmap = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=0, vmax=1), cmap='YlGnBu')\n\n# Set the values of the heatmap to the values in rand_mat\nim = ax.imshow(rand_mat, cmap=cmap, aspect='auto', origin='lower')\n\n# Add colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\ncbar.ax.set_ylabel('Heatmap values', rotation=-90, va=\"bottom\")\n\n# Set the x and y tick labels\nax.set_xticks(numpy.arange(4)+0.5, xlabels)\nax.set_yticks(numpy.arange(4)+0.5, ylabels[::-1]) # Invert the order of ylabels\nax.set_xticklabels(xlables, rotation=45, ha='right')\nax.set_yticklabels(ylabels[::-1], rotation=0, ha='right') # Invert the order of ylabels\n\n# Make the x-axis tick labels appear on top of the heatmap\nfor i, txt in enumerate(ax.get_xticklabels()):\n    txt.set_y(1.02*i)\n\n# Make the y-axis tick labels appear on top of the heatmap\nfor i, txt in enumerate(ax.get_yticklabels()):\n    txt.set_y(1.02*i)\n\nplt.show()\n",
        "\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n",
        "\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nax[0].plot(x, y)\nax[0].set_title(\"Y\")\n\nax[1].plot(x, y)\nax[1].set_title(\"Y\")\n\n",
        "\n\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n\n# make a seaborn scatter plot of bill_length_mm and bill_depth_mm\n# use markersize 30 for all data points in the scatter plot\nscatter_plot = sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", markersize=30)\n\n# show the plot\nplt.show()\n",
        "\n\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n\n# make scatter plot of a over b and annotate each data point with correspond numbers in c\nfig, ax = plt.subplots()\nax.scatter(b, a, c=c, s=50, cmap='viridis')\nfor i, txt in enumerate(c):\n    ax.annotate(txt, xy=(b[i], a[i]), ha='center', va='bottom', color='black')\n\nplt.xlabel('b')\nplt.ylabel('a')\nplt.title('Scatter plot of a over b')\nplt.show()\n",
        "\n# Add a title to the plot\nplt.title(\"y over x\")\n",
        "\n",
        "\n\n# Create a histogram of x with outline of each bar\nplt.hist(x, edgecolor='black', linewidth=1.2)\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Make two subplots. Make the first subplot three times wider than the second subplot but they should have the same height.\nfig, axs = plt.subplots(1, 2, figsize=(10, 6))\n\n# Make the first subplot three times wider than the second subplot\naxs[0].set_size_inches(6, 3)\n\n# Plot the data in the first subplot\naxs[0].plot(x, y)\naxs[0].set_title('First Subplot')\n\n# Plot the data in the second subplot\naxs[1].plot(x, y)\naxs[1].set_title('Second Subplot')\n\nplt.show()\n",
        "\n\nx = np.random.rand(10)\ny = np.random.rand(10)\nbins = np.linspace(-1, 1, 100)\n\n# Plot two histograms of x and y on a single chart with matplotlib\n# Set the transparency of the histograms to be 0.5\nfig, ax = plt.subplots()\nax.hist(x, alpha=0.5, bins=bins)\nax.hist(y, alpha=0.5, bins=bins)\nplt.show()\n\n",
        "\n\nx = np.random.rand(10)\ny = np.random.rand(10)\n\n# Plot a grouped histograms of x and y on a single chart with matplotlib\n# Use grouped histograms so that the histograms don't overlap with each other\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].hist2d(x, y, bins=20, cmap='Blues', range=[[0, 1], [0, 1]])\naxs[0].set_title('Histogram of x and y')\naxs[1].hist(x, bins=10, histtype='step', cmap='Blues')\naxs[1].hist(y, bins=10, histtype='step', cmap='Blues')\naxs[1].set_title('Histograms of x and y')\nplt.show()\n",
        "\nx = [a, c]\ny = [b, d]\nplt.plot(x, y, color='red')\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n",
        "\n\n# create two subplots\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# create colormap with x\naxs[0].imshow(x, cmap='viridis', extent=[0, 1, 0, 1])\naxs[0].set_title('x')\n\n# create colormap with y\naxs[1].imshow(y, cmap='viridis', extent=[0, 1, 0, 1])\naxs[1].set_title('y')\n\n# share a single colorbar\ncbar_ax = plt.subplot(122, sharex=axs[0], frameon=False)\ncb = plt.colorbar(ticks=None, cax=cbar_ax, orientation='vertical')\ncb.ax.set_ylabel('z', rotation=-90, va='bottom')\n\n",
        "\n\ncolors = ['r', 'b']\nlabels = ['a', 'b']\n\nfor i, col in enumerate(x):\n    plt.plot(col, label=labels[i], color=colors[i])\n\nplt.legend()\nplt.show()\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n\n# plot y over x and z over a in two different subplots\n# Set \"Y and Z\" as a main title above the two subplots\n\nfig, ax1 = plt.subplots()\nax1.plot(x, y, label='Y over X')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.legend()\n\nax2 = ax1.twinx()\nax2.plot(a, z, label='Z over A')\nax2.set_ylabel('Z')\nax2.legend()\n\nplt.title('Y and Z')\nplt.tight_layout()\nplt.show()\n",
        "\n\nplt.plot(points)\nplt.yscale('log')\nplt.show()\n\n",
        "\n\nplt.plot(x, y)\nplt.xlabel('X-axis', fontsize=18)\nplt.ylabel('Y-axis', fontsize=18)\nplt.title('Plot of y over x', fontsize=20)\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\nf = plt.figure()\nax = f.add_subplot(111)\n\n# plot y over x, show tick labels (from 1 to 10)\n# use the `ax` object to set the tick labels\nax.set_xticks(np.arange(1, 11))\nax.set_yticks(np.arange(1, 11))\nax.set_xticklabels(np.arange(1, 11))\nax.set_yticklabels(np.arange(1, 11))\n\n# plot y over x\nax.plot(x, y)\n\nplt.show()\n",
        "\n\n# Plot line segments according to the positions specified in lines\n# Use the colors specified in c to color each line segment\nplt.plot(lines[:, 0], lines[:, 1], colors=c)\n\n",
        "\n\nx = np.arange(0, 1000, 50)\ny = np.arange(0, 1000, 50)\n\n# plot y over x on a log-log plot\n# mark the axes with numbers like 1, 10, 100. do not use scientific notation\nfig, ax = plt.subplots(figsize=(10, 7))\n\nax.loglog(x, y)\nax.set_xlabel('X')\nax.set_ylabel('Y')\n\nax.set_xscale('log')\nax.set_yscale('log')\n\nax.grid(True)\n\n# mark the axes with numbers like 1, 10, 100\nax.set_xticks([1, 10, 100])\nax.set_yticks([1, 10, 100])\n\nplt.show()\n",
        "\n\n# create four subplots\nfig, axs = plt.subplots(2, 2)\n\n# plot the data in each subplot\naxs[0, 0].plot(df.A)\naxs[0, 0].scatter(df.index, df.A)\naxs[0, 1].plot(df.B)\naxs[0, 1].scatter(df.index, df.B)\naxs[1, 0].plot(df.C)\naxs[1, 0].scatter(df.index, df.C)\naxs[1, 1].plot(df.D)\naxs[1, 1].scatter(df.index, df.D)\n\n# set labels and title\naxs[0, 0].set_xlabel(\"Date\")\naxs[0, 0].set_ylabel(\"A\")\naxs[0, 1].set_ylabel(\"B\")\naxs[1, 0].set_ylabel(\"C\")\naxs[1, 1].set_ylabel(\"D\")\naxs[0, 0].set_title(\"A\")\naxs[0, 1].set_title(\"B\")\naxs[1, 0].set_title(\"C\")\naxs[1, 1].set_title(\"D\")\n\n# tight layout\nplt.tight_layout()\n\n# show the plot\nplt.show()\n\n",
        "\n\n# renormalize the data\ndata_sum = sum(data)\ndata_norm = [d/data_sum for d in data]\n\n# make histogram\nplt.hist(data_norm, bins=range(min(data_norm), max(data_norm)+1, 100))\n\n# format y tick labels\nplt.yticks(np.arange(0, max(data_norm)*1.1, 10))\nplt.yticks(np.arange(0, max(data_norm)*1.1, 10), [str(int(x*10))+'%' for x in plt.yticks()])\n\n",
        "\n\n# Create a scatter plot of x and y with marker='o' and alpha=0.5\nplt.scatter(x, y, marker='o', alpha=0.5)\n\n# Plot the line\nplt.plot(x, y)\n\n",
        "\n# Create a single figure-level legend using the figlegend function\nfiglegend(axs)\n",
        "\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n\n# First subplot: bill_depth_mm over bill_length_mm\naxs[0].scatter(df[\"bill_length_mm\"], df[\"bill_depth_mm\"])\naxs[0].set_title(\"Bill Depth vs Bill Length\")\naxs[0].set_xlabel(\"Bill Length (mm)\")\naxs[0].set_ylabel(\"Bill Depth (mm)\")\nsns.regression(axs[0], x=\"bill_length_mm\", y=\"bill_depth_mm\")\n\n# Second subplot: flipper_length_mm over bill_length_mm\naxs[1].scatter(df[\"bill_length_mm\"], df[\"flipper_length_mm\"])\naxs[1].set_title(\"Flipper Length vs Bill Length\")\naxs[1].set_xlabel(\"Bill Length (mm)\")\naxs[1].set_ylabel(\"Flipper Length (mm)\")\nsns.regression(axs[1], x=\"bill_length_mm\", y=\"flipper_length_mm\")\n\nplt.tight_layout()\n\n",
        "\n# create a new axis object for the second x-axis\nax2 = ax.twinx()\n# set the limits for the second x-axis\nax2.set_xlim(1, 10)\n# set the tick labels for the second x-axis\nax2.set_xticklabels(['second'])\n",
        "\n\nplt.plot(x, y)\nplt.legend(label='\u03bb')\nplt.show()\n\n",
        "\n\nextra_ticks = [2.1, 3, 7.6]\nplt.xticks(range(0, 10, 2), extra_ticks)\n\n",
        "\n\nplt.xticks(rotation=-60, ha='left')\n\n",
        "\n\nplt.xticks(rotation=60, ha='right')\nplt.gca().yaxis.tick_right()\nplt.gca().yaxis.set_label_position('right')\nplt.gca().yaxis.tick_label_pad = 10\nplt.gca().yaxis.set_label_coords(-0.1, 0.5)\n\n",
        "\n# Set the transparency of the grid to be 0.2\nplt.gca().set_aspect('equal', adjustable='box')\nplt.grid(alpha=0.2)\n",
        "\nplt.gca().xaxis.set_major_formatter(plt.NullFormatter())\nplt.gca().yaxis.set_major_formatter(plt.NullFormatter())\nplt.tight_layout(pad=0)\n",
        "\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().yaxis.set_tick_params(labelleft=True, pad=10)\nplt.gca().xaxis.set_tick_params(labelbottom=True, pad=10)\n",
        "\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].plot(x, y)\naxs[0].set_title('Subplot 1')\n\naxs[1].plot(x, y)\naxs[1].set_title('Subplot 2')\n\nplt.tight_layout()\nplt.gca().set_aspect('equal', adjustable='box')\n\nplt.suptitle('Figure', fontsize=14)\n\n",
        "\n\nplt.plot(df.index, df.values)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Line Chart\")\nplt.show()\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Make a scatter plot with x and y\n# Use vertical line hatch for the marker and make the hatch dense\nfig, ax = plt.subplots()\nax.scatter(x, y, s=50, marker='v', hatch='///', edgecolors='black')\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nax.set_xticks([])\nax.set_yticks([])\nplt.show()\n",
        "\n\nscatter_kwargs = {'s': 50, 'facecolors': 'none', 'edgecolors': 'black', 'marker': 'v', 'linestyle': 'none'}\nplt.scatter(x, y, **scatter_kwargs)\n\n",
        "\n\n# Create a scatter plot with x and y\nplt.scatter(x, y, s=50, c='r', marker='*')\n\n# Set the hatch pattern for the markers\nplt.rcParams['patch.force_edgecolor'] = True\nplt.rcParams['patch.edgecolor'] = 'k'\nplt.rcParams['patch.facecolor'] = 'red'\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=['red'])\n\n",
        "\n\n# Define a custom marker using the star and vertical line hatch\nmarker = 'o:^'\n\n# Plot the scatter plot with the custom marker\nplt.scatter(x, y, marker=marker, s=100)\n\n",
        "\n\nxlim = (1, 5)\nylim = (1, 4)\n\nplt.imshow(data, extent=(xlim[0], xlim[1], ylim[0], ylim[1]), cmap='hot', interpolation='nearest')\n\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xlim)\nplt.ylim(ylim)\n\n",
        "\n\nx = np.linspace(0.1, 2 * np.pi, 41)\ny = np.exp(np.sin(x))\n\n# make a stem plot of y over x and set the orientation to be horizontal\nplt.stem(x, y, basefmt='', markerfmt='k', orientation='horizontal')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\n\nd = {\"a\": 4, \"b\": 5, \"c\": 7}\nc = {\"a\": \"red\", \"c\": \"green\", \"b\": \"blue\"}\n\n# Make a bar plot using data in `d`. Use the keys as x axis labels and the values as the bar heights.\n# Color each bar in the plot by looking up the color in colors\n\nplt.bar(d.keys(), d.values(), color=c.values())\nplt.xlabel('')\nplt.ylabel('')\nplt.title('')\nplt.show()\n",
        "\n\nx = [1, 2, 3, 4, 5]\ny = [1, 2, 3, 4, 5]\n\nplt.plot(x, y, 'o')\nplt.vlines(3, 0, 10, color='r', linestyle='-', label='Cutoff')\nplt.legend()\n\n",
        "\n\nlabels = [\"a\", \"b\"]\nheight = [3, 4]\n\n# Use polar projection for the figure and make a bar plot with labels in `labels` and bar height in `height`\nfig = plt.figure(figsize=(8, 6), facecolor='w', edgecolor='k')\nax = fig.add_axes([0, 0, 1, 1], polar=True)\n\n# Create a bar plot with the given labels and heights\ntheta = np.linspace(0, 2 * np.pi, len(height), endpoint=False)\nr = np.array(height)\nax.bar(theta, r, width=0.7 * 2 * np.pi / len(height), edgecolor='k', linewidth=0.5, align='edge')\n\n# Add labels to the plot\nfor i, label in enumerate(labels):\n    ax.text(theta[i] + np.pi / 2, 1.01 * r[i], label, ha='center', va='bottom', fontsize=12, color='k')\n\n# Set the limits of the plot\nax.set_rlim(0, max(height) + 1)\nax.set_theta_zero_location('N')\nax.set_theta_direction(-1)\n\n# Set the title and axis labels\nplt.title('Polar Bar Plot', fontsize=16)\nplt.xlabel('Angle (radians)', fontsize=14)\nplt.ylabel('Height', fontsize=14)\n\n# Display the plot\nplt.show()\n",
        "\n\nwedges = [plt.wedges.Wedge(x=0, y=1, width=x, fc='b') for x, _ in zip(data, l)]\nplt.pie(wedges, wedgeprops=dict(width=0.4), startangle=90)\nplt.axis('equal')\nplt.show()\n\n#",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and show blue dashed grid lines\nplt.plot(x, y)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Y over X')\nplt.grid(color='blue', linestyle='-', linewidth=1)\nplt.show()\n",
        "\n# Turn on minor grid lines\nplt.grid(which='minor', alpha=0.2, linestyle='-')\n",
        "\n\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\n\nfig, ax = plt.subplots()\nax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n\nfor i, p in enumerate(ax.containers):\n    ax.text(p.center[0], p.center[1] + 0.35 * p.height, labels[i], ha='center', va='center', fontweight='bold', color=colors[i])\n\nplt.show()\n",
        "\n\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\n\nfig, ax = plt.subplots()\nax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n\n# Add a legend\nlegend = ax.legend(labels, loc='center left', bbox_to_anchor=(1, 0.5))\nlegend.get_frame().set_alpha(0.5)\n\nfor text in legend.get_texts():\n    text.set_fontsize('small')\n    text.set_bbox(dict(facecolor='white', alpha=0.5))\n\nplt.axis('equal')\nplt.show()\n",
        "\n\n# Create a scatter plot of the data with transparent markers and non-transparent edges\nplt.scatter(x, y, alpha=0.5, edgecolors='black')\n\n",
        "\n# Create a new figure and axis objects\nfig, ax = plt.subplots()\n\n# Add vertical line at 55 with green color\nax.axvline(x=55, color='green')\n",
        "\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Calculate the maximum value among the two sets of bars\nmax_val = max(max(blue_bar), max(orange_bar))\n\n# Plot the blue bars\nax.bar(range(len(blue_bar)), blue_bar, color='b', align='edge', width=0.5)\n\n# Add some padding to avoid overlap\nfor i in range(len(blue_bar)):\n    ax.text(i, blue_bar[i] + 0.02 * max_val, str(blue_bar[i]), ha='center', va='bottom')\n\n# Plot the orange bars\nax.bar(range(len(orange_bar)), orange_bar, color='orange', align='edge', width=0.5)\n\n# Add some padding to avoid overlap\nfor i in range(len(orange_bar)):\n    ax.text(i, orange_bar[i] + 0.02 * max_val, str(orange_bar[i]), ha='center', va='bottom')\n\n",
        "\n\nx = np.arange(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\na = np.arange(10)\n\n# Make two subplots\n# Plot y over x in the first subplot and plot z over a in the second subplot\n# Label each line chart and put them into a single legend on the first subplot\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot y over x in the first subplot\naxs[0].plot(x, y, label='y')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].legend()\n\n# Plot z over a in the second subplot\naxs[1].plot(a, z, label='z')\naxs[1].set_xlabel('a')\naxs[1].set_ylabel('z')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n",
        "\n\nscatter_cmap = matplotlib.cm.get_cmap('Spectral')\nscatter_colors = scatter_cmap(y)\n\nplt.scatter(x, y, c=scatter_colors, cmap=scatter_cmap)\n\n",
        "\n\nplt.plot(x, y)\nplt.xticks(np.arange(0, 10, 1))\nplt.show()\n\n",
        "\n\ng = sns.FacetGrid(df, hue=\"species\", height=3)\ng.map(plt.bar, \"bill_length_mm\", \"sex\", fill=False)\ng.set_axis_labels(\"Sex\", None, None, \"Bill Length (mm)\")\n\n",
        "\nx = [0.25, 0.5, 0.75]\ny = [0.25, 0.5*0.5, 0.75*0.5]\nplt.plot(x, y, 'r-', lw=2)\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and use the greek letter phi for title. Bold the title and make sure phi is bold.\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_title(r'$\\phi$', fontweight='bold')\nax.set_title(fontweight='bold', fontsize=16)\nplt.show()\n",
        "\n\nplt.plot(x, y, label='Line')\nplt.legend(bbox_to_anchor=(1.05, 1), ncol=1, fontsize=10, handlelength=1, borderaxespad=0.1)\n\n",
        "\n# Adjust the size of the figure\nplt.figure(figsize=(8, 6))\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y, label=\"Line\")\nplt.plot(y, x, label=\"Flipped\")\n\n# Show a two columns legend of this plot\nplt.legend(bbox_to_anchor=(1.04, 1), ncol=2, fontsize=10)\n\n",
        "\nplt.legend()\nplt.scatter(x, y, color='red', marker='s', s=100, label='Scatter')\n",
        "\n\ndata = np.random.random((10, 10))\n\n# plot the 2d matrix data with a colorbar\nfig, ax = plt.subplots()\nim = ax.imshow(data, cmap='coolwarm')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\nplt.colorbar(im, cax=cax)\nplt.show()\n",
        "\n\nplt.plot(x, y)\nplt.title(\"**Figure** 1\", fontweight='bold')\n\n",
        "\n\ndf = pd.DataFrame(\n    {\n        \"id\": [\"1\", \"2\", \"1\", \"2\", \"2\"],\n        \"x\": [123, 22, 356, 412, 54],\n        \"y\": [120, 12, 35, 41, 45],\n    }\n)\n\n# Use seaborn to make a pairplot of data in `df` using `x` for x_vars, `y` for y_vars, and `id` for hue\n# Hide the legend in the output figure\nsns.pairplot(df, x_vars=['x'], y_vars=['y'], hue='id', diag_kind='kde')\nplt.legend(bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0)\nplt.show()\n",
        "\n\nplt.plot(x, y)\nplt.xscale('reverse')\n\n",
        "\n# Create a grid of x and y values\ngrid_x, grid_y = np.meshgrid(x, y)\n\n# Calculate the distance from each grid point to the origin\ndistances = np.sqrt(grid_x**2 + grid_y**2)\n\n# Set the color of each grid point based on the distance\ncmap = plt.get_cmap('coolwarm')\ncolors = cmap(distances)\n\n# Plot the grid with the color based on the distance\nplt.pcolor(grid_x, grid_y, colors)\nplt.colorbar()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot a scatter plot with values in x and y\n# Plot the data points to have red inside and have black border\nplt.scatter(x, y, c='red', edgecolors='black')\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# plot y over x on a 2 by 2 subplots with a figure size of (15, 15)\n# repeat the plot in each subplot\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor i in range(2):\n    for j in range(2):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_title(f\"Subplot ({i+1}, {j+1})\")\n\nplt.show()\n",
        "\n\nhist, bins = np.histogram(x, bins=5, range=(0, 10), density=False)\n\nplt.bar(bins[:-1], hist, align='edge', width=2)\n\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n\n",
        "\n\nplt.scatter(x, y, c=error, cmap='Blues')\nplt.colorbar()\nplt.show()\n\n",
        "\n# create a mask for the x=0 and y=0 lines\nmask = (x == 0) & (y == 0)\n\n# set the values of the mask to NaN, which will be plotted in white\nz[mask] = np.nan\n\n",
        "\n\nax.errorbar(box_position, box_height, yerr=box_errors, color=c)\n\n",
        "\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nax[0].plot(x, y)\nax[0].set_title('Y')\n\nax[1].plot(a, z)\nax[1].set_title('Z')\n\nax[1].set_position([0.5, 0.45, 0.5, 0.5])\n\n",
        "\n\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xticks(np.arange(10))\n        axs[i, j].set_yticks(np.arange(10))\n        axs[i, j].set_xticklabels(np.arange(1, 11))\n        axs[i, j].set_yticklabels(np.arange(1, 11))\n        axs[i, j].set_xlim(0, 10)\n        axs[i, j].set_ylim(0, 10)\n        axs[i, j].set_aspect('equal')\n        axs[i, j].set_facecolor('white')\n        axs[i, j].grid(True)\n        axs[i, j].axis('on')\n\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\nplt.show()\n\n",
        "\n\nplt.figure(figsize=(8, 8))\nplt.imshow(d, cmap='gray')\nplt.show()\n\n",
        "\n\ntable = plt.table(cellText=df.values, colLabels=df.columns, bbox=[0, 0, 1, 1])\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x in a line chart. Show x axis tick labels on both top and bottom of the figure.\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xticks([])  # remove top x-axis tick labels\nax.set_yticks([])  # remove left y-axis tick labels\nax.invert_yaxis()  # flip y-axis for bottom tick labels\nplt.show()\n",
        "\n\n# Create a subplot with two rows and one column\nfig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n# Plot y over x in the first row, first column\nax[0].plot(x, y)\nax[0].set_xticks([])  # Hide x axis ticks\nax[0].set_yticks([])  # Hide y axis ticks\n\n# Show x axis ticks on both top and bottom of the figure\nax[0].tick_params(axis='both', which='both', length=0)\n\n# Plot x over y in the second row, first column\nax[1].plot(y, x)\nax[1].set_xticks([])  # Hide x axis ticks\nax[1].set_yticks([])  # Hide y axis ticks\n\n# Show y axis ticks on both left and right of the figure\nax[1].tick_params(axis='both', which='both', length=0)\n\n",
        "\nplt.plot(x, y)\nplt.xticks(x, ['{}'.format(i) for i in x], rotation=45)\nplt.gca().xaxis.set_major_locator(plt.NullLocator())\nplt.gca().yaxis.grid(True, which='major', color='gray', linestyle='-')\nplt.show()\n",
        "\n\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", dodge=False, data=df, kind=\"scatter\", col=\"diet\")\nplt.suptitle(\"Group: Fat\")\n\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", dodge=False, data=df, kind=\"scatter\", col=\"diet\", height=0.5, ax=plt.subplot(121))\nplt.suptitle(\"Group: No Fat\")\n\n",
        "\n\ncatplot = sns.catplot(x='time', y='pulse', hue='kind', col='diet', data=df, kind='scatter', height=4, aspect=1.5)\ncatplot.set(xlabel='Exercise Time', ylabel='Exercise Time')\n\n",
        "\n\ng = sns.catplot(x='time', y='pulse', hue='kind', col='diet', data=df, kind='scatter', height=4, aspect=1.5)\ng.set(ylabel=None)\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# plot y over x with label \"y\"\n# make the legend fontsize 8\nplt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n\n# show the plot\nplt.show()\n",
        "\nfig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\n",
        "\n\nplt.plot(x, y, label='y')\nplt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\n\n",
        "\nfrom numpy import *\n\nt = linspace(0, 2 * math.pi, 400)\na = sin(t)\nb = cos(t)\nc = a + b\n\n# Plot a, b, c in the same figure\nfig, axs = plt.subplots(nrows=3, ncols=1, sharex=True)\naxs[0].plot(t, a)\naxs[0].set_title('a(t) = sin(t)')\naxs[1].plot(t, b)\naxs[1].set_title('b(t) = cos(t)')\naxs[2].plot(t, c)\naxs[2].set_title('c(t) = a(t) + b(t)')\nplt.xlabel('t')\nplt.show()\n",
        "\n\nsns.stripplot(x=df[\"sex\"], y=df[\"bill_length_mm\"], hue=df[\"species\"], dodge=False)\n\n# Remove the legend\nplt.legend(bbox_to_anchor=(1.04, 1), loc=2, borderaxespad=0.)\n\n",
        "\n\ng = sns.FaceGrid(df, row=\"b\")\ng.map(sns.pointplot, \"c\", \"a\", xtick_interval=1, xtick_labels=df[\"a\"][::2])\n\n",
        "\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.random.random(10)\ny = np.random.random(10)\nz = np.random.random(10)\n\n# Make a 3D scatter plot of x,y,z\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\n\n# Change the view of the plot to have 100 azimuth and 50 elevation\nax.view_init(elev=50, azim=100)\n\nplt.show()\n",
        "\n\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Line Chart\")\nplt.xticks(())\nplt.yticks(())\n\n",
        "\nfrom matplotlib import gridspec\n\nx = np.random.random((10, 10))\n\nnrow = 2\nncol = 2\n\nfig = plt.figure(figsize=(ncol + 1, nrow + 1))\n\ngs = gridspec.GridSpec(nrow, ncol)\n\nfor i in range(nrow * ncol):\n    ax = plt.subplot(gs[i])\n    img = ax.imshow(x[:, i % 10], cmap='gray')\n    ax.axis('off')\n\nplt.show()\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\n# result = result[:,0]\n",
        "\nimport tensorflow as tf\n\nlabels = [0, 6, 5, 4, 2]\nmissing_code = []\nfor i in range(len(labels)):\n    missing_code.append(tf.one_hot(labels[i], depth=10))\nresult = tf.concat(missing_code, axis=0)\n\nprint(result)\n",
        "\nimport tensorflow as tf\n\nlabels = [0, 6, 5, 4, 2]\nresult = tf.one_hot(indices=labels, depth=10)\n# Reshape to be a 2D tensor of shape (5, 10)\nresult = tf.reshape(result, [5, 10])\n\nprint(result)\n",
        "\nimport tensorflow as tf\n\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    result = tf.zeros((len(labels), 10), dtype=tf.int32)\n    for i, label in enumerate(labels):\n        result[i, label] = 1\n    return result\n",
        "\nimport tensorflow as tf\n\nlabels = [0, 6, 5, 4, 2]\ntarget_index = tf.cast(tf.argmax(tf.one_hot(labels, depth=10), axis=-1), tf.int32)\nresult = tf.expand_dims(target_index, axis=-1)\nresult = tf.repeat(result, tf.shape(labels)[0], axis=0)\nresult = tf.reshape(result, [-1, 10])\n\nprint(result)\n",
        "\nimport tensorflow as tf\n\n\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [i, i+1, i+2]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = ds.make_one_shot_iterator().get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n",
        "\n        result.extend([i, i+1, i+2])\n",
        "\nimport tensorflow as tf\n\n\nlengths = [4, 3, 5, 2]\nresult = []\nfor i in range(max(lengths)):\n    row = [1 for j in range(8)]\n    row[7-i] = 0\n    for j in range(len(lengths)):\n        row[:lengths[j]] = [0 for k in range(lengths[j])]\n        result.append(row)\nprint(result)\n",
        "\nimport tensorflow as tf\n\n\nlengths = [4, 3, 5, 2]\nresult = tf.concat([tf.ones([1, 8 - len(lengths[i]) - 1]), tf.ones([1, lengths[i]])], axis=1)\nfor i in range(len(lengths)):\n    result = tf.concat([result, tf.zeros([1, 8 - len(lengths[i]) - 1])], axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\n\nlengths = [4, 3, 5, 2]\n\nmask = tf.concat([tf.zeros([8 - i,], dtype=tf.float32) + tf.ones([i,], dtype=tf.float32) for i in lengths], axis=0)\nresult = mask\n\nprint(result)\n",
        "\nimport tensorflow as tf\n\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    result = []\n    for i, l in enumerate(lengths):\n        mask = [1] * l + [0] * (8 - l)\n        result.append(mask)\n    return result\n",
        "\nimport tensorflow as tf\n\nlengths = [4, 3, 5, 2]\nmask = tf.concat([tf.ones([1, lengths[0]]), tf.zeros([1, lengths[0]])], axis=1)\nfor i in range(1, len(lengths)):\n    mask = tf.concat([mask, tf.ones([1, lengths[i]]), tf.zeros([1, lengths[i]])], axis=1)\n\nresult = tf.reshape(mask, [-1])\n\nprint(result)\n",
        "\nresult = tf.math.multiply(tf.expand_dims(a, 1), tf.expand_dims(b, 0))\n",
        "\n    combinations = tf.reshape(result, [-1])\n",
        "\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 1, 512))\n\nresult = tf.reshape(a, [50, 100, 512])\n# or\n# result = tf.reshape(a, [-1, 512])\n# or\n# result = tf.reshape(a, [50, 100, 512])\n\nprint(result)\n",
        "\nresult = tf.reshape(a, [50, 100, 1, 512])\n",
        "\nresult = tf.reshape(a, [1, 50, 100, 1, 512])\n",
        "\nsum_along_axis_1 = tf.reduce_sum(A, axis=1)\n",
        "\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\n\nresult = tf.reduce_prod(A, axis=1)\n\n### output your answer to the variable 'result'\nprint(result)\n",
        "\nimport tensorflow as tf\n\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\n\nresult = tf.math.reciprocal(A)\n\n### output your answer to the variable 'result'\nprint(result)\n",
        "\nsquared_diff = tf.square(tf.sub(a, b))\n",
        "\nsquared_diff = tf.square(tf.sub(a, b))\ncolumn_sum = tf.reduce_sum(squared_diff, axis=1)\n",
        "\n    squared_diff = tf.square(tf.sub(A,B))\n    # [Missing Code]\n    sum_of_squared_diff = tf.reduce_sum(squared_diff, axis=1)\n",
        "\nimport tensorflow as tf\n\nx = [[1, 2, 3], [4, 5, 6]]\ny = [0, 1]\nz = [1, 2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\n\nresult = tf.gather(x, y, batch_dims=1)\nresult = tf.gather(result, z, batch_dims=1)\n\n# print(result)  # uncomment this line to see the result\n",
        "\nimport tensorflow as tf\n\nx = [[1,2,3],[4,5,6]]\nrow = [0,0]\ncol = [1,2]\nx = tf.constant(x)\nrow = tf.constant(row)\ncol = tf.constant(col)\n\nresult = x[[row,col]]\n\nprint(result)\n",
        "\nimport tensorflow as tf\n\nexample_x = [[1,2,3],[4,5,6]]\nexample_y = [0,1]\nexample_z = [1,2]\nexample_x = tf.constant(example_x)\nexample_y = tf.constant(example_y)\nexample_z = tf.constant(example_z)\ndef f(x=example_x,y=example_y,z=example_z):\n    result = tf.gather(x, y[:, tf.newaxis], axis=1)\n    result = tf.gather(result, z, axis=0)\n    return result\n",
        "\nresult = tf.reshape(A, [-1, A.get_shape().as_list[-1]])\nresult = tf.matmul(result, B)\nresult = tf.reshape(result, [tf.shape(A)[0], tf.shape(A)[0], -1])\n",
        "\nresult = tf.matmul(A, B)\n",
        "\nimport tensorflow as tf\n\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\n\nresult = tf.compat.v1.python_dispatch.byte_string_to_string(x)\n\nprint(result)\n",
        "\n    # result = tf.string_join(result, separator=',')\n",
        "\nimport tensorflow as tf\n\nx = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nx = tf.convert_to_tensor(x, dtype=tf.float32)\n\nresult = tf.nn.functional.sum(x, axis=-2) / tf.math.count_nonzero(x, axis=-2)\n\nprint(result)\n",
        "\nimport tensorflow as tf\n\nx = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nx = tf.convert_to_tensor(x, dtype=tf.float32)\n\nresult = tf.nn.moments(x, axes=[-2], keep_dims=True)\nresult = result / tf.cast(tf.not_equal(x, 0), dtype=result.dtype)\n\nprint(result)\n",
        "import tensorflow as tf\n\nexample_x = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nexample_x = tf.convert_to_tensor(example_x, dtype=tf.float32)\n\ndef f(x=example_x):\n    x_shape = tf.shape(x)\n    batch_size = x_shape[0]\n    features = x_shape[-1]\n    non_zero_mask = tf.math.equal(x, 0)\n    non_zero_features = tf.cast(tf.reshape(tf.count_nonzero(non_zero_mask, axis=-1), [-1, 1, 1, features]), x.dtype)\n    non_zero_sum = tf.reduce_sum(non_zero_features, axis=-2)\n    non_zero_mean = non_zero_sum / tf.cast(batch_size, x.dtype)\n    result = tf.where(non_zero_mask, x, non_zero_mean)\n    return result\n\nresult = f()\nprint(result)",
        "\nwith tf.compat.v1.Session() as sess:\n",
        "\n# result = result.reshape(-1, 1)\n#",
        "\n# result = tf.cast(result, tf.int32)\n",
        "\n    argmax_scores = tf.argmax(scores, axis=-1)\n",
        "\n# result = tf.reshape(tf.argmin(a, axis=1), [-1])\n",
        "\n#Save the model in \"my_model\"\nmodel.save(\"my_model\")\n",
        "\n### [Missing Code]\n##",
        "\nimport tensorflow as tf\n\nseed_x = 10\n# Set random seed\ntf.random.set_seed(seed_x)\n\n# Generate 114 random integers as a tensor\nresult = tf.random.uniform([114], minval=2, maxval=5, dtype=tf.int32)\n\n# Print the tensor\nprint(result)\n",
        "import tensorflow as tf\n\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform([10], maxval=4, dtype=tf.int32)\n    return result\n\nf()",
        "\n# result = \n"
    ],
    "Scipy": [
        "\nimport numpy as np\nimport scipy\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\nresult = np.array(scipy.optimize.curve_fit(lambda x, a, b: a*np.log(x) + b, x, y))\nprint(result)\n",
        "\np = np.polyfit(x, y, 1)\n",
        "\ndef func(p, x):\n    A, B, C = p\n    return A * np.exp(B * x) + C\n",
        "\ntest_statistic, p_value = stats.kstest(x, y)\n",
        "\ntest_stat, p_value = stats.kstest(x, y)\nresult = np.bool_(p_value < alpha)\n",
        "\nimport scipy.optimize as optimize\nfrom math import *\n\ninitial_guess = [-1, 0, -3]\n\nresult = optimize.minimize(f, initial_guess)\n\nprint(result.x)\n\ndef f(x):\n    a, b, c = x\n    return ((a + b - c) - 2)**2 + ((3 * a - b - c))**2 + sin(b) + cos(b) + 4\n",
        "\nimport numpy as np\nimport scipy.stats\nz_scores = np.array([-3, -2, 0, 2, 2.5])\np_values = 2 * (1 - scipy.stats.norm.cdf(z_scores))\nprint(p_values)\n",
        "\nleft_tail = 0.5\np_values = []\nfor z_score in z_scores:\n    p_value = scipy.stats.norm.sf(z_score, loc=mu, scale=sigma) * 2 * left_tail\n    p_values.append(p_value)\n",
        "\n",
        "import numpy as np\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\nx = 25\nresult = stats.lognorm.cdf(x, s=stddev, loc=mu)",
        "\ndist = stats.lognorm(s=stddev, loc=mu)\nexpected_value = dist.mean()\nmedian = np.median(dist)\n",
        "\n# result = sa * sb\n",
        "\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n    result = sA.multiply(sB)\n    return result\n",
        "\nimport numpy as np\nimport scipy.interpolate\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30]])\n\n# Define the interpolator for x, y and z\nx, y, z = np.meshgrid(points[:,0], points[:,1], points[:,2])\nx_interp, y_interp, z_interp = np.meshgrid(x, y, z)\ncart_coords = np.dstack((x_interp.flatten(), y_interp.flatten(), z_interp.flatten()))\ninterpolator = scipy.interpolate.LinearNDInterpolator(cart_coords, V)\n\n# Get the interpolated value for the request point\nresult = interpolator([[25, 20, -30]])\n\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.interpolate\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Define the interpolator for x, y and z dimensions\nx_interpolator = scipy.interpolate.LinearNDInterpolator(points[:,0], points[:,1])\ny_interpolator = scipy.interpolate.LinearNDInterpolator(points[:,0], points[:,2])\nz_interpolator = scipy.interpolate.LinearNDInterpolator(points[:,0], points[:,3])\n\n# Interpolate x, y, and z for the request points\nx_interpolated = x_interpolator(request[:,0])\ny_interpolated = y_interpolator(request[:,0])\nz_interpolated = z_interpolator(request[:,0])\n\n# Calculate the moisture value for each request point\nresult = np.zeros((2,1))\nfor i in range(2):\n    x_point = x_interpolated[i]\n    y_point = y_interpolated[i]\n    z_point = z_interpolated[i]\n    point_index = np.argmin(np.sqrt((points[:,0]-x_point)**2 + (points[:,1]-y_point)**2 + (points[:,2]-z_point)**2))\n    result[i,0] = V[point_index]\n\nprint(result)\n",
        "\nxrot, yrot = x0 * np.cos(angle) - y0 * np.sin(angle), x0 * np.sin(angle) + y0 * np.cos(angle)\n",
        "\nresult = M.diags()\n",
        "\nresult, p_value = stats.kstest(times, \"uniform\", args=(0, T))\n",
        "\n    result, p_value = stats.kstest(times, 'uniform', args=(0, T))\n",
        "\nresult, p_value = stats.kstest(times, 'uniform', args=(T,))\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\nFeature = sparse.vstack([c1, c2])\n\n#print(Feature)\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\nFeature = sparse.vstack([c1, c2])\n\n#print(Feature)\n",
        "\n# Define the objective function for the optimization problem\ndef objective_function(x):\n    assignment = np.zeros(N)\n    assignment[np.argmin([np.linalg.norm(points1[i] - points2[j]) for j in range(N)] - x)] = i\n    return np.sum([np.linalg.norm(points1[assignment[i]] - points2[i]) for i in range(N)])\n# Use the scipy.optimize.minimize function to find the optimal assignment\nresult = scipy.optimize.minimize(objective_function, np.zeros(N)).x\n",
        "import numpy as np\nimport scipy.spatial\nimport scipy.optimize\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\nresult = scipy.optimize.linear_sum_assignment(np.sqrt(np.sum(np.square(points1[:,0]-points2[:,0]),axis=1)**2 + np.sum(np.square(points1[:,1]-points2[:,1]),axis=1)**2)))\nprint(result)",
        "\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\nb.elim_diag()\n",
        "import numpy as np\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\nresult = 0\nregions = []\n\nfor i in range(512):\n    row = img[i]\n    for j in range(512):\n        if row[j] > threshold:\n            regions.append((i, j))\n            result += 1\n            # Connect all the cells in the region\n            queue = [(i, j)]\n            while queue:\n                x, y = queue.pop(0)\n                if x > 0 and img[x - 1][y] > threshold:\n                    queue.append((x - 1, y))\n                if x < 511 and img[x + 1][y] > threshold:\n                    queue.append((x + 1, y))\n                if y > 0 and img[x][y - 1] > threshold:\n                    queue.append((x, y - 1))\n                if y < 511 and img[x][y + 1] > threshold:\n                    queue.append((x, y + 1))\n\n# Print the number of regions\nprint(result)\n```",
        "import numpy as np\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\nresult = 0\nregions = []\nfor i in range(512):\n    for j in range(512):\n        if img[i, j] < threshold:\n            regions.append((i, j))\n            result += 1\n            # mark all the adjacent cells as visited\n            visited = np.zeros((512, 512))\n            visited[i, j] = 1\n            queue = [(i, j)]\n            while queue:\n                x, y = queue.pop(0)\n                for dx, dy in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n                    nx, ny = x + dx, y + dy\n                    if 0 <= nx < 512 and 0 <= ny < 512 and not visited[nx, ny]:\n                        visited[nx, ny] = 1\n                        queue.append((nx, ny))\n\n# print(regions)\n# print(result)\n```",
        "\n    regions = ndimage.label(img > threshold)\n    num_regions = np.unique(regions).shape[0]\n",
        "\nregions = ndimage.measurements.label(img > threshold)\nunique_labels, counts = np.unique(regions, return_counts=True)\ncentroids = np.array([np.mean(np.where(regions == l), axis=1) for l in unique_labels])\ndistances = np.abs(centroids - (0, 0))\n",
        "\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy import sparse\n\nM= sparse.random(10, 10, density=0.1, format='lil')\n\nM.make_symmetric()\n\nprint(M)\n",
        "\n    sA.setdiag(0)\n    # [Missing Code]\n    sA.sort()\n    sA.setdiag(1)\n",
        "\nlabel = np.zeros(square.shape, dtype=np.int32)\nlabel = scipy.ndimage.label(square)\nblobs = scipy.ndimage.find_objects(label)\nfor blob in blobs:\n if blob.shape == (1, 1):\n square[blob] = 0\n",
        "\nlabel_image = np.zeros(square.shape)\nlabel_image[square != 0] = 1\nlabeled_image, num_labels = scipy.ndimage.label(label_image)\nfor label in np.unique(labeled_image):\n if label_image.max() == 1:\n square[labeled_image == label] = 0\n",
        "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\nmean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\nprint(mean)\nprint(standard_deviation)",
        "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\nMax = np.max(col)\nMin = np.min(col)\n",
        "\nmedian = np.median(col.toarray())\nmode = np.mode(col.toarray())\n",
        "from scipy.optimize import curve_fit\nimport numpy as np\ns = '''1.000000000000000021e-03,2.794682735905079767e+02\n4.000000000000000083e-03,2.757183469104809888e+02\n1.400000000000000029e-02,2.791403179603880176e+02\n2.099999999999999784e-02,1.781413355804160119e+02\n3.300000000000000155e-02,-2.798375517344049968e+02\n4.199999999999999567e-02,-2.770513900380149721e+02\n5.100000000000000366e-02,-2.713769422793179729e+02\n6.900000000000000577e-02,1.280740698304900036e+02\n7.799999999999999989e-02,2.800801708984579932e+02\n8.999999999999999667e-02,2.790400329037249776e+02'''.replace('\\n', ';')\narr = np.matrix(s)\nz = np.array(arr[:, 0]).squeeze()\nUa = np.array(arr[:, 1]).squeeze()\ntau = 0.045\ndegree = 15\na = np.ones(degree+1)\npopt, pcov = curve_fit(lambda x, a: np.sum([a[i] * np.cos(i * np.pi / tau * x) for i in range(1, degree+1)] + [a[0] * np.cos(0 * np.pi / tau * x)], axis=0), z, Ua)\nprint(popt, pcov)",
        "\nresult = np.zeros((example_array.shape[0], example_array.shape[0]))\nfor i in range(example_array.shape[0]):\n    for j in range(i+1, example_array.shape[0]):\n        distances = scipy.spatial.distance.cdist(example_array[i,:], example_array[j,:])\n        result[i,j] = distances.min()\n        result[j,i] = distances.min()\n",
        "\nresult = np.zeros((len(example_array), len(example_array)))\nfor i in range(len(example_array)):\n    for j in range(i+1, len(example_array)):\n        result[i][j] = scipy.spatial.distance.cdist(example_array[i], example_array[j], 'manhattan').min()\n",
        "import numpy as np\nimport scipy.spatial.distance\nexample_arr = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\ndef f(example_array = example_arr):\n    result = np.zeros((len(example_array), len(example_array), 2))\n    for i in range(len(example_array)):\n        for j in range(len(example_array)):\n            if i != j:\n                dist = np.linalg.norm(example_array[i] - example_array[j])\n                result[i, j, :] = [i, j, dist]\n    return result\n```",
        "\nfrom scipy import interpolate\nimport numpy as np\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85.36, 93.24],\n              [61.48, 69.31, 75.36, 81.87, 89.35],\n              [57.61, 65.75, 71.7, 79.1, 86.13],\n              [55.12, 63.34, 69.32, 77.29, 83.88],\n              [54.58, 62.54, 68.7, 76.72, 82.92],\n              [56.58, 63.87, 70.3, 77.69, 83.53],\n              [61.67, 67.79, 74.41, 80.43, 85.86],\n              [70.08, 74.62, 80.93, 85.06, 89.84]])\nx_val = np.linspace(-1, 1, 100)\nresult = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    result[i, :] = interpolate.splev(x_val, tck, der = 0)\nplt.figure(figsize = (5.15,5.15))\nplt.subplot(111)\nfor i in range(5):\n    plt.plot(x[:, i], y[:, i], linestyle = '', marker = 'o')\n    plt.plot(x_val, result[i, :], linestyle = ':', linewidth = 0.25, color =  'black')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.stats as ss\nx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\nx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\nresult = ss.anderson_ksamp(x1, x2)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\n\ndef tau(A, k):\n    tau_list = []\n    for i in range(A.shape[1]-k+1):\n        col = A.iloc[:, i:i+k]\n        col = col.values.T.flatten()\n        tau_val, p_val = stats.kendalltau(col, col)\n        tau_list.append(tau_val)\n        \n    return pd.Series(tau_list, index=A.columns[k:])\n\ndf[['AB', 'AC', 'BC']] = tau(df, 3)\nprint(df)\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sa.sum() == 0\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'lil')\nis_empty = len(sa.nonzero()[0]) == 0\nresult = is_empty\n",
        "\n# result = block_diag(a[0], a[1],a[2])\n",
        "\n# p_value = \n",
        "\n    p_value = result.pvalue\n",
        "\nkurtosis_result = np.mean((a - np.mean(a))**4)\n",
        "\n# [Missing Code]\n",
        "\ninterpolator = scipy.interpolate.interp2d(t, s, z)\nresult = interpolator(t, s)\n",
        "\nimport numpy as np\nimport scipy.interpolate\nexampls_s = np.linspace(-1, 1, 50)\nexample_t = np.linspace(-2, 0, 50)\ndef f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    result = scipy.interpolate.interp2d(t, s, z, kind='cubic')(t, s)\n    return result\n",
        "\nimport scipy.spatial\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n\nresult = np.zeros(1)\nfor point in extraPoints:\n    region = vor.points_in_polyhedron([point])[0]\n    result = np.append(result, region)\n\nprint(result)\n",
        "import numpy as np\n\nresult = np.zeros(len(extraPoints))\nfor i, point in enumerate(extraPoints):\n    region = vor.points_in_region(point)[0]\n    result[i] = region\n\nprint(result)",
        "\n# result = sparse.vstack(vectors)\n# for i in range(len(vectors)):\n#     while len(vectors[i]) < max_vector_size:\n#         vectors[i] = np.concatenate((vectors[i], vectors[i]))\n",
        "\nb = scipy.ndimage.median_filter(a, 3, mode='constant', cval=0)\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = 2\ncolumn = 3\n\nresult = M.getrow(row).toarray()[column-1]\n\n",
        "\nresult = M.getrow(row).toarray()[column]\n",
        "\n# new_array = np.zeros((1000, 100, 100))\n# for i in range(100):\n#     for j in range(100):\n#         new_array[:, i, j] = interp1d(x, array[:, i, j])(x_new)\n",
        "\nP_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n",
        "\n    P_inner = scipy.integrate.quad(NDfx,-(x-u)/o2,(x-u)/o2)[0]\n",
        "\nimport numpy as np\nimport scipy.fft as sf\nN = 8\nresult = np.zeros((N,N))\nfor k in range(N):\n    for n in range(N):\n        result[k][n] = np.cos(2*np.pi*k*n/N)\n",
        "\nfrom scipy import sparse\nimport numpy as np\nmatrix = np.array([[3.5,   13. ,   28.5,   50. ,   77.5],\n                   [-5. ,  -23. ,  -53. ,  -95. , -149. ],\n                   [2.5,   11. ,   25.5,   46. ,   72.5]])\n\nresult = diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.stats\nN = 3\np = 0.5\nresult = np.zeros((N+1,N+1))\nresult[np.arange(N+1), np.arange(N,0,-1)] = scipy.stats.binom.pmf(np.arange(N,0,-1), N, p)\nresult[np.diag_indices_from(result)] = 0\nprint(result)\n",
        "\nzscore_df = df.apply(lambda x: stats.zscore(x))\n",
        "\nresult = df.apply(lambda x: stats.zscore(x))\n",
        "\nresult = result.rename(columns={1:'zscore'})\n",
        "\nimport pandas as pd\nimport io\nimport numpy as np\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\nresult = pd.DataFrame()\nfor column in df.columns:\n    zscore = np.round(stats.zscore(df[column]),3)\n    result[f\"zscore_{column}\"] = zscore\n    result[column] = df[column]\n\nresult = result.dropna()\n\nprint(result)\n",
        "\nimport scipy\nimport scipy.optimize\nimport numpy as np\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\n\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([[3, 3], [4.5, 4.5]]) # center point\nresult = distance.cdist(np.array([[0, 0], [0.5, 0.5]]).T, mid) ** 2\nresult = np.expand_dims(result, axis=1)\nresult = np.expand_dims(result, axis=2)\nresult = np.tile(result, (1, 1, shape[0], shape[1]))\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([[3, 3], [3, 3]]) # center point\nresult = distance.cdist(np.dstack((np.arange(shape[0]), np.arange(shape[1]))))\nprint(result)\n",
        "import numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    mid = np.array([[shape[0]/2, 0], [0, shape[1]/2]])\n    result = distance.cdist(np.dstack(np.indices(shape).T), np.dstack((mid, mid))).reshape(*shape, 2)\n    return result\n```",
        "\nimport numpy as np\nimport scipy.ndimage\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\nresult = scipy.ndimage.zoom(x, float(shape[0]/3), float(shape[1]/3), order=1)\n",
        "\ndef objective(x):\n    return np.dot(a, x**2) - y\n",
        "import scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\nresult = scipy.optimize.minimize(\n    func=lambda x: func(x, a),\n    x0=x0,\n    args=(),\n    method='L-BFGS-B',\n    jac=None,\n    hess=None,\n    bounds=[(lb, ub) for lb, ub in zip(x0 - x_lower_bounds, x0 + x_lower_bounds)],\n    options={'maxiter': 1000}\n)\nprint(result.x)",
        "\ndef dN1_dt(t, N1, input_func):\n    return -100 * N1 + input_func(t)\n",
        "\nt_eval = np.linspace(time_span[0], time_span[1], 1000)\n# [Missing Code]\nsol = solve_ivp(fun=dN1_dt, t_span=time_span, y0=N0, t_eval=t_eval)\n",
        "\ndef dN1_dt_sinusoid(t, N1):\n    return -100 * N1 - np.cos(t)\n",
        "\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef function(x):\n    return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\n\nI=np.array((20,50,50,80))\nx0=I\n\ncons=[]\nsteadystate={'type':'eq', 'fun': lambda x: x.sum()-I.sum() }\ncons.append(steadystate)\n\nfor t in range(4):\n    y = x[t]\n    cons.append({'type':'ineq', 'fun': lambda x: y - I[t]})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n\nresult = sparse.vstack([sa, sb])\n# Alternatively, you can also use the hstack function:\n# result = sparse.hstack([sa, sb])\n\nprint(result)\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n\nresult = sparse.hstack((sa, sb))\n# Alternatively, you can also use the vstack function to merge the matrices vertically\n# result = sparse.vstack((sa, sb))\n\nprint(result)\n",
        "\nI = []\nfor n in range(len(c)):\n    eqn = 2*x*c[n]\n    result, error = scipy.integrate.quad(lambda x: eqn, low, high)\n    I.append(result)\n",
        "\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n",
        "\nV_dense = V.toarray()\nV_dense[V_dense == 0] = np.nan\nx_dense = np.nan\n# [Missing Code]\nV_dense[V_dense == np.nan] = x_dense\n",
        "\nV_coo = V.tocoo()\nV_coo_row = V_coo.row\nV_coo_col = V_coo.col\nV_coo_data = V_coo.data\n\nx_coo = sparse.coo_matrix((x, V_coo_row, V_coo_col), shape=V.shape)\n\nV_new_coo = sparse.coo_matrix((V_coo_data + x_coo.data), (V_coo_row, V_coo_col), shape=V.shape)\n\n",
        "\nC = A + B\n",
        "\n# Iterate through each column of the matrix\nfor i in range(sa.shape[1]):\n    # Extract the ith column of the matrix\n    column = sa[:,i].data\n    # Calculate the length of the column\n    length = math.sqrt(np.dot(column, column))\n    # Normalize the column by dividing each element by its length\n    column /= length\n    # Update the ith column of the matrix with the normalized column\n    sa[:,i].data = column\n",
        "\n# Iterate through each row of the column\nfor i in range(sa.shape[0]):\n    # Get the row\n    row = sa[i, Col]\n    # Calculate the length of the row\n    len_row = math.sqrt(np.dot(row, row))\n    # Normalize the row\n    sa[i, Col] = row / len_row\n",
        "\nbinary_matrix = np.where(a > 0, 1, 0)\n",
        "\nb = np.zeros(a.shape)\nb[a > 0] = 1\n",
        "\ndists = scipy.spatial.distance.pdist(data, centroids)\nresult = np.argmin(dists, axis=1)\n",
        "\nresult = np.apply_along_axis(lambda x: np.min(np.sqrt(np.sum((x - centroids)**2, axis=1)), axis=1), axis=1)\n",
        "\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nk = 3\nresult = np.empty((len(centroids),), dtype=np.intp)\nfor i, centroid in enumerate(centroids):\n    dists = np.linalg.norm(data - centroid, axis=1)\n    mask = dists >= np.partition(dists, k-1)[:k-1]\n    result[i] = mask.nonzero()[0]\nprint(result)\n",
        "\nfrom scipy.optimize import fsolve\n\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\n\ndef eqn(a, x, b):\n    return x + 2*a - b**2\n\nresult = np.empty((len(xdata), len(bdata)), dtype=float)\nfor i in range(len(xdata)):\n    for j in range(len(bdata)):\n        a0 = fsolve(eqn, 0.5, args=(xdata[i], bdata[j]))\n        result[i, j] = a0\n\nprint(result)\n",
        "\nresult = []\nfor i in range(len(xdata)):\n    b0 = adata[i]\n    roots = fsolve(lambda b: eqn(xdata[i], adata[i], b) , b0)\n    result.append([xdata[i], adata[i], roots])\n",
        "\nsample_data_range = np.linspace(range_start, range_end, 1000)\nsample_data_range_pdf = np.zeros_like(sample_data)\nsample_data_range_cdf = np.zeros_like(sample_data)\nfor i in range(len(sample_data)):\n    sample_data_range_pdf[i], _ = integrate.quad(lambda x: bekkers(x, estimated_a, estimated_m, estimated_d), range_start, sample_data[i])\n    sample_data_range_cdf[i] = sample_data_range_pdf[i]\nsample_data_range_cdf_pdf = sample_data_range_pdf/sample_data_range_pdf.sum()\n",
        "import numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\np = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\nreturn(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\nresult = sp.stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d), ddof=len(sample_data)-3)[1]\nprint(result)",
        "\nrolling_integral_df = df.groupby(pd.Grouper(freq='25S'))['A'].apply(lambda x: integrate.trapz(x, x.index))\n",
        "\nimport scipy.interpolate\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\neval = [(2.7, 2.3)]\nresult = scipy.interpolate.griddata(x, y, eval, fill_value='extrapolate')\n",
        "\nfrom scipy.optimize import minimize\ndef multinomial_loglikelihood(x):\n    n, k = x.shape\n    return -np.sum(np.log(x) * np.array(a.value_counts()).values)\n",
        "\nimport numpy as np\nimport scipy.optimize as sciopt\nx = np.array([[ 1247.04,  1274.9 ,  1277.81,  1259.51,  1246.06,  1230.2 ,\n     1207.37,  1192.  ,  1180.84,  1182.76,  1194.76,  1222.65],\n   [  589.  ,   581.29,   576.1 ,   570.28,   566.45,   575.99,\n      601.1 ,   620.6 ,   637.04,   631.68,   611.79,   599.19]])\ny = np.array([ 1872.81,  1875.41,  1871.43,  1865.94,  1854.8 ,  1839.2 ,\n    1827.82,  1831.73,  1846.68,  1856.56,  1861.02,  1867.15])\nfp   = lambda p, x: p[0]*x[0]+p[1]*x[1]\ne    = lambda p, x, y: ((fp(p,x)-y)**2).sum()\npmin = np.array([0.5,0.7]) # mimimum bounds\npmax = np.array([1.5,1.8]) # maximum bounds\nresult = sciopt.fminbound(e, pmin, pmax, args=(x,y))\n",
        "\nresult = []\nfor i in range(n, len(arr)):\n    if arr[i] >= max(arr[i-n:i+1]):\n        result.append(i)\n",
        "\nresult = []\nfor i in range(n, arr.shape[0] - n):\n    if all(arr[i][j] >= max(arr[i][j - n:j + n + 1]) for j in range(n, arr.shape[1] - n + 1)):\n        result.append([i - n, i + n])\n",
        "\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\ndf = pd.DataFrame({'NUM1': np.random.randn(50)*100,\n                   'NUM2': np.random.uniform(0,1,50),                   \n                   'NUM3': np.random.randint(100, size=50),                                             \n                   'CAT1': [\"\".join(np.random.choice(LETTERS,1)) for _ in range(50)],\n                   'CAT2': [\"\".join(np.random.choice(['pandas', 'r', 'julia', 'sas', 'stata', 'spss'],1)) for _ in range(50)],              \n                   'CAT3': [\"\".join(np.random.choice(['postgres', 'mysql', 'sqlite', 'oracle', 'sql server', 'db2'],1)) for _ in range(50)]\n                  })\noutliers = np.abs(stats.zscore(df[df.select_dtypes(include='number').columns])) > 3\ndf = df[~outliers.any(axis=1)]\nprint(df)\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names, index=data.target_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names, index=data.target_names)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndef solve(data):\n    X = data.data\n    y = data.target\n    target_names = np.unique(y).astype(str)\n    frame = pd.DataFrame(data=X, columns=data.feature_names)\n    frame['target'] = frame.index.map(target_names)\n    return frame\ndata1 = solve(data)\nprint(data1)\n",
        "\ndf_out = pd.get_dummies(df['Col3'], drop_first=True)\n",
        "import pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\ndf_out = pd.get_dummies(df['Col3'], drop_first=True)\ndf_out.columns = ['Apple', 'Orange', 'Banana', 'Grape']\ndf[['Col2']] = df_out.sum(axis=1).reset_index(drop=True)\ndf.drop(['Col3'], axis=1, inplace=True)\ndf_out.columns = ['Apple', 'Orange', 'Banana', 'Grape']\nprint(df)",
        "import pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\ndf_out = df.copy()\ndf_out['Apple'] = np.where(df['Col4'].str.contains('Apple'), 1, 0)\ndf_out['Banana'] = np.where(df['Col4'].str.contains('Banana'), 1, 0)\ndf_out['Grape'] = np.where(df['Col4'].str.contains('Grape'), 1, 0)\ndf_out['Orange'] = np.where(df['Col4'].str.contains('Orange'), 1, 0)\ndf_out['Suica'] = np.where(df['Col4'].str.contains('Suica'), 1, 0)\nprint(df_out)",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\ndf_out = df.copy()\ncol_to_split = df_out.columns[-1]\ndf_out[col_to_split] = df_out[col_to_split].apply(lambda x: pd.Series(x))\ndf_out[col_to_split.split('[')] = df_out[col_to_split.split('[')].apply(lambda x: x.str.get(0))\ndf_out[col_to_split.split(']')] = df_out[col_to_split.split(']')].apply(lambda x: x.str.get(1).str.split(','))\ndf_out = pd.concat([df_out] * len(df_out[col_to_split.split('[')].unique()), axis=1)\ndf_out.drop(col_to_split.split('['), axis=1, inplace=True)\ndf_out.drop(col_to_split.split(']'), axis=1, inplace=True)\ndf_out.columns = [f'{col_to_split}_{elem}' for elem in df_out[col_to_split.split('[')].unique()]\nprint(df_out)\n",
        "import pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\ndf_out = df.copy()\ndf_out = pd.get_dummies(df_out, columns=df_out.columns[-1])\nprint(df_out)",
        "import numpy as np\nimport pandas as pd\nimport sklearn.svm as suppmach\nX, y, x_test = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_test) == np.ndarray\nsvmmodel=suppmach.LinearSVC()\nproba = np.where(svmmodel.decision_function(X) >= 0, 1 / (1 + np.exp(-svmmodel.decision_function(X))), 1 - 1 / (1 + np.exp(-svmmodel.decision_function(X))))\nprint(proba)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm\nX, y, x_predict = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_predict) == np.ndarray\nmodel = svm.LinearSVC()\nproba = np.where(model.decision_function(x_predict) > 0, 1 / (1 + np.exp(-model.decision_function(x_predict))), 0)\n",
        "\ndf_merged = pd.concat([df_origin, pd.DataFrame(transform_output.toarray())], axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ndf = pd.concat([df_origin, pd.DataFrame(transform_output.toarray())], axis=1)\n",
        "import pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndf_origin, transform_output = load_data()\ndef solve(df, transform_output):\n    result = pd.concat([df, pd.DataFrame(transform_output.toarray())], axis=1)\n    return result\ndf = solve(df_origin, transform_output)\nprint(df)",
        "\nsteps = clf.named_steps()\nprint(steps)\n# Delete the 'poly' step\nclf.remove_step('poly')\n# Print the updated steps\nprint(clf.named_steps())\n",
        "\n# Delete the step 'dim_svm'\nclf.steps.pop('dim_svm')\n",
        "\nsteps = clf.named_steps()\ndel steps['pOly']\n",
        "\n# Insert a step 'feature_selection' with SelectKBest feature selection\n# before the 'svm' step\nfeature_selection = SelectKBest(k=1)\nclf.steps[-1] = ('feature_selection', feature_selection)\n",
        "\n# Insert a step at index 1\nclf.steps.insert(1, ('reduce_poly', PolynomialFeatures()))\n",
        "\nclf.steps[1:2] = ['t1919810', PCA()]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\ngridsearch, testX, testY, trainX, trainY = load_data()\nassert type(gridsearch) == sklearn.model_selection._search.GridSearchCV\nassert type(trainX) == list\nassert type(trainY) == list\nassert type(testX) == list\nassert type(testY) == list\n# [Missing Code]\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.1, 0.3, 0.5],\n    'max_depth': [3, 5, 7],\n    'n_estimators': [50, 100, 200],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'objective': ['reg:squarederror']\n}\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params=fit_params)\ngridsearch.fit(trainX,trainY)\nb = gridsearch.score(trainX, trainY)\nc = gridsearch.predict(trainX)\nprint(b)\nprint(c)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\ngridsearch, testX, testY, trainX, trainY = load_data()\nassert type(gridsearch) == sklearn.model_selection._search.GridSearchCV\nassert type(trainX) == list\nassert type(trainY) == list\nassert type(testX) == list\nassert type(testY) == list\n# Define the XGBoost model\nmodel = xgb.XGBRegressor()\n\n# Define the parameter grid for GridSearchCV\nparamGrid = {\n    'learning_rate': [0.1, 0.3, 0.5],\n    'max_depth': [3, 5, 7],\n    'n_estimators': [50, 100, 200]\n}\n\n# Add early stopping parameters to fit_params\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n\n# Perform GridSearchCV with early stopping parameters\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params=fit_params)\ngridsearch.fit(trainX, trainY)\n\nb = gridsearch.score(trainX, trainY)\nc = gridsearch.predict(trainX)\nprint(b)\nprint(c)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba_test = logreg.predict_proba(X_test)\n    proba.append(proba_test)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba_test = logreg.predict_proba(X_test)\n    proba.append(proba_test)\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    model = LinearRegression()\n    model.fit(scaled[:, :-1], scaled[:, -1])\n    # [Missing Code]\n    t_pred = model.predict(scaled)\n",
        "\ndf = pd.DataFrame({'Model': [model_name], 'Mean Score': [scores.mean()]})\n",
        "\nscores = cross_val_score(model, X, y, cv=5)\n# [Missing Code]\nprint(f'Name Model: {model_name}, Mean Score: {scores.mean()}')\n",
        "\ndf = pd.DataFrame({'Model': [model_name], \n 'Mean score': [scores.mean()]})\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\nintermediate_result = tf_idf_out\n",
        "\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nclf = GridSearchCV(bc, param_grid, cv=5)\n",
        "\nregressor.fit(X[:,None], y)\n",
        "\nregressor.fit(X[:, np.newaxis], y)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef preprocess(s):\n    return s.upper()\ntfidf = TfidfVectorizer(preprocessor=preprocess)\nprint(tfidf.preprocessor)\n",
        "\ntfidf = TfidfVectorizer(preprocessor=prePro)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\ndf_out = pd.DataFrame(preprocessing.scale(data))\ndf_out.columns = data.columns\ndf_out.index = data.index\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\ndf_out = pd.DataFrame(preprocessing.scale(data))\ndf_out.columns = data.columns\ndf_out.index = data.index\nprint(df_out)\n",
        "\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ncoef = grid.best_estimator_.get_params()['model__coef_']\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n# [Missing Code]\ncolumn_indices = np.argsort(X_new.toarray())[::-1]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = model.get_support()\n",
        "\nselected_columns = model.get_support()\n",
        "\nselected_columns = []\nfor i, import_ance in enumerate(model.scores_):\n    if import_ance > 0.5:\n        selected_columns.append(X.columns[i])\n",
        "\nclosest_50_samples = km.predict(X)[:50]\n",
        "\nclosest_50_samples = km.predict(X[:50])\n",
        "\nclosest_100_samples = []\ndistances = km.inertia_\nfor i in range(p):\n    closest_samples = np.argsort(distances[i])[:100]\n    closest_100_samples.extend(X[closest_samples])\n",
        "\ndef get_samples(p, X, km):\n    samples = []\n    for i in range(km.cluster_centers_.shape[0]):\n        distances = np.linalg.norm(X - km.cluster_centers_, axis=1)\n        if i == p:\n            index = np.argsort(distances)[-50:]\n        else:\n            index = np.argsort(distances)\n        samples.extend(X[index])\n    return samples\nclosest_50_samples = get_samples(p, X, km)\nprint(closest_50_samples)\n",
        "import pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# load data in the example\nX_train, y_train = load_data()\nX_train[0] = ['a'] * 40 + ['b'] * 40\n\n# create dummy variable\nX_train_dummy = pd.get_dummies(X_train)\n\n# merge original data with dummy variable\nX_train_merged = pd.concat([X_train[1:], X_train_dummy], axis=1)\n\n# insert fake categorical variable\nX_train_merged[0] = ['a'] * 40 + ['b'] * 40\n\n# model\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train_merged, y_train)",
        "\n# clf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train_merged, y_train)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_approximation import GaussianProcessRegressor\n\n# Use default arguments\nsvr = SVR()\ngpr = GaussianProcessRegressor()\n\n# fit, then predict X\nsvr.fit(X, y)\ngpr.fit(X, y)\n\npredict_svr = svr.predict(X)\npredict_gpr = gpr.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\nsvr = SVR(kernel='poly', degree=2, coef0=0.0, tol=0.001, caching=True, epsilon=0.1, gamma='auto')\nsvr.fit(X_train, y_train)\n\npredict = svr.predict(X_test)\nmse = mean_squared_error(y_test, predict)\n",
        "\nfrom sklearn.svm import SVR\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\n# cosine_similarities_of_queries = cosine_similarities_of_queries.tolist()\n",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ntfidf = TfidfVectorizer()\ntfidf.fit_transform(documents)\ncosine_similarities_of_queries = np.array([])\nfor query in queries:\n    query_tfidf = tfidf.transform([query])\n    cosine_similarity = np.dot(query_tfidf, tfidf.transform(documents)).toarray()[0][0]\n    cosine_similarities_of_queries = np.vstack([cosine_similarities_of_queries, cosine_similarity])\ncosine_similarities_of_queries = np.delete(cosine_similarities_of_queries, 0, axis=0)\nprint(cosine_similarities_of_queries)\n```",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    cosine_similarities = []\n    for query in queries:\n        query_tfidf = tfidf.transform([query])\n        cosine_similarity = np.dot(query_tfidf, tfidf.transform(documents)).toarray()[0][0]\n        cosine_similarities.append(cosine_similarity)\n    return cosine_similarities\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)",
        "import numpy as np\nfeatures = load_data()\nnew_features = np.array([[1 if f in feature else 0 for f in features[0]] for feature in features[1:] for f in feature])\nnew_features = np.column_stack([new_features] * len(features[0]))\nprint(new_features)",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nf = load_data()\nnew_f = np.zeros((len(f), len(set(f[0]))))\nfor i, row in enumerate(f):\n    for j, val in enumerate(row):\n        new_f[i, j] = 1\nprint(new_f)\n",
        "\n# features = pd.DataFrame(features)\n# new_features = features.values\n",
        "\n    feature_names = [f for feature in features for f in feature]\n    num_samples = len(features)\n    num_features = len(feature_names)\n    # [Missing Code]\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\nnew_features = np.zeros((len(features), max(len(feat) for feat in features)))\nfor i, feat in enumerate(features):\n    for j, f in enumerate(feat):\n        new_features[i, j] = 1\nprint(new_features)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.cluster\ndata_matrix = load_data()\n# Initialize the linkage matrix with the distance matrix\nlinkage_matrix = np.array(data_matrix)\n# Perform hierarchical clustering using AgglomerativeClustering\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2, linkage='ward').fit_predict(linkage_matrix)\n# Convert the linkage matrix to a dendrogram\ndendrogram = sklearn.cluster.Dendrogram(linkage_matrix, labels=cluster_labels)\n# Plot the dendrogram\nplt.figure(figsize=(8, 6))\nplt.title(\"Hierarchical Clustering Dendrogram\")\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Cluster distance\")\nplt.plot(dendrogram.distance, dendrogram.labels, 'ob')\nplt.show()\n",
        "\n# Create an instance of AgglomerativeClustering with the 'ward' linkage method\n# and a distance metric of 'euclidean'\nclustering = sklearn.cluster.AgglomerativeClustering(n_clusters=2, linkage='ward', distance_metric='euclidean')\n# Fit the clustering to the data_matrix\nclustering.fit(data_matrix)\n# Get the cluster labels\ncluster_labels = clustering.labels_\n",
        "\n# Create a similarity matrix from the given distance matrix\nsimilarity_matrix = -1 * simM.astype('float32')\n",
        "\n# Perform hierarchical clustering on the data matrix\n# using scipy.cluster.hierarchy.dendrogram\nwcss = scipy.cluster.hierarchy.linkage(data_matrix, method='ward')\n# Use the WCSS linkage matrix to create a dendrogram\ndendrogram = scipy.cluster.hierarchy.dendrogram(wcss, labels=None, orientation='top')\n# Find the optimal number of clusters using the method of David B. Dunlap and Paul J. Katz\noptimal_num_clusters = scipy.cluster.hierarchy.optimal_leaf_order(dendrogram)\n# Use the optimal number of clusters to cut the dendrogram\ncluster_labels = scipy.cluster.hierarchy.leaves_list(dendrogram, optimize_order=True, p=optimal_num_clusters)\n",
        "\n# Hierarchical clustering with 2 clusters\nZ = linkage(data_matrix, 'ward')\n# Get the cluster labels\ncluster_labels = np.argmin(Z, axis=1)\n",
        "\n# Perform hierarchical clustering on the similarity matrix\nZ = linkage(simM, 'ward')\n# Get the cluster labels\ncluster_labels = pd.Series(Z[:, -1])\n",
        "\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# [Missing Code]\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\nbox_cox_data = sklearn.preprocessing.BoxCox1D().fit_transform(data)",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\nbox_cox_data = sklearn.preprocessing.BoxCox1D().fit_transform(data)\n",
        "import numpy as np\nimport pandas as pd\nimport sklearn\ndata = load_data()\nassert type(data) == np.ndarray\nyeo_johnson_data = sklearn.preprocessing.PowerTransformer(method='yeojohnson').fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nyeo_johnson_data = scaler.fit_transform(data)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ntext = load_data()\n# Use the tokenizer parameter to preserve punctuation marks\nvectorizer = CountVectorizer(tokenizer=lambda x: x)\ntransformed_text = vectorizer.fit_transform(text)\n# Print the transformed text\nprint(transformed_text)\n",
        "\ntrain_size = int(0.8 * len(dataset))\nx_train, y_train, x_test, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=train_size, random_state=42)\n",
        "\nimport numpy as np\nimport pandas as pd\ndata = load_data()\n# Split the dataframe into training and testing sets\ntrain_test_split = np.random.rand(data.shape[0]) < 0.8\ntrain_set = data[train_test_split]\ntest_set = data[~train_test_split]\n# Split each set into x and y\nx_train, y_train = train_set.drop(columns=['target']), train_set['target']\nx_test, y_test = test_set.drop(columns=['target']), test_set['target']\n# print the results\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\ntrain_size = int(0.6 * len(dataset))\ntest_size = len(dataset) - train_size\nindex = np.random.permutation(len(dataset))\ntrain_index = index[:train_size]\ntest_index = index[train_size:]\nx_train = dataset.iloc[train_index][:-1]\ny_train = dataset.iloc[train_index][-1]\nx_test = dataset.iloc[test_index][:-1]\ny_test = dataset.iloc[test_index][-1]\n",
        "\n    train_size = int(0.8 * len(data))\n    # Split the dataframe into training and testing sets\n    x_train, x_test = data.iloc[:train_size, :-1], data.iloc[train_size:, :-1]\n    y_train, y_test = x_train.pop(-1), x_test.pop(-1)\n",
        "\nX = X.reshape(-1, 1)\n",
        "\nfrom sklearn.cluster import KMeans\ndf = load_data()\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\n",
        "\nfeatureSelector = sklearn.feature_selection.SelectKBest(score_func=LinearSVC().loss, k=1)\nfeatureSelector.fit(X, y)\nselected_feature_indices = featureSelector.get_support()\nselected_feature_names = vectorizer.get_feature_names()[selected_feature_indices]\n",
        "\nfeature_selector = SelectKBest(f_classif, k=1000)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[feature_selector.get_support()]\n",
        "import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ncorpus, y = load_data()\nassert type(corpus) == list\nassert type(y) == list\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\ndef solve(corpus, y, vectorizer, X):\n    clf = LinearSVC(penalty='l1')\n    clf.fit(X, y)\n    coefficients = np.abs(clf.coef_)\n    indices = np.argpartition(coefficients, -10)[-10:]\n    selected_feature_names = vectorizer.get_feature_names()[indices]\n    return selected_feature_names\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)",
        "\nfeature_names = feature_names[vectorizer.vocabulary_.argsort()]\n",
        "\nvectorizer.vocabulary = vectorizer.vocabulary.order()\n",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n'We are looking for Java developer',\n'Frontend developer with knowledge in SQL and Jscript',\n'And this is the third one.',\n'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = np.array([[1 if feature_names.index(word) in feature_names else 0 for word in document.split()] for document in corpus])\nprint(feature_names)\nprint(X)",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n'We are looking for Java developer',\n'Frontend developer with knowledge in SQL and Jscript',\n'And this is the third one.',\n'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = np.array([[1 if feature in vectorizer.get_feature_names() else 0 for feature in vectorizer.get_feature_names()] for vector in X])\nprint(feature_names)\nprint(X)",
        "\nfor col in df1.columns[2:]:\n    df2 = df1[~np.isnan(df1[col])] \n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) \n    m = slope.coef_[0]\n    series= np.concatenate((series, m), axis = 0)\n",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nseries = np.array([])\nfor col in df1.columns:\n    if col[0] >= 'A' and col[0] <= 'Z':\n        df2 = df1[~np.isnan(df1[col])]\n        df3 = df2[['Time', col]]\n        npMatrix = np.matrix(df3)\n        X, Y = npMatrix[:,0], npMatrix[:,1]\n        slope = LinearRegression().fit(X,Y)\n        m = slope.coef_[0]\n        series= np.concatenate((series, m), axis = 0)\nslopes = series.tolist()\nprint(slopes)",
        "\n# df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "def Transform(df):\n    transformed_df = df.copy()\n    le = LabelEncoder()\n    transformed_df['Sex'] = le.fit_transform(transformed_df['Sex'])\n    return transformed_df\ntransformed_df = Transform(df)",
        "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n",
        "\ntransformed = MinMaxScaler().fit_transform(np_array)\n",
        "\ntransformed = MinMaxScaler().fit_transform(np_array)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nnp_array = load_data()\ndef Transform(a):\n    new_a = np.array(a).reshape(-1,1)\n    scaler = MinMaxScaler()\n    scaler.fit(new_a)\n    return scaler.transform(new_a)\ntransformed = Transform(np_array)\nprint(transformed)\n",
        "from sklearn import tree\nimport pandas as pd\nimport pandas_datareader as web\nimport numpy as np\n\ndf = web.DataReader('goog', 'yahoo', start='2012-5-1', end='2016-5-20')\n\ndf['B/S'] = (df['Close'].diff() < 0).astype(int)\n\nclosing = (df.loc['2013-02-15':'2016-05-21'])\nma_50 = (df.loc['2013-02-15':'2016-05-21'])\nma_100 = (df.loc['2013-02-15':'2016-05-21'])\nma_200 = (df.loc['2013-02-15':'2016-05-21'])\nbuy_sell = (df.loc['2013-02-15':'2016-05-21'])  # Fixed\n\nclose = pd.DataFrame(closing)\nma50 = pd.DataFrame(ma_50)\nma100 = pd.DataFrame(ma_100)\nma200 = pd.DataFrame(ma_200)\nbuy_sell = pd.DataFrame(buy_sell)\n\nclf = tree.DecisionTreeRegressor()\nx = np.concatenate([close, ma50, ma100, ma200], axis=1)\ny = buy_sell\n\nclf.fit(x, y)\n\npredict = clf.predict([close[:-1], ma_50[:-1], ma_100[:-1], ma_200[:-1]])\n\n",
        "\nle = LabelEncoder()\nnew_X = np.array([[le.fit_transform(row[0]), row[1]] for row in X])\n",
        "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['asdf', '1'], ['asdf', '0']]\nclf = DecisionTreeClassifier()\n\nnew_X = np.array([[0, 1], [0, 0]])\nclf.fit(new_X, ['2', '3'])",
        "\nnew_y = np.array(['4', '5'])",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\n# dataframe = df\n# Git rid of the name of the animal\n# And change the hunter/scavenger to 0/1\ndataframe = dataframe.drop([\"Name\"], axis=1)\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n# Creating an instance of Logistic Regression\nlogReg = LogisticRegression()\n# Fitting the data to the model\nlogReg.fit(X[:-1], y)\n# Creating an array of independent variables\nX_new = np.array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\ndataframe = dataframe.drop([\"Name\"], axis=1)\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:-1,-1]\nlogReg = LogisticRegression()\nlogReg.fit(X[:None], y)\npredict = logReg.predict(X)\nprint(predict)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n\n# Split data into train and test sets, ensuring that test set is newer than train set\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, test_size=np.int(features_dataframe.shape[0]*0.8), random_state=42)\ntrain_dataframe = train_dataframe.sort_values(by=[\"date\"])\ntest_dataframe = test_dataframe.sort_values(by=[\"date\"])\n\n# Print train and test dataframes\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\n\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, test_size=1 - train_size)\n\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n\n# Split the data into train and test sets such that the test set is older than the train set\ntrain_cutoff_date = int(train_dataframe.shape[0] * 0.8)\ntest_cutoff_date = int(train_dataframe.shape[0] * 0.2)\n\ntrain_dataframe = train_dataframe.iloc[:train_cutoff_date]\ntest_dataframe = test_dataframe.iloc[train_cutoff_date:]\n\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfeatures_dataframe = load_data()\ndef solve(features_dataframe):\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort(\"date\")\n    test_dataframe = test_dataframe.sort(\"date\")\n    return train_dataframe, test_dataframe\ntrain_dataframe, test_dataframe = solve(features_dataframe)\nprint(train_dataframe)\nprint(test_dataframe)",
        "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndf = pd.DataFrame({\n    'Month': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],\n    'X1': [12, 10, 100, 55, 65, 60, 35, 25, 10, 15, 30, 40, 50],\n    'X2': [10, 15, 24, 32, 8, 6, 10, 23, 24, 56, 45, 10, 56],\n    'X3': [12, 90, 20, 40, 10, 15, 30, 40, 60, 42, 2, 4, 10]\n})\nscaler = MinMaxScaler()\ncols = ['X2', 'X3']\ndf[cols + '_scale'] = df.groupby('Month').apply(lambda x: scaler.fit_transform(x[cols]))\nprint(df)",
        "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nmyData = pd.DataFrame({\n    'Month': [3, 3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8],\n    'A1': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],\n    'A2': [31, 13, 13, 13, 33, 33, 81, 38, 18, 38, 18, 18, 118],\n    'A3': [81, 38, 18, 38, 18, 18, 118, 31, 13, 13, 13, 33, 33],\n    'A4': [1, 1, 1, 1, 1, 1, 8, 8, 8, 8, 8, 8, 8],\n})\nscaler = MinMaxScaler()\ncols = ['A2', 'A3']\nmyData[['new_A2', 'new_A3']] = myData.groupby('Month').apply(lambda x: scaler.fit_transform(x[cols]))\nprint(myData)",
        "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nwords = load_data()\nfeature_names = []\nfor i, word in enumerate(words):\n feature_names.extend(word.split())\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform(feature_names)\nprint(count.get_feature_names())",
        "import re\n\nfeature_names = []\nfor token, frequency in count.get_feature_names_out():\n    if token.startswith('@') or token in ['#', ',']:\n        feature_names.append(token)\n    else:\n        feature_names.extend(re.split('[.,]', token))\nprint(feature_names)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\n\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results['rank'] = np.arange(1, len(full_results)+1) + 1\nfull_results = full_results[['params', 'mean_test_score', 'rank']]\nprint(full_results)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\n\nfull_results = pd.DataFrame()\n\nfor model_name, model_results in GridSearch_fitted.cv_results_.items():\n    for i, (params, mean_fit_time, mean_score, fit_time, score) in enumerate(zip(model_results['params'], model_results['mean_fit_time'], model_results['mean_score'], model_results['fit_time'], model_results['score'])):\n        row_data = locals()\n        row_data.pop('model_name')\n        row_data.pop('i')\n        row_data.pop('row_data')\n        full_results = full_results.append(pd.DataFrame(row_data))\n\nfull_results = full_results.sort_values(by=['mean_fit_time'])\nprint(full_results)\n",
        "import pickle\n\nwith open('sklearn_model', 'wb') as f:\n    pickle.dump(fitted_model, f)",
        "import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndf = load_data()\ntfidf = TfidfVectorizer()\ncosine_similarity_matrix = cosine_similarity(tfidf.fit_transform(df['description']))\nprint(cosine_similarity_matrix)\n\nNote: The `cosine_similarity` function from `sklearn.metrics.pairwise` is used to calculate the cosine similarity between the transformed vectors."
    ],
    "Pytorch": [
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = load_data()\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.001\n",
        "\ndef update_learning_rate(optimizer, new_lr):\n    for group in optimizer.param_groups:\n        group['lr'] = new_lr\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = load_data()\nfor group in optim.state.groups():\n    for key, value in group.items():\n        if key == 'params':\n            for param_group in optim.param_groups:\n                param_group['lr'] = 0.0005\n",
        "\ndef update_learning_rate(optimizer, new_lr):\n    for group in optimizer.param_groups:\n        group['lr'] = new_lr\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\ninput_Tensor = load_data()\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\nembedded_input = torch.tensor([word2vec.wv.vector_for_word(w) for w in input_Tensor])\nprint(embedded_input)\n",
        "import numpy as np\nimport pandas as pd\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\ninput_Tensor = load_data()\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\ndef get_embedded_input(input_Tensor):\n    embedded_input = np.array([word2vec.wv[str(i)] for i in input_Tensor])\n    return torch.from_numpy(embedded_input)\nembedded_input = get_embedded_input(input_Tensor)\nprint(embedded_input)",
        "import numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x)\npx = px.apply(lambda x: x.numpy())\nprint(px)",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x)\npx[0:4,0:4] = np.array(px[0:4,0:4]).flatten()\nprint(px)\n",
        "\npx[0] = np.asscalar(px[0].detach().numpy())\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\nC = B[:, A_log.long()]\nprint(C)\n",
        "\n# [Missing Code]\nC = torch.LongTensor(C)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\nC = B[:, A_log.long()] # convert the byte tensor to long tensor\nprint(C)\n",
        "\nprint(C)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    C = B[:, A_log]\n    return C\nC = solve(A_log, B)\nprint(C)\n",
        "\nprint(C)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nidx, B = load_data()\nC = torch.index_select(B, dim=0, index=idx)\nprint(C)\n",
        "\n# x_tensor = x_tensor.type(torch.float16)\n",
        "\n# x_tensor = torch.from_numpy(np.array(x_array))\n",
        "\nimport pandas as pd\nimport torch\nimport numpy as np\nx_array = load_data()\ndef Convert(a):\n    t = torch.from_numpy(np.array(a))\n    return t\nx_tensor = Convert(x_array)\nprint(x_tensor)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\nmask = torch.zeros(len(lens), max(lens))\n# use numpy to fill in the mask with 1's up to the length of each sentence\nmask = torch.BoolTensor(np.repeat(np.arange(max(lens)+1).reshape(-1,1), lens.reshape(-1,1)).astype(np.bool))\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\nmask = torch.zeros(len(lens), max(lens))\n# use numpy to fill the mask with 1's up to the length of each sentence\nmask = torch.tensor(np.concatenate([np.repeat(np.arange(l), l) for l in lens]))\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\nmask = torch.zeros(len(lens), max(lens)).long()\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\ndef get_mask(lens):\n    mask = torch.zeros(len(lens), max(lens))\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\nmask = get_mask(lens)\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nTensor_2D = load_data()\nTensor_3D = torch.reshape(Tensor_2D, (Tensor_2D.shape[0], 1, -1))\nprint(Tensor_3D)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nTensor_2D = load_data()\ndef Convert(t):\n    result = torch.zeros(t.size(0), t.size(1), t.size(1))\n    result[:,:,0] = t\n    return result\nTensor_3D = Convert(Tensor_2D)\nprint(Tensor_3D)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nab = torch.stack((a, b), dim=0)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nab = torch.stack((a,b),0)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\ndef solve(a, b):\n    ab = torch.stack((a,b),0)\n    return ab\nab = solve(a, b)\nprint(ab)\n",
        "\na[:, lengths:, :] = 0\n",
        "\na[:, lengths:, :] = 2333\n",
        "\na[:, :lengths, :] = 0\n",
        "\na[:, :lengths, :] = 2333\n",
        "\n# tensor_of_tensors = torch.tensor(list_of_tensors)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist = load_data()\nnew_tensors = torch.stack(list)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\ndef Convert(lt):\n    tt = torch.stack(lt)\n    return tt\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = t.clone() # create a new tensor to store the result\nfor i in range(t.size(0)):\n    result[i,:] = t[i,:] * idx\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\nresult = t[idx[0], idx[1]]\nresult = t[idx[2]]\nprint(result)\n",
        "\n# result = x.gather(1,ids)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nids, x = load_data()\nresult = x.gather(1, torch.argmax(ids, dim=1, keepdim=True).unsqueeze(-1))\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nids, x = load_data()\nresult = np.zeros((70,2))\nfor i in range(70):\n    if ids[i,0] == 1:\n        result[i,0] = x[i,0,0]\n        result[i,1] = x[i,0,1]\n    else:\n        result[i,0] = x[i,1,0]\n        result[i,1] = x[i,1,1]\nresult = torch.from_numpy(result)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ny = torch.argmax(softmax_output, dim=1)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ny = torch.argmax(softmax_output, dim=1)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ny = torch.argmin(softmax_output, dim=1)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ndef solve(softmax_output):\n    y_pred = torch.argmax(softmax_output, dim=1)\n    return y_pred.unsqueeze(1)\ny = solve(softmax_output)\nprint(y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ndef solve(softmax_output):\n    y_prob = torch.argmin(softmax_output, dim=1)\n    y = torch.tensor(y_prob.numpy(), dtype=torch.long)\n    return y\ny = solve(softmax_output)\nprint(y)\n",
        "import numpy as np\nimport pandas as pd\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn.functional as F\n\nimages, labels = load_data()\n\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    n, c, w, z = input.size()\n    log_p = F.log_softmax(input, dim=1)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)\n    log_p = log_p[target.view(n, w, z, 1).repeat(0, 0, 0, c) >= 0]\n    log_p = log_p.view(-1)\n    target = target.view(n, w, z)\n    target = target.float()\n    target = target.cuda()\n    target = target.unsqueeze(1)\n    target = target.expand(n, c, w, z)\n    target = target.float()\n    target = target.cuda()\n    target = target.unsqueeze(2)\n    target = target.expand(n, c, w, z)\n    target = target.float()\n    loss = F.nll_loss(input, target, weight=weight, reduction='sum')\n    return loss\n\nloss = cross_entropy2d(images, labels)\nprint(loss)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_equal = np.sum(np.array(A) == np.array(B))\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_equal = np.sum(np.equal(A, B))\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_not_equal = (torch.tensor(A) != torch.tensor(B)).sum().item()\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ndef Count(A, B):\n    cnt_equal = 0\n    for i in range(len(A)):\n        if A[i] == B[i]:\n            cnt_equal += 1\n    return cnt_equal\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\n_, x = A.shape\n_, _ = B.shape\ncnt_equal = np.sum((A[-x:] == B[-x:]).all(axis=1))\nprint(cnt_equal)\n",
        "\ncnt_not_equal = (A[-x:] != B[-x:]).sum()\n",
        "\nstep = a.size(3) // chunk_dim\nfor i in range(a.size(3)):\n    slice_start = i * chunk_dim\n    slice_end = slice_start + chunk_dim\n    if i * chunk_dim + chunk_dim > a.size(3):\n        slice_end = a.size(3)\n    tensor = a[:, :, :, slice_start:slice_end]\n    tensors_31.append(tensor)\n",
        "\nfor i in range(0, 40, chunk_dim):\n    tensor = a[:, :, i:i+chunk_dim, :, :]\n    tensors_31.append(tensor)\n",
        "\noutput[mask==1] = clean_input_spectrogram[mask==1]\n",
        "\noutput[mask==0] = clean_input_spectrogram[mask==0]\n",
        "import numpy as np\nimport pandas as pd\nimport torch\nx, y = load_data()\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(min == torch.abs(x), torch.abs(x), torch.abs(y)) * sign_x * sign_y\nprint(signed_min)",
        "\nselected_x = torch.where(signed_max_x > 0, signed_max_x, torch.zeros_like(signed_max_x))\nselected_y = torch.where(signed_max_y > 0, signed_max_y, torch.zeros_like(signed_max_y))\n",
        "import numpy as np\nimport pandas as pd\nimport torch\nx, y = load_data()\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min = torch.min(torch.abs(x), torch.abs(y))\n    index = torch.argmin(torch.abs(x), dim=0)\n    signed_min_x = torch.zeros_like(x)\n    signed_min_y = torch.zeros_like(y)\n    sign_x[index] = torch.sign(x[index])\n    sign_y[index] = torch.sign(y[index])\n    signed_min_x[index] = torch.abs(x[index]) * sign_x[index]\n    signed_min_y[index] = torch.abs(y[index]) * sign_y[index]\n    return signed_min_x, signed_min_y\nsigned_min = solve(x, y)\nprint(signed_min)",
        "import numpy as np\nimport pandas as pd\nimport torch\nMyNet = torch.nn.Sequential(torch.nn.Linear(4, 15),\n                            torch.nn.Sigmoid(),\n                            torch.nn.Linear(15, 3),\n                            )\nMyNet.load_state_dict(torch.load(\"my_model.pt\"))\ninput = load_data()\nassert type(input) == torch.Tensor\nconfidence_score = np.exp(output) / np.sum(np.exp(output))\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nresult = torch.cat((a[:, :2], b[:, -2:]), dim=1)\nresult[-2, -1] = (a[-1, 2] + b[0, 0]) / 2\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\ndef solve(a, b):\n    result = torch.cat((a[:, :2], b[:, -2:]), dim=1)\n    middle_col = (a[:, -1] + b[:, 0]) / 2\n    result[:, 2] = middle_col\n    return result\nresult = solve(a, b)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[0., 0., 0.,0.]])\nr = torch.stack([t,new])\nresult = r.unsqueeze(0)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.tensor([[1, 2],\n [3, 4]])\nnew = torch.tensor([[0, 0, 0, 0]])\nresult = torch.stack([t, new])\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(8).reshape(1,4,2).float()\nnew=torch.tensor([[-1, -1, -1, -1,]])\nr = torch.stack([t,new], dim=0)\nprint(r)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nhid_dim = 32\ndata = torch.randn(10, 2, 3, hid_dim)\ndata = data.view(10, 2 * 3, hid_dim)\nW = torch.randn(hid_dim)\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n"
    ]
}