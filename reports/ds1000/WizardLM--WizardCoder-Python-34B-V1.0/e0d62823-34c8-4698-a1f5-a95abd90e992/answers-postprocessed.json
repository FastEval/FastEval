{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\ndf_shuffled = df.iloc[List]\nresult = len(df_shuffled[df_shuffled['Type'] != df['Type']])\n",
        "\n# Replace values in Qu1, Qu2, Qu3 columns according to value_counts() >= 2\nfor col in df.columns:\n    counts = df[col].value_counts()\n    df[col] = df[col].apply(lambda x: x if counts[x] >= 2 else 'other')\n",
        "\n# Replace values in columns Qu1,Qu2,Qu3 according to value_counts() when value count great or equal 3\nfor col in df.columns:\n    counts = df[col].value_counts()\n    others = counts[counts < 3].index\n    df[col] = df[col].replace(others, 'other')\n",
        "\n    for col in df.columns:\n        value_counts = df[col].value_counts()\n        for value, count in value_counts.items():\n            if count >= 2:\n                df.loc[df[col] == value, col] = value\n            else:\n                df.loc[df[col] == value, col] = 'other'\n    ",
        "\n# Replace values in Qu1 column\nqu1_value_counts = pd.value_counts(df.Qu1)\nqu1_others = qu1_value_counts[qu1_value_counts < 3].index\ndf.Qu1 = df.Qu1.replace(qu1_others, 'other')\n# Replace values in Qu2 and Qu3 columns\nfor col in ['Qu2', 'Qu3']:\n    col_value_counts = pd.value_counts(df[col])\n    col_others = col_value_counts[col_value_counts < 2].index\n    df[col] = df[col].replace(col_others, 'other')\nresult = df\n",
        "\n# Replace values in Qu1 column\nvalue_counts = pd.value_counts(df.Qu1)\nfor value, count in value_counts.items():\n    if count >= 3:\n        df.loc[df.Qu1 != value, 'Qu1'] = 'other'\n# Replace values in Qu2 and Qu3 columns\nfor col in ['Qu2', 'Qu3']:\n    value_counts = pd.value_counts(df[col])\n    for value, count in value_counts.items():\n        if count >= 2:\n            df.loc[df[col] != value, col] = 'other'\n# Reserve 'apple' values in Qu1 column\ndf.loc[df.Qu1 == 'other', 'Qu1'] = 'apple'\nresult = df\n",
        "\ndf = df.sort_values(by='keep_if_dup', ascending=False)\ndf = df.drop_duplicates(subset='url', keep='first')\nresult = df.sort_values(by='id').reset_index(drop=True)\n",
        "\ndf = df.sort_values(by='drop_if_dup', ascending=False)\ndf = df.drop_duplicates(subset='url', keep='first')\nresult = df.sort_values(by='id').reset_index(drop=True)\n",
        "\ndf = df.sort_values(by='keep_if_dup', ascending=False)\ndf = df.drop_duplicates(subset='url', keep='first')\nresult = df.sort_values(by='id').reset_index(drop=True)\n",
        "\nresult = {}\nfor index, row in df.iterrows():\n    if row['name'] not in result:\n        result[row['name']] = {}\n    if row['v1'] not in result[row['name']]:\n        result[row['name']][row['v1']] = {}\n    result[row['name']][row['v1']][row['v2']] = row['v3']\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    ",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf = df.sort_values(by='datetime')\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\nimport pandas as pd\nimport re\ndef extract_key_value_pairs(message):\n    key_value_pairs = re.findall(r'\\b\\w+: \\w+', message)\n    key_value_dict = {}\n    for pair in key_value_pairs:\n        key, value = pair.split(': ')\n        key_value_dict[key] = value\n    return key_value_dict\ndef expand_dataframe(df):\n    key_value_dicts = df['message'].apply(extract_key_value_pairs)\n    expanded_df = pd.DataFrame(key_value_dicts.tolist())\n    expanded_df = expanded_df.fillna('none')\n    result = pd.concat([df, expanded_df], axis=1)\n    result = result.drop(columns=['message'])\n    return result\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\nresult = expand_dataframe(df)\nprint(result)\n",
        "\nmultiplier = 10\ndf.loc[df['product'].isin(products), 'score'] *= multiplier\n",
        "\nmask = df['product'].isin(products)\ndf.loc[~mask, 'score'] = df.loc[~mask, 'score'] * 10\n",
        "\nfor product_range in products:\n    df.loc[(df['product'] >= product_range[0]) & (df['product'] <= product_range[1]), 'score'] *= 10\n",
        "\n# Min-Max Normalization\nmin_score = df[df['product'].isin(products)]['score'].min()\nmax_score = df[df['product'].isin(products)]['score'].max()\ndf.loc[df['product'].isin(products), 'score'] = (df[df['product'].isin(products)]['score'] - min_score) / (max_score - min_score)\n",
        "\ndf['category'] = df.apply(lambda row: row.index[row == 1].tolist()[0], axis=1)\n",
        "\ndf['category'] = df.apply(lambda row: row.index[row == 0].tolist()[0], axis=1)\n",
        "\ndef binary_to_categorical(row):\n    categories = []\n    for col in row.index:\n        if row[col] == 1:\n            categories.append(col)\n    return categories\ndf['category'] = df.apply(binary_to_categorical, axis=1)\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\nstart_date = pd.to_datetime(List[0])\nend_date = pd.to_datetime(List[1])\nresult = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y %A')\n",
        "\ndf.iloc[0, 0] = df.iloc[-1, 0]\ndf.iloc[-1, 0] = 11.6985\n",
        "\n# Shift the last row of the first column up 1 row\ndf.iloc[-1, 0] = df.iloc[-2, 0]\n# Shift the first row of the first column to the last row, first column\ndf.iloc[0, 0] = df.iloc[-1, 0]\n",
        "\ndf.iloc[0, 0] = df.iloc[-1, 0]\ndf.iloc[-1, 0] = df.iloc[0, 0]\ndf.iloc[0, 1] = df.iloc[-1, 1]\ndf.iloc[-1, 1] = df.iloc[0, 1]\n",
        "\nimport numpy as np\n# Shift the first row of the first column down 1 row\ndf.iloc[0, 0] = df.iloc[1, 0]\n# Shift the last row of the first column to the first row, first column\ndf.iloc[0, 0] = df.iloc[-1, 0]\n# Calculate the R^2 values for the first and second columns\nr2_1 = np.corrcoef(df['#1'], df.index)[0, 1] ** 2\nr2_2 = np.corrcoef(df['#2'], df.index)[0, 1] ** 2\n# Calculate the number of times we can get a Dataframe that minimizes the R^2 values\nmin_r2 = min(r2_1, r2_2)\nnum_times = 0\nfor _ in range(len(df)):\n    # Shift the first row of the first column down 1 row\n    df.iloc[0, 0] = df.iloc[1, 0]\n    # Shift the last row of the first column to the first row, first column\n    df.iloc[0, 0] = df.iloc[-1, 0]\n    # Calculate the R^2 values for the first and second columns\n    r2_1 = np.corrcoef(df['#1'], df.index)[0, 1] ** 2\n    r2_2 = np.corrcoef(df['#2'], df.index)[0, 1] ** 2\n    if min(r2_1, r2_2) == min_r2:\n        num_times += 1\nprint(num_times)\n",
        "\ndf.columns = [f'{col}X' for col in df.columns]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\nnew_columns = {}\nfor col in df.columns:\n    if not col.endswith('X'):\n        new_columns[col] = 'X' + col + 'X'\n    else:\n        new_columns[col] = col\ndf.rename(columns=new_columns, inplace=True)\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"mean\" for col in df.columns if \"val\" in col}})\n",
        "\ngrouped = df.groupby('group')\nresult = grouped.agg({\"group_color\": \"first\", **{col: \"sum\" for col in df.columns if \"val\" in col}})\n",
        "\nagg_dict = {}\nfor col in df.columns:\n    if col.endswith('2'):\n        agg_dict[col] = 'mean'\n    else:\n        agg_dict[col] = 'sum'\nresult = df.groupby('group').agg(agg_dict)\n",
        "\nresult = df.loc[row_list, column_list].mean()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = pd.Series(index=df.columns)\nfor col in df.columns:\n    counts = df[col].value_counts()\n    result[col] = counts.index.tolist() + counts.tolist()\n",
        "\nresult = df.isnull().sum()\n",
        "\nresult = \"\"\nfor col in df.columns:\n    result += \"---- \" + col + \" ----\\n\" + str(df[col].value_counts()) + \"\\n\\n\"\n",
        "\ndf.columns = df.iloc[0]\ndf = df.drop(0)\ndf.columns = df.columns.str.strip()\n",
        "\ndf.columns = df.iloc[0]\ndf = df.drop(0)\ndf.reset_index(drop=True, inplace=True)\n",
        "\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n",
        "\nmask = df['value'] < thresh\ndf_small = df.loc[mask]\ndf_large = df.loc[~mask]\nsum_small = df_small['value'].sum()\ndf_small = pd.DataFrame({'lab': ['X'], 'value': [sum_small]}).set_index('lab')\nresult = pd.concat([df_large, df_small])\n",
        "\n# create a boolean mask to filter rows with value > threshold\nmask = df['value'] > thresh\n# create a new dataframe with rows that satisfy the mask\nnew_df = df.loc[mask]\n# calculate the average of the 'value' column of the new dataframe\navg_value = new_df['value'].mean()\n# create a new row with label 'X' and the average value\nnew_row = pd.DataFrame({'value': [avg_value]}, index=['X'])\n# concatenate the new row with the original dataframe\nresult = pd.concat([df.loc[~mask], new_row])\n",
        "\ndf_filtered = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\ndf_aggregated = pd.DataFrame({'lab': 'X', 'value': df[(df['value'] < section_left) | (df['value'] > section_right)]['value'].mean()}).set_index('lab')\nresult = pd.concat([df_filtered, df_aggregated])\n",
        "\nresult = df.apply(lambda x: 1 / x)\nresult.columns = ['inv_' + col for col in result.columns]\ndf = pd.concat([df, result], axis=1)\n",
        "\nexp_cols = [\"exp_A\", \"exp_B\"]\ndf[exp_cols] = np.exp(df)\n",
        "\nresult = df.apply(lambda x: 1 / x)\nresult.columns = ['inv_' + col for col in result.columns]\nresult = pd.concat([df, result], axis=1)\n",
        "\nfor col in df.columns:\n    df[f\"sigmoid_{col}\"] = 1 / (1 + np.exp(-df[col]))\n",
        "\nmax_loc = df.idxmax()\nmin_loc = df.idxmin()\nresult = pd.Series(index=df.columns)\nfor col in df.columns:\n    max_loc_col = max_loc[col]\n    min_loc_col = min_loc[col]\n    if max_loc_col < min_loc_col:\n        result[col] = max_loc_col\n    else:\n        result[col] = df.loc[:min_loc_col, col].idxmax()\n",
        "\nmax_loc = df.idxmax()\nmin_loc = df.idxmin()\nresult = pd.Series(index=df.columns)\nfor col in df.columns:\n    max_before_min = df.loc[max_loc[col]:min_loc[col], col]\n    result[col] = max_before_min[max_before_min == df[col].max()].index[0]\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nnew_df = pd.DataFrame(columns=df.columns)\nfor user in df['user'].unique():\n    user_df = df[df['user'] == user]\n    user_min_date = user_df['dt'].min()\n    user_max_date = user_df['dt'].max()\n    \n    date_range = pd.date_range(start=user_min_date, end=user_max_date)\n    user_df = user_df.set_index('dt')\n    user_df = user_df.reindex(date_range).rename_axis('dt').reset_index()\n    user_df['user'] = user\n    user_df['val'] = user_df['val'].fillna(0)\n    \n    new_df = new_df.append(user_df)\n    \nresult = new_df.reset_index(drop=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nnew_df = pd.DataFrame()\nfor user in df['user'].unique():\n    user_df = df[df['user'] == user]\n    user_df = user_df.set_index('dt')\n    user_df = user_df.reindex(pd.date_range(min_date, max_date, freq='D'), fill_value=0)\n    user_df['user'] = user\n    new_df = new_df.append(user_df)\nnew_df = new_df.reset_index()\nnew_df.columns = ['dt', 'val', 'user']\nresult = new_df\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame(columns=df.columns)\nfor user in df['user'].unique():\n    user_df = df[df['user'] == user]\n    min_user_date = user_df['dt'].min()\n    max_user_date = user_df['dt'].max()\n    \n    new_dates = pd.date_range(start=min_user_date, end=max_user_date)\n    new_df = pd.DataFrame({'user': user, 'dt': new_dates, 'val': 233})\n    \n    result = result.append(new_df)\nresult = result.sort_values(by=['user', 'dt'])\nresult = result.reset_index(drop=True)\n",
        "\n# Create a new dataframe with all dates for each user\nusers = df['user'].unique()\ndates = pd.date_range(df['dt'].min(), df['dt'].max())\nnew_df = pd.DataFrame(columns=df.columns)\nfor user in users:\n    user_df = df[df['user'] == user]\n    user_dates = pd.date_range(user_df['dt'].min(), user_df['dt'].max())\n    missing_dates = dates[~dates.isin(user_dates)]\n    missing_df = pd.DataFrame({'user': user, 'dt': missing_dates, 'val': user_df['val'].max()})\n    new_df = new_df.append(user_df).append(missing_df).sort_values(by=['user', 'dt'])\nresult = new_df.reset_index(drop=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame(columns=df.columns)\nfor user in df['user'].unique():\n    user_df = df[df['user'] == user]\n    min_user_date = user_df['dt'].min()\n    max_user_date = user_df['dt'].max()\n    max_val = user_df['val'].max()\n    \n    date_range = pd.date_range(start=min_user_date, end=max_user_date)\n    date_df = pd.DataFrame({'dt': date_range, 'user': user, 'val': max_val})\n    \n    result = result.append(date_df)\n    \nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n",
        "\nname_to_id = {name: i+1 for i, name in enumerate(df['name'].unique())}\ndf['name'] = df['name'].map(name_to_id)\nresult = df\n",
        "\ndf['a'] = pd.factorize(df['name'])[0] + 1\n",
        "\n    name_map = {name: i for i, name in enumerate(df['name'].unique())}\n    df['name'] = df['name'].map(name_map)\n    ",
        "\nname_a_map = {(name, a): i for i, (name, a) in enumerate(df[['name', 'a']].drop_duplicates().to_numpy(), 1)}\ndf['ID'] = df[['name', 'a']].apply(lambda x: name_a_map[tuple(x)], axis=1)\ndf = df.drop(['name', 'a'], axis=1)\n",
        "\ndf = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\ndf = df.sort_values(['user', 'date'])\n",
        "\ndf_melted = pd.melt(df, id_vars=['user'], var_name='others', value_name='value')\nresult = df_melted.loc[df_melted['others'] != 'someBool']\n",
        "\ndf = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.dropna()\ndf['date'] = pd.to_datetime(df['date'])\n",
        "\nresult = df[df.c > 0.5][columns]\n",
        "\nlocs = [df.columns.get_loc(_) for _ in columns]\nresult = df[df.c > 0.45].iloc[:, locs]\n",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5].values[:, locs]\n    ",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    df_subset = df[df.c > 0.5][locs]\n    result = df_subset.sum(axis=1)\n    ",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5].iloc[:, locs]\n    ",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef remove_overlapping_rows(df, X):\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values(by='date')\n    filter_dates = []\n    for index, row in df.iterrows():\n        if index == 0:\n            continue\n        if (row['date'] - df.iloc[index - 1]['date']).days <= X:\n            filter_dates.append(row['date'])\n    df = df[~df['date'].isin(filter_dates)]\n    return df\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 120\nresult = remove_overlapping_rows(df, X)\nprint(result)\n",
        "\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef remove_overlapping_rows(df, X):\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values(by='date')\n    filter_dates = []\n    for index, row in df.iterrows():\n        if index == 0:\n            continue\n        for i in range(1, X+1):\n            filter_dates.append((row['date'] - timedelta(weeks=i)).date())\n    df = df[~df['date'].isin(filter_dates)]\n    return df\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\nresult = remove_overlapping_rows(df, X)\nprint(result)\n",
        "\nresult = remove_overlap(df, X)\n",
        "\nbins = [0, 3, 6]\nlabels = [0.5, 1.5, 2.5]\ndf['col1'] = pd.cut(df['col1'], bins=bins, labels=labels)\n",
        "\nresult = pd.DataFrame(df.groupby(df.index // 3)['col1'].sum())\n",
        "\nresult = pd.DataFrame(df.groupby(df.index // 4)['col1'].sum()).reset_index(drop=True)\n",
        "\nresult = pd.DataFrame(df['col1'][::-1].rolling(3).mean()[::-1][1:], columns=['col1'])\n",
        "\nresult = pd.DataFrame(columns=['col1'])\nfor i in range(0, len(df), 3):\n    if i + 2 < len(df):\n        result = result.append({'col1': df.iloc[i:i+3]['col1'].sum()}, ignore_index=True)\n    if i + 4 < len(df):\n        result = result.append({'col1': df.iloc[i+3:i+5]['col1'].mean()}, ignore_index=True)\n",
        "\ndef process_data(df):\n    result = pd.DataFrame(columns=df.columns)\n    for i in range(0, len(df), 3):\n        if i + 2 >= len(df):\n            break\n        result.loc[len(result)] = [df.iloc[i:i+3].sum().values[0]]\n        result.loc[len(result)] = [df.iloc[i+1:i+3].mean().values[0]]\n    return result\nresult = process_data(df)\n",
        "\ndf.loc[df['A'] == 0, 'A'] = df['A'][df['A'] != 0].shift(1)\n",
        "\ndf.replace(0, method='ffill')\n",
        "\nfor i in range(1, len(df)-1):\n    if df.iloc[i]['A'] == 0:\n        df.iloc[i]['A'] = max(df.iloc[i-1]['A'], df.iloc[i+1]['A'])\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'([a-zA-Z]+)', expand=False)\ndf['time_days'] = df.time.replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'([a-zA-Z]+)', expand=False)\ndf['time_day'] = df.time.replace({'year': 365, 'month': 30, 'week': 7, 'day': 1})\n",
        "\n    df['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\n    df['time'] = df.duration.str.extract(r'([a-zA-Z]+)', expand=False)\n    time_days_dict = {'year': 365, 'month': 30, 'week': 7, 'day': 1}\n    df['time_days'] = df['time'].map(time_days_dict)\n    ",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'([a-zA-Z]+)', expand=False)\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True) * df['number']\n",
        "\ncheck = np.where([df1[column] != df2[column] for column in columns_check_list])\nresult = check[0]\n",
        "\ncheck = np.where(df1[columns_check_list] == df2[columns_check_list]).all(axis=1)\nresult = check.tolist()\n",
        "\ndf.index = pd.MultiIndex.from_tuples([(t[0], pd.to_datetime(t[1])) for t in df.index])\n",
        "\ndf.index = pd.MultiIndex.from_tuples([(name, pd.to_datetime(date)) for name, date in df.index], names=('name', 'datetime'))\n",
        "\n    df.index = pd.to_datetime(df.index)\n    df = df.reset_index()\n    df = df.values\n    ",
        "\n    df.index = pd.to_datetime(df.index.get_level_values(1))\n    df = df.swaplevel(0, 1).sort_index()\n    ",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value')\ndf = df.reset_index()\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(['Country', 'Variable', 'year'], ascending=[True, True, False])\ndf = df.reset_index(drop=True)\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nfiltered_df = df[(df[columns].abs() < 1).all(axis=1)]\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value')]\nfiltered_df = df[(df[columns].abs() > 1).any(axis=1)]\n",
        "\ncolumns = [col for col in df.columns if col.startswith('Value_')]\nfor col in columns:\n    df[col] = df[col].apply(lambda x: abs(x))\n    df = df[df[col] <= 1]\n    df.rename(columns={col: col.replace('Value_', '')}, inplace=True)\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT;', '<', regex=True)\n",
        "\n    df = df.replace('&AMP;', '&', regex=True)\n    ",
        "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\nname_df['first_name'] = name_df['name'].apply(validate_single_space_name)\nname_df['last_name'] = name_df['name'].apply(lambda x: x.split(' ')[-1] if validate_single_space_name(x) else None)\nname_df['first_name'] = name_df['first_name'].apply(lambda x: x.split(' ')[0] if x else x)\nname_df['last_name'] = name_df['last_name'].apply(lambda x: x.split(' ')[0] if x else x)\n",
        "\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x.split()[-1] if validate_single_space_name(x) else '')\ndf.drop('name', axis=1, inplace=True)\n",
        "\ndf[['first_name', 'middle_name', 'last_name']] = df['name'].apply(split_name)\n",
        "\nresult = pd.merge_asof(df2, df1, on='Timestamp', direction='nearest')\n",
        "\nresult = pd.merge(df1, df2, on='Timestamp', how='left')\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\nerror_values = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        error_values.append(row['Field1'])\nresult = error_values\nprint(result)\n",
        "\nint_list = []\nfor index, row in df.iterrows():\n    if isinstance(row['Field1'], int):\n        int_list.append(row['Field1'])\nresult = int_list\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    result = []\n    for index, row in df.iterrows():\n        if not isinstance(row['Field1'], int):\n            result.append(row['Field1'])\n    return result\nprint(f())\n",
        "\ndf_percent = df.set_index('cat')\ndf_percent = df_percent.apply(lambda x: x / x.sum(), axis=1)\ndf_percent = df_percent.reset_index()\n",
        "\ndf_pct = df.set_index('cat')\ndf_pct = df_pct.apply(lambda x: x / x.sum(), axis=1)\ndf_pct = df_pct.reset_index()\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\n",
        "\n    result = df.loc[test]\n    result = result.drop_duplicates()\n    ",
        "\ndef nearest_neighbour(group):\n    distances = []\n    nearest = min(distances, key=lambda x: x[2])\n    return nearest\ndf2 = df.groupby('time').apply(nearest_neighbour)\ndf2 = pd.DataFrame(df2.tolist(), columns=['car', 'nearest_neighbour', 'euclidean_distance'])\ndf2['time'] = df2.index\nresult = df2[['time', 'car', 'nearest_neighbour', 'euclidean_distance']]\n",
        "\ndef farmost_neighbour(group):\n    distances = pdist(group[['x', 'y']])\n    farmost_index = distances.argmax()\n    farmost_distance = distances[farmost_index]\n    farmost_neighbour_car = group['car'].iloc[farmost_index]\n    return pd.Series({'farmost_neighbour': farmost_neighbour_car, 'euclidean_distance': farmost_distance})\ndf2 = df.groupby(['time', 'car']).apply(farmost_neighbour).reset_index()\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[cols].apply(lambda row: \",\".join(row.dropna().tolist()), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\nsample_rows = df.sample(frac=0.2, random_state=0)\naltered_index = sample_rows.index\ndf.loc[altered_index, 'Quantity'] = 0\n",
        "\nsample_rows = df.sample(frac=0.2, random_state=0)\naltered_index = sample_rows.index\ndf.loc[altered_index, 'ProductId'] = 0\n",
        "\nuser_groups = df.groupby('UserId')\naltered_rows = []\nfor user_id, user_df in user_groups:\n    sampled_rows = user_df.sample(frac=0.2, random_state=0)\n    altered_rows.extend(sampled_rows.index.tolist())\n    user_df.loc[sampled_rows.index, 'Quantity'] = 0\nresult = df.loc[altered_rows]\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index - duplicate.groupby(['col1', 'col2']).cumcount()\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = df.merge(duplicate[['col1', 'col2', 'index_original']], on=['col1', 'col2'], how='left')\nresult.fillna(-1, inplace=True)\nresult['index_original'] = result['index_original'].astype(int)\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = df.index[duplicate_bool == True]\n    ",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nduplicate.index = duplicate['val']\nduplicate = duplicate.drop('val', axis=1)\nresult = duplicate.reset_index()\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate.reset_index(drop=True)\n",
        "\ngrouped = df.groupby(['Sp', 'Mt'])\nresult = grouped.apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\ngrouped = df.groupby(['Sp','Mt'])\nmax_count = grouped['count'].transform('max')\nresult = df[df['count'] == max_count]\n",
        "\ngrouped = df.groupby(['Sp', 'Mt'])\nresult = grouped.apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\ngrouped = df.groupby(['Sp', 'Value'])\nresult = grouped.apply(lambda x: x[x['count'] == x['count'].max()]).reset_index(drop=True)\n",
        "\nresult = df.query(\"Category in @filter_list\")\n",
        "\nresult = df.query(\"Category not in @filter_list\")\n",
        "\nvalue_vars = [(col[0], col[1], col[2]) for col in df.columns]\nresult = pd.melt(df, value_vars=value_vars)\n",
        "\nresult = pd.melt(df, value_vars=list(zip(df.columns.get_level_values(0), df.columns.get_level_values(1), df.columns.get_level_values(2))), var_name=['variable_0', 'variable_1', 'variable_2'], value_name='value')\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum().apply(lambda y: y if y >= 0 else 0))\n",
        "\nresult = df.groupby('l')['v'].sum()\nresult.loc['right'] = np.nan\n",
        "\nresult = df.groupby('r')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            x = df[col1].values\n            y = df[col2].values\n            if len(set(zip(x, y))) == len(x):\n                result.append(f'{col1} {col2} one-to-one')\n            elif len(set(zip(x, y))) == len(set(x)):\n                result.append(f'{col1} {col2} one-to-many')\n            elif len(set(zip(y, x))) == len(y):\n                result.append(f'{col1} {col2} many-to-one')\n            else:\n                result.append(f'{col1} {col2} many-to-many')\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            x = df[col1].values\n            y = df[col2].values\n            if len(set(x)) == len(x) and len(set(y)) == len(y):\n                result.append(f'{col1} {col2} one-2-one')\n            elif len(set(x)) == len(x):\n                result.append(f'{col1} {col2} one-2-many')\n            elif len(set(y)) == len(y):\n                result.append(f'{col1} {col2} many-2-one')\n            else:\n                result.append(f'{col1} {col2} many-2-many')\n",
        "\ndef get_relationship(df):\n    columns = df.columns\n    relationship_df = pd.DataFrame(columns=columns, index=columns)\n    for i in range(len(columns)):\n        for j in range(len(columns)):\n            if i == j:\n                relationship_df.iloc[i, j] = \"one-to-one\"\n            else:\n                column1 = df[columns[i]]\n                column2 = df[columns[j]]\n                if len(column1.unique()) == len(column1):\n                    if len(column2.unique()) == len(column2):\n                        relationship_df.iloc[i, j] = \"one-to-one\"\n                    else:\n                        relationship_df.iloc[i, j] = \"one-to-many\"\n                else:\n                    if len(column2.unique()) == len(column2):\n                        relationship_df.iloc[i, j] = \"many-to-one\"\n                    else:\n                        relationship_df.iloc[i, j] = \"many-to-many\"\n    return relationship_df\nresult = get_relationship(df)\n",
        "\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 == col2:\n            result.loc[col1, col2] = 'one-2-one'\n        else:\n            x = df[col1].value_counts()\n            y = df[col2].value_counts()\n            xy = pd.crosstab(df[col1], df[col2])\n            if len(x) == len(y):\n                result.loc[col1, col2] = 'one-2-one'\n            elif len(x) == 1:\n                result.loc[col1, col2] = 'many-2-one'\n            elif len(y) == 1:\n                result.loc[col1, col2] = 'one-2-many'\n            else:\n                result.loc[col1, col2] = 'many-2-many'\n",
        "\n# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last'))\n# save unique records with bank account\ndf_uniq = df.loc[uniq_indx.index]\n",
        "\nlocale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\nresult = pd.to_numeric(s.astype(str).str.replace(',', ''), errors='coerce')\n",
        "\nconditions = [(df['SibSp'] > 0) | (df['Parch'] > 0), (df['SibSp'] == 0) & (df['Parch'] == 0)]\nchoices = ['Has Family', 'No Family']\ndf['Family'] = pd.Series(np.select(conditions, choices, default='Unknown'))\nresult = df.groupby('Family')['Survived'].mean()\n",
        "\nconditions = [(df['Survived'] > 0) | (df['Parch'] > 0), (df['Survived'] == 0) & (df['Parch'] == 0)]\nchoices = ['Has Family', 'No Family']\ndf['Family'] = pd.Series(np.select(conditions, choices, default='Unknown'))\nresult = df.groupby('Family')['SibSp'].mean()\n",
        "\nconditions = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n              (df['SibSp'] == 0) & (df['Parch'] == 0),\n              (df['SibSp'] == 0) & (df['Parch'] == 1),\n              (df['SibSp'] == 1) & (df['Parch'] == 0)]\nchoices = ['Has Family', 'No Family', 'New Family', 'Old Family']\ndf['Group'] = pd.Series(np.select(conditions, choices, default='Unknown'))\nresult = df.groupby('Group')['Survived'].mean()\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A')).reset_index(drop=True)\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A')).reset_index(drop=True)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\nresult = df.unstack(level=0)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\nbird_counts = pd.DataFrame(someTuple[1], index=someTuple[0], columns=['birdCount'])\nbird_counts.index.name = 'birdType'\nresult = bird_counts.reset_index()\n",
        "\ndef stdMeann(x):\n    return np.std(np.mean(x))\nresult = df.groupby('a').b.agg(['mean', stdMeann])\n",
        "\ndef stdMeann(x):\n    return np.std(np.mean(x))\nresult = df.groupby('b').a.agg(['mean', stdMeann])\n",
        "\ndef normalization(x):\n    x['softmax'] = softmax(x['b'])\n    x['min-max'] = minmax_scale(x['b'])\n    return x\nresult = df.groupby('a').apply(normalization)\n",
        "\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf = df.loc[(df != 0).any(axis=1), :]\nresult = df\n",
        "\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf = df.loc[(df != 0).any(axis=1), :]\nresult = df.reset_index(drop=True)\n",
        "\nmax_value = 2\nresult = df[(df <= max_value).all(axis=1)]\nresult = result.drop(columns=df.columns[(df == max_value).all()])\n",
        "\ndf[df != 2] = 0\n",
        "\ns = s.sort_values(ascending=False)\nresult = s.sort_index()\n",
        "\ns = s.sort_values(ascending=False)\ndf = s.reset_index()\ndf = df.sort_values(['index'], ascending=True)\n",
        "\nresult = df[pd.to_numeric(df['A'], errors='coerce').notnull()]\n",
        "\nstring_mask = df['A'].apply(lambda x: isinstance(x, str))\nresult = df[string_mask]\n",
        "\ngrouped = df.groupby(['Sp', 'Mt'])\nresult = grouped.apply(lambda x: x[x['count'] == x['count'].max()]).reset_index(drop=True)\n",
        "\ngrouped = df.groupby(['Sp', 'Mt'])\nmax_count = grouped['count'].transform('max')\nresult = df[df['count'] == max_count]\n",
        "\ngrouped = df.groupby(['Sp', 'Mt'])\nresult = grouped.apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\ngrouped = df.groupby(['Sp', 'Value'])\nresult = grouped.apply(lambda x: x[x['count'] == x['count'].max()]).reset_index(drop=True)\n",
        "\ndf.loc[df['Member'].isin(dict.keys()), 'Date'] = df['Member'].map(dict)\n",
        "\ndf.loc[df['Member'].isin(dict.keys()), 'Date'] = df['Member'].map(dict)\ndf.fillna({'Date': '17/8/1926'}, inplace=True)\n",
        "\n    df['Date'] = df['Member'].map(dict)\n    df['Date'] = df['Date'].fillna(df['Member'])\n    ",
        "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(pd.Timestamp('1926-08-17'))\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%d-%b-%Y') if not pd.isnull(x) else np.nan)\n",
        "\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.map(df.groupby(df.Date.dt.to_period('M')).size())\ndf['Count_y'] = df.Date.map(df.groupby(df.Date.dt.year).size())\n",
        "\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.map(df.groupby(df.Date.dt.to_period('M')).size())\ndf['Count_y'] = df.Date.map(df.groupby(df.Date.dt.year).size())\ndf['Count_Val'] = df.Val.map(df.groupby('Val').size())\n",
        "\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.dt.month.map(df.groupby(df.Date.dt.month).size())\ndf['Count_y'] = df.Date.dt.year.map(df.groupby(df.Date.dt.year).size())\ndf['Count_w'] = df.Date.dt.dayofweek.map(df.groupby(df.Date.dt.dayofweek).size())\ndf['Count_Val'] = df.Val.map(df.groupby('Val').size())\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: (x == 0).sum())\nresult2 = df.groupby('Date').apply(lambda x: (x != 0).sum())\n",
        "\nresult1 = df.groupby('Date').apply(lambda x: x % 2 == 0).sum()\nresult2 = df.groupby('Date').apply(lambda x: x % 2 != 0).sum()\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\nimport dask.dataframe as dd\nimport pandas as pd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\n# Convert pandas dataframe to dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n# Define a function to split the column into multiple rows\ndef split_rows(row):\n    var2_list = row['var2'].split(',')\n    return pd.DataFrame({'var1': [row['var1']] * len(var2_list), 'var2': var2_list})\n# Apply the function to each row of the dask dataframe\nresult = dask_df.apply(split_rows, axis=1, meta={'var1': 'str', 'var2': 'str'}).reset_index(drop=True)\n# Convert dask dataframe back to pandas dataframe\nresult = result.compute()\nprint(result)\n",
        "\nimport dask.dataframe as dd\nimport pandas as pd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\n# Convert pandas dataframe to dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n# Define a function to split the column into multiple rows\ndef split_rows(row):\n    var1 = row['var1']\n    var2 = row['var2']\n    return pd.DataFrame({'var1': [var1] * len(var2.split(',')), 'var2': var2.split(',')})\n# Apply the function to each row of the dask dataframe\nresult = dask_df.apply(split_rows, axis=1, meta={'var1': 'str', 'var2': 'str'}).reset_index(drop=True)\n# Convert dask dataframe back to pandas dataframe\nresult = result.compute()\nprint(result)\n",
        "\nimport dask.dataframe as dd\ndef split_rows(row):\n    var1 = row['var1']\n    var2 = row['var2']\n    return pd.DataFrame({'var1': [var1] * len(var2), 'var2': var2.split('-')})\ndask_df = dd.from_pandas(df, npartitions=1)\nresult = dask_df.apply(split_rows, axis=1, meta={'var1': 'str', 'var2': 'str'}).reset_index(drop=True)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df.apply(lambda row: count_special_char(row['str']), axis=1)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf[\"new\"] = df.apply(lambda row: count_special_char(row['str']), axis=1)\nresult = df\nprint(result)\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[6:]\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(n=1, expand=True)\n",
        "\ndf[['fips', 'medi', 'row']] = df['row'].str.split(expand=True)\n",
        "\ndf = df.set_index('Name')\ndf = df.replace(0, pd.NA)\ndf = df.cumsum(axis=1) / df.notna().sum(axis=1)\n",
        "\ndf = df.set_index('Name')\ndf = df.replace(0, np.nan)\ndf = df.iloc[:, ::-1]\ndf = df.cumsum(axis=1)\ndf = df.iloc[:, ::-1]\ndf = df.div(df.count(axis=1), axis=0)\ndf = df.reset_index()\n",
        "\n    df_new = df.set_index('Name')\n    result = pd.DataFrame()\n    for col in df_new.columns:\n        result[col] = df_new[col].replace(0, pd.NA).cumsum() / (df_new[col] != 0).cumsum()\n    result = result.reset_index()\n    ",
        "\ndf = df.set_index('Name')\ndf = df.replace(0, np.nan)\ndf = df.apply(lambda x: x.dropna()[::-1].cumsum()[::-1] / (np.arange(len(x.dropna()))+1))\ndf = df.fillna(0)\n",
        "\ndf['Label'] = 0\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\n",
        "\ndf['label'] = df['Close'].diff().fillna(0).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf.loc[df.index[0], 'label'] = 1\n",
        "\ndf['label'] = 0\ndf['label'].iloc[0] = 1\nfor i in range(1, len(df)):\n    if df['Close'].iloc[i] > df['Close'].iloc[i-1]:\n        df['label'].iloc[i] = 1\n    elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:\n        df['label'].iloc[i] = -1\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time'].shift(1)\ndf['Duration'] = df['Duration'].fillna(pd.Timedelta(seconds=0))\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time'].shift(1)\ndf['Duration'] = df['Duration'].dt.seconds\n",
        "\nimport pandas as pd\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n# Convert arrival_time and departure_time to datetime64[ns]\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n# Calculate the time difference in seconds\ndf['Duration'] = df['departure_time'].shift(-1) - df['arrival_time'].shift(1)\ndf['Duration'] = df['Duration'].dt.total_seconds()\n# Format the arrival_time and departure_time columns\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\n# Fill the missing values in the Duration column\ndf['Duration'].fillna(value=pd.Timedelta(seconds=0), inplace=True)\n# Output the result\nresult = df\nprint(result)\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'one').sum())\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'two').sum())\n",
        "\ncounts = df[df['key2'].str.endswith('e')].groupby('key1').size()\nresult = counts.reindex(df['key1'].unique(), fill_value=0)\n",
        "\nmax_result = df.index.max()\nmin_result = df.index.min()\n",
        "\nmode_result = df.index.mode().values[0]\nmedian_result = df.index.median()\n",
        "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n",
        "\ndf1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\nresult = df.merge(df1, on=[\"item\", \"diff\"], how=\"inner\")\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.rsplit('_', 1)[0])\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n",
        "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.rsplit('_', 1)[0])\n    ",
        "\nnum_nan = df['Column_x'].isna().sum()\nnum_zeros = int(num_nan / 2)\nnum_ones = num_nan - num_zeros\ndf.loc[df['Column_x'].isna()[:num_zeros], 'Column_x'] = 0\ndf.loc[df['Column_x'].isna()[num_zeros:], 'Column_x'] = 1\n",
        "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n# Calculate the number of NaN values to fill in each category\nfill_0_count = int(nan_count * 0.3)\nfill_05_count = int(nan_count * 0.3)\nfill_1_count = nan_count - fill_0_count - fill_05_count\n# Fill the NaN values with the desired categories\ndf.loc[df['Column_x'].isna()[:fill_0_count].index, 'Column_x'] = 0\ndf.loc[df['Column_x'].isna()[fill_0_count:fill_0_count+fill_05_count].index, 'Column_x'] = 0.5\ndf.loc[df['Column_x'].isna()[fill_0_count+fill_05_count:].index, 'Column_x'] = 1\n",
        "\nnum_nan = df['Column_x'].isna().sum()\nnum_zeros = int(num_nan / 2)\nnum_ones = num_nan - num_zeros\ndf.loc[df['Column_x'].isna(), 'Column_x'] = np.concatenate([np.zeros(num_zeros), np.ones(num_ones)]).astype(int)\n",
        "\na_b = pd.DataFrame(columns=['one', 'two'])\nfor i in range(len(a)):\n    row = []\n    for j in range(len(a.columns)):\n        row.append((a.iloc[i][j], b.iloc[i][j]))\n    a_b.loc[i] = row\n",
        "\na_b_c = pd.DataFrame(columns=a.columns)\nfor i in range(len(a)):\n    row = []\n    for j in range(len(a.columns)):\n        row.append(tuple(df.iat[i, j] for df in [a, b, c]))\n    a_b_c.loc[i] = row\n",
        "\na_b = pd.DataFrame(columns=['one', 'two'])\nfor i in range(max(a.shape[0], b.shape[0])):\n    row_a = a.iloc[i] if i < a.shape[0] else [np.nan] * a.shape[1]\n    row_b = b.iloc[i] if i < b.shape[0] else [np.nan] * b.shape[1]\n    a_b.loc[i] = list(zip(row_a, row_b))\n",
        "\ngroups = df.groupby(['username'])\nresult = groups.views.apply(lambda x: pd.value_counts(pd.cut(x, bins=bins), sort=False)).fillna(0)\n",
        "\nresult = df.groupby(['username', pd.cut(df.views, bins)]).size().unstack(fill_value=0)\n",
        "\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack(fill_value=0)\n",
        "\nresult = pd.DataFrame({'text': [', '.join(df['text'])]})\n",
        "\ndf['text'] = df['text'].apply(lambda x: '-'.join(df['text']))\nresult = pd.DataFrame({'text': [df['text'].values[0]]})\n",
        "\nresult = pd.DataFrame({'text': [', '.join(df['text'][::-1])]})\n",
        "\nresult = ', '.join(df['text'])\n",
        "\nresult = pd.Series(df['text'].str[::-1].sum()[::-1])\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.merge(df1[['id', 'city', 'district']], on='id', how='left')\nresult = result.fillna(method='ffill')\nresult = result.fillna(method='bfill')\nresult = result.sort_values(by='id')\nresult = result[['id', 'city', 'district', 'date', 'value']]\n",
        "\ndf1['date'] = pd.to_datetime(df1['date'], format='%Y/%m/%d')\ndf2['date'] = pd.to_datetime(df2['date'], format='%Y/%m/%d')\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(by=['id', 'date'])\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\nresult = result.fillna(method='ffill')\nresult = result.drop_duplicates(subset=['id', 'date'], keep='first')\nresult = result.sort_values(by=['date', 'id'])\nresult = result[['id', 'city', 'district', 'date', 'value']]\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(by=['id', 'date'])\nresult['city'] = result['city'].fillna(method='ffill')\nresult['district'] = result['district'].fillna(method='ffill')\nresult = result.drop_duplicates(subset=['id', 'date'], keep='first')\nresult = result.sort_values(by=['id', 'date'])\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['B'] = result['B_y'].fillna(result['B_x']).astype(int)\nresult = result[['A', 'B']]\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['B'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(['B_x', 'B_y'], axis=1)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['B'] = result['B_y'].fillna(result['B_x'])\nresult['dulplicated'] = result['B_y'].notnull()\nresult = result[['A', 'B', 'dulplicated']].reset_index(drop=True)\n",
        "\nresult = df.groupby('user').apply(lambda x: list(zip(x['time'], x['amount']))).reset_index(name='transactions')\n",
        "\nresult = df.groupby('user').apply(lambda x: list(zip(x['time'], x['amount']))).reset_index(name='amount-time-tuple')\n",
        "\nresult = df.groupby('user').apply(lambda x: sorted(zip(x['time'], x['amount']), key=lambda y: y[0]))\n",
        "\ndf = pd.DataFrame(series.to_list(), index=series.index)\n",
        "\ndf = pd.DataFrame(series.tolist(), index=series.index)\ndf = df.rename_axis('name').reset_index()\n",
        "\nresult = [col for col in df.columns if s in col and col != s]\n",
        "\ncols = [col for col in df.columns if s in col]\nresult = df[cols]\n",
        "\ncols = [col for col in df.columns if s in col]\ndf_spike = df[cols]\ndf_spike.columns = [f\"{s}{i+1}\" for i in range(len(cols))]\n",
        "\ndf = df['codes'].apply(pd.Series)\ndf.columns = ['code_' + str(i) for i in range(df.shape[1])]\nresult = df.join(df.apply(pd.Series.first_valid_index).apply(pd.Series))\n",
        "\ndf = df['codes'].apply(pd.Series)\ndf.columns = ['code_' + str(i+1) for i in range(df.shape[1])]\nresult = df.fillna(value=float('nan'))\n",
        "\ndf['code_1'] = df['codes'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['code_2'] = df['codes'].apply(lambda x: x[1] if len(x) > 1 else None)\ndf['code_3'] = df['codes'].apply(lambda x: x[2] if len(x) > 2 else None)\n",
        "\nresult = [item for sublist in df['col1'].values.tolist() for item in sublist]\n",
        "\ndef reverse_and_join(lst):\n    return ','.join(str(x) for x in lst[::-1])\ndf['col1'] = df['col1'].apply(reverse_and_join)\nresult = df['col1'].str.cat(sep=',')\n",
        "\ndf['col1'] = df['col1'].apply(lambda x: ','.join(str(i) for i in x))\nresult = df['col1'].values.tolist()\n",
        "\ndf = df.groupby(pd.Grouper(key='Time', freq='2min')).mean().reset_index()\ndf['Time'] = df['Time'].dt.floor('2min')\n",
        "\ndf = df.groupby(pd.Grouper(key='Time', freq='3min')).agg({'Value': 'sum'})\ndf = df.resample('3min').asfreq().fillna(0)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n",
        "\nresult = df[df.index.get_level_values('a').isin(filt[filt].index)]\n",
        "\nresult = df[df.index.get_level_values('a').isin(filt[filt].index)]\n",
        "\ndiff_cols = df.columns[(df.iloc[0] != df.iloc[8]) & (df.iloc[0].notna() | df.iloc[8].notna())]\n",
        "\nrow1 = df.iloc[0]\nrow2 = df.iloc[8]\nresult = row1.index[row1 == row2]\n",
        "\nrow1 = df.iloc[0]\nrow2 = df.iloc[8]\nresult = [col for col in df.columns if not (np.isnan(row1[col]) and np.isnan(row2[col]))]\n",
        "\nresult = []\nfor col in df.columns:\n    if df.iloc[0][col] != df.iloc[8][col]:\n        result.append((df.iloc[0][col], df.iloc[8][col]))\n",
        "\nimport pandas as pd\ndates = ['2016-1-{}'.format(i)for i in range(1,21)]\nvalues = [i for i in range(20)]\ndata = {'Date': dates, 'Value': values}\ndf = pd.DataFrame(data)\ndf['Date'] = pd.to_datetime(df['Date'])\n# Sort the DataFrame by the 'Date' column\ndf = df.sort_values(by='Date')\n# Create the Series using the sorted DataFrame\nts = pd.Series(df['Value'], index=df['Date'])\nresult = ts\nprint(result)\n",
        "\nresult = df.unstack().to_frame().T\n",
        "\nresult = df.unstack().to_frame().T\n",
        "\ndf = df.apply(lambda x: x.fillna(0) if x.dtype == np.float64 else x)\ndf['dogs'] = df['dogs'].round(2)\n",
        "\ndf['dogs'] = pd.to_numeric(df['dogs']).round(2)\ndf['cats'] = pd.to_numeric(df['cats']).round(2)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\ndf = df.sort_index(level='time')\n",
        "\nresult = df.sort_values(by=['VIM'], ascending=True)\n",
        "\ndf = df[(df.index < '2020-02-17') | (df.index > '2020-02-18')]\n",
        "\ndf = df[(df.index < '2020-02-17') | (df.index > '2020-02-18')]\n",
        "\nresult = corr.where(corr > 0.3)\nresult = result.dropna(how='all')\nresult = result.dropna(how='all', axis=1)\nresult.columns = ['Pearson Correlation Coefficient']\n",
        "\nresult = corr.unstack().to_frame().reset_index()\nresult = result[(result[0] > 0.3) & (result['level_0'] != result['level_1'])]\nresult = result.set_index(['level_0', 'level_1'])[0]\n",
        "\nnew_columns = df.columns.values.tolist()\nnew_columns[-1] = 'Test'\ndf.columns = new_columns\n",
        "\nnew_columns = df.columns.to_list()\nnew_columns[0] = 'Test'\ndf.columns = new_columns\n",
        "\ndf['frequent'] = df.mode(axis=1).iloc[:, 0]\ndf['freq_count'] = df.apply(lambda x: x.tolist().count(x['frequent']), axis=1)\n",
        "\ndef find_frequent_value(row):\n    values, counts = np.unique(row, return_counts=True)\n    max_count = np.max(counts)\n    frequent_values = values[counts == max_count]\n    return frequent_values[0] if len(frequent_values) == 1 else np.nan\ndf['frequent'] = df.apply(find_frequent_value, axis=1)\ndf['freq_count'] = df.apply(lambda row: np.sum(row == row['frequent']), axis=1)\n",
        "\ndef find_frequent_values(row):\n    values, counts = np.unique(row, return_counts=True)\n    max_count = np.max(counts)\n    frequent_values = values[counts == max_count]\n    return frequent_values.tolist(), len(frequent_values)\ndf[['frequent', 'freq_count']] = df.apply(lambda row: find_frequent_values(row), axis=1, result_type='expand')\n",
        "\nres = df.groupby([\"id1\",\"id2\"])[[\"foo\", \"bar\"]].mean()\nres = res.reset_index()\nres['bar'] = res['bar'].fillna(res['foo'])\nres = res.set_index(['id1', 'id2'])\n",
        "\nres = df.groupby([\"id1\",\"id2\"])[[\"foo\", \"bar\"]].mean()\nres = res.reset_index()\n# Fill missing values with 0\nres = res.fillna(0)\n# Convert 'id1' and 'id2' to int\nres['id1'] = res['id1'].astype(int)\nres['id2'] = res['id2'].astype(int)\n",
        "\nresult = pd.merge(df_a, df_b, on='EntityNum', how='inner')[['EntityNum', 'foo', 'a_col']]\n",
        "\nresult = pd.merge(df_a, df_b, on='EntityNum', how='inner')[['EntityNum', 'foo', 'b_col']]\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = x[~np.isnan(x)]\n",
        "\nx = np.where(np.isnan(x), np.inf, x)\n",
        "\nx = x[~np.isnan(x)]\nresult = x.tolist()\n",
        "\nmax_num = np.max(a)\nb = np.zeros((len(a), max_num + 1), dtype=int)\nb[np.arange(len(a)), a] = 1\n",
        "\nn = a.max() + 1\nb = np.zeros((a.size, n), dtype=int)\nb[np.arange(a.size), a] = 1\n",
        "\na_min = a.min()\na_max = a.max()\nb = np.zeros((len(a), a_max - a_min + 1), dtype=int)\nb[np.arange(len(a)), a - a_min] = 1\n",
        "\nunique_a = np.unique(a)\nb = np.zeros((len(a), len(unique_a)), dtype=int)\nfor i, val in enumerate(unique_a):\n    b[:, i] = (a == val)\n",
        "\nunique_values = np.unique(a)\nnum_values = len(unique_values)\nb = np.zeros((a.shape[0], a.shape[1], num_values), dtype=int)\nfor i in range(num_values):\n    b[:, :, i] = (a == unique_values[i])\n",
        "\nresult = np.percentile(a, p)\n",
        "\nB = A.reshape((-1, ncol))\n",
        "\nB = A.reshape(nrow, -1)\n",
        "\nnrow = len(A) // ncol\nB = A[:nrow*ncol].reshape(nrow, ncol)\n",
        "\nnrow = len(A) // ncol\nB = np.reshape(A[:nrow*ncol], (nrow, ncol))\n",
        "\nresult = np.roll(a, shift)\nresult[:shift] = np.nan\n",
        "\nresult = np.zeros_like(a)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        if i == 0:\n            if j - shift >= 0:\n                result[i, j] = a[i, j - shift]\n            else:\n                result[i, j] = np.nan\n        else:\n            if j - shift >= 0:\n                result[i, j] = a[i, j - shift]\n            else:\n                result[i, j] = np.nan\n",
        "\ndef shift_rows(arr, shifts):\n    result = np.zeros_like(arr)\n    for i, shift in enumerate(shifts):\n        result[i, :] = np.roll(arr[i, :], shift)\n    return result\nresult = shift_rows(a, shift)\n",
        "\nnp.random.seed(0)\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nnp.random.seed(0)\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n",
        "\nresult = np.argmax(a.ravel())\n",
        "\nresult = np.argmin(a.ravel())\n",
        "\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    flat_index = np.argmax(a.ravel())\n    result = np.unravel_index(flat_index, a.shape)\n    ",
        "\nresult = np.unravel_index(np.argpartition(a.ravel(), -2)[-2], a.shape)\n",
        "\nz = np.any(np.isnan(a), axis=0)\na = a[:, ~z]\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nresult = np.unravel_index(a.argmin(), a.shape)\n",
        "\nrow, col = np.unravel_index(a.argmax(), a.shape)\nresult = (row, col)\n",
        "\nresult = np.where(a == np.min(a))\nresult = np.transpose(result)\n",
        "\nresult = np.sin(np.deg2rad(degree))\n",
        "\nresult = np.cos(np.deg2rad(degree))\n",
        "\nif np.sin(number * np.pi / 180) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n",
        "\ndegree = np.degrees(np.arcsin(value))\n",
        "\npad_len = length - len(A)\nif pad_len > 0:\n    A = np.pad(A, (0, pad_len), 'constant', constant_values=0)\n",
        "\npad_len = length - len(A)\nif pad_len > 0:\n    A = np.pad(A, (0, pad_len), 'constant', constant_values=0)\nresult = A\n",
        "\na = a ** power\n",
        "\n    result = a ** power\n    ",
        "\ndef gcd(a, b):\n    while b:\n        a, b = b, a % b\n    return a\ndef reduce_fraction(numerator, denominator):\n    gcd_value = gcd(numerator, denominator)\n    return numerator // gcd_value, denominator // gcd_value\nnumerator, denominator = reduce_fraction(numerator, denominator)\nresult = (numerator, denominator)\n",
        "\n    gcd = np.gcd(numerator, denominator)\n    numerator //= gcd\n    denominator //= gcd\n    result = (numerator, denominator)\n    ",
        "\nif denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    gcd = np.gcd(numerator, denominator)\n    result = (numerator // gcd, denominator // gcd)\n",
        "\nresult = np.mean([a, b, c], axis=0)\n",
        "\nresult = np.maximum(a, np.maximum(b, c))\n",
        "\ndiagonal = np.diag_indices(5)\nresult = a[diagonal][::-1]\n",
        "\ndiagonal = np.diag_indices(5)\nresult = a[diagonal][::-1]\n",
        "\ndiagonal = np.diag_indices(5)\nresult = np.array([a[diagonal], a[diagonal[::-1]]])\n",
        "\ndiagonal = np.diag_indices(5)\nresult = np.array([a[diagonal], a[diagonal[::-1]]])\n",
        "\nresult = []\nfor row in X:\n    for element in row:\n        result.append(element)\n",
        "\nresult = []\nfor row in X:\n    for element in row:\n        result.append(element)\n",
        "\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for row in X:\n        for element in row:\n            result.append(element)\n    return result\n",
        "\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n",
        "\nresult = np.fromstring(mystr, dtype=int, sep='')\n",
        "\ncolumn = a[:, col] * multiply_number\nresult = np.cumsum(column)\n",
        "\nrow_array = a[row, :] * multiply_number\nresult = np.cumsum(row_array)\n",
        "\na[row, :] = a[row, :] / divide_number\nresult = np.prod(a[row, :])\n",
        "\n_, s, vh = np.linalg.svd(a)\nresult = vh[s > 1e-5].T\n",
        "\nresult = a.shape[1]\n",
        "\nn_a = len(a)\nn_b = len(b)\nmean_a = np.mean(a)\nmean_b = np.mean(b)\nstd_dev_a = np.std(a)\nstd_dev_b = np.std(b)\nstd_err_a = std_dev_a / np.sqrt(n_a)\nstd_err_b = std_dev_b / np.sqrt(n_b)\nt_statistic = (mean_a - mean_b) / (std_err_a + std_err_b)\ndf = n_a + n_b - 2\np_value = scipy.stats.t.sf(np.abs(t_statistic), df) * 2\n",
        "\na_nan = np.isnan(a)\nb_nan = np.isnan(b)\na = a[~a_nan]\nb = b[~b_nan]\nn_a = len(a)\nn_b = len(b)\nmean_a = np.mean(a)\nmean_b = np.mean(b)\nstd_dev_a = np.std(a, ddof=1)\nstd_dev_b = np.std(b, ddof=1)\nweighted_mean = (mean_a * n_a + mean_b * n_b) / (n_a + n_b)\nweighted_std_dev = np.sqrt(((n_a - 1) * std_dev_a**2 + (n_b - 1) * std_dev_b**2) / (n_a + n_b - 2))\nt_value = (mean_a - mean_b) / (weighted_std_dev * np.sqrt(1/n_a + 1/n_b))\np_value = scipy.stats.t.sf(np.abs(t_value), n_a + n_b - 2) * 2\n",
        "\npooled_var = ((anobs - 1) * avar + (bnobs - 1) * bvar) / (anobs + bnobs - 2)\nt_stat = (amean - bmean) / np.sqrt(pooled_var * (1 / anobs + 1 / bnobs))\np_value = scipy.stats.t.sf(np.abs(t_stat), anobs + bnobs - 2) * 2\n",
        "\noutput = []\nfor row in A:\n    if row not in B:\n        output.append(row)\n",
        "\noutput = np.concatenate((A[~np.in1d(A, B)], B[~np.in1d(B, A)]))\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = np.zeros_like(b)\nfor i in range(3):\n    c[i] = b[sort_indices[i]]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = np.take_along_axis(b, sort_indices, axis=0)\n",
        "\nsort_indices = np.argsort(-a, axis=0)\nc = np.take_along_axis(b, sort_indices, axis=0)\n",
        "\nsums = np.sum(a, axis=(1, 2))\nindices = np.argsort(sums)\nresult = b[indices]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0, 2], axis=1)\n",
        "\nvalid_cols = np.array([i for i in range(a.shape[1]) if i not in del_col])\nresult = a[:, valid_cols]\n",
        "\na = np.insert(a, pos, element)\n",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\n    a = np.insert(a, pos, element)\n    ",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\nimport copy\nresult = copy.deepcopy(array_of_arrays)\n",
        "\nresult = np.all(np.all(a == a[0], axis=1))\n",
        "\nresult = np.all(np.apply_along_axis(lambda x: len(set(x)) == 1, 0, a))\n",
        "\nresult = np.all(np.all(a == a[0], axis=1))\n",
        "\nZ = (np.cos(X)**4 + np.sin(Y)**2).T\nresult = simps(simps(Z, y), x)\n",
        "\n    X, Y = np.meshgrid(x, y)\n    result = simps(simps(np.cos(X)**4 + np.sin(Y)**2, x), y)\n    ",
        "\ndef ecdf(x):\n    x = np.sort(x)\n    n = len(x)\n    y = np.arange(1, n+1) / n\n    return x, y\nx, y = ecdf(grades)\nresult = y[np.searchsorted(x, 90, side='left')]\n",
        "\necdf = norm.cdf(grades)\nresult = ecdf(eval)\n",
        "\necdf_grades = ecdf(grades)\nlow = np.max(grades[ecdf_grades < threshold])\nhigh = np.min(grades[ecdf_grades >= threshold])\n",
        "\nnums = np.zeros(size, dtype=int)\nnums[:int(size*one_ratio)] = 1\nnp.random.shuffle(nums)\n",
        "\na_np = np.array(a)\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\na_np = np.array(a)\n",
        "\na_tf = tf.constant(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nindexes = np.argsort(a)[::-1][:N]\nresult = indexes\n",
        "\nresult = A ** n\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\nresult = np.array(result)\n",
        "\nresult = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        result.append(a[i:i+2, j:j+2])\nresult = np.array(result)\n",
        "\nresult = []\nfor i in range(0, a.shape[0], 2):\n    for j in range(0, a.shape[1], 2):\n        result.append(a[i:i+2, j:j+2])\nresult = np.array(result)\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size])\nresult = np.array(result)\n",
        "\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i, j] = a[i // 2, j // 3, i % 2 + j % 3]\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        result.append(a[i:i+patch_size, j:j+patch_size])\nresult = np.array(result)\n",
        "\nresult = a[:,low:high]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:,low:high]\n",
        "\na = np.array([float(x) for x in string.split()])\na = a.reshape((2,2))\n",
        "\nbase = 10\nsamples = np.random.uniform(min_val, max_val, n)\nresult = np.power(base, samples)\n",
        "\nlog_min = np.log(min_val)\nlog_max = np.log(max_val)\nlog_uniform_samples = np.exp(np.random.uniform(log_min, log_max, n))\n",
        "\n    log_min = np.log(min)\n    log_max = np.log(max)\n    log_uniform_samples = np.random.uniform(log_min, log_max, n)\n    result = np.exp(log_uniform_samples)\n    ",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a * A[0]\nfor i in range(1, len(A)):\n    B[i] = a * A[i] + b * B[i-1]\n",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nlinear_index = np.ravel_multi_index(index, dims, order='F')\nresult = linear_index - 1\n",
        "\nresult = np.ravel_multi_index(index, dims, order='C')\n",
        "\nvalues = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n",
        "\nresult = np.zeros(np.max(accmap) + 1)\nfor i in range(len(a)):\n    result[accmap[i]] += a[i]\n",
        "\nunique_index = np.unique(index)\nresult = np.zeros(len(unique_index))\nfor i in range(len(unique_index)):\n    result[i] = np.max(a[index == unique_index[i]])\n",
        "\naccmap_max = np.max(accmap)\nresult = np.zeros(accmap_max + 1)\nfor i in range(len(a)):\n    result[accmap[i]] += a[i]\nresult = result[:accmap_max + 1]\n",
        "\nunique_index = np.unique(index)\nresult = np.zeros(len(unique_index))\nfor i, idx in enumerate(unique_index):\n    result[i] = np.min(a[index == idx])\n",
        "\nimport numpy as np\nx = np.array(x)\ny = np.array(y)\nz = np.zeros_like(x)\nfor i in range(len(x)):\n    for j in range(len(x[0])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n",
        "\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\n",
        "\nresult = np.pad(a, ((max(0, -low_index), high_index - a.shape[0]), (max(0, -low_index), high_index - a.shape[1])), mode='constant', constant_values=0)\n",
        "\nx = x[x >= 0]\n",
        "\nresult = x[np.iscomplex(x)]\n",
        "\nnum_bins = int(np.ceil(len(data) / bin_size))\nbin_data = np.array_split(data, num_bins)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\nbin_data_max = np.max(bin_data, axis=1)\n",
        "\nn_bins = data.shape[1] // bin_size\nbin_data = np.array([data[:, i:i+bin_size] for i in range(0, n_bins*bin_size, bin_size)])\nbin_data_mean = np.mean(bin_data, axis=2)\n",
        "\nbin_data = []\nbin_data_mean = []\nfor i in range(len(data)//bin_size):\n    bin_data.append(data[-(i+1)*bin_size:][:bin_size])\n    bin_data_mean.append(np.mean(data[-(i+1)*bin_size:][:bin_size]))\n",
        "\nbin_data = []\nfor row in data:\n    row = row[::-1]\n    bins = [row[i:i+bin_size] for i in range(0, len(row), bin_size)]\n    bin_data.append(bins)\nbin_data_mean = np.array([[np.mean(bin) for bin in row] for row in bin_data])\n",
        "\ndef bin_and_mean(data, bin_size):\n    bin_data = []\n    bin_data_mean = []\n    for row in data:\n        row_bin_data = []\n        row_bin_data_mean = []\n        for i in range(len(row) // bin_size):\n            bin = row[i * bin_size: (i + 1) * bin_size]\n            row_bin_data.append(bin)\n            row_bin_data_mean.append(np.mean(bin))\n        bin_data.append(row_bin_data)\n        bin_data_mean.append(row_bin_data_mean)\n    return np.array(bin_data), np.array(bin_data_mean)\nbin_data, bin_data_mean = bin_and_mean(data, bin_size)\n",
        "\nresult = smoothclamp(x, x_min, x_max)\n",
        "\nresult = smoothclamp(x, N=N)\n",
        "\ndef circular_corr(a, b):\n    a_ext = np.concatenate((a, a))\n    b_ext = np.concatenate((b, b))\n    result = np.correlate(a_ext, b_ext, mode='valid')\n    return result\nresult = circular_corr(a, b)\n",
        "\nresult = df.to_numpy().reshape(4, 15, 5)\n",
        "\nresult = np.array(df.unstack().values.tolist()).reshape(15,4,5)\n",
        "\nresult = np.zeros((len(a), m), dtype=int)\nfor i, num in enumerate(a):\n    binary = format(num, f'0{m}b')\n    result[i] = [int(bit) for bit in binary]\n",
        "\nresult = np.zeros((len(a), m), dtype=int)\nfor i, num in enumerate(a):\n    binary = format(num, f'0{m}b')\n    result[i] = [int(bit) for bit in binary]\n",
        "\nresult = np.zeros((len(a), m), dtype=int)\nfor i, num in enumerate(a):\n    binary = format(num, f'0{m}b')\n    result[i] = [int(bit) for bit in binary]\nresult = np.array(result)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nstart = mean - 3 * std\nend = mean + 3 * std\nresult = (start, end)\n",
        "\nmean = np.mean(a)\nstd = np.std(a)\nstart = mean - 2 * std\nend = mean + 2 * std\nresult = (start, end)\n",
        "\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3 * std, mean + 3 * std)\n    ",
        "\nmean = np.mean(a)\nstd = np.std(a)\nupper_bound = mean + 2 * std\nlower_bound = mean - 2 * std\nresult = np.logical_or(a > upper_bound, a < lower_bound)\n",
        "\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\nprob = np.percentile(masked_data, percentile)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[1, :] = 0\na[:, 0] = 0\n",
        "\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(a.shape[0]), np.argmax(a, axis=1)] = True\n",
        "\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(len(a)), a.argmin(axis=1)] = True\n",
        "\npearson_coefficient, _ = np.polyfit(distance, post, 1)\nresult = pearson_coefficient\n",
        "\nresult = np.zeros((X.shape[1], X.shape[0], X.shape[0]))\nfor i in range(X.shape[1]):\n    result[i] = X[:, i].dot(X[:, i].T)\n",
        "\nX = np.zeros((3, 4))\nfor i in range(4):\n    X[:, i] = np.linalg.eigvals(Y[i]).real\n",
        "\nis_contained = number in a\n",
        "\nC = np.setdiff1d(A, B)\n",
        "\nC = np.array([x for x in A if x in B])\n",
        "\nmask = np.logical_or(np.logical_and(A >= B[0], A < B[1]), np.logical_and(A > B[0], A <= B[2]))\nC = A[mask]\n",
        "\na_rev = a[::-1]\nresult = rankdata(a_rev, method='ordinal').astype(int)\n",
        "\na_unique = np.unique(a)\na_unique_rank = rankdata(-a_unique, method='ordinal')\nresult = np.zeros(len(a))\nfor i, val in enumerate(a_unique):\n    result[a == val] = a_unique_rank[i]\n",
        "\n    a = np.array(a)\n    result = rankdata(a, method='min')\n    result = len(a) - result + 1\n    result = result.astype(int)\n    ",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\nresult = a[:, second, third]\n",
        "\narr = np.zeros((20,10,10,2))\n",
        "\nl1 = X.sum(axis=1)\nresult = X / l1.reshape(5,1)\n",
        "\nnorm_X = X / np.sqrt(np.sum(X**2, axis=1))[:, np.newaxis]\n",
        "\nx = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = X / x[:, np.newaxis]\n",
        "\nconditions = [df['a'].str.contains(target)]\ndf['result'] = np.select(conditions, choices, default='')\n",
        "\ndistances = squareform(pdist(a, 'euclidean'))\nresult = distances\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\nresult = squareform(pdist(a))\n",
        "\nfrom scipy.spatial.distance import pdist, squareform\nresult = squareform(pdist(a))\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=float)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A, dtype=np.float64)\nNA = np.where(np.isnan(NA), np.inf, NA)\nAVG = np.nanmean(NA, axis=0)\nprint(AVG)\n",
        "\nNA = NA.astype(np.float)\n",
        "\na = np.trim_zeros(a)\na = np.unique(a)\nresult = []\nfor i in range(len(a)-1):\n    if a[i] != a[i+1]:\n        result.append(a[i])\nresult.append(a[-1])\n",
        "\nresult = []\nprev_val = None\nfor i in range(len(a)):\n    if a[i][0] != 0 and a[i][0] != prev_val:\n        result.append(a[i])\n        prev_val = a[i][0]\nresult = np.array(result)\n",
        "\ndf = pd.DataFrame(columns=['lat', 'lon', 'val'])\nfor i in range(len(lat)):\n    for j in range(len(lat[0])):\n        df.loc[len(df)] = [lat[i][j], lon[i][j], val[i][j]]\n",
        "\n    lat_flat = lat.flatten()\n    lon_flat = lon.flatten()\n    val_flat = val.flatten()\n    df = pd.DataFrame({'lat': lat_flat, 'lon': lon_flat, 'val': val_flat})\n    ",
        "\ndata = np.concatenate((lat, lon, val), axis=1)\ndf = pd.DataFrame(data, columns=['lat', 'lon', 'val'])\ndf['maximum'] = df.max(axis=1)\n",
        "\ndef get_window(a, size):\n    result = []\n    for i in range(a.shape[0]):\n        for j in range(a.shape[1]):\n            window = a[max(0, i-size[0]//2):i+size[0]//2+1, max(0, j-size[1]//2):j+size[1]//2+1]\n            result.append(window)\n    return result\nresult = get_window(a, size)\n",
        "\ndef get_window(a, size):\n    result = []\n    for i in range(a.shape[0]):\n        for j in range(a.shape[1]):\n            window = a[max(0, i-size[0]+1):i+1, max(0, j-size[1]+1):j+1]\n            result.append(window)\n    return result\nresult = get_window(a, size)\n",
        "\nresult = np.mean(a)\n",
        "\n    result = np.mean(a)\n    ",
        "\nresult = Z[(*[slice(None, -1) for _ in range(Z.ndim)], -1)]\n",
        "\nresult = a[-1:]\n",
        "\nresult = c in CNTS\n",
        "\nresult = c in CNTS\n",
        "\nf = intp.interp2d(x_new, y_new, a, kind='linear')\nx_new = np.linspace(0, 2, 4*2)\ny_new = np.linspace(0, 2, 4*2)\nresult = f(x_new, y_new)\n",
        "\ndf[name] = df.groupby('D').Q.apply(lambda x: np.cumsum(x))\n",
        "\ni_diagonal = np.zeros((4, 4))\nnp.fill_diagonal(i_diagonal, i)\n",
        "\na[np.triu_indices_from(a, 1)] = 0\n",
        "\ndelta = (tf - t0) / (n - 1)\nresult = pd.date_range(start=t0, end=tf, periods=n)\n",
        "\nindex = -1\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        index = i\n        break\n",
        "\nindices = np.where((x == a) & (y == b))\nresult = np.array(indices).flatten()\n",
        "\nA = np.vander(x, 3)\nresult, residuals, rank, singular_values = np.linalg.lstsq(A, y, rcond=None)\n",
        "\nA = np.vander(x, degree+1)\nresult, residuals, rank, singular_values = np.linalg.lstsq(A, y, rcond=None)\nresult = result[::-1]\n",
        "\ndf = df.apply(lambda row: row - a, axis=1)\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(-1, 1)).reshape(a.shape)\n",
        "\nscaler = MinMaxScaler()\nresult = np.array([scaler.fit_transform(row.reshape(-1, 1)).reshape(1, -1)[0] for row in arr])\n",
        "\nscaler = MinMaxScaler()\nresult = np.array([scaler.fit_transform(matrix).tolist() for matrix in a])\n",
        "\narr_temp = arr.copy()\nmask = arr_temp < -10\nmask2 = arr_temp >= 15\nmask3 = np.logical_xor(mask, mask2)\narr[mask] = 0\narr[mask3] = arr_temp[mask3] + 5\narr[~mask2] = 30\n",
        "\nfor i in range(len(arr)):\n    mask = arr[i] < n1[i]\n    mask2 = arr[i] < n2[i]\n    mask3 = np.logical_xor(mask, mask2)\n    arr[i][mask] = 0\n    arr[i][mask3] = arr[i][mask3] + 5\n    arr[i][~mask2] = 30\n",
        "\nresult = np.nonzero(np.abs(s1 - s2) > 1e-12)[0].shape[0]\n",
        "\nresult = np.sum(np.isclose(s1, s2, equal_nan=True))\n",
        "\nresult = all(np.array_equal(a[0], arr) for arr in a)\n",
        "\nresult = all(np.isnan(arr).all() for arr in a)\n",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\npad_width = ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1]))\nresult = np.pad(a, pad_width, 'constant', constant_values=0)\n",
        "\npad_width = ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1]))\nresult = np.pad(a, pad_width, 'constant', constant_values=element)\n",
        "\n    result = np.zeros(shape)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n    ",
        "\npad_top = (shape[0] - a.shape[0]) // 2\npad_bottom = shape[0] - a.shape[0] - pad_top\npad_left = (shape[1] - a.shape[1]) // 2\npad_right = shape[1] - a.shape[1] - pad_left\nresult = np.pad(a, ((pad_top, pad_bottom), (pad_left, pad_right)), 'constant', constant_values=0)\n",
        "\na = a.reshape(int(a.shape[0]/3), 3)\n",
        "\nresult = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        result[i, j] = a[i, j, b[i, j]]\n",
        "\nresult = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        result[i, j] = a[i, j, b[i, j]]\n",
        "\nresult = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        result[i, j] = a[i, j, b[i, j]]\n",
        "\nresult = np.sum(a[np.arange(3)[:, None], np.arange(3), b])\n",
        "\nresult = np.sum(a[np.arange(3)[:,None], np.arange(3), b])\n",
        "\nmask = (df['a'] > 1) & (df['a'] <= 4)\ndf.loc[~mask, 'b'] = np.nan\nresult = df['b'].values.tolist()\n",
        "\nresult = []\nfor i in range(im.shape[1]):\n    if np.all(im[:,i] == 0):\n        continue\n    else:\n        result.append(im[:,i])\nresult = np.array(result).T\n",
        "\nrows, cols = np.where(A != 0)\nmin_row, max_row = min(rows), max(rows)\nmin_col, max_col = min(cols), max(cols)\nresult = A[min_row:max_row+1, min_col:max_col+1]\n",
        "\n# Check if the image is completely black\nif np.all(im == 0):\n    result = np.array([])\nelse:\n    # Remove the peripheral non-zeros that fill an entire row/column\n    rows_to_remove = np.where(np.all(im == 0, axis=1))[0]\n    cols_to_remove = np.where(np.all(im == 0, axis=0))[0]\n    result = np.delete(np.delete(im, rows_to_remove, axis=0), cols_to_remove, axis=1)\n",
        "\nresult = []\nfor i in range(im.shape[0]):\n    if not np.all(im[i] == 0):\n        result.append(im[i, np.nonzero(im[i])[0][0]:np.nonzero(im[i])[0][-1]+1])\nresult = np.array(result)\n"
    ],
    "Matplotlib": [
        "\nplt.scatter(x, y, label=\"x-y\")\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n",
        "\n# Get the current axis\nax = plt.gca()\n# Set minor ticks on y-axis\nax.yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\nax = plt.gca()\nax.minorticks_on()\n",
        "\nax = plt.gca()\nax.minorticks_on()\nax.xaxis.set_minor_locator(plt.AutoMinorLocator())\n",
        "\nfor ls in ['-', '--', '-.', ':']:\n    y = np.random.rand(10)\n    plt.plot(x, y, ls)\nplt.show()\n",
        "\nfor ls in ['-', '--', '-.', ':']:\n    y = np.random.rand(10)\n    plt.plot(x, y, ls)\nplt.show()\n",
        "\nplt.plot(x, y, marker='D', markersize=4, linewidth=1, color='blue')\nplt.show()\n",
        "\nplt.plot(x, y, marker='D', markersize=10, linewidth=2)\nplt.show()\n",
        "\nax.set_ylim(0, 40)\n",
        "\nplt.axvspan(2, 4, color='red', alpha=0.5)\n",
        "\nplt.plot([0, 1], [0, 2])\nplt.show()\n",
        "\nplt.plot([0, 1], [0, 2])\nplt.show()\n",
        "\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df)\n",
        "\nsns.set()\nplt.plot(x, y)\nplt.show()\n",
        "\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', linewidth=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.plot(x, y)\nplt.legend(title='xyz', title_fontsize=20)\nplt.show()\n",
        "\nl.set_markerfacecolor((1, 1, 1, 0.2))\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30, markeredgecolor='black')\nplt.show()\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"red\")\nplt.show()\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=-45)\n",
        "\nplt.xticks(np.arange(0, 2 * np.pi, 2))\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H, cmap='RdBu')\nplt.colorbar()\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.xlabel(\"X\")\nplt.xlim(0, 2 * np.pi)\nplt.xticks([0, np.pi, 2 * np.pi], [\"0\", \"$\\pi$\", \"$2\\pi$\"])\n",
        "\nfor tick in g.get_xticklabels():\n    tick.set_rotation(90)\n",
        "\ndef wrap_title(title, max_width=40):\n    return \"\\n\".join(textwrap.wrap(title, max_width))\nmyTitle = \"Some really really long long long title I really really need - and just can't - make it any - simply any - shorter - at all.\"\nwrapped_title = wrap_title(myTitle)\nplt.title(wrapped_title)\nplt.show()\n",
        "\nplt.plot(x, -y)\nplt.ylim(0, 2)\nplt.xlabel('x')\nplt.ylabel('-y')\nplt.title('Upside down plot')\nplt.show()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n# plot x, then y then z, but so that x covers y and y covers z\nplt.fill_between(np.arange(10), x, y, alpha=0.5)\nplt.fill_between(np.arange(10), y, z, alpha=0.5)\nplt.plot(x, 'o-', label='x')\nplt.plot(y, 'o-', label='y')\nplt.plot(z, 'o-', label='z')\nplt.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolor='black', facecolor='blue')\nplt.show()\n",
        "\nplt.xticks(x)\nplt.yticks(np.arange(0, 2.1, 0.5))\n",
        "\nplt.ticklabel_format(style='plain', axis='y')\n",
        "\n# Set the linestyle to '--'\nax.lines[0].set_linestyle('--')\n",
        "\nfig, axs = plt.subplots(2, sharex=True)\naxs[0].plot(x, y1)\naxs[0].set_title('Sine Wave')\naxs[1].plot(x, y2)\naxs[1].set_title('Cosine Wave')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, sharex=True)\naxs[0].plot(x, y1)\naxs[0].set_title('Sine Wave')\naxs[0].set_ylabel('y1')\naxs[0].set_frame_on(False)\naxs[1].plot(x, y2)\naxs[1].set_title('Cosine Wave')\naxs[1].set_ylabel('y2')\naxs[1].set_frame_on(False)\nplt.setp(axs, xticks=[], yticks=[])\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.sin(x)\ndf = pd.DataFrame({\"x\": x, \"y\": y})\nsns.lineplot(x=\"x\", y=\"y\", data=df, xlabel='')\nplt.show()\n",
        "\nplt.xticks([])\n",
        "\nplt.xticks([3, 4])\nplt.axvline(3, color='gray', linestyle='--')\nplt.axvline(4, color='gray', linestyle='--')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y', color='grey', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "\nplt.yticks([3, 4])\nplt.axhline(y=3, color='gray', linestyle='--', alpha=0.5)\nplt.axhline(y=4, color='gray', linestyle='--', alpha=0.5)\nplt.xticks([1, 2])\nplt.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\nplt.axvline(x=2, color='gray', linestyle='--', alpha=0.5)\n",
        "\nplt.grid()\n",
        "\nplt.legend(loc='lower right')\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\nplt.show()\nplt.clf()\n",
        "\nx = np.arange(10)\ny = np.arange(10, 20)\nz = np.arange(10)\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\nplt.show()\n",
        "\nax.xaxis.tick_top()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", labelpad=20)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().invert_yaxis()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.gca().yaxis.tick_left()\nplt.gca().yaxis.set_label_position(\"right\")\nplt.show()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"green\", marginal_kws=dict(color=\"blue\"))\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"g\", joint_kws={\"color\":\"g\"})\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg')\n",
        "\nplt.bar(df['celltype'], df['s1'], label='s1')\nplt.bar(df['celltype'], df['s2'], bottom=df['s1'], label='s2')\nplt.xticks(rotation=0)\nplt.xlabel('celltype')\nplt.ylabel('values')\nplt.legend()\nplt.show()\n",
        "\nplt.bar(df['celltype'], df['s1'], label='s1')\nplt.bar(df['celltype'], df['s2'], bottom=df['s1'], label='s2')\nplt.xticks(rotation=45)\nplt.xlabel('celltype')\nplt.ylabel('values')\nplt.legend()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('X')\nplt.xticks(color='r')\nplt.xlabel('X', color='r')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.axhline(color=\"red\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(fontsize=10, rotation='vertical')\nplt.yticks(fontsize=10)\nplt.show()\n",
        "\nfor x in [0.22058956, 0.33088437, 2.20589566]:\n    plt.axvline(x, color='r', linestyle='--')\n",
        "\nfig, ax = plt.subplots()\nim = ax.imshow(rand_mat, cmap='YlOrRd')\nax.set_xticks(numpy.arange(len(xlabels)))\nax.set_yticks(numpy.arange(len(ylabels)))\nax.set_xticklabels(xlabels)\nax.set_yticklabels(ylabels[::-1])\nax.tick_params(top=True, bottom=False,\n                   labeltop=True, labelbottom=False)\ncbar = ax.figure.colorbar(im, ax=ax)\nplt.show()\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nh1, l1 = ax.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\nax.legend(h1+h2, l1+l2, loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title(\"Y\")\naxs[1].plot(x, y)\naxs[1].set_title(\"Y\")\nplt.show()\n",
        "\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, s=30)\n",
        "\nplt.scatter(b, a)\nfor i, txt in enumerate(c):\n    plt.annotate(txt, (b[i], a[i]))\nplt.xlabel('b')\nplt.ylabel('a')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"y over x\")\nplt.show()\n",
        "\nplt.plot(x, y, label='y over x')\nplt.legend(title='Legend', title_fontsize='large', fontsize='medium')\nplt.title('y over x', fontweight='bold')\nplt.show()\n",
        "\nplt.hist(x, edgecolor='black', linewidth=1.2)\nplt.show()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw={'width_ratios': [3, 1]})\n",
        "\nplt.hist(x, bins, alpha=0.5)\nplt.hist(y, bins, alpha=0.5)\nplt.show()\n",
        "\nplt.hist([x, y], bins=10, histtype='bar', stacked=True, label=['x', 'y'])\nplt.legend(loc='upper right')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Grouped Histogram of x and y')\n",
        "\nplt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nim1 = ax1.imshow(x, cmap='Reds')\nim2 = ax2.imshow(y, cmap='Blues')\nfig.colorbar(im1, ax=[ax1, ax2], label='Colorbar')\n",
        "\nfor i in range(x.shape[1]):\n    plt.plot(x[:, i], label=['a', 'b'][i])\nplt.legend()\nplt.show()\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_xlabel('X')\naxs[0].set_ylabel('Y')\naxs[0].set_title('Y over X')\naxs[1].plot(a, z)\naxs[1].set_xlabel('A')\naxs[1].set_ylabel('Z')\naxs[1].set_title('Z over A')\nfig.suptitle('Y and Z')\nplt.show()\n",
        "\nx, y = zip(*points)\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title('Title', fontsize=20)\nplt.xlabel('X-axis', fontsize=18)\nplt.ylabel('Y-axis', fontsize=16)\nplt.show()\n",
        "\nax.plot(x, y)\nax.set_xticks(x)\nax.set_yticks(y)\nax.set_xticklabels(x+1)\nax.set_yticklabels(y+1)\n",
        "\nfor i, line in enumerate(lines):\n    x, y = zip(*line)\n    plt.plot(x, y, c=c[i])\nplt.show()\n",
        "\nplt.loglog(x, y)\nplt.xticks([1, 10, 100])\nplt.yticks([1, 10, 100])\nplt.show()\n",
        "\nfor col in df.columns:\n    plt.plot(df.index, df[col], label=col)\n    plt.scatter(df.index, df[col])\nplt.legend()\nplt.show()\n",
        "\n# Normalize the data to sum up to 1\ndata_norm = np.array(data) / np.sum(data)\n# Make a histogram of the normalized data\nplt.hist(data_norm, bins=len(data))\n# Set y tick labels as percentage\nplt.yticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n           ['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n# Set x label as \"Value\" and y label as \"Frequency (%)\"\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency (%)\")\n",
        "\nplt.plot(x, y, marker='o', alpha=0.5, ls='-')\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y, label='y')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('y over x')\nax2.plot(z, a, label='a')\nax2.set_xlabel('z')\nax2.set_ylabel('a')\nax2.set_title('a over z')\nhandles, labels = ax1.get_legend_handles_labels()\nhandles2, labels2 = ax2.get_legend_handles_labels()\nfig.legend(handles + handles2, labels + labels2, loc='upper center')\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=False)\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nfig, ax = plt.subplots(1, 1)\nplt.xlim(1, 10)\nplt.xticks(range(1, 10), [''] + ['second' if i == 1 else i for i in range(2, 10)])\nax.plot(y, x)\nplt.show()\n",
        "\nplt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()\n",
        "\nextra_ticks = [2.1, 3, 7.6]\nxticks = list(plt.xticks()[0])\nxticks.extend(extra_ticks)\nxticks = sorted(xticks)\nplt.xticks(xticks)\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.xticks(rotation=90, ha='right')\nplt.yticks(rotation=-60)\n",
        "\nfor label in plt.gca().get_xticklabels():\n    label.set_alpha(0.5)\n",
        "\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n",
        "\nplt.gca().margins(x=0.01)\nplt.gca().margins(y=0.05)\nplt.gca().yaxis.set_tick_params(which='major', direction='in')\nplt.gca().xaxis.set_tick_params(which='major', direction='in')\nplt.gca().yaxis.set_ticks_position('left')\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(True)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().spines['left'].set_position(('outward', 10))\nplt.gca().spines['bottom'].set_position(('outward', 10))\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\nfor i in range(2):\n    axs[i].plot(x, y)\nplt.suptitle(\"Figure\")\nplt.show()\n",
        "\ndf.plot(kind='line', xlabel='X', ylabel='Y')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='\\\\', hatch='//', alpha=0.5)\n",
        "\nplt.scatter(x, y, edgecolor='none', marker='|')\n",
        "\nplt.scatter(x, y, marker='*', hatch='//')\nplt.show()\n",
        "\nplt.scatter(x, y, s=100, marker='*', hatch='*//')\n",
        "\nplt.xlim(1, 5)\nplt.ylim(1, 4)\nplt.imshow(data, cmap='hot', interpolation='nearest')\nplt.colorbar()\n",
        "\nplt.stem(x, y, use_line_collection=True, basefmt=\" \", orientation=\"horizontal\")\nplt.show()\n",
        "\nplt.bar(d.keys(), d.values(), color=[c[key] for key in d.keys()])\nplt.xticks(d.keys())\nplt.show()\n",
        "\nplt.axvline(x=3, label='cutoff', linestyle='--')\nplt.legend()\n",
        "\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.bar(labels, height)\n",
        "\nplt.pie(data, labels=l, wedgeprops={'width': 0.4})\nplt.axis('equal')\nplt.show()\n",
        "\nplt.plot(x, y, color='blue', linestyle='--')\nplt.grid(color='blue', linestyle='--')\nplt.show()\n",
        "\nplt.minorticks_on()\nplt.grid(b=True, which='minor', color='gray', linestyle=':')\nplt.grid(b=False, which='major')\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\n# Add bold font to labels\nfor label in labels:\n    plt.text(1.1, 1.1, label, ha='center', va='center', fontweight='bold')\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\n# Add bold font to labels\nfor label in labels:\n    plt.text(1.1, 1.1, label, ha='center', va='center', fontweight='bold')\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markerfacecolor='none', markeredgecolor='blue', markeredgewidth=2, linestyle='-', color='blue')\nplt.show()\n",
        "\nplt.axvline(x=55, color=\"green\")\n",
        "\nx = np.arange(len(blue_bar))\nwidth = 0.35\nfig, ax = plt.subplots()\nblue_rects = ax.bar(x - width/2, blue_bar, width, label='Blue')\norange_rects = ax.bar(x + width/2, orange_bar, width, label='Orange')\nax.set_xticks(x)\nax.set_xticklabels(['A', 'B', 'C'])\nax.set_ylabel('Value')\nax.set_title('Blue vs. Orange')\nax.legend()\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 1)\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\naxs[1].plot(x, y)\naxs[1].plot(a, z)\n",
        "\ncmap = matplotlib.cm.get_cmap('Spectral')\nnorm = matplotlib.colors.Normalize(vmin=min(y), vmax=max(y))\ncolors = [cmap(norm(value)) for value in y]\nplt.scatter(x, y, c=colors)\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(10))\nplt.show()\n",
        "\ng = sns.catplot(x=\"sex\", y=\"bill_length_mm\", data=df, kind=\"bar\", col=\"species\", sharey=False)\ng.set_titles(\"{col_name}\")\n",
        "\ncircle = plt.Circle((0.5, 0.5), 0.2)\nax = plt.gca()\nax.add_artist(circle)\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\bf{\\phi}$', fontsize=18)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.1)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\nplt.show()\n",
        "\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n            ncol=2, fancybox=True, shadow=True)\n",
        "\nplt.legend()\nplt.plot(x[4], y[4], marker=\"o\", color=\"red\")\nplt.plot(x[7], y[7], marker=\"o\", color=\"green\")\n",
        "\nplt.imshow(data, cmap='viridis')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Figure 1\", fontweight=\"bold\", fontsize=16)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n",
        "\nsns.pairplot(data=df, x_vars=[\"x\"], y_vars=[\"y\"], hue=\"id\", palette=\"Set1\", legend=False)\n",
        "\nplt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n",
        "\nplt.scatter(x, y)\nplt.axis('off')\nplt.axis('tight')\nplt.axis('image')\n",
        "\nplt.scatter(x, y, s=100, c='red', edgecolor='black')\n",
        "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor row in range(2):\n    for col in range(2):\n        axs[row, col].plot(x, y)\n",
        "\nplt.hist(x, bins=5, range=(0, 10), align='left', rwidth=2)\nplt.xlabel('x')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n",
        "\nplt.plot(x, y, 'o-')\nplt.fill_between(x, y-error, y+error, alpha=0.2)\n",
        "\nplt.axhline(y=0, color='white', linestyle='-')\nplt.axvline(x=0, color='white', linestyle='-')\n",
        "\nfor i in range(len(box_position)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], ecolor=c[i], elinewidth=2, capsize=3)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title(\"Y\")\nax2.plot(a, z)\nax2.set_title(\"Z\")\nax2.title.set_position([0.5, 1.05])\nplt.show()\n",
        "\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\nplt.tight_layout()\nplt.show()\n",
        "\nplt.figure(figsize=(8, 8))\nplt.matshow(d)\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.table(cellText=df.values, colLabels=df.columns, loc=\"center\", bbox=[0, 0, 1, 1])\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(rotation=45)\nplt.tick_params(axis='x', which='both', top=True, bottom=True, labeltop=True, labelbottom=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(rotation=90)\nplt.tick_params(axis='x', top=True, bottom=True, labeltop=True, labelbottom=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(x, labels=x, rotation=90)\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True, labeltop=False)\nplt.show()\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.set_titles(\"Group: {col_name}\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\ng.set_xlabels(\"Exercise Time\")\ng.set_ylabels(\"Pulse\")\n",
        "\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\", height=4, aspect=1, ylabel=False)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\nplt.show()\n",
        "\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.xlabel('t')\nplt.ylabel('y')\nplt.title('Plot of sin(t), cos(t), and sin(t) + cos(t)')\nplt.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df)\nplt.legend([], [], frameon=False)\nplt.show()\n",
        "\ng = sns.FacetGrid(df, row=\"b\", hue=\"b\", aspect=4)\ng.map(sns.pointplot, \"a\", \"c\")\nfor ax in g.axes.flat:\n    ax.set_xticks(np.arange(1, 31, 1))\n    ax.set_xticklabels(np.arange(2, 32, 2))\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(elev=50, azim=100)\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tick_params(axis=\"both\", which=\"both\", bottom=False, left=False, labelbottom=True, labelleft=True)\nplt.show()\n",
        "\ngs = gridspec.GridSpec(nrow, ncol, wspace=0, hspace=0)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x)\n        ax.set_xticks([])\n        ax.set_yticks([])\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nnum_classes = 10\none_hot_labels = tf.one_hot(labels, num_classes)\nresult = tf.cast(one_hot_labels, tf.int32)\n",
        "\none_hot_labels = tf.one_hot(labels, depth=10, dtype=tf.int32)\nresult = tf.reduce_sum(one_hot_labels, axis=0)\n",
        "\none_hot_labels = tf.one_hot(labels, depth=10)\nresult = tf.cast(one_hot_labels, tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ",
        "\none_hot_labels = tf.one_hot(labels, depth=10)\nresult = tf.cast(tf.reverse(one_hot_labels, axis=[1]), tf.int32)\n",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [[i, i+1, i+2]]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nresult = [i for sublist in result for i in sublist]\nprint(result)\n",
        "\n    def my_map_func(i):\n        return [[i, i+1, i+2]]\n    ds = tf.data.Dataset.from_tensor_slices(input)\n    ds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n        func=my_map_func, inp=[input], Tout=[tf.int64]\n    ))\n    element = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n    result = []\n    with tf.compat.v1.Session() as sess:\n        for _ in range(9):\n            result.append(sess.run(element))\n    result = [int(x) for sublist in result for x in sublist]\n    ",
        "\nmax_length = max(lengths)\nmasks = []\nfor length in lengths:\n    mask = tf.concat([tf.ones(length), tf.zeros(max_length - length)], axis=0)\n    masks.append(mask)\nresult = tf.stack(masks)\n",
        "\nmax_length = 8\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\nresult = tf.reverse(mask, axis=[1])\n",
        "\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n",
        "\nimport tensorflow as tf\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    max_len = 8\n    result = tf.sequence_mask(lengths, max_len, dtype=tf.int32)\n    return result\n",
        "\nmask = tf.sequence_mask(lengths, max_len, dtype=tf.float32)\n",
        "\nimport itertools\nresult = tf.constant(list(itertools.product(a.numpy(), b.numpy())))\n",
        "\n    a = tf.expand_dims(a, axis=1)\n    b = tf.expand_dims(b, axis=0)\n    result = tf.reshape(a * b, [-1])\n    ",
        "\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\nnew_shape = tf.concat([tf.shape(a)[:-1], [1], tf.shape(a)[-1:]], axis=0)\nresult = tf.reshape(a, new_shape)\n",
        "\nnew_shape = (1, 50, 100, 1, 512)\nresult = tf.reshape(a, new_shape)\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\nl2_distance = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\ndiff = tf.square(tf.subtract(a, b))\nresult = tf.reduce_sum(diff, axis=0)\n",
        "\n    l2_dist = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n    ",
        "\nm = tf.gather_nd(x, tf.stack((y, z), axis=1))\n",
        "\nm = tf.gather_nd(x, tf.stack((row, col), axis=1))\n",
        "\n    result = tf.gather_nd(x, tf.stack((y, z), axis=1))\n    ",
        "\nC = tf.einsum('bik,bjk->bij', A, B)\n",
        "\nC = tf.einsum('bik,bjk->bij', A, B)\n",
        "\nx = tf.constant(x)\nresult = tf.strings.unicode_decode(x, 'UTF-8')\n",
        "\n    result = [tf.strings.unicode_decode(x_i, 'UTF-8').numpy() for x_i in x]\n    ",
        "\nmask = tf.cast(tf.reduce_max(x, axis=-1) > 0, tf.float32)\nmask = tf.expand_dims(mask, -1)\nx_masked = tf.multiply(x, mask)\nresult = tf.reduce_sum(x_masked, axis=-2) / tf.reduce_sum(mask, axis=-2)\n",
        "\nmask = tf.cast(tf.reduce_max(x, axis=-1) != 0, tf.float32)\nmask = tf.expand_dims(mask, -1)\nx_masked = x * mask\nx_mean = tf.reduce_sum(x_masked, axis=-2) / tf.reduce_sum(mask, axis=-2)\nresult = tf.reduce_mean(tf.square(x_mean - tf.reduce_mean(x_mean, axis=-2, keepdims=True)), axis=-1)\n",
        "\n    mask = tf.cast(tf.not_equal(x, 0), tf.float32)\n    masked_x = tf.multiply(x, mask)\n    sum_x = tf.reduce_sum(masked_x, axis=-1)\n    sum_mask = tf.reduce_sum(mask, axis=-1)\n    result = tf.divide(sum_x, sum_mask)\n    ",
        "\nwith Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    result = tf.argmax(a, axis=1)\n    ",
        "\nresult = tf.argmin(a, axis=1)\n",
        "\n#Save the model in \"export/1\"\ntf.saved_model.save(model, \"export/1\")\n",
        "\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n",
        "\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nseed_x = 10\ntf.random.set_seed(seed_x)\ndist = tfp.distributions.Uniform(low=2, high=6)\nresult = dist.sample(114)\n",
        "\nimport tensorflow as tf\ndef f(seed_x=10):\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n    return result\n",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nA, B = scipy.optimize.curve_fit(lambda x, A, B: A * np.log(x) + B, x, y)[0]\nresult = np.array([A, B])\n",
        "\ndef func(x, A, B):\n    return A + B * np.log(x)\npopt, pcov = scipy.optimize.curve_fit(func, x, y)\nresult = popt\n",
        "\ndef f(x, A, B, C):\n    return A * np.exp(B * x) + C\nresult, _ = scipy.optimize.curve_fit(f, x, y, p0=p0)\n",
        "\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\ntest_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n",
        "\nresult = optimize.minimize(f, initial_guess)\n",
        "\np_values = scipy.stats.norm.cdf(z_scores)\n",
        "\np_values = []\nfor z in z_scores:\n    p_value = scipy.stats.norm.cdf(z, loc=mu, scale=sigma)\n    p_values.append(p_value)\n",
        "\nz_scores = scipy.stats.norm.ppf(1 - np.array(p_values))\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nresult = sa.dot(sb)\n",
        "\n    result = sA.dot(sB)\n    ",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ncenter = np.array([data_orig.shape[0]//2, data_orig.shape[1]//2])\nshift = center - np.array([x0,y0])\nshift_rot = np.dot(rotate(np.array([[1,0],[0,1]], dtype=float), angle, reshape=False), shift)\nxrot, yrot = center + shift_rot\n",
        "\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, 'uniform', args=(0, T))\n",
        "\n    uniform_times = np.random.uniform(0, T, len(times))\n    result = stats.ks_2samp(times, uniform_times)\n    ",
        "\nkstest_result = stats.kstest(times, 'uniform', args=(0, T))\nresult = kstest_result[1] >= 0.95\n",
        "\nFeature = sparse.hstack((c1, c2))\n",
        "\nFeature = sparse.hstack((c1, c2))\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\ndef objective(x):\n    return np.sum(np.sqrt(np.sum((points1 - points2[x])**2, axis=1)))\nx0 = np.arange(N)\nresult = scipy.optimize.minimize(objective, x0, method='SLSQP', constraints={'type': 'eq', 'fun': lambda x: np.unique(x, return_counts=True)[1] - 1}).x\nresult = np.round(result).astype(int)\n",
        "\ndef objective(assignment):\n    return np.sum(np.sqrt(np.sum((points1[assignment] - points2)**2, axis=1)))\ndef constraint(assignment):\n    return np.unique(assignment).size - assignment.size\nassignment = np.zeros(N, dtype=int)\nresult = scipy.optimize.minimize(objective, assignment, constraints={'type': 'eq', 'fun': constraint})\nresult = np.asarray(result.x, dtype=int)\n",
        "\nb.setdiag(0)\nb.eliminate_zeros()\n",
        "\nlabels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n",
        "\nlabels, n_labels = ndimage.label(img < threshold)\nresult = n_labels\n",
        "\nlabels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n",
        "\n# Find the regions of cells which value exceeds a given threshold\nlabel_im, nb_labels = ndimage.label(img > threshold)\n# Determine the distance between the center of mass of such regions and the top left corner\ndistances = []\nfor i in range(1, nb_labels + 1):\n    y, x = ndimage.measurements.center_of_mass(img, labels=label_im, index=i)\n    distance = np.sqrt(x**2 + y**2)\n    distances.append(distance)\nresult = distances\n",
        "\nM = M + M.T\n",
        "\n    sA[triu(sA, 1).nonzero()] = 0\n    sA = sA + tril(sA, -1).T\n    ",
        "\nstruct = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\neroded = scipy.ndimage.binary_erosion(square, structure=struct)\ndilated = scipy.ndimage.binary_dilation(eroded, structure=struct)\nsquare[dilated] = 0\n",
        "\nisolated_cells = np.zeros_like(square, dtype=bool)\nfor i in range(square.shape[0]):\n    for j in range(square.shape[1]):\n        if square[i, j] != 0 and np.sum(square[max(0, i-1):i+2, max(0, j-1):j+2]) == 5:\n            isolated_cells[i, j] = True\nsquare[isolated_cells] = 0\n",
        "\nmean = col.mean()\nstandard_deviation = col.std()\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nMedian = np.median(col.toarray().flatten())\nMode = mode(col.toarray().flatten())[0][0]\n",
        "\ndef fourier(x, *a):\n    ret = a[0] * np.cos(1 * np.pi / tau * x)\n    for i in range(1, degree):\n        ret += a[i] * np.cos((i+1) * np.pi / tau * x)\n    return ret\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*(degree+1))\n",
        "\n# Create a list of unique IDs\nunique_ids = np.unique(example_array)\n# Create a list of all possible combinations of unique IDs\nid_combinations = list(combinations(unique_ids, 2))\n# Calculate pairwise distances between all regions\nresult = []\nfor id1, id2 in id_combinations:\n    # Get the indices of the regions with the given IDs\n    id1_indices = np.argwhere(example_array == id1)\n    id2_indices = np.argwhere(example_array == id2)\n    \n    # Calculate the pairwise distances between all cells in the regions\n    distances = scipy.spatial.distance.cdist(id1_indices, id2_indices, 'euclidean')\n    \n    # Get the minimum distance between the regions\n    min_distance = np.min(distances)\n    \n    # Add the result to the output list\n    result.append((id1, id2, min_distance))\nprint(result)\n",
        "\nunique_ids = np.unique(example_array)\nunique_ids = unique_ids[unique_ids != 0]\n# Create a list of tuples containing the unique IDs and their corresponding coordinates\nid_coords = []\nfor id in unique_ids:\n    id_coords.append((id, np.argwhere(example_array == id)))\n# Calculate pairwise Manhattan distances between all regions\ndistances = []\nfor i in range(len(id_coords)):\n    for j in range(i+1, len(id_coords)):\n        dist = scipy.spatial.distance.cdist(id_coords[i][1], id_coords[j][1], metric='cityblock')\n        distances.append((id_coords[i][0], id_coords[j][0], np.min(dist)))\nresult = np.array(distances)\n",
        "\n    unique_ids = np.unique(example_array)\n    unique_ids = unique_ids[unique_ids != 0]\n    pairwise_distances = []\n    for id1, id2 in combinations(unique_ids, 2):\n        mask1 = example_array == id1\n        mask2 = example_array == id2\n        coords1 = np.argwhere(mask1)\n        coords2 = np.argwhere(mask2)\n        distances = scipy.spatial.distance.cdist(coords1, coords2, metric='euclidean')\n        min_distance = np.min(distances)\n        pairwise_distances.append((id1, id2, min_distance))\n    result = np.array(pairwise_distances)\n    ",
        "\nfrom scipy import interpolate\nimport numpy as np\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85.36, 93.24],\n              [61.48, 69.31, 75.36, 81.87, 89.35],\n              [57.61, 65.75, 71.7, 79.1, 86.13],\n              [55.12, 63.34, 69.32, 77.29, 83.88],\n              [54.58, 62.54, 68.7, 76.72, 82.92],\n              [56.58, 63.87, 70.3, 77.69, 83.53],\n              [61.67, 67.79, 74.41, 80.43, 85.86],\n              [70.08, 74.62, 80.93, 85.06, 89.84]])\nx_val = np.linspace(-1, 1, 100)\nresult = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k=2, s=4)\n    result[i] = interpolate.splev(x_val, tck)\nprint(result)\n",
        "\nsamples = [x1, x2, x3, x4]\nstatistic, critical_values, significance_level = ss.anderson_ksamp(samples)\n",
        "\nresult = ss.anderson_ksamp([x1, x2])\n",
        "\ndf['AB'] = pd.rolling_apply(df['A'], 3, lambda x: tau1(x, df['B']))\ndf['AC'] = pd.rolling_apply(df['A'], 3, lambda x: tau2(x, df['C']))\ndf['BC'] = pd.rolling_apply(df['B'], 3, lambda x: tau3(x, df['C']))\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = len(sa.nonzero()[0]) == 0\n",
        "\nresult = block_diag(*a)\n",
        "\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\n",
        "\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    ",
        "\nn = len(a)\nmean = np.mean(a)\nstd = np.std(a, ddof=1)\nkurtosis_result = np.sum(((a - mean) / std) ** 4) / n\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=True)\n",
        "\nf = scipy.interpolate.interp2d(s, t, z, kind='cubic')\ns_vals = np.array([-0.5, 0.25])\nt_vals = np.array([-1.5, -0.75])\nresult = f(s_vals, t_vals)\n",
        "\n    f = scipy.interpolate.interp2d(x, y, z, kind='cubic')\n    result = f(s, t)\n    ",
        "\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nresult = count_points_in_regions(vor, extraPoints)\n",
        "\nimport numpy as np\ndef find_region(vor, point):\n    for i, region in enumerate(vor.regions):\n        if -1 not in region and scipy.spatial.Voronoi.point_region(vor, point) == i:\n            return i\n    return None\ndef count_points_in_region(vor, extraPoints):\n    result = np.zeros(len(vor.points))\n    for point in extraPoints:\n        region = find_region(vor, point)\n        if region is not None:\n            result[region] += 1\n    return result\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nresult = count_points_in_region(vor, extraPoints)\nprint(result)\n",
        "\nsparse_matrix = sparse.lil_matrix((len(vectors), max_vector_size))\nfor i, vector in enumerate(vectors):\n    sparse_matrix[i, :len(vector)] = vector\nresult = sparse_matrix.tocsr()\n",
        "\nimport numpy as np\nimport scipy.ndimage\na = np.random.binomial(n=1, p=1/2, size=(9, 9))\nb = scipy.ndimage.median_filter(a, 3, origin=[1, 1])\nprint(b)\n",
        "\nrow_vector = M.getrow(row)\nresult = row_vector[0, column]\n",
        "\nresult = []\nfor i in range(len(row)):\n    result.append(M[row[i], column[i]])\n",
        "\nnew_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[:, i, j])\n        new_array[:, i, j] = f(x_new)\n",
        "\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nprob = NormalDistro(u,o2,x)\n",
        "\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\ndef f(x = 2.5, u = 1, o2 = 3):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n",
        "\ndct_matrix = np.zeros((N, N))\nfor k in range(N):\n    for n in range(N):\n        if k == 0:\n            dct_matrix[k, n] = 1 / np.sqrt(N)\n        else:\n            dct_matrix[k, n] = np.sqrt(2 / N) * np.cos((np.pi * k * (2 * n + 1)) / (2 * N))\n",
        "\nresult = sparse.dia_matrix((matrix, [-1, 0, 1]), shape=(5, 5)).toarray()\n",
        "\nM = np.zeros((N+1, N+1))\nfor i in range(N+1):\n   for j in range(i+1):\n      M[i,j] = scipy.stats.binom.pmf(j, i, p)\n",
        "\nresult = df.apply(stats.zscore, axis=1, result_type='broadcast')\n",
        "\nresult = df.apply(stats.zscore)\n",
        "\nzscore = stats.zscore(df, axis=1, ddof=1)\nresult = pd.concat([df, zscore], axis=1, keys=['data', 'zscore'])\n",
        "\nzscore = pd.DataFrame(stats.zscore(df, axis=1, nan_policy='omit'), index=df.index, columns=df.columns)\nresult = pd.concat([df, zscore], keys=['data', 'zscore'], axis=1)\nresult = result.round(3)\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([shape[0] // 2, shape[1] // 2])\ny, x = np.indices(shape)\nresult = distance.cdist(np.dstack((y, x)), mid)\n",
        "\nmid = np.array([shape[0] // 2, shape[1] // 2])\ny, x = np.mgrid[:shape[0], :shape[1]]\nresult = np.abs(y - mid[0]) + np.abs(x - mid[1])\n",
        "\n    mid = np.array([shape[0]//2, shape[1]//2])\n    y, x = np.mgrid[:shape[0], :shape[1]]\n    result = distance.cdist(np.dstack((y, x)), mid[None, :])\n    ",
        "\nzoom_factor = (shape[0] / x.shape[0], shape[1] / x.shape[1])\nresult = scipy.ndimage.zoom(x, zoom_factor, order=1)\n",
        "\ndef func(x, a):\n    return a.dot(x ** 2)\ndef residual(x, a, y):\n    return np.sum((y - func(x, a)) ** 2)\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\n",
        "\ndef func(x, a):\n    return a.dot(x ** 2)\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\nout = scipy.optimize.minimize(residual, x0, args=(a, y), bounds=[(lb, None) for lb in x_lower_bounds], method='L-BFGS-B')\n",
        "\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\n",
        "\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\n",
        "\nfor t in range(4):\n    def const(x, t=t):\n        return x[t]\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sparse.vstack([sa, sb])\n",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\ndef integrand(x):\n    return 2 * c * x\nresult, error = scipy.integrate.quad(integrand, low, high)\n",
        "\nimport scipy.integrate\ndef f(c=5, low=0, high=1):\n    def integrand(x):\n        return 2 * c * x\n    result, error = scipy.integrate.quad(integrand, low, high)\n    return result\n",
        "\nfor key in V.keys():\n    V[key] += x\n",
        "\nV_new = V.tocsr()\nV_new.data += x\n",
        "\nA = V + x\nB = A + y\n",
        "\nfor Col in range(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    sa[:,Col] = sa[:,Col] / Len\n",
        "\nfor col in range(sa.shape[1]):\n    column = sa[:, col].toarray().flatten()\n    list_ = [x**2 for x in column]\n    len_ = math.sqrt(sum(list_))\n    sa[:, col] = sparse.csr_matrix((1 / len_) * column)\n",
        "\na[a > 0] = 1\n",
        "\na[a > 0] = 1\n",
        "\ndistances = scipy.spatial.distance.cdist(centroids, data)\nresult = np.argmin(distances, axis=1)\n",
        "\ndistances = scipy.spatial.distance.cdist(centroids, data)\nresult = []\nfor i in range(len(centroids)):\n    idx = np.argmin(distances[i])\n    result.append(data[idx])\n",
        "\ndistances = scipy.spatial.distance.cdist(centroids, data)\nresult = np.argsort(distances, axis=1)[:, k]\n",
        "\nresult = []\nfor x, b in zip(xdata, bdata):\n    root = fsolve(eqn, x0=0.5, args=(x, b))\n    result.append(root)\n",
        "\nresult = []\nfor i in range(len(xdata)):\n    x = xdata[i]\n    a = adata[i]\n    b = fsolve(lambda b: eqn(x, a, b), x0=0.5)\n    result.append([x, b[0]])\n",
        "\ncdf = lambda x: integrate.quad(bekkers, range_start, x, args=(estimated_a, estimated_m, estimated_d))[0]\nks_stat, p_value = stats.kstest(sample_data, cdf)\nresult = (ks_stat, p_value)\n",
        "\n# Calculate the KS-test statistic and p-value\nks_stat, p_value = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d))\n# Check if the null hypothesis can be rejected at 95% confidence level\nresult = p_value < 0.05\n",
        "\ndf['Time'] = pd.to_datetime(df['Time'])\ndf = df.set_index('Time')\ndef rolling_integral(data, window):\n    data = data.rolling(window).apply(integrate.trapz)\n    return data\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral, window='25S')\n",
        "\ngrid_x, grid_y = zip(*x)\ngrid_z = y\ninterp = scipy.interpolate.griddata(x, y, eval, method='linear')\nresult = interp[0]\n",
        "\nimport scipy.optimize as sciopt\nimport numpy as np\nimport pandas as pd\na=pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\n# Define the log-likelihood function\ndef log_likelihood(weights, data):\n    return np.sum(np.log(weights[data]))\n# Define the constraints for the optimization problem\ndef constraint_max(weights):\n    return np.sum(weights) - 1\ndef constraint_min(weights):\n    return 1 - np.sum(weights)\n# Define the bounds for the optimization problem\nbounds = [(0, 1) for _ in range(len(a['A1'].unique()))]\n# Define the initial guess for the optimization problem\ninit_guess = [1/len(a['A1'].unique())] * len(a['A1'].unique())\n# Define the constraints for the optimization problem\nconstraints = ({'type': 'eq', 'fun': constraint_max},\n                {'type': 'eq', 'fun': constraint_min})\n# Run the optimization problem\nresult = sciopt.minimize(fun=log_likelihood,\n                        x0=init_guess,\n                        args=(a['A1'],),\n                        method='SLSQP',\n                        bounds=bounds,\n                        constraints=constraints)\n# Print the weights\nweights = result.x\nprint(weights)\n",
        "\npopt, _ = sciopt.curve_fit(fp, x, y, bounds=(pmin, pmax))\n",
        "\nresult = []\nfor i in range(n, len(arr)-n):\n    if arr[i] <= np.max(arr[i-n:i]) and arr[i] <= np.max(arr[i:i+n]):\n        result.append(i)\n",
        "\nresult = []\nfor i in range(len(arr)):\n    for j in range(len(arr[i])):\n        if j - n >= 0 and j + n < len(arr[i]):\n            if arr[i][j] <= arr[i][j - n] and arr[i][j] <= arr[i][j + n]:\n                result.append([i, j])\n",
        "\n# Select only numeric columns\nnum_cols = df.select_dtypes(include=np.number).columns\n# Calculate z-score for each row of numeric columns\nzscores = np.abs(stats.zscore(df[num_cols]))\n# Create a boolean mask to filter out rows with z-score greater than 3\nmask = (zscores < 3).all(axis=1)\n# Apply the mask to the original dataframe\ndf = df[mask]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = pd.Series(data.target)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n    data1 = pd.DataFrame(data.data, columns=data.feature_names)\n    data1['target'] = pd.Series(data.target)\n    ",
        "\ndf_out = pd.get_dummies(df.Col3.apply(pd.Series).stack()).sum(level=0)\ndf_out.columns = df_out.columns.str.split('_').str[1]\ndf_out = df_out.join(df.drop('Col3', axis=1))\n",
        "\ndf_out = df.drop('Col3', axis=1)\nfor fruit in df['Col3'].explode().unique():\n    df_out[fruit] = df['Col3'].apply(lambda x: 1 if fruit in x else 0)\n",
        "\ndf_out = df.join(pd.get_dummies(df.iloc[:, -1].apply(pd.Series).stack()).sum(level=0))\ndf_out.drop(df.columns[-1], axis=1, inplace=True)\n",
        "\ndf_out = df.copy()\nfor elem in set(sum(df[df.columns[-1]], [])):\n    df_out[elem] = df_out.apply(lambda row: int(elem in row[df.columns[-1]]), axis=1)\ndf_out.drop(df.columns[-1], axis=1, inplace=True)\n",
        "\ndf_out = df.copy()\nfor elem in set(sum(df[df.columns[-1]], [])):\n    df_out[elem] = np.where(df_out[df.columns[-1]].apply(lambda x: elem in x), 1, 0)\ndf_out.drop(df.columns[-1], axis=1, inplace=True)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.svm as suppmach\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nX, y, x_test = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_test) == np.ndarray\n# Fit model:\nsvmmodel = suppmach.LinearSVC()\nsvmmodel.fit(X, y)\n# Get decision scores\nscores = svmmodel.decision_function(x_test)\n# Convert decision scores to probabilities using logistic function\nproba = 1 / (1 + np.exp(-scores))\n# Use CalibratedClassifierCV to improve probability estimates\ncalibrated_svm = CalibratedClassifierCV(base_estimator=svmmodel, method='sigmoid', cv=5)\ncalibrated_svm.fit(X, y)\n# Get calibrated probabilities\ncalibrated_proba = calibrated_svm.predict_proba(x_test)[:, 1]\n# Alternatively, you can use LogisticRegression to get calibrated probabilities\nlogreg = LogisticRegression()\nlogreg.fit(scores.reshape(-1, 1), y)\nlogreg_proba = logreg.predict_proba(scores.reshape(-1, 1))[:, 1]\nprint(proba)\nprint(calibrated_proba)\nprint(logreg_proba)\n",
        "\nmodel.fit(X, y)\npredicted_test = model.predict(x_predict)\npredicted_test_scores = model.decision_function(x_predict)\n# Convert decision scores to probabilities using logistic function\nproba = 1 / (1 + np.exp(-predicted_test_scores))\n# Use CalibratedClassifierCV to get probability estimates\nfrom sklearn.calibration import CalibratedClassifierCV\ncalibrated_svc = CalibratedClassifierCV(base_estimator=model, cv=5)\ncalibrated_svc.fit(X, y)\nproba_calibrated = calibrated_svc.predict_proba(x_predict)\n",
        "\nencoder = OneHotEncoder()\nsparse_matrix = encoder.fit_transform(transform_output)\none_hot_matrix = sparse_matrix.toarray()\none_hot_df = pd.DataFrame(one_hot_matrix, columns=encoder.get_feature_names())\ndf = pd.concat([df_origin, one_hot_df], axis=1)\n",
        "\ndf = pd.DataFrame(transform_output.toarray())\ndf = pd.concat([df_origin, df], axis=1)\n",
        "\n    transform_output_df = pd.DataFrame(transform_output.toarray())\n    result = pd.concat([df, transform_output_df], axis=1)\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Delete the 'poly' step\nestimators_without_poly = [estimator for estimator in estimators if estimator[0] != 'poly']\nclf_without_poly = Pipeline(estimators_without_poly)\n# Insert a new step\nestimators_with_new_step = [('reduce_dim', PCA()), ('new_step', PolynomialFeatures()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf_with_new_step = Pipeline(estimators_with_new_step)\nprint(len(clf.steps))  # Output: 3\nprint(len(clf_without_poly.steps))  # Output: 2\nprint(len(clf_with_new_step.steps))  # Output: 4\n",
        "\nestimators = clf.steps\nestimators.pop(1)  # remove the second step (index 1)\nclf = Pipeline(estimators)\n",
        "\nsteps = clf.named_steps.copy()\ndel steps['pOly']\nestimators = list(steps.items())\nclf = Pipeline(estimators)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\n# Define the original pipeline\nestimators = [('reduce_dim', PCA()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Define the new pipeline with the inserted step\nestimators_with_poly = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf_with_poly = Pipeline(estimators_with_poly)\nprint(len(clf.steps))  # Output: 2\nprint(len(clf_with_poly.steps))  # Output: 3\n",
        "\nclf.steps.insert(1, ('reduce_poly', PolynomialFeatures()))\n",
        "\nsteps = clf.named_steps.copy()\nsteps.update({'t1919810': PCA()})\nestimators = [(name, step) for name, step in steps.items()]\nclf = Pipeline(estimators)\n",
        "\ncv = TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY])\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\ngridsearch.fit(trainX, trainY, cv=cv, **fit_params)\n",
        "\nparamGrid = {'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300]}\nmodel = xgb.XGBRegressor()\nfit_params = {\"early_stopping_rounds\": 42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params=fit_params)\ngridsearch.fit(trainX, trainY)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5)\nlogreg = LogisticRegression()\nproba = []\nfor train_index, test_index in cv.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict(X_test)\n    proba.append(logreg.predict_proba(X_test))\n    \n    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(proba)\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nmodels = [LinearRegression()]\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5)\n    model_name = type(model).__name__\n    print(f'Name model: {model_name} , Mean score: {scores.mean()}')\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nmodels = [LinearRegression()]\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5)\n    model_name = type(model).__name__\n    print(f'Name model: {model_name} , Mean score: {scores.mean()}')\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nmodels = [LinearSVC()]\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5)\n    model_name = type(model).__name__\n    print(f'Name model: {model_name} , Mean score: {scores.mean()}')\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\npipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)\n",
        "\nclf = GridSearchCV(bc, param_grid)\nclf.fit(X_train, y_train)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nregressor.fit(X.reshape(-1, 1), y)\npredict = regressor.predict(X_test.reshape(-1, 1))\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nregressor.fit(X.reshape(-1, 1), y)\n",
        "\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef prePro(text):\n    return text.lower()\ntfidf = TfidfVectorizer(preprocessor=prePro)\nprint(tfidf.preprocessor)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"model\", SGDClassifier(random_state=42))\n])\ngrid = GridSearchCV(pipe, param_grid={\"model__alpha\": [1e-3, 1e-2, 1e-1, 1]}, cv=5)\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\nprint(coef)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"model\", RidgeClassifier(random_state=24))\n])\ngrid = GridSearchCV(pipe, param_grid={\"model__alpha\": [2e-4, 3e-3, 4e-2, 5e-1]}, cv=7)\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps[\"model\"].coef_\nprint(coef)\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n# Get the selected column indices\nselected_indices = model.get_support(indices=True)\n# Get the column names of the selected columns\ncolumn_names = X.columns[selected_indices]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n",
        "\ncentroids = km.cluster_centers_\ndistances = np.linalg.norm(X - centroids[p], axis=1)\nclosest_50_samples = X[np.argsort(distances)[:50]]\n",
        "\nkm.fit(X)\ncenters = km.cluster_centers_\np_center = centers[p-1]\ndistances = np.linalg.norm(X - p_center, axis=1)\nclosest_50_samples = X[np.argsort(distances)[:50]]\n",
        "\ncentroids = km.fit(X).cluster_centers_\ndistances = np.linalg.norm(X - centroids[p], axis=1)\nclosest_100_samples = X[np.argsort(distances)[:100]]\n",
        "\n    km.fit(X)\n    labels = km.labels_\n    centers = km.cluster_centers_\n    center_p = centers[p]\n    distances = np.linalg.norm(X - center_p, axis=1)\n    indices = np.argsort(distances)\n    samples = X[indices[:50]]\n    ",
        "\n# Convert categorical variable to matrix and merge back with original training data\nX_train = pd.get_dummies(X_train)\n",
        "\nX_train = pd.get_dummies(X_train)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVR\ndef load_data():\n    # Load your data here\n    pass\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\n# Fit an SVM regression model with a Gaussian kernel\nsvr_model = SVR(kernel='rbf', C=1e3, gamma=0.1)\nsvr_model.fit(X, y)\n# Predict the output for the input data X\npredict = svr_model.predict(X)\nprint(predict)\n",
        "\nsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\nsvr_rbf.fit(X, y)\npredict = svr_rbf.predict(X)\n",
        "\nsvr_model = SVR(kernel='poly', degree=2)\nsvr_model.fit(X, y)\npredict = svr_model.predict(X)\n",
        "\nsvr_poly = SVR(kernel='poly', degree=2)\nsvr_poly.fit(X, y)\npredict = svr_poly.predict(X)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef load_data():\n    # This function should load the queries and documents from a file or database\n    # and return them as lists of strings\n    pass\ndef get_term_frequency_inverse_data_frequency(documents):\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(documents)\n    return matrix\ndef get_tf_idf_query_similarity(documents, queries):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    vectorizer = TfidfVectorizer()\n    query_matrix = vectorizer.fit_transform(queries)\n    cosine_similarities = cosine_similarity(query_matrix, tfidf)\n    return cosine_similarities\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ncosine_similarities_of_queries = get_tf_idf_query_similarity(documents, queries)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.transform(documents).T).toarray()\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef load_data():\n    # This function loads the queries and documents from a file or database\n    # and returns them as lists of strings\n    # You can replace this function with your own implementation\n    # to load the queries and documents from your specific source\n    queries = [\n        \"What is the capital of France?\",\n        \"Who is the president of the United States?\",\n        \"What is the highest mountain in the world?\"\n    ]\n    documents = [\n        \"Paris is the capital of France.\",\n        \"Barack Obama is the president of the United States.\",\n        \"Mount Everest is the highest mountain in the world.\",\n        \"The capital of France is Paris.\",\n        \"The president of the United States is Barack Obama.\"\n    ]\n    return queries, documents\ndef get_term_frequency_inverse_data_frequency(documents):\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(documents)\n    return matrix\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = cosine_similarity(query_tfidf, tfidf).flatten()\n    return cosine_similarities\ndef solve(queries, documents):\n    cosine_similarities_of_queries = []\n    for query in queries:\n        cosine_similarities = get_tf_idf_query_similarity(documents, query)\n        cosine_similarities_of_queries.append(cosine_similarities)\n    return cosine_similarities_of_queries\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n",
        "\nnew_features = pd.DataFrame(features).fillna(0).astype(int)\n",
        "\nnew_f = pd.DataFrame(f).fillna(0).astype(int)\n",
        "\nnew_features = pd.DataFrame(features).fillna(0).astype(int)\n",
        "\n    df = pd.DataFrame(features)\n    df = df.fillna(0)\n    new_features = df.values\n    ",
        "\nnew_features = pd.DataFrame(features).fillna(0).astype(int)\n",
        "\n# Convert the distance matrix to a condensed form\ncondensed_matrix = np.concatenate([y for x, y in enumerate(np.tril(data_matrix, -1)) if x != 0])\n# Create an AgglomerativeClustering object and fit it to the condensed matrix\nac = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\nac.fit(condensed_matrix)\n# Get the labels for each professor\ncluster_labels = ac.labels_\n",
        "\n# Create a hierarchical clustering object\nhc = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n# Fit the hierarchical clustering object to the data matrix\nhc.fit(data_matrix)\n# Get the cluster labels\ncluster_labels = hc.labels_\n",
        "\n# Convert the distance matrix to a condensed form\ndistM = simM[np.triu_indices(simM.shape[0], k=1)]\n# Perform hierarchical clustering\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\nmodel.fit(distM.reshape(-1, 1))\n# Get the labels for each fruit\ncluster_labels = model.labels_\n",
        "\n# Calculate the linkage matrix\nZ = linkage(data_matrix, 'ward')\n# Define the cutoff height for the dendrogram\ncutoff_height = 0.5\n# Get the labels for each data point using fcluster\ncluster_labels = fcluster(Z, cutoff_height, criterion='distance')\n",
        "\n    cluster_labels = sch.fcluster(sch.linkage(data_matrix, method='ward'), 2, criterion='maxclust')\n    ",
        "\n# Calculate the linkage matrix\nZ = linkage(simM, 'ward')\n# Define the cutoff height for the dendrogram\ncutoff_height = 0.5\n# Get the labels for each fruit\ncluster_labels = fcluster(Z, cutoff_height, criterion='distance')\n",
        "\n# Apply Box-Cox transformation\npt = PowerTransformer(method='box-cox', standardize=False)\ndata_boxcox = pt.fit_transform(data)\n# Center and scale the data\nscaler = StandardScaler()\ndata_centered_scaled = scaler.fit_transform(data_boxcox)\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\ntransformer = PowerTransformer(method='box-cox', standardize=True)\ntransformed_data = transformer.fit_transform(data)\nbox_cox_data = pd.DataFrame(transformed_data, columns=data.columns)\n",
        "\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n",
        "\ntransformer = PowerTransformer(method='yeo-johnson', standardize=True)\ntransformed_data = transformer.fit_transform(data)\n",
        "\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\ntransformed_text = vectorizer.fit_transform(text)\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n",
        "\n# [Missing Code]\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the dataset into training and testing sets\ntrain_size = 0.75\ntest_size = 0.25\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], train_size=train_size, test_size=test_size, random_state=42)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Load data\ndataset = pd.read_csv('example.csv', header=None, sep=',')\ndef solve(data):\n    # Split data into features and target\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    # Split data into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef load_data():\n    df = pd.read_csv(\"generate_csv/all_data_device.csv\", parse_dates=[\"date\"])\n    return df\ndf = load_data()\nf1 = df['mse'].values\n# generate another list\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n# Centroid values\ncentroids = kmeans.cluster_centers_\n#print(centroids)\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(X[:, 0], X[:, 1], c=labels)\nax.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='#050505', s=1000)\nplt.title('K Mean Classification')\nplt.show()\n",
        "\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\ndef load_data():\n    df = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\n    return df\ndef get_clusters(df):\n    f1 = df['mse'].values\n    f2 = list(range(0, len(f1)))\n    X = np.array(list(zip(f1, f2)))\n    kmeans = KMeans(n_clusters=2).fit(X)\n    labels = kmeans.predict(X)\n    centroids = kmeans.cluster_centers_\n    return labels, centroids\ndef main():\n    df = load_data()\n    labels, centroids = get_clusters(df)\n    print(labels)\nif __name__ == \"__main__\":\n    main()\n",
        "\nfeatureSelector = LinearSVC(penalty='l1', dual=False)\nfeatureSelector.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.coef_[0] != 0]\n",
        "\nfeature_selector = LinearSVC(penalty='l1', dual=False)\nfeature_selector.fit(X, y)\nselected_features = feature_selector.coef_[0].nonzero()[0]\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_features]\n",
        "\n    clf = LinearSVC(penalty='l1', dual=False)\n    clf.fit(X, y)\n    selected_feature_indices = np.where(clf.coef_ != 0)[1]\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n    ",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef load_data():\n    data = {'Time': [5.00, 5.50, 6.00],\n            'A1': [np.nan, 7.44, 7.62],\n            'A2': [np.nan, 7.63, 7.86],\n            'A3': [np.nan, 7.58, 7.71],\n            'B1': [np.nan, 7.54, np.nan],\n            'B2': [7.40, np.nan, np.nan],\n            'B3': [7.51, np.nan, np.nan]}\n    return pd.DataFrame(data)\ndf1 = load_data()\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\nprint(slopes)\n",
        "\ntransformed_df = transform_sex(df)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n# Load data\ndf = pd.read_csv('data.csv')\n# Create a LabelEncoder object\nle = LabelEncoder()\n# Fill missing code\ndf['Sex'] = le.fit_transform(df['Sex'])\n# Print the transformed dataframe\nprint(df)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef load_data():\n    # Load the data here\n    return df\ndef Transform(df):\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\ntransformed_df = Transform(load_data())\nprint(transformed_df)\n",
        "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1)).reshape(np_array.shape)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1)).reshape(np_array.shape)\n",
        "\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a.reshape(-1, 1)).reshape(a.shape)\n    ",
        "\nclose_buy1 = close[:-1]\nm5 = ma50[:-1]\nm10 = ma100[:-1]\nma20 = ma200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict(b)\n",
        "\nX = np.array(X)\nle = LabelEncoder()\nX[:, 0] = le.fit_transform(X[:, 0])\nX[:, 1] = le.fit_transform(X[:, 1])\n",
        "\nnew_X = np.array(X).astype(str)\n",
        "\nle = LabelEncoder()\nnew_X = np.array([le.fit_transform(x) for x in X])\n",
        "\n# Data preprocessing\ndataframe = dataframe.dropna()\n# Separating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n",
        "\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n",
        "\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n",
        "\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort_values(by=\"date\")\n    test_dataframe = test_dataframe.sort_values(by=\"date\")\n    # Find the latest date in the train set\n    latest_train_date = train_dataframe[\"date\"].max()\n    # Filter the test set to only include dates greater than the latest train date\n    test_dataframe = test_dataframe[test_dataframe[\"date\"] > latest_train_date]\n    ",
        "\ncols = ['X2', 'X3']\ndf[cols] = df.groupby('Month')[cols].apply(lambda x: scaler.fit_transform(x))\ndf[['X2_scale', 'X3_scale']] = df[cols]\n",
        "\ncols = ['A2', 'A3']\nmyData[cols] = myData.groupby('Month')[cols].apply(lambda x: scaler.fit_transform(x))\n",
        "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n",
        "\ncv_results = GridSearch_fitted.cv_results_\ncv_results_df = pd.DataFrame(cv_results)\nfull_results = cv_results_df.sort_values(by='mean_fit_time')\n",
        "\nimport numpy as np\nimport pandas as pd\nimport pickle\nfitted_model = load_data()\n# Save the model in the file named \"sklearn_model\"\nwith open('sklearn_model', 'wb') as file:\n    pickle.dump(fitted_model, file)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef load_data():\n    data = {\n        'items': ['1fgg', '2hhj', '3jkl'],\n        'description': ['abcd ty', 'abc r', 'r df']\n    }\n    return pd.DataFrame(data)\ndef calculate_cosine_similarity(tfidf_matrix):\n    cosine_similarity_matrix = []\n    for i in range(tfidf_matrix.shape[0]):\n        row = []\n        for j in range(tfidf_matrix.shape[0]):\n            row.append(cosine_similarity(tfidf_matrix[i], tfidf_matrix[j]))\n        cosine_similarity_matrix.append(row)\n    return np.array(cosine_similarity_matrix)\ndf = load_data()\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = calculate_cosine_similarity(tfidf_matrix)\nprint(cosine_similarity_matrix)\n"
    ],
    "Pytorch": [
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\nif loss > prev_loss:\n    for g in optim.param_groups:\n        g['lr'] *= 0.1\n        print(\"Learning rate updated to:\", g['lr'])\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\nif loss_increases:\n    for param_group in optim.param_groups:\n        param_group['lr'] = 0.0005\n",
        "\nweights = torch.FloatTensor(word2vec.wv.vectors)\nembedding_layer = torch.nn.Embedding.from_pretrained(weights)\nembedded_input = embedding_layer(input_Tensor)\n",
        "\n    embedded_input = []\n    for sentence in input_Tensor:\n        embedded_sentence = []\n        for word in sentence:\n            if word in word2vec.wv.vocab:\n                embedded_sentence.append(word2vec.wv[word])\n            else:\n                embedded_sentence.append(np.zeros(100))\n        embedded_input.append(embedded_sentence)\n    ",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nC = B[:, A_log.bool()]\n",
        "\nC = B[:, A_logical.nonzero(as_tuple=True)]\n",
        "\nC = B[:, A_log.bool()]\n",
        "\nC = B[:, A_log.bool()]\n",
        "\n    C = B[:, A_log.nonzero().squeeze()]\n    ",
        "\nC = B[:, A_log.bool()]\n",
        "\nC = B[:, idx]\n",
        "\nx_tensor = torch.tensor(x_array.tolist())\n",
        "\nx_tensor = torch.tensor(x_array, dtype=torch.double)\n",
        "\n    t = torch.tensor(a.tolist())\n    ",
        "\nmax_len = max(lens)\nmask = torch.LongTensor(np.array([[1]*l + [0]*(max_len-l) for l in lens]))\n",
        "\nmax_len = max(lens)\nmask = torch.zeros((len(lens), max_len), dtype=torch.long)\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmax_len = max(lens)\nmask = torch.LongTensor(np.array([[1]*l + [0]*(max_len-l) for l in lens]))\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = np.arange(max_len) < np.array(lens)[:, None]\n    mask = torch.from_numpy(mask).long()\n    return mask\nmask = get_mask(lens)\nprint(mask)\n",
        "\nTensor_3D = torch.zeros(Tensor_2D.shape[0], Tensor_2D.shape[1], Tensor_2D.shape[1])\nfor i in range(Tensor_2D.shape[0]):\n    Tensor_3D[i] = torch.diag(Tensor_2D[i])\n",
        "\n    batch_size, diag_ele = t.shape\n    result = torch.zeros(batch_size, diag_ele, diag_ele)\n    for i in range(batch_size):\n        result[i] = torch.diag(t[i])\n    ",
        "\nif a.shape[0] == b.shape[0]:\n    ab = torch.stack((a, b), 0)\nelse:\n    ab = torch.cat((a, b.expand(a.shape[0], -1)), 0)\n",
        "\n# Convert the tensors to numpy arrays\na_np = a.detach().numpy()\nb_np = b.detach().numpy()\n# Concatenate the numpy arrays\nab_np = np.concatenate((a_np, b_np), axis=0)\n# Convert the concatenated numpy array back to a tensor\nab = torch.from_numpy(ab_np)\n",
        "\n    c = torch.zeros_like(a)\n    ab = torch.stack((a, b, c), 0)\n    ",
        "\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 0\n",
        "\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 2333\n",
        "\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 0\n",
        "\nfor i in range(len(lengths)):\n    a[i, :lengths[i], :] = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\n    tensor_of_tensors = torch.stack(lt)\n    ",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx.nonzero()]\n",
        "\nresult = t[torch.tensor(idx.tolist())]\n",
        "\nresult = t[torch.tensor(idx)]\n",
        "\nresult = x.gather(1, ids.unsqueeze(-1).expand(-1, -1, x.shape[2]))\nresult = result.squeeze(1)\n",
        "\nresult = x.gather(1, ids.expand(-1, 114, -1).transpose(1, 2))[:, 0, :]\n",
        "\nresult = torch.gather(x, 1, ids.unsqueeze(2).expand(-1, -1, x.shape[2]))\nresult = result.reshape(ids.shape[0], -1)\n",
        "\ny = torch.argmax(torch.tensor(softmax_output), dim=1).reshape(-1, 1)\n",
        "\ny = torch.argmax(softmax_output, dim=1).reshape(-1, 1)\n",
        "\ny = torch.argmin(softmax_output, dim=1).reshape(-1, 1)\n",
        "\n    y = torch.argmax(softmax_output, dim=1).reshape(-1, 1)\n    ",
        "\n    softmax_output = torch.tensor(softmax_output)\n    y = torch.argmin(softmax_output, dim=1)\n    ",
        "\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, w, z), target: (n, w, z)\n    n, c, w, z = input.size()\n    # log_p: (n, c, w, z)\n    log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*w*z, c)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)  # make class dimension last dimension\n    log_p = log_p[\n        target.view(n, w, z, 1).repeat(0, 0, 0, c) >= 0]  # this looks wrong -> Should rather be a one-hot vector\n    log_p = log_p.view(-1, c)\n    # target: (n*w*z,)\n    mask = target >= 0\n    target = target[mask]\n    loss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False)\n    if size_average:\n        loss /= mask.data.sum()\n    return loss\nimages = Variable(torch.randn(5, 3, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(3))\nloss = cross_entropy2d(images, labels)\nprint(loss)\n",
        "\ncnt_equal = 0\nfor i in range(1000):\n    if A[i] == B[i]:\n        cnt_equal += 1\n",
        "\ncnt_equal = 0\nfor i in range(len(A)):\n    if A[i] == B[i]:\n        cnt_equal += 1\n",
        "\ncnt_not_equal = 0\nfor i in range(len(A)):\n    if A[i] != B[i]:\n        cnt_not_equal += 1\n",
        "\n    cnt_equal = 0\n    for i in range(len(A)):\n        if A[i] == B[i]:\n            cnt_equal += 1\n    ",
        "\ncnt_equal = 0\nfor i in range(x):\n    if A[-(i+1)][0] == B[-(i+1)][0]:\n        cnt_equal += 1\n",
        "\ncnt_not_equal = 0\nfor i in range(x):\n    if A[-(i+1)][0] != B[-(i+1)][0]:\n        cnt_not_equal += 1\n",
        "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor = a[:, :, :, i::chunk_dim, :]\n    tensors_31.append(tensor)\n",
        "\ntensors_31 = []\nfor i in range(0, a.shape[2], chunk_dim):\n    tensor = a[:, :, i:i+chunk_dim, :, :]\n    tensors_31.append(tensor)\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x), torch.abs(y))\nmin_y = torch.min(torch.abs(x), torch.abs(y))\nsigned_min_x = sign_x * min_x\nsigned_min_y = sign_y * min_y\nsigned_min = torch.where(torch.abs(x) < torch.abs(y), signed_min_x, signed_min_y)\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x), torch.abs(y))\nmax_y = torch.max(torch.abs(x), torch.abs(y))\nsigned_max_x = torch.mul(max_x, sign_x)\nsigned_max_y = torch.mul(max_y, sign_y)\nsigned_max = torch.max(signed_max_x, signed_max_y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nx, y = load_data()\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_abs = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = min_abs * sign_x * sign_y\n    return signed_min\nsigned_min = solve(x, y)\nprint(signed_min)\n",
        "\noutput = MyNet(input)\nsoftmax = Softmax(dim=1)\nconfidence_score = softmax(output).max().item()\n",
        "\na_last_col = a[:, -1].unsqueeze(1)\nb_first_col = b[:, 0].unsqueeze(1)\noverlap_col = (a_last_col + b_first_col) / 2\nresult = torch.cat((a, overlap_col, b[:, 1:]), dim=1)\n",
        "\n    a_last_col = a[:, -1].unsqueeze(1)\n    b_first_col = b[:, 0].unsqueeze(1)\n    avg_col = (a_last_col + b_first_col) / 2\n    result = torch.cat((a, avg_col, b), dim=1)\n    ",
        "\nnew = torch.zeros(1, 4, 2)\nnew[:, :, 0] = 0\nnew[:, :, 1] = t[:, :, 1]\nresult = torch.cat([new, t], dim=1)\n",
        "\nnew = torch.zeros(1, 2, 4)\nnew[:, :, 1:3] = t\nr = new\n",
        "\nt = torch.cat([new, t, new], dim=1)\nt = torch.cat([new, t, new], dim=0)\n",
        "\nW = W.unsqueeze(0).expand(10, -1)\nresult = torch.bmm(data, W).squeeze()\n"
    ]
}