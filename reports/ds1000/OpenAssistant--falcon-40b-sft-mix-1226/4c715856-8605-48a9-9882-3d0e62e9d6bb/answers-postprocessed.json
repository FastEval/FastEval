{
    "Pandas": [
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\nresult = df[List]\nprint(result)\n",
        "\nresult = df.iloc[List]\n",
        "\n# Create a dictionary to map the values to their counts\ncounts = pd.value_counts(df.Qu1)\n# Create a new column with the mapped values\ndf['Qu1'] = df['Qu1'].map(counts)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\nresult = df.apply(lambda x: 'other' if x.value_counts()[x] >= 3 else x, axis=1)\nprint(result)\n",
        "\n    result = df.copy()\n    result['Qu1'] = result['Qu1'].apply(lambda x: 'other' if x in ['apple', 'egg'] else x)\n    result['Qu2'] = result['Qu2'].apply(lambda x: x)\n    result['Qu3'] = result['Qu3'].apply(lambda x: 'other' if x in ['apple', 'egg'] else x)\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\ndef change_values(df):\n    for col in df.columns:\n        if col == 'Qu1':\n            value_counts = df[col].value_counts()\n            if value_counts[col] >= 3:\n                df[col] = 'cheese'\n            else:\n                df[col] = 'other'\n        elif col == 'Qu2':\n            value_counts = df[col].value_counts()\n            if value_counts[col] >= 2:\n                df[col] = 'other'\n            else:\n                df[col] = col\n        else:\n            value_counts = df[col].value_counts()\n            if value_counts[col] >= 3:\n                df[col] = 'cheese'\n            else:\n                df[col] = 'other'\n    return df\nresult = change_values(df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\ndef replace_values(df):\n    value_counts = df.value_counts()\n    for col in df.columns:\n        if col == 'Qu1':\n            if value_counts[col] >= 3:\n                df[col] = 'cheese'\n            else:\n                df[col] = 'other'\n        elif col == 'Qu2':\n            if value_counts[col] >= 2:\n                df[col] = 'other'\n        else:\n            if value_counts[col] >= 3:\n                df[col] = 'cheese'\n            else:\n                df[col] = 'other'\n    return df\nresult = replace_values(df)\nprint(result)\n",
        "\nkeep_if_dup = df['keep_if_dup'].shift(1)\ndf = df.drop_duplicates(subset='url', keep=keep_if_dup)\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[result['drop_if_dup'] == 'No']\nprint(result)\n",
        "\nkeep_if_dup = df['keep_if_dup'].shift(1)\ndf = df.drop_duplicates(subset='url', keep=keep_if_dup)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\nresult = {}\nfor name, row in df.iterrows():\n    result[name] = {}\n    for col, value in zip(df.columns, row):\n        result[name][col] = {value: value}\nprint(result)\n",
        "\nimport pandas as pd\nimport datetime\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\nresult = df['datetime'].dt.tz_localize(None)\nprint(result)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\nexample_df['datetime'] = pd.to_datetime(example_df['datetime'])\ndef f(df=example_df):\n    # Convert datetime to local timezone\n    result = df['datetime'].dt.tz_localize(None)\n    \n    # Remove timezone information\n    result = result.dt.tz_convert('UTC')\n    \n    return result\n",
        "\nimport pandas as pd\nimport datetime\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndef remove_tz(dt):\n    return dt.replace(tzinfo=None)\ndf['datetime'] = df['datetime'].apply(remove_tz)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nimport datetime\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\nresult = df['datetime'].dt.tz_localize(None)\nprint(result)\n",
        "\nimport pandas as pd\ndef extract_key_value_pairs(message):\n    key_value_pairs = {}\n    for key, value in message.split(', '):\n        key_value_pairs[key] = value\n    return key_value_pairs\ndef expand_dataframe(df):\n    for index, row in df.iterrows():\n        message = row['message']\n        key_value_pairs = extract_key_value_pairs(message)\n        for key, value in key_value_pairs.items():\n            if value == 'none':\n                df.at[index, f'{key}_value'] = None\n            else:\n                df.at[index, f'{key}_value'] = value\n    return df\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': ['[job:  , money: none, wife: none]',\n                               '[group: band, wife: yes, money: 10000]',\n                               '[job: none, money: none, wife:  , kids: one, group: jail]']})\nresult = expand_dataframe(df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1066490, 1077784]\nfor product in products:\n    result = df.loc[df['product'] == product, 'score']\n    result *= 10\n    df = df.assign(score=result)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1066490, 1077784]\nresult = df.loc[df['product'].isin(products)]\nresult['score'] *= 10\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [[1069104, 1069105], [1066489, 1066491]]\nfor product_list in products:\n    for product in product_list:\n        df.loc[df['product'] == product, 'score'] *= 10\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1066490, 1077784, 1179741]\nfor product in products:\n    df.loc[df['product'] == product, 'score'] = df.loc[df['product'] == product, 'score'].max() - df.loc[df['product'] == product, 'score'].min()\nresult = df\nprint(result)\n",
        "result = df.apply(lambda x: 'A' if x == 1 else 'B' if x == 0 else 'C' if x == 2 else 'D', axis=1)\nresult = pd.DataFrame(result)\nresult = result.apply(lambda x: 'category' if x == 'A' else 'B' if x == 'B' else 'C' if x == 'C' else 'D', axis=1)\nresult = result.drop('A', axis=1)\nresult = result.drop('B', axis=1)\nresult = result.drop('C', axis=1)\nresult = result.drop('D', axis=1)\nresult = result.reset_index(drop=True)\n",
        "\n# Convert binary columns to categorical column\nresult['category'] = df.apply(lambda x: 'A' if x['A'] == 1 else 'B' if x['B'] == 1 else 'C' if x['C'] == 1 else 'D', axis=1)\n",
        "\ncategory = df.apply(lambda x: [i for i in x if x[i] == 1], axis=1)\nresult = pd.concat([df, category], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\nresult = df.apply(lambda x: x.dt.strftime('%B-%Y'), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\nresult = df.apply(lambda x: x.dt.strftime('%B-%Y'), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\nList = ['2019-01-17', '2019-02-20']\nfor date in List:\n    date_obj = pd.to_datetime(date)\n    result = df[(df['Date'] >= date_obj) & (df['Date'] < date_obj + pd.Timedelta(days=1))]\n    result['Month'] = result['Date'].dt.month_name()\n    result['Year'] = result['Date'].dt.year\n    result['Day'] = result['Date'].dt.day\n    print(result)\n",
        "\nresult = df.shift(1, axis=0)\nprint(result)\n",
        "\nresult = df.shift(1, axis=0)\n",
        "\nresult = df.shift(1, axis=0)\nresult = result.dropna()\nresult = result.reset_index(drop=True)\n",
        "\n# Shift the first row of the first column (11.6985) down 1 row\ndf['#1'] = df['#1'].shift(1)\n# Shift the last row of the first column (72.4399) to the first row, first column\ndf.loc[df.index == '1980-01-05', '#1'] = 72.4399\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\nresult = df.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX'})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\nresult = df.rename(columns={'HeaderA': 'XHeaderA', 'HeaderB': 'XHeaderB', 'HeaderC': 'XHeaderC'})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457],\n     \"HeaderX\": [345]})\nresult = df.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX', 'HeaderX': 'HeaderX'})\nprint(result)\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"mean\", \"val2\": \"mean\", \"val3\": \"mean\"})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", \"val3\": \"sum\"})\n",
        "\nvalue_columns = ['val1', 'val2', 'val42']\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"mean\"})\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.loc[row_list,column_list].mean(axis=0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.sum(axis=0, index=row_list, columns=column_list)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.sum(axis=0, level=row_list, subset=column_list)\nprint(result)\n",
        "\nresult = df.value_counts()\n",
        "\nresult = df.apply(lambda x: x.notnull().sum(), axis=1)\n",
        "\nresult = df.value_counts()\n",
        "\nresult = df.iloc[[0,1]].combine_first(axis=1)\n",
        "\nresult = df.iloc[[0,1]].combine_first(axis=1)\n",
        "\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n",
        "\nresult = df.loc[df['value'] < thresh].sum()\n",
        "\nresult = df.loc[df['value'] < thresh].sum()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nsection_left = 4\nsection_right = 38\ndef average_section(df, section_left, section_right):\n    section = df.loc[section_left:section_right]\n    if section.empty:\n        return None\n    else:\n        return section.mean()\nresult = average_section(df, section_left, section_right)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nfor col in df.columns:\n    result[f\"inv_{col}\"] = 1/df[col]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \": [e**1, e**2, e**3], \"exp_B \": [e**4, e**5, e**6]})\nprint(result)\n",
        "\ninv_cols = [\"A\", \"B\"]\nfor col in inv_cols:\n    result[f\"inv_{col}\"] = 1 / df[col]\n",
        "\nimport pandas as pd\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nresult = pd.DataFrame({\"A\": df[\"A\"], \"B\": df[\"B\"], \"sigmoid_A\": df[\"A\"].apply(sigmoid), \"sigmoid_B\": df[\"B\"].apply(sigmoid)})\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\nidx = pd.date_range('2017', periods=a.shape[0])\ndf = pd.DataFrame(a, index=idx, columns=list('abc'))\n# Get the index of the minimum value\nmin_idx = df.idxmin()\n# Get the index of the last occurrence of the column-wise maximum, up to the location of the minimum\nresult = df.index[min_idx:].shift(-1)\nprint(result)\n",
        "\nmask = df.idxmin() == df.index\nresult = df.mask(mask).idxmax()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date, freq='D')})\nresult['user'] = df['user']\nresult['val'] = 0\nprint(result)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndf['dt'] = pd.date_range(min_date, max_date, freq='D')\ndf['val'] = 0\nprint(result)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nfor i in range(min_date, max_date+pd.Timedelta(days=1)):\n    df = df.append({'user': 'a', 'dt': i, 'val': 233}, ignore_index=True)\n    df = df.append({'user': 'b', 'dt': i, 'val': 233}, ignore_index=True)\nprint(result)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nfor i in range(min_date, max_date+pd.Timedelta(1,'D')):\n    df = df.append({'user': 'a', 'dt': i, 'val': 33}, ignore_index=True)\nfor i in range(min_date, max_date+pd.Timedelta(1,'D')):\n    df = df.append({'user': 'b', 'dt': i, 'val': 2}, ignore_index=True)\nprint(df)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nfor i in range(min_date, max_date+pd.Timedelta(days=1)):\n    if i not in df['dt'].index:\n        df = df.append({'dt': i, 'user': 'a', 'val': 33}, ignore_index=True)\nfor i in range(min_date, max_date+pd.Timedelta(days=1)):\n    if i not in df['dt'].index:\n        df = df.append({'dt': i, 'user': 'b', 'val': 2}, ignore_index=True)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\nresult = df.assign(name=df['name'].apply(lambda x: str(x)))\nprint(result)\n",
        "\nresult = df.assign(a=df['a'].apply(lambda x: str(x)))\n",
        "\n    result = df.set_index('name')\n    result = result.to_dict()\n    result = {k: v for k, v in result.items() if v == 'Aaron'}\n    result = pd.DataFrame(result)\n    result = result.reset_index()\n    result = result.rename(columns={'index': 'name'})\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\nresult = df.groupby(['name', 'a']).agg({'b': 'sum', 'c': 'sum'}).reset_index()\nresult.columns = ['ID', 'b', 'c']\nprint(result)\n",
        "\ndate_col = ['01/12/15', '02/12/15']\nvalue_col = ['value']\nresult = pd.concat([df[date_col], df[value_col]], axis=1)\n",
        "\nresult = df.pivot_table(index='user', columns=['01/12/15', '02/12/15'], values='someBool', aggfunc='sum')\n",
        "\ndate_col = ['01/12/15', '02/12/15']\nvalue_col = ['01/12/15', '02/12/15']\nresult = pd.concat([df[date_col], df[value_col]], axis=1)\nresult = result.rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['b','e']\nresult = df[df.c > 0.5][columns]\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['a','b','e']\nresult = df[df.c > 0.45][columns]\nprint(result)\n",
        "\n    result = df[df.c > 0.5][columns]\n    ",
        "\nimport pandas as pd\ndef f(df, columns=['b', 'e']):\n    # Select rows where column 'c' is greater than 0.5\n    mask = df['c'] > 0.5\n    result = df[mask][columns]\n    return result\n",
        "\n    result = df[df.c > 0.5].ix[:, columns]\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 120\ndef filter_dates(df, X):\n    filter_dates = []\n    for index, row in df.iterrows():\n        if observation_time == 'D':\n            for i in range(1, X):\n                filter_dates.append((index.date() + timedelta(days=i)))\n    df = df[~df.index.isin(filter_dates)]\n    return df\nresult = filter_dates(df, X)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\ndef filter_dates(df, X):\n    filter_dates = []\n    for index, row in df.iterrows():\n        if observation_time == 'D':\n            for i in range(1, observation_period):\n                filter_dates.append((index.date() + timedelta(months=i)))\n    df = df[~df.index.isin(filter_dates)]\n    return df\nresult = filter_dates(df, X)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\ndef filter_dates(df, X):\n    filter_dates = []\n    for index, row in df.iterrows():\n        if observation_time == 'D':\n            for i in range(1, X):\n                filter_dates.append((index.date() + timedelta(months=i)))\n    df = df[~df.index.isin(filter_dates)]\n    return df\nresult = filter_dates(df, X)\nprint(result)\n",
        "\nresult = df.groupby(3).mean()\n",
        "\nresult = df.groupby(3).sum()\n",
        "\nresult = df.groupby(4).sum().reset_index()\n",
        "\nresult = df.groupby(3).apply(lambda x: x.mean())\n",
        "\nresult = df.groupby(3).sum().reset_index()\nresult = pd.concat([result, df.groupby(2).mean().reset_index()], axis=1)\nprint(result)\n",
        "\nresult = df.groupby(3).sum().reset_index()\nresult = pd.concat([result, df.groupby(2).mean().reset_index()], axis=1)\nprint(result)\n",
        "\ndf['A'] = df['A'].fillna(df['A'].shift())\n",
        "\ndf = df.fillna(df.shift())\n",
        "\n# Fill the zeros with the maximum between the previous and next non-zero value\ndf['A'] = df['A'].fillna(df['A'].shift(1).fillna(df['A'].shift(-1)))\n",
        "\ndf['number'] = df['duration'].str.extract(r'\\d+')\ndf['time'] = df['duration'].str.extract(r'\\w+')\n",
        "\ndf['time'] = df['duration'].str.extract(r'\\d+')\ndf['number'] = df['duration'].str.extract(r'\\w+')\n",
        "\n    df['number'] = df.duration.str.extract(r'\\d+', expand=False)\n    df['time'] = df.duration.str.extract(r'\\w+', expand=False)\n    df['time_days'] = df.duration.str.extract(r'\\d+', expand=False)\n    df['time_days'] = df['time_days'].astype(int)\n    df['time_days'] = df['time_days'].replace(r'year', '365')\n    df['time_days'] = df['time_days'].replace(r'month', '30')\n    df['time_days'] = df['time_days'].replace(r'week', '7')\n    df['time_days'] = df['time_days'].replace(r'day', '1')\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n# Create a new column for the time\ndf['time'] = df['duration'].str.extract(r'\\d+')\n# Create a new column for the number\ndf['numer'] = df['duration'].str.extract(r'\\d+')\n# Create a new column for the time_day\n# Multiply the time_day by the number\ndf['time_day'] *= df['numer']\n# Print the result\nprint(df)\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\ncolumns_check_list = ['A','B','C','D','E','F']\ndef check_columns(df1, df2, columns_check_list):\n    result = []\n    for column in columns_check_list:\n        result.append(df1[column] != df2[column])\n    return result\nresult = check_columns(df1, df2, columns_check_list)\nprint(result)\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\ncolumns_check_list = ['A','B','C','D','E','F']\nresult = []\nfor column in columns_check_list:\n    result.append(np.where((df1[column] == df2[column]) | (df1[column] == 'yes') | (df2[column] == 'yes'), True, False))\nprint(result)\n",
        "\nimport pandas as pd\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                 names=('id', 'date'))\ndf = pd.DataFrame({'x': [100, 90, 80], 'y':[7, 8, 9]}, index=index)\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\nresult = df\nprint(result)\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n",
        "\n    date_index = df.index.str.slice(0, 10)\n    date_index = date_index.to_datetime()\n    df['date'] = date_index\n    ",
        "\n    df = df.set_index(['date', 'id'])\n    df = df.unstack(fill_value=0)\n    ",
        "\nresult = pd.melt(df, id_vars='Country', value_name='Var1', var_name='year')\nresult = pd.concat([result, pd.melt(df, id_vars='Country', value_name='Var2', var_name='year')], axis=1)\n",
        "\nresult = df.melt(id_vars='Country', value_name='Var1', var_name='year')\nresult = result.sort_values(by='year')\nresult = result.reset_index(drop=True)\nprint(result)\n",
        "\nabsolute_values = df.abs()\nresult = absolute_values[absolute_values < 1]\n",
        "\n# Filter rows where absolute value of any columns (Value_A, Value_B, Value_C, ....) is more than 1\nresult = df[abs(df) > 1]\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\ncolumns = df.columns[df.columns.startswith('Value_')]\nfor column in columns:\n    df[column] = df[column].abs()\nresult = df\nprint(result)\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n",
        "\ndf['A'] = df['A'].str.replace('&LT;', '<')\n",
        "\n    result = df.apply(lambda x: x.replace('&AMP;', '&'))\n    ",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['A'] = df['A'].str.replace('&LT;', '<')\ndf['A'] = df['A'].str.replace('&GT;', '>')\nresult = df\nprint(result)\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n",
        "\nimport pandas as pd\nimport re\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\nresult = df.apply(validate_single_space_name, axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport re\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\nresult = df.apply(validate_single_space_name, axis=1)\nresult = result.where(result != None, other='')\nprint(result)\n",
        "\nimport pandas as pd\nimport re\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\nresult = df.apply(validate_single_space_name, axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n# Create a new column in df1 that contains the data from df2 at the same timestamp\ndf1['data_from_df2'] = df2.loc[df1.index].reset_index(drop=True)['data']\n# Drop the original data column from df1\ndf1 = df1.drop('data', axis=1)\n# Merge the two dataframes on the timestamp column\nresult = pd.merge(df1, df2, on='Timestamp')\n# Print the resulting dataframe\nprint(result)\n",
        "\nresult = df1.merge(df2, on='Timestamp')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'datetime': ['2021-04-10 01:00:00', '2021-04-10 02:00:00', '2021-04-10 03:00:00', '2021-04-10 04:00:00', '2021-04-10 05:00:00'],\n                   'col1': [25, 25, 25, 50, 100],\n                   'col2': [50, 50, 100, 50, 100],\n                   'col3': [50, 50, 50, 100, 100]})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndef max_of_col1_col2_col3(row):\n    if row['col2'] <= 50 and row['col3'] <= 50:\n        return row['col1']\n    elif row['col2'] <= 50:\n        return row['col2']\n    elif row['col3'] <= 50:\n        return row['col3']\n    else:\n        return 100\nresult = df.apply(max_of_col1_col2_col3, axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'datetime': ['2021-04-10 01:00:00', '2021-04-10 02:00:00', '2021-04-10 03:00:00', '2021-04-10 04:00:00', '2021-04-10 05:00:00'],\n                   'col1': [25, 25, 25, 50, 100],\n                   'col2': [50, 50, 100, 50, 100],\n                   'col3': [50, 50, 50, 100, 100]})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndef calculate_state(row):\n    if row['col2'] + row['col3'] > 50:\n        return row['col1']\n    else:\n        return row['col1'] + row['col2'] + row['col3']\nresult = df.apply(calculate_state, axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\nresult = df.apply(lambda x: [x] if isinstance(x, int) else [\"error\"], axis=1)\nprint(result)\n",
        "\nresult = df.apply(lambda x: [int(y) for y in x['Field1']], axis=1)\n",
        "\n    result = []\n    for index, row in df.iterrows():\n        if not pd.isnumeric(row[\"Field1\"]):\n            result.append(row[\"Field1\"])\n    return result\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\nresult = df.apply(lambda x: x['val1']/x['val1'].sum(), axis=1)\nresult = result.apply(lambda x: x*100)\nresult = result.round(2)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\nresult = df.apply(lambda x: x / df['val1'].sum(), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport io\ndata = io.StringIO(\"\"\"\nrs  alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP7      A/T      0    7      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP15     C/A      0   15      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n\"\"\")\ndf = pd.read_csv(data, delim_whitespace=True).set_index('rs')\ntest = ['TP3', 'TP7', 'TP18']\nresult = df.loc[test]\nprint(result)\n",
        "\ntest = ['TP3', 'TP18']\nresult = df.loc[test]\nprint(result)\n",
        "\ntest = ['TP3', 'TP7', 'TP18']\nfor row in test:\n    df = df.drop(row)\nprint(df)\n",
        "\n    result = df[df.index.isin(test)]\n    ",
        "\n# Calculate pairwise distances between cars\ndistances = df.apply(lambda x: ((x['x'] - x['x'])**2 + (x['y'] - x['y'])**2)**0.5, axis=1)\n# Create a new DataFrame with the distances and the car index\ndf_distances = pd.DataFrame({'distance': distances, 'car': df['car']})\n# Find the nearest neighbor for each car\nnearest_neighbors = df_distances.groupby('car')['distance'].apply(lambda x: x.idxmin())\n# Merge the nearest neighbor information with the original DataFrame\nresult = pd.merge(df, df_distances, on='car')\n",
        "\n# Calculate pairwise distances between cars\ndistances = df.apply(lambda x: ((x['x'] - x['x'])**2 + (x['y'] - x['y'])**2)**0.5, axis=1)\n# Create a new column for the farmost neighbour\ndf['farmost_neighbour'] = df.apply(lambda x: x['car'] + 1, axis=1)\n# Calculate the Euclidean distance between each car and its farmost neighbour\ndf['euclidean_distance'] = distances.shift(-1) - distances\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\ndef join_columns(cols):\n    return \",\".join(cols)\nresult = df\nresult[\"keywords_all\"] = result[\"keywords_all\"].apply(join_columns, axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\ndef concat_columns(cols):\n    return \"-\".join(cols)\nresult = df.apply(concat_columns, axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\nresult = df[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\nresult = df[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\nprint(result)\n",
        "\nsample_size = int(df.shape * 0.2)\nsample = df.sample(sample_size, random_state=0)\nsample['Quantity'] = 0\nresult = df.append(sample)\nprint(result)\n",
        "\nsample_index = df.sample(n=int(df.shape * 0.2)).index\nresult = df.loc[sample_index]\nresult['ProductId'] = 0\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'UserId': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1, 1, 4, 7, 4, 2, 1, 1, 4, 7],\n                   'Quantity': [6, 1, 3, 2, 7, 2, 6, 1, 3, 2, 7, 2, 6, 1, 3]})\n# Sample 20% of rows of each user\nsample_size = int(df.groupby('UserId').size() * 0.2)\nsample_indices = df.groupby('UserId').apply(lambda x: np.random.choice(x.index, size=sample_size, replace=False))\ndf = df.merge(sample_indices, on='UserId')\n# Set Quantity to zero for selected rows\ndf.loc[sample_indices, 'Quantity'] = 0\n# Print result\nprint(df)\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    result = duplicate.reset_index(drop=True)\n    result['index_original'] = duplicate.index\n    ",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nprint(duplicate)\n",
        "\nresult = df.groupby(['Sp', 'Mt']).max()\n",
        "\nresult = df.groupby(['Sp','Mt']).max()\n",
        "\nresult = df.groupby(['Sp', 'Mt']).agg({'count': 'min'}).reset_index()\n",
        "\nresult = df.groupby(['Sp','Value']).max()\n",
        "\nimport pandas as pd\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\nresult = df.query(\"Category.isin(filter_list)\")\nprint(result)\n",
        "\nresult = df.query(\"Catergory!=filter_list\")\nprint(result)\n",
        "\nimport pandas as pd\ndef melt_df(df, column_levels):\n    value_vars = [(col_level, col_level, col_level) for col_level in column_levels]\n    return pd.melt(df, value_vars=value_vars)\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\ncolumn_levels = [list('A'), list('B'), list('C'), list('D'), list('E'), list('F'), list('G'), list('H'), list('I'), list('J')]\nresult = melt_df(df, column_levels)\nprint(result)\n",
        "import pandas as pd\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n# Define a list of tuples for the value_vars\nvalue_vars = [(list('AAAAAA'), list('BBCCDD'), list('EFGHIJ'))]\n# Define a list of tuples for the id_vars\nid_vars = [list('col1'), list('col2'), list('col3')]\n# Define a list of tuples for the var_name\nvar_name = ['variable_0', 'variable_1', 'variable_2']\n# Define a list of tuples for the value\nvalue = ['a', 'b', 'c']\n# Define a list of tuples for the id\nid = [0, 1, 2]\n# Define a list of tuples for the index\nindex = [0, 1, 2]\n# Define a list of tuples for the columns\ncolumns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n# Define a list of tuples for the data\ndata = [df.iloc[0]]\n# Define a list of tuples for the index_name\nindex_name = ['index']\n# Define a list of tuples for the column_name\ncolumn_name = ['variable_0', 'variable_1', 'variable_2']\n# Define a list of tuples for the row_name\nrow_name = ['row']\n# Define a list of tuples for the level_name\nlevel_name = ['level']\n# Define a list of tuples for the level_num\nlevel_num = [0, 1, 2]\n# Define a list of tuples for the level_size\nlevel_size = [1, 1, 1]\n# Define a list of tuples for the level_start\nlevel_start = [0, 1, 2]\n# Define a list of tuples for the level_end\nlevel_end = [1, 2, 3]\n# Define a list of tuples for the level_str\nlevel_str = ['A', 'B', 'C']\n# Define a list of tuples for the level_num_str\nlevel_num_str = ['0', '1', '2']\n# Define a list of tuples for the level_size_str\nlevel_size_str = ['1', '1', '1']\n# Define a list of tuples for the level_start_str\nlevel_start_str = ['0', '1', '2']\n# Define a list of tuples for the level_end_str\nlevel_end_str = ['1', '2', '3']\n# Define a list of tuples for the level_name_str\nlevel_name_str = ['A', 'B', 'C']\n# Define a list of tuples for the level_num_str\nlevel_num_str = ['0', '1', '2']\n# Define a list of tuples for the level_size_str\nlevel_size_str = ['1', '1', '1']\n# Define a list of tuples for the level_start_str\nlevel_start_str = ['0', '1', '2']\n# Define a list of tuples for the level_end_str\nlevel_end_str = ['1', '2', '3']\n# Define a",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n",
        "\ndf['cumsum'] = df['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=True)\nprint(result)\n",
        "\n# Create a new column with the same values as 'v' but with all NaN values replaced with 0\ndf['v_nan_replaced'] = df['v'].fillna(0)\n# Group the dataframe by 'r' and sum the 'v_nan_replaced' column\nresult = df.groupby('r')['v_nan_replaced'].sum()\n",
        "\n# Create a new column with the same values as 'v' but with all NaN values replaced with 0\ndf['v_nan_replaced'] = df['v'].fillna(0)\n# Group the dataframe by 'l' and sum the 'v_nan_replaced' column\nresult = df.groupby('l')['v_nan_replaced'].sum()\n",
        "\nimport pandas as pd\ndef relationship_type(df):\n    result = []\n    for i in range(len(df)):\n        for j in range(i+1, len(df)):\n            if df.iloc[i, :] == df.iloc[j, :]:\n                result.append(f\"{df.iloc[i, :]} {df.iloc[j, :]} one-to-one\")\n            elif df.iloc[i, :] in df.iloc[j, :]:\n                result.append(f\"{df.iloc[i, :]} {df.iloc[j, :]} one-to-many\")\n            elif df.iloc[i, :] in df.iloc[j, :] and df.iloc[j, :] in df.iloc[i, :]:\n                result.append(f\"{df.iloc[i, :]} {df.iloc[j, :]} many-to-many\")\n            else:\n                result.append(f\"{df.iloc[i, :]} {df.iloc[j, :]} many-to-one\")\n    return result\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\nresult = relationship_type(df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndef get_relationship_type(df):\n    relationships = []\n    for col1, col2 in zip(df.columns, df.columns):\n        if df[col1].nunique() == 1 and df[col2].nunique() == len(df):\n            relationships.append(f\"{col1} {col2} one-2-one\")\n        elif df[col1].nunique() == len(df) and df[col2].nunique() == 1:\n            relationships.append(f\"{col1} {col2} one-2-many\")\n        elif df[col1].nunique() == df[col2].nunique() and df[col1].nunique() > 1:\n            relationships.append(f\"{col1} {col2} many-2-many\")\n        else:\n            relationships.append(f\"{col1} {col2} many-2-one\")\n    return relationships\nresult = get_relationship_type(df)\nprint(result)\n",
        "\nimport pandas as pd\ndef relationship_type(df):\n    # Create a dictionary to store the relationship types\n    relationship_types = {\n        'one-to-one': '1:1',\n        'one-to-many': '1:N',\n        'many-to-one': 'N:1',\n        'many-to-many': 'N:N'\n    }\n    \n    # Create a list to store the relationship types for each column\n    relationship_types_list = []\n    \n    # Iterate through the columns\n    for col in df.columns:\n        # Get the unique values in the column\n        unique_values = df[col].unique()\n        \n        # Iterate through the unique values\n        for value in unique_values:\n            # Get the index of the value in the column\n            index = df[col].index[df[col] == value]\n            \n            # If the index is not empty, add the relationship type to the list\n            if index.size > 0:\n                relationship_types_list.append(relationship_types[value])\n    \n    # Create a DataFrame to store the relationship types\n    relationship_types_df = pd.DataFrame({'Column': df.columns, 'Relationship Type': relationship_types_list})\n    \n    # Merge the relationship types DataFrame with the original DataFrame\n    result = pd.merge(df, relationship_types_df, on='Column')\n    \n    return result\n",
        "\nimport pandas as pd\ndef relationship_type(df):\n    # Create a dictionary to store the relationship types\n    relationship_types = {\n        'one-2-one': '1:1',\n        'one-2-many': '1:N',\n        'many-2-one': 'N:1',\n        'many-2-many': 'N:N'\n    }\n    \n    # Create a new DataFrame with the relationship types\n    result = pd.DataFrame({\n        'Column1': [relationship_types[col1] for col1 in df['Column1']],\n        'Column2': [relationship_types[col2] for col2 in df['Column2']],\n        'Column3': [relationship_types[col3] for col3 in df['Column3']],\n        'Column4': [relationship_types[col4] for col4 in df['Column4']],\n        'Column5': [relationship_types[col5] for col5 in df['Column5']]\n    })\n    \n    return result\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\nresult = relationship_type(df)\nprint(result)\n",
        "\n# Convert bank to string\ndf['bank'] = df['bank'].astype(str)\n# Drop duplicates based on firstname, lastname, and email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n            .applymap(lambda s: s.lower() if type(s) == str else s)\n            .applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n            .drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# Save unique records\ndfiban_uniq = df.loc[uniq_indx]\n",
        "\nresult = s.astype(float)\n",
        "\ngroup_by_conditions = (df['SibSp'] > 0) | (df['Parch'] > 0)\nresult = df.groupby(group_by_conditions).mean()\n",
        "\ngrouped = df.groupby(['Survived', 'Parch'])\nresult = grouped.mean()\n",
        "\ngroup_by_conditions = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                       (df['SibSp'] == 0) & (df['Parch'] == 0),\n                       (df['SibSp'] == 0) & (df['Parch'] == 1),\n                       (df['SibSp'] == 1) & (df['Parch'] == 0)]\nresult = df.groupby(group_by_conditions).mean()\nprint(result)\n",
        "\nresult = df.groupby('cokey').sort_values('A')\n",
        "\nresult = df.groupby('cokey').sort_values('A')\n",
        "\ndf = df.set_index(pd.MultiIndex.from_tuples(l, names=['Caps', 'Lower', 'A', 'B']))\nresult = df\nprint(result)\n",
        "\ndf = df.set_index(pd.MultiIndex.from_tuples(df.columns.tolist()))\nresult = df\nprint(result)\n",
        "\ndf = df.set_index(pd.MultiIndex.from_tuples(l, names=['Caps', 'Middle', 'Lower']))\nresult = df\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nnp.random.seed(123)\nbirds = np.random.choice(['African Swallow', 'Dead Parrot', 'Exploding Penguin'], size=int(5e4))\nsomeTuple = np.unique(birds, return_counts=True)\nresult = pd.DataFrame({'birdType': someTuple, 'birdCount': someTuple})\nprint(result)\n",
        "\nstdMeann = lambda x: np.std(np.mean(x))\nresult = df.groupby('a').b.apply(stdMeann)\nprint(result)\n",
        "\nstdMeann = lambda x: np.std(np.mean(x))\nresult = df.groupby('b').a.apply(stdMeann)\nprint(result)\n",
        "\n# Softmax normalization\ndf['softmax'] = np.exp(df['b'].apply(lambda x: -1 * np.log(x)))\n# Min-Max normalization\ndf['min-max'] = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n",
        "\nresult = df.dropna(axis=1)\n",
        "\nresult = df.drop(df.sum(axis=1) == 0, axis=1)\n",
        "\nresult = df.drop(df.idxmax(axis=1))\n",
        "\nresult = df.apply(lambda x: [0 if x == 2 else x for x in x], axis=1)\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n          index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\nresult = s.sort_values(ascending=True)\nresult = result.reset_index(drop=True)\nprint(result)\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n              index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\nresult = df.sort_values(by=['index','value'])\nprint(result)\n",
        "\nresult = df[df['A'].isin([1, 2, 3])]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\nresult = df[df['A'].str.contains('s')]\nprint(result)\n",
        "\nresult = df.groupby(['Sp', 'Mt']).max()\n",
        "\nresult = df.groupby(['Sp','Mt']).max()\n",
        "\nresult = df.groupby(['Sp', 'Mt']).agg({'count': 'min'}).reset_index()\n",
        "\nresult = df.groupby(['Sp','Value']).max()\n",
        "\nimport pandas as pd\nimport numpy as np\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\nresult = df.assign(Date=dict.get(df['Member'], df['Date']))\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\nresult = df.assign(Date=dict.get(df['Member'], df['Date']))\nprint(result)\n",
        "\nimport pandas as pd\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\ndef f(dict=example_dict, df=example_df):\n    # Fill NaNs in the 'Member' column\n    df['Member'] = df['Member'].fillna(df['Member'])\n    \n    # Create a new column 'Date' with the values from the dict\n    result = df.assign(Date=dict.values())\n    \n    # Drop the original 'Date' column\n    result = result.drop('Date', axis=1)\n    \n    return result\n",
        "\nimport pandas as pd\nimport numpy as np\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\n# Fill NaNs with the value in the 'Member' column\ndf['Member'] = df['Member'].fillna(df['Member'])\n# Map the values in the dict to replace the 'Date' column\nfor key, value in dict.items():\n    df[key] = value\n# Convert the 'Date' column to a string\ndf['Date'] = df['Date'].astype(str)\n# Set the format of the 'Date' column to '%d-%b-%Y'\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n# Drop the original 'Date' column\ndf = df.drop('Date', axis=1)\n# Print the result\nprint(df)\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Month'] = df['Date'].dt.month\ndf['Year'] = df['Date'].dt.year\ndf['Count_d'] = df.groupby(['Date','Month','Year']).size()\ndf['Count_m'] = df.groupby(['Date','Month']).size()\ndf['Count_y'] = df.groupby(['Date','Year']).size()\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n# Group by month and year and count the number of occurrences of each val\ndf1 = df.groupby(['year', 'month']).agg({'count': 'sum'})\n# Merge the two dataframes\nresult = pd.merge(df, df1, on=['year', 'month'])\n# Add new columns for the count of each val per month and year\nresult['Count_d'] = result['Date'].map(df.groupby('Date').size())\nresult['Count_m'] = result['Date'].map(df.groupby('Date').size())\nresult['Count_y'] = result['Date'].map(df.groupby('Date').size())\nresult['Count_Val'] = result['Val'].map(df.groupby('Val').size())\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n# Group by date, val, and count\nresult = df.groupby(['Date', 'Val', 'Date.dt.year', 'Date.dt.month', 'Date.dt.weekday']).agg({'count': 'sum'})\n# Add new columns for count_d, count_m, count_y, and count_w\nresult['count_d'] = result.groupby('Date')['count'].cumsum()\nresult['count_m'] = result.groupby('Date.dt.month')['count'].cumsum()\nresult['count_y'] = result.groupby('Date.dt.year')['count'].cumsum()\nresult['count_w'] = result.groupby('Date.dt.weekday')['count'].cumsum()\n# Print the result\nprint(result)\n",
        "\nresult1 = df.groupby('Date')['B'].sum()\nresult2 = df.groupby('Date')['C'].sum()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\nresult1 = df.groupby(['Date', 'B']).sum().reset_index()\nresult2 = df.groupby(['Date', 'C']).sum().reset_index()\nprint(result1)\nprint(result2)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D':np.sum,'E':np.mean})\nprint(result)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.sum, np.mean])\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\ndf = pd.DataFrame({\n'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n'B' : ['A', 'B', 'C'] * 4,\n'D' : np.random.randn(12),\n'E' : np.random.randn(12)\n})\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.sum, np.mean])\nprint(result)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.max, np.min])\n",
        "\nimport pandas as pd\nimport dask.dataframe as dd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\nresult = df.apply(lambda x: x.split(','), axis=1)\nresult = result.to_dask_dataframe()\nprint(result)\n",
        "\nimport pandas as pd\nimport dask.dataframe as dd\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\nresult = df.apply(lambda x: x.split(','), axis=1)\nresult = result.to_dask_dataframe()\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndef split_column(df):\n    # Split the column into a list of strings\n    column = df['var2'].tolist()\n    \n    # Split each string into a list of characters\n    characters = [list(x) for x in column]\n    \n    # Convert the list of characters into a numpy array\n    characters_array = np.array(characters)\n    \n    # Split the numpy array into rows\n    rows = np.split(characters_array, 1)\n    \n    # Convert the list of rows into a pandas dataframe\n    result = pd.DataFrame(rows, columns=['var1', 'var2'])\n    \n    # Drop the original column\n    df = df.drop('var2', axis=1)\n    \n    # Concatenate the original dataframe with the new dataframe\n    result = pd.concat([df, result], axis=1)\n    \n    return result\ndf = pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1,2,3], columns=['var1', 'var2'])\nresult = split_column(df)\nprint(result)\n",
        "\nimport pandas as pd\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\nresult = df.apply(count_special_char, axis = 0)\nprint(result)\n",
        "\nimport pandas as pd\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\nresult = df.apply(count_special_char, axis = 0)\nprint(result)\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\nresult = df\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\nresult = df\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['medi'] = df['row'].str[6:10]\ndf['row'] = df['row'].str[11:]\nresult = df\n",
        "\nresult = df.apply(lambda x: x.fillna(x.mean()).cumsum(), axis=1)\nprint(result)\n",
        "\nresult = df.apply(lambda x: x.rolling(window=6, min_periods=1).mean().fillna(x.rolling(window=6, min_periods=1).mean()))\nprint(result)\n",
        "\n    result = df.groupby('Name').cummin().fillna(0)\n    ",
        "\nresult = df.apply(lambda x: x.rolling(window=6, min_periods=1).mean().fillna(0), axis=1)\nprint(result)\n",
        "\ndf['Label'] = 0\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 1)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\nresult = df.assign(label=lambda x: 1 if x['Close'] > x['Close'].shift() else 0 if x['Close'] == x['Close'].shift() else -1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\nresult = df.assign(label=lambda x: -1 if x['Close'] < x['Close'].shift() else 1 if x['Close'] > x['Close'].shift() else 0)\nprint(result)\n",
        "\nimport pandas as pd\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 12:50:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n# Convert datetime strings to datetime objects\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n# Calculate the time difference between the first row departure time and the second row arrival time\ndf['Duration'] = df.departure_time.iloc[1:] - df.arrival_time.iloc[:-1]\n# Print the result\nprint(df)\n",
        "\n# Convert datetime strings to datetime objects\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\n# Calculate the time difference in seconds\ndf['Duration'] = df['departure_time'].iloc[i+1] - df['arrival_time'].iloc[i]\n",
        "\nimport pandas as pd\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n# Convert datetime strings to datetime objects\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n# Calculate time difference in seconds\ndf['Duration'] = df['departure_time'].iloc[1:] - df['arrival_time'].iloc[:-1]\n# Format datetime objects as strings\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%Y-%m-%d %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n# Print result\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\nresult = df.groupby(['key1']).size().reset_index()\nresult['count'] = result['key2'].eq('one').cumsum()\nprint(result)\n",
        "\nresult = df.groupby(['key1']).size().reset_index()\nresult['count'] = result['key2'].eq('two').cumsum()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'gee', 'two', 'three', 'two']})\nresult = df.groupby(['key1']).size().reset_index()\nresult['count'] = result['key2'].apply(lambda x: len(x.split('e')) if 'e' in x else 0)\nprint(result)\n",
        "min_date = df.index.min()\nmax_date = df.index.max()\nprint(max_result,min_result)",
        "mode_result = df.mode(axis=0)\nmedian_result = df.median(axis=0)\nprint(mode_result, median_result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\nresult = df[(99 <= df['closing_price'] <= 101)]\nprint(result)\n",
        "\ndf = df[~(df['closing_price'] <= 99) & ~(df['closing_price'] >= 101)]\n",
        "\nresult = df.groupby([\"item\", \"otherstuff\"], as_index=False)[\"diff\"].min()\nprint(result)\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[1:]\n",
        "\n    result = df['SOURCE_NAME'].str.split('_').str[0]\n    ",
        "\ndef fill_nan(df):\n    n = len(df)\n    half = int(n / 2)\n    df['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace=True)\n    df['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace=True)\n    return df\n",
        "\ndef fill_nan(df):\n    nan_count = df['Column_x'].isna().sum()\n    if nan_count == 0:\n        return df\n    else:\n        first_30 = int(nan_count * 0.3)\n        middle_30 = int(nan_count * 0.3)\n        last_30 = nan_count - first_30 - middle_30\n        first_30_values = [0] * first_30\n        middle_30_values = [0.5] * middle_30\n        last_30_values = [1] * last_30\n        df['Column_x'] = df['Column_x'].fillna(first_30_values + middle_30_values + last_30_values, inplace=True)\n        return df\n",
        "\n# Fill NaN values with \"0\" or \"1\" so that the number of \"0\" is 50%(round down) and the number of \"1\" is 50%(round down)\n# Meanwhile, please fill in all zeros first and then all ones\n# Create a list of \"0\" and \"1\" values with the desired proportions\nzero_proportion = 0.5\none_proportion = 0.5\nzero_list = [0] * int(zero_proportion * len(df))\none_list = [1] * int(one_proportion * len(df))\n# Iterate through the NaN values and replace them with the corresponding value from the list\nfor i in range(len(df)):\n    if df.iloc[i] == np.nan:\n        df.iloc[i] = zero_list[i % len(zero_list)] if np.random.rand() < zero_proportion else one_list[i % len(one_list)]\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nresult = pd.concat([a, b], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nc = pd.DataFrame(np.array([[9, 10],[11, 12]]), columns=['one', 'two'])\nresult = pd.concat([a, b, c], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8],[9, 10]]), columns=['one', 'two'])\nresult = pd.DataFrame()\nfor i in range(len(a)):\n    result = result.append({'one': a.iloc[i], 'two': b.iloc[i]}, ignore_index=True)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\nresult = pd.DataFrame()\nfor bin in bins:\n    result = result.append({'username': df['username'].unique(), 'views': df.groupby(df['views'].apply(lambda x: bin if x >= bin else 0)).sum()}, ignore_index=True)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\nresult = df.groupby(pd.cut(df.views, bins))['username'].count()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['tom', 'tom', 'tom', 'tom', 'jack', 'jack', 'jack', 'jack'],\n                   'post_id': [10, 8, 7, 6, 5, 4, 3, 2],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n# Group the DataFrame by username and views\ngroups = df.groupby(pd.cut(df.views, bins))\n# Count the number of views in each bin for each username\nresult = groups.username.count()\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df.text.str.cat(sep=', ')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep='-')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df.text.str.cat(sep=', ')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep=', ')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep='-')\nprint(result)\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n# Concatenate the two DataFrames\nresult = pd.concat([df1, df2], axis=0)\n# Fill in the missing city and district values from df1\nresult['city'] = result['id'].apply(lambda x: df1.loc[df1['id'] == x]['city'])\nresult['district'] = result['id'].apply(lambda x: df1.loc[df1['id'] == x]['district'])\n# Drop the original id column\nresult = result.drop('id', axis=1)\n# Print the result\nprint(result)\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date'])\nresult = result.set_index('id')\nresult = result.sort_index()\nresult = result.reset_index()\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date', 'district'])\nresult = result.drop_duplicates(keep=False)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult.drop(columns=['B_x'], inplace=True)\nprint(result)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult.fillna(result.B_x, inplace=True)\nprint(result)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A')\nresult['dulplicated'] = result['B_x'].notna()\nprint(result)\n",
        "\nresult = df.groupby('user')[['time', 'amount']].apply(list)\nresult = result.apply(lambda x: sorted(x, key=lambda y: y[0]))\nprint(result)\n",
        "\nresult = df.groupby('user')['time', 'amount'].apply(list)\nresult = result.reset_index().rename(columns={'index': 'time-amount-tuple'})\nprint(result)\n",
        "\nresult = df.groupby('user')['time', 'amount'].apply(list).reset_index()\nresult = result.sort_values(['user', 'time', 'amount'])\nresult = result.apply(lambda x: [x['time'], x['amount']], axis=1)\nresult = result.reset_index()\n",
        "\nresult = pd.concat([series, series, series], axis=1)\n",
        "\ndf_concatenated = pd.DataFrame(series.values, index=series.index, columns=['0', '1', '2', '3'])\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\nresult = []\nfor col in df.columns:\n    if s in col:\n        result.append(col)\nprint(result)\n",
        "\nresult = df[df.columns.str.contains(s, case=False)]\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\ndef find_column(df, s):\n    for col in df.columns:\n        if s in col:\n            return col\n    return None\nresult = []\nfor col in df.columns:\n    if find_column(df, s) is not None:\n        result.append(col)\ndf = df[result]\nprint(df)\n",
        "\nresult = df.apply(lambda x: [x] if len(x) == 1 else x, axis=1)\n",
        "\nresult = df.apply(lambda x: [x] if len(x) == 1 else x, axis=1)\n",
        "\nresult = df.apply(lambda x: [x] if len(x) == 1 else x, axis=1)\n",
        "\nresult = df.loc[0:index, 'User IDs'].values.tolist()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3],[4,5]]))\nids = [str(reverse(list(i))) for i in df.loc[0:index, 'User IDs'].values.tolist()]\nresult = ','.join(ids)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n# Convert the list column to a string\nids = str(df.loc[0:index, 'User IDs'].values.tolist())\n# Remove the square brackets\nids = ids.replace('[', '').replace(']', '')\n# Concatenate the list values into one string\nresult = ','.join(ids)\nprint(result)\n",
        "\n# bin the time into 2-minute intervals\ntime_bins = df['Time'].dt.floor('2min')\n# group the data by the time bins and calculate the mean value for each bin\nresult = df.groupby(time_bins).mean()\n",
        "\n# bin the time into 3-minute intervals\ntime_bins = df['Time'].dt.floor('3min')\n# group the data by the time bins and sum the values\nresult = df.groupby(time_bins).sum()\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\nresult = df.groupby('ID')['TIME'].rank(ascending=True)\nprint(result)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\nresult = df.groupby('ID')['TIME'].rank(ascending=False)\nprint(result)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['TIME'] = df['TIME'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\nresult = df.c.loc[filt]\n",
        "\nresult = df.c.loc[filt]\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\nresult = df.apply(lambda x: [i for i, v in enumerate(x) if not equalp(v, x[0])], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\nresult = df.apply(lambda x: x.columns[np.where(x.isna().values, True, False)], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\nresult = []\nfor i in range(df.shape):\n    for j in range(i+1, df.shape):\n        if equalp(df.iloc[i], df.iloc[j]):\n            result.append(str(df.columns[i]))\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\nresult = []\nfor i in range(df.shape):\n    for j in range(i+1, df.shape):\n        if df.iloc[i, :] != df.iloc[j, :] and (df.iloc[i, :] != df.iloc[i, :] or df.iloc[j, :] != df.iloc[j, :]):\n            result.append((df.iloc[i, :], df.iloc[j, :]))\nprint(result)\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\nresult = ts\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\nresult = df.stack().reset_index()\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\nresult = df.stack().reset_index()\nprint(result)\n",
        "\ndf['dogs'] = df['dogs'].round(2)\n",
        "\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\nresult = df[list_of_my_columns].sum(axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\ndef avg_columns(columns):\n    return df[columns].mean(axis=1)\nresult = avg_columns(list_of_my_columns)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\ndef avg(columns):\n    return df[columns].mean(axis=1)\nresult = df[list_of_my_columns].apply(avg, axis=1)\nprint(result)\n",
        "\nresult = df.sort_index(level=0)\n",
        "\nresult = df.sort_index(level=0, axis=1)\n",
        "\n# Drop the specific dates from the Date column\ndf = df.drop(df[df['Date'].isin(['2020-02-17 15:30:00', '2020-02-18 15:30:00'])].index)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n# Drop the specific dates\ndf = df[~df.index.isin(['2020-02-17 15:30:00', '2020-02-18 15:33:00'])]\n",
        "\n# Filter the correlation matrix to only include values above 0.3\nresult = corr[corr > 0.3]\n",
        "\n# Filter the correlation matrix to only include columns where the value is above 0.3\nresult = corr[corr > 0.3]\n",
        "\nlast_column = df.columns[-1]\ndf.rename(columns={last_column: 'Test'}, inplace=True)\n",
        "\ndf.columns = ['Test']\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'bit1': [0, 1, 1],\n                   'bit2': [0, 1, 0],\n                   'bit3': [1, 0, 1],\n                   'bit4': [1, 0, 1],\n                   'bit5': [0, 1, 1]})\nfrequent = df.groupby('bit1', as_index=False).agg({'bit2': 'count'}).reset_index()\nfrequent['frequent'] = 1\nfreq_count = df.groupby('bit1', as_index=False).agg({'bit2': 'count'}).reset_index()\nfreq_count['freq_count'] = freq_count['bit2']\nresult = pd.concat([df, frequent, freq_count], axis=1)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4]})\nfrequent = df.groupby('bit1', as_index=False).agg({'bit2': 'count'})\nfreq_count = df.groupby('bit1', as_index=False).agg({'bit2': 'sum'})\nresult = pd.concat([df, frequent, freq_count], axis=1)\nprint(result)\n",
        "\nfrequent = df.apply(lambda x: x.value_counts().index[x.value_counts().index > 1], axis=1)\nresult = pd.concat([df, pd.DataFrame({'frequent': frequent, 'freq_count': frequent.value_counts()})], axis=1)\n",
        "\nres = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\ngroupedFrame = res.groupby([\"id1\",\"id2\"])\naggrFrame = groupedFrame.aggregate(numpy.mean)\nresult = pd.concat([aggrFrame, df], axis=1)\nprint(result)\n",
        "\nres = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nresult = pd.DataFrame({\"foo\":res[\"foo\"], \"bar\":res[\"bar\"], \"id1\":res[\"id1\"], \"id2\":res[\"id2\"]})\nprint(result)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum')\nresult = result[['EntityNum','foo','a_col']]\nprint(result)\n",
        "\nimport pandas as pd\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\nresult = pd.merge(df_a, df_b, on='EntityNum')\nresult = result[['EntityNum','foo','b_col']]\nprint(result)\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nimport numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = x[~np.isnan(x)]\nprint(x)\n",
        "\nimport numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx[np.isnan(x)] = np.inf\nprint(x)\n",
        "\nresult = [list(row) for row in x if not np.isnan(row)]\n",
        "b = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    b[i, a[i]] = 1\nprint(b)",
        "\nb = np.zeros((len(a)+1, len(a)+1))\nb[a,:] = 1\nprint(b)\n",
        "\nb = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    b[i, a[i]] = 1\n",
        "\nb = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(len(a)):\n        if a[i] == a[j]:\n            b[i][j] = 1\n",
        "\nb = np.zeros((a.shape, a.shape))\nfor i in range(a.shape):\n    for j in range(a.shape):\n        if a[i,j] == 1:\n            b[i,j] = 1\n        elif a[i,j] == 2:\n            b[i,j] = 2\n        elif a[i,j] == 3:\n            b[i,j] = 3\n        else:\n            b[i,j] = 0\n",
        "\nresult = np.percentile(a, p)\nprint(result)\n",
        "\nB = np.array([A,A,A,A])\nB = B.reshape(ncol,4)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nnrow = 3\nB = np.array([A[i:i+nrow] for i in range(0, len(A), nrow)])\nprint(B)\n",
        "\nB = np.array(A).reshape((ncol, -1))\n",
        "\nB = np.array(A).reshape((ncol, -1))\n",
        "\nimport numpy as np\na = np.array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\nshift = 3\nresult = np.roll(a, shift)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = 3\nresult = a + shift\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = [-2, 3]\nresult = np.roll(a, shift, axis=0)\nprint(result)\n",
        "\nimport numpy as np\nimport random\ndef generate_array():\n    r = np.random.randint(3, size=(100, 2000)) - 1\n    r_old = np.copy(r)\n    r_new = np.copy(r)\n    return r, r_old, r_new\nr, r_old, r_new = generate_array()\nprint(r, r_old, r_new)\n",
        "\nresult = np.argmax(a, axis=0)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(a.max(), a.shape)\nprint(result)\n",
        "\n    result = np.argmax(a, axis=0)\n    return result.ravel()\n    ",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nz = any(isnan(a), axis=0)\ndelete(a, z, axis = 1)\n",
        "\na = a[~np.isnan(a).any(axis=1)]\n",
        "\nresult = np.array(a)\n",
        "\nperm_matrix = np.array([[0, 1, 2, 3, 4], [4, 0, 3, 2, 1]])\na_perm = np.dot(a, perm_matrix)\n",
        "\nresult = np.moveaxis(a, permutation, 0)\nprint(result)\n",
        "\nmin_val = np.min(a)\nrow_index = np.unravel_index(min_val, a.shape)\ncol_index = row_index[1]\nresult = (row_index, col_index)\nprint(result)\n",
        "\nresult = np.unravel_index(a.max(), a.shape)\nprint(result)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nimport numpy as np\ndegree = 90\nresult = np.sin(degree * np.pi / 180)\nprint(result)\n",
        "\nimport numpy as np\ndegree = 90\nresult = np.cos(degree * np.pi / 180)\nprint(result)\n",
        "\nif np.sin(number) > 0:\n    result = 0\nelse:\n    result = 1\n",
        "\nimport numpy as np\nvalue = 1.0\nresult = np.degrees(np.arctan(value))\nprint(result)\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0, 0))\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0, 0))\n",
        "\na = a**power\n",
        "\n    result = a**power\n    ",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\nresult = np.divide(numerator, denominator)\nprint(result)\n",
        "\n    result = np.divide(numerator, denominator)\n    ",
        "\nresult = np.divide(numerator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, b, c)\n",
        "\ndiagonal = np.diag_indices(a.shape)\nresult = a[diagonal[::-1]]\n",
        "\ndiagonal = np.diag_indices(a.shape)\nresult = a[diagonal[::-1]]\n",
        "\ndiagonal = np.diag_indices(a.shape)\nresult = a[diagonal]\nresult = result.T\nprint(result)\n",
        "\ndiagonal = np.diag_indices(a.shape)\nresult = a[diagonal]\nresult = np.roll(result, 1, axis=1)\nresult = np.roll(result, -1, axis=0)\nprint(result)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape):\n    result.append(X[i])\nprint(result)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape):\n    result.append(X[i])\nprint(result)\n",
        "\n    result = []\n    for i in range(X.shape):\n        result.append(X[i])\n    ",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape):\n    result.append(X[i])\nprint(result)\n",
        "\nresult = np.fromstring(mystr, dtype=int, sep='')\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\ncol = 2\nmultiply_number = 5.2\nresult = a[:, col] * multiply_number\nresult = np.cumsum(result)\nprint(result)\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\nresult = a[row] * multiply_number\ncumulative_sum = np.cumsum(result)\nprint(cumulative_sum)\n",
        "\nresult = a[row] / divide_number\n",
        "\nresult = np.column_stack((a, np.zeros((a.shape, 1))))\n",
        "\nimport numpy as np\na = np.random.rand(np.random.randint(5, 10), np.random.randint(6, 10))\nrow_size = a.shape\nresult = []\nfor i in range(row_size):\n    result.append(a[0, i])\nprint(result)\n",
        "\n# Calculate the mean and standard deviation of each sample\na_mean = np.mean(a)\na_std = np.std(a)\nb_mean = np.mean(b)\nb_std = np.std(b)\n# Create a weighted t-test object\nt_test = scipy.stats.ttest_ind(a, b, equal_var=False)\n# Get the p-value\np_value = t_test.pvalue\n# Print the p-value\nprint(p_value)\n",
        "\n# Calculate the mean and standard deviation of each sample\na_mean = np.mean(a)\na_std = np.std(a)\nb_mean = np.mean(b)\nb_std = np.std(b)\n# Create a weighted t-test object\nt_test = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n# Get the p-value\np_value = t_test.pvalue\n# Print the p-value\nprint(p_value)\n",
        "\nimport numpy as np\nimport scipy.stats\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\n# Calculate the pooled variance\npooled_var = (anobs * avar + bnobs * bvar) / (anobs + bnobs)\n# Calculate the t-statistic\nt_statistic = (bmean - amean) / np.sqrt(pooled_var)\n# Calculate the p-value\np_value = scipy.stats.t.sf(t_statistic, df=anobs + bnobs - 2)\nprint(p_value)\n",
        "\noutput = []\nfor i in range(len(A)):\n    for j in range(len(B[0])):\n        if A[i][j] not in B:\n            output.append(A[i][:])\nprint(output)\n",
        "\noutput = np.where(np.in1d(A,B), A, B)\n",
        "\nimport numpy as np\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\nsort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n",
        "\nimport numpy as np\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\nsort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n",
        "\nimport numpy as np\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\nsort_indices = numpy.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n",
        "\nresult = np.argsort(a, axis=0)\nb = b[result]\n",
        "\na = a[:, :-1]\n",
        "\na = a[:-1,:]\n",
        "\na = a[:, 1:]\n",
        "\nresult = a[:, del_col]\n",
        "\nimport numpy as np\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\na[pos] = element\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\npos = 1\nelement = [3,5]\na = np.insert(a, pos, element, axis=1)\nprint(a)\n",
        "\n    a[pos] = element\n    ",
        "\ninserted_rows = np.insert(a, pos, element, axis=1)\n",
        "\nresult = [np.array(arr) for arr in array_of_arrays]\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\nresult = np.all(a == a[0])\nprint(result)\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\nresult = np.all(np.array_equal(a, a))\nprint(result)\n",
        "\n    result = np.all([np.array_equal(a[0], a[i]) for i in range(1, len(a))])\n    ",
        "\nweights = np.zeros((30, 20))\nfor i in range(30):\n    for j in range(20):\n        weights[i, j] = (x[j+1] - x[j]) * (y[i+1] - y[i])\nresult = np.sum(weights * (np.cos(x)**4 + np.sin(y)**2))\nprint(result)\n",
        "\n    result = np.zeros((example_x.shape, example_y.shape))\n    for i in range(example_x.shape):\n        for j in range(example_y.shape):\n            result[i, j] = (x[i] - 0.5)**4 + (y[j] - 0.5)**2\n    return result\n",
        "\nresult = np.cumsum(grades)\n",
        "\necdf = np.cumsum(grades / np.sum(grades))\nresult = ecdf(eval)\nprint(result)\n",
        "\necdf = np.cumsum(grades)\nlow, high = np.searchsorted(ecdf, [threshold, 1])\nprint(low, high)\n",
        "\nnums = np.random.randint(2, size=size)\nnums = nums * (1 - one_ratio) + 1 * (one_ratio)\nprint(nums)\n",
        "\nimport torch\nimport numpy as np\na = torch.ones(5)\na_np = a.numpy()\nprint(a_np)\n",
        "\nimport torch\nimport numpy as np\na = np.ones(5)\na_pt = torch.from_numpy(a)\nprint(a_pt)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = tf.ones([2,3,4])\na_np = a.numpy()\nprint(a_np)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.constant(a)\nprint(a_tf)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = [3, 4, 0, 5, 1, 2]\nfor i in range(len(a)-1, -1, -1):\n    result.append(i)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = []\nfor i in range(len(a)):\n    result.append(a.tolist().index(a[i]))\nprint(result)\n",
        "\nresult = np.argsort(a)[::-1][:N]\nprint(result)\n",
        "\nimport numpy as np\nA = np.arange(16).reshape(4, 4)\nn = 5\nresult = np.power(A, n)\nprint(result)\n",
        "\npatches = []\nfor i in range(0, a.shape, 2):\n    for j in range(0, a.shape, 2):\n        patches.append(a[i:i+2, j:j+2])\nresult = patches\n",
        "\npatches = []\nfor i in range(0, a.shape, 2):\n    for j in range(0, a.shape, 2):\n        patches.append(a[i:i+2, j:j+2])\n",
        "\nresult = []\nfor i in range(0, a.shape, 2):\n    for j in range(i, a.shape, 2):\n        result.append(a[i:i+2, j:j+2])\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\ndef extract_patches(a, patch_size):\n    patches = []\n    for i in range(0, a.shape, patch_size):\n        for j in range(0, a.shape, patch_size):\n            patches.append(a[i:i+patch_size, j:j+patch_size])\n    return patches\nresult = extract_patches(a, patch_size)\nprint(result)\n",
        "\nresult = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i, j] = a[i//2, j//2][i%2, j%2]\n",
        "\nimport numpy as np\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\ndef extract_patches(a, patch_size):\n    patches = []\n    for i in range(0, a.shape, patch_size):\n        for j in range(0, a.shape, patch_size):\n            patches.append(a[i:i+patch_size, j:j+patch_size])\n    return patches\nresult = extract_patches(a, patch_size)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 5\nresult = a[:, low:high]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 0\nhigh = 2\nresult = a[low:high,:]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 10\nresult = a[:, low:high]\nprint(result)\n",
        "\na = np.array([list(map(float, line.strip().split())) for line in string.split(\",\")])\n",
        "\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\nresult = np.random.loguniform(min, max, size=n)\nprint(result)\n",
        "\nimport numpy as np\nmin = 0\nmax = 1\nn = 10000\ndef log_uniform_sample(min, max, n):\n    log_range = np.log(max) - np.log(min)\n    log_samples = np.random.uniform(np.log(min), np.log(max), size=n)\n    return np.exp(log_samples)\nresult = log_uniform_sample(min, max, n)\nprint(result)\n",
        "\n    result = np.random.loguniform(min, max, size=n)\n    ",
        "\nB = np.zeros(len(A))\nB[0] = a*A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\nprint(B)\n",
        "\nB = np.zeros(len(A))\nB[0] = a*A[0]\nfor i in range(1, len(A)):\n    B[i] = a*A[i] + b*B[i-1] + c*B[i-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.zeros((3,0))\n",
        "\nlinearInd = sub2ind(dims, 2, 1, 2)\nresult = a[linearInd]\n",
        "\nrow, col, _ = index\nlinear_index = row * dims[1] + col\n",
        "\nvalues = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df)\n",
        "\nresult = np.bincount(accmap[a])\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.max(a[index==1], axis=1)\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.zeros(len(accmap))\nfor i, v in enumerate(accmap):\n    result[accmap[i]] += a[i]\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.min(a[index >= 0], axis=0)\nprint(result)\n",
        "\nimport numpy as np\nx = [[2, 2, 2],\n     [2, 2, 2],\n     [2, 2, 2]]\ny = [[3, 3, 3],\n     [3, 3, 3],\n     [3, 3, 1]]\ndef elementwise_function(element_1, element_2):\n    return (element_1 + element_2)\nz = np.empty_like(x)\nfor i in range(len(x)):\n    for j in range(len(x[i])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\nprint(z)\n",
        "\nnp.random.choice(lista_elegir, size=samples, p=probabilit)\n",
        "\nresult = np.pad(a, ((0, high_index - low_index), (0, 0)), 'constant')\n",
        "\nimport numpy as np\nx = np.array([-2, -1.4, -1.1, 0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2])\nresult = x[x >= 0]\nprint(result)\n",
        "\nimport numpy as np\nx = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\nresult = x[x != 0]\nprint(result)\n",
        "\nbin_data = [(4,2),(5,6),(7,5)]\nbin_data_mean = [3,5.5,6]\n",
        "\nbin_data = [(4,2),(5,6),(7,5)]\nbin_data_max = [4,6,7]\n",
        "\nbin_data = []\nfor i in range(0, len(data), bin_size):\n    bin_data.append(data[i:i+bin_size])\n",
        "\nbin_data = [(3,5,7),(7,5,4),(2,5,6)]\nbin_data_mean = [5,5.33,4.33]\n",
        "\nbin_data = []\nfor i in range(len(data)-bin_size+1):\n    bin_data.append(data[i:i+bin_size])\n",
        "\nbin_data = []\nfor i in range(0, len(data), bin_size):\n    bin_data.append(data[i:i+bin_size])\n",
        "\ndef smoothclamp(x):\n    x_smooth = (x - x_min) / (x_max - x_min)\n    return (x_min + x_max) * x_smooth + (1 - x_smooth) * (x_min + x_max)\n",
        "\nsmoothstep = np.interp(x, [x_min, x_max], [0, 1], left=0, right=1)\nresult = smoothstep**N\n",
        "\nresult = np.correlate(a, b, mode='full')\n",
        "\nimport numpy as np\nimport pandas as pd\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('colu'))\nresult = np.array([df.loc[i,:] for i in range(df.shape)])\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('colu'))\n# Create a 3D array with shape (15, 4, 5)\nresult = np.zeros((15, 4, 5))\n# Loop through the rows of the DataFrame\nfor i in range(df.shape):\n    # Get the major and timestamp for the current row\n    major = df.loc[i, 'major']\n    timestamp = df.loc[i, 'timestamp']\n    \n    # Get the values for the current row\n    values = df.loc[i, :]\n    \n    # Loop through the columns of the current row\n    for j in range(4):\n        # Get the value for the current column\n        value = values[j]\n        \n        # Set the value in the 3D array\n        result[major, j, timestamp] = value\nprint(result)\n",
        "\nresult = np.unpackbits(np.uint8(a))\n",
        "\nresult = np.unpackbits(np.uint8(a))\nresult = result.reshape((len(a), m))\nprint(result)\n",
        "\nresult = np.zeros((m, m))\nfor i in range(m):\n    result[i] = np.unpackbits(np.uint8(a[i]))\n",
        "\nmu = np.mean(a)\nsigma = np.std(a)\nthird_sigma = 3 * sigma\nstart = mu - third_sigma\nend = mu + third_sigma\nresult = (start, end)\nprint(result)\n",
        "\nmu = np.mean(a)\nsigma = np.std(a)\nresult = (mu - 2 * sigma, mu + 2 * sigma)\nprint(result)\n",
        "\nimport numpy as np\nexample_a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\ndef f(a = example_a):\n    # Calculate the mean\n    mu = np.mean(a)\n    \n    # Calculate the standard deviation\n    sigma = np.std(a)\n    \n    # Calculate the 3rd standard deviation\n    sigma_3 = sigma * 3\n    \n    # Calculate the start and end of the 3rd standard deviation interval\n    start = mu - sigma_3\n    end = mu + sigma_3\n    \n    # Return the start and end of the 3rd standard deviation interval\n    return (start, end)\n",
        "\ndef calculate_2nd_std(a):\n    mean = np.mean(a)\n    std = np.std(a)\n    sigma = std * 2\n    result = (a - mean) / sigma\n    return result\n",
        "\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\nprob = np.percentile(masked_data, percentile)\n",
        "\nimport numpy as np\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = 0\nzero_cols = 0\nfor i in range(a.shape):\n    for j in range(a.shape):\n        if i == zero_rows or j == zero_cols:\n            a[i][j] = 0\nprint(a)\n",
        "\nfor i in range(len(a)):\n    for j in range(len(a[0])):\n        if i in zero_rows or j in zero_cols:\n            a[i][j] = 0\n",
        "\na[1,:] = 0\na[:,0] = 0\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.zeros_like(a)\nmask[np.amax(a, axis=1) == a] = 1\nprint(mask)\n",
        "\nmask = np.where(a.min(axis=1) == a, True, False)\n",
        "\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(post, distance)\n# Print the result\nprint(result)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = np.stack([xi.dot(xi.T) for xi in X], axis=2)\nprint(result)\n",
        "\nX = np.array([[81, 63, 63],\n        [63, 49, 49],\n        [63, 49, 49]])\nfor i in range(len(Y)):\n    X = np.dot(X, Y[i])\n",
        "\nimport numpy as np\na = np.array([9, 2, 7, 0])\nnumber = 0\nis_contained = np.all(a == number)\nprint(is_contained)\n",
        "\nC = np.delete(A, np.in1d(A, B))\n",
        "\nC = np.where(np.isin(A, B), A, 0)\n",
        "\nC = np.zeros(len(A))\nfor i in range(len(A)):\n    if A[i] in B:\n        C[i] = A[i]\nprint(C)\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\nresult = rankdata(a).astype(int)\nresult = result[::-1]\nprint(result)\n",
        "\nresult = []\nfor i in range(len(a)):\n    rank = 0\n    for j in range(i+1, len(a)):\n        if a[i] == a[j]:\n            rank += 1\n    result.append(rank)\n",
        "\n    result = np.flipud(rankdata(a))\n    ",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, 1, -2],\n                 [ -1,  0, 1],\n                 [ -2,  1,  0]])\ndists = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        dists[i, j] = (x_dists[i], y_dists[j])\nprint(dists)\n",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ndists = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        dists[i, j] = (x_dists[i, j], y_dists[i, j])\nprint(dists)\n",
        "\nresult = a[second, third]\n",
        "arr = np.zeros((20,10,10,2))",
        "\nl1 = X.sum(axis=1)\nresult = X/l1.reshape(5,1)\nprint(result)\n",
        "\nx = np.array([LA.norm(v,ord=2) for v in X])\n",
        "\nx = np.array([LA.norm(v,ord=np.inf) for v in X])\n",
        "\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nchoices = ['XX']\nconditions = [a[\"a\"].str.contains(target),\n               a[\"a\"].str.contains('blog'),\n               a[\"a\"].str.contains('credit-card-readers/|machines|poss|team|transaction_fees'),\n               a[\"a\"].str.contains('signup|sign-up|create-account|continue|checkout'),\n               a[\"a\"].str.contains('complete'),\n               a[\"a\"].str.contains('/za/|/')]\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n",
        "\nresult = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(i+1, len(a)):\n        result[i,j] = result[j,i] = np.sqrt(np.sum((a[i] - a[j])**2))\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\nfor i in range(a.shape):\n    for j in range(i+1, a.shape):\n        result[i,j] = result[j,i] = np.linalg.norm(a[i] - a[j])\nprint(result)\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\nfor i in range(a.shape):\n    for j in range(i+1, a.shape):\n        result[i, j] = np.linalg.norm(a[i] - a[j])\nprint(result)\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nNA = np.array(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nresult = np.delete(a, np.where(np.diff(a) == 0))\n",
        "\nresult = np.unique(a[~np.isinf(a)])\n",
        "\nimport numpy as np\nimport pandas as pd\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\ndf = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\nprint(df)\n",
        "\n    df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\n    df = df.sort_index()\n    return df\n    ",
        "\ndf = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\ndf['max'] = df.max(axis=1)\nprint(df)\n",
        "\nresult = []\nfor i in range(a.shape):\n    for j in range(a.shape):\n        window = a[i:i+size, j:j+size]\n        if i >= 0 and j >= 0 and i < a.shape and j < a.shape:\n            result.append(window)\nprint(result)\n",
        "\nresult = []\nfor i in range(a.shape):\n    for j in range(a.shape):\n        window = a[i:i+size, j:j+size]\n        if i == 0 and j == 0:\n            result.append(window)\n        elif i == a.shape-1 and j == a.shape-1:\n            result.append(window)\n        elif i == a.shape-1 and j != a.shape-1:\n            result.append(window[1:, 1:])\n        elif i != a.shape-1 and j == a.shape-1:\n            result.append(window[1:, :])\n        else:\n            result.append(window)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\nresult = np.mean(a)\nprint(result)\n",
        "\n    result = np.mean(a)\n    ",
        "\nresult = Z[:,:,:,-1:]\n",
        "\nresult = a[-1:, :]\n",
        "\nimport numpy as np\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ 57, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  78, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ 57, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, 727]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ 66, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\nfor cnt in CNTS:\n    if c in cnt:\n        return True\nprint(result)\n",
        "\nfor cnt in CNTS:\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue\n    if np.all(np.isnan(c)):\n        continue\n    if np.all(np.isnan(cnt)):\n        continue",
        "\ninterpolator = intp.RectBivariateSpline(a, a, a)\nresult = interpolator(x_new, y_new)\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n",
        "\ndiag_i = np.diag(i)\ni = diag_i\n",
        "\na[np.triu_indices_from(a)] = 0\n",
        "\ndt_index = pd.date_range(start=start, end=end, freq=\"1s\")\nresult = pd.DatetimeIndex(dt_index)\n",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 1, 3, 4, 5])\na = 1\nb = 4\nresult = np.where(x == a)\nresult = result[0]\nprint(result)\n",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nresult = []\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result.append(i)\nprint(result)\n",
        "\n# Calculate the gradient of the function\ng = np.gradient(y, x)\n# Calculate the hessian matrix of the function\nH = np.zeros((len(x), len(x)))\nfor i in range(len(x)):\n    for j in range(len(x)):\n        H[i, j] = np.gradient(g[i, j], x)\n# Use the scipy.optimize.minimize function to minimize the squared error\nfrom scipy.optimize import minimize\nresult = minimize(fun=lambda a, b, c: np.sum((y - f(x, a, b, c))**2), x0=[0, 0, 0], args=(x, y), method='SLSQP')\n",
        "\ncoeffs = np.polyfit(x, y, degree)\nresult = np.polyval(coeffs, x)\nprint(result)\n",
        "\ntemp_df = pd.DataFrame(temp_arr)\nfor i in range(len(df)):\n    df.iloc[i] = df.iloc[i] - temp_df.iloc[temp_arr[i-1]]\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\na = np.array([[-1, 2], [-0.5, 6]])\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\nprint(result)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\narr = np.array([[1.0,2.0,3.0],[0.1, 5.1, 100.1],[0.01, 20.1, 1000.1]])\nresult = MinMaxScaler(arr)\nprint(result)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\na = np.array([[[1, 0.5, -2], [-0.5,1, 6], [1,1,1]], [[-2, -3, 1], [-0.5, 10, 6], [1,1,1]]])\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\nprint(result)\n",
        "\nmask = arr < -10\nmask2 = arr < 15\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narry[~mask2] = 30\n",
        "\nimport numpy as np\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\nmask = arr_temp < n1\nmask2 = arr_temp < n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arry[mask3] + 5\narry[~mask2] = 30\nprint(arr)\n",
        "\ns1_abs_diff = np.abs(s1 - s2)\nresult = np.sum(s1_abs_diff > 1e-6)\n",
        "s1 = np.append(s1, np.nan)\ns2 = np.append(s2, np.nan)\nresult = np.where(np.isnan(s1), 0, s1 - s2)\nprint(result)",
        "\nresult = np.all(a == a)\n",
        "\nresult = all(np.isnan(arr) for arr in a)\n",
        "\nresult = np.pad(a, ((0, shape[1] - a.shape[1]), (0, shape[0] - a.shape[0])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), mode='constant')\n",
        "\nresult = np.pad(a, ((0, shape - a.shape), (0, shape - a.shape)), mode='constant', constant_values=element)\n",
        "\n    result = np.zeros((93,13))\n    result[:arr.shape, :] = arr\n    ",
        "\nresult = np.zeros((93, 13))\nresult[:a.shape, :] = a\n",
        "\na = a.reshape((a.shape[0]//3,3))\n",
        "\nresult = np.zeros((a.shape, b.shape))\nfor i in range(a.shape):\n    for j in range(b.shape):\n        result[i, j] = a[i, b[i, j]]\n",
        "\nresult = a[b]\n",
        "\nresult = a[b]\n",
        "\nresult = np.zeros((a.shape, b.shape))\nfor i in range(a.shape):\n    for j in range(b.shape):\n        result[i][j] = a[i][b[j]]\n",
        "\nresult = np.sum(a[b], axis=0)\n",
        "\nmask = (df['a'] >= 1) & (df['a'] <= 4)\nresult = np.where(mask, df['b'], np.nan)\n",
        "\nresult = im[1:-1,1:-1]\n",
        "\nresult = A[A != 0]\n",
        "\nimport numpy as np\ndef remove_peripheral_zeros(im):\n    # Get the dimensions of the image\n    m, n = im.shape\n    # Create an empty array to store the result\n    result = np.zeros((m, n))\n    # Iterate over each row and column of the image\n    for i in range(m):\n        for j in range(n):\n            # Check if the current pixel is non-zero\n            if im[i][j] != 0:\n                # If it is, set the corresponding pixel in the result array to the current pixel\n                result[i][j] = im[i][j]\n    return result\n# Test the function\nim = np.array([[1,1,1,1,1,5],\n               [1,0,0,1,2,0],\n               [2,1,0,0,1,0],\n               [1,0,0,7,1,0],\n               [1,0,0,0,0,0]])\nresult = remove_peripheral_zeros(im)\nprint(result)\n",
        "\nimport numpy as np\ndef remove_peripheral_zeros(im):\n    # Get the dimensions of the image\n    m, n = im.shape\n    # Create an empty array to store the result\n    result = np.zeros((m-2, n-2))\n    # Iterate over the rows and columns of the image\n    for i in range(1, m-1):\n        for j in range(1, n-1):\n            # Check if the current pixel is non-zero\n            if im[i, j] != 0:\n                # If it is, add it to the result array\n                result[i-1, j-1] = im[i, j]\n    # Return the result array\n    return result\n# Test the function\nim = np.array([[0,0,0,0,0,0],\n               [0,0,5,1,2,0],\n               [0,1,8,0,1,0],\n               [0,0,0,7,1,0],\n               [0,0,0,0,0,0]])\nresult = remove_peripheral_zeros(im)\nprint(result)\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n# turn on minor ticks on y axis only\nax.set_xticks(np.arange(0, 10, 1))\nax.set_minorticks(np.arange(0, 10, 0.5))\n",
        "\nplt.xticks(np.arange(0, 11, 1))\nplt.yticks(np.arange(0, 11, 1))\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\nplt.xticks(np.arange(0, 10, 1))\n",
        "\nfor i in range(5):\n    plt.plot(x, np.random.randn(10), label=f\"Line {i}\")\n",
        "\nfor i in range(5):\n    plt.plot(x, np.random.randn(10), label=f\"Line {i}\")\n",
        "\nplt.plot(x, y, 'o-', markersize=10)\nplt.show()\n",
        "\nplt.plot(x, y, 'o-', markersize=10)\nplt.show()\n",
        "\nax.set_ylim(bottom=0)\n",
        "\nplt.fill_between(x, x.min(), x.max(), color='red', alpha=0.5)\nplt.plot(x)\n",
        "\nplt.plot([0, 1], [0, 2], 'k-')\n",
        "\nx1, y1 = 0, 0\nx2, y2 = 1, 2\nplt.plot([x1, x2], [y1, y2], 'r-')\n",
        "\nsns.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", data=df, kind=\"scatter\", hue=\"Gender\")\n",
        "\nsns.lineplot(x, y)\nplt.show()\n",
        "\nsns.lineplot(x=x, y=y)\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', markeredgewidth=7)\n",
        "\nplt.legend(fontsize=20)\nplt.show()\n",
        "\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Cosine Function')\nplt.legend(fontsize=20)\n",
        "\nl.set_facecolor(\"none\")\nl.set_edgecolor(\"black\")\nl.set_alpha(0.2)\n",
        "\nfor i in range(len(l)):\n    l[i].set_edgecolor(\"black\")\n",
        "\nl.set_color(\"red\")\nl.set_marker(\"o\")\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(np.arange(0, 2 * np.pi, 2))\nplt.xlabel(\"Angle\")\nplt.ylabel(\"Value\")\nplt.show()\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H)\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.xlabel('X')\nplt.xlabel('X')\n",
        "\ng.set_xticklabels(rotation=90)\n",
        "\nplt.title(myTitle, fontsize=10, linewidth=0.5)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\n",
        "\nplt.gca().invert_yaxis()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n# plot x, then y then z, but so that x covers y and y covers z\nx_min, x_max = plt.xlim()\ny_min, y_max = plt.ylim()\nz_min, z_max = plt.zlim()\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.zlim(z_min, z_max)\nplt.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, s=50, c='blue', edgecolors='black')\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n# make all axes ticks integers\nx_ticks = np.arange(0, 11, 1)\nplt.xticks(x_ticks, x_ticks)\nplt.xlim(0, 10)\n",
        "\nsns.set_style(\"whitegrid\")\nsns.factorplot(y=\"coverage\", x=\"reports\", kind=\"bar\", data=df, label=\"Total\")\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\nplt.show()\n",
        "\nax.set_dashes([(5, 5), (5, 5)])\n",
        "\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\naxs[0].plot(x, y1)\naxs[0].set_title(\"y1 vs x\")\naxs[1].plot(x, y2)\naxs[1].set_title(\"y2 vs x\")\n",
        "\nplt.subplots_adjust(top=1, bottom=0, left=0, right=1)\nplt.subplots_adjust(hspace=0, wspace=0)\nplt.xlabel('x')\nplt.ylabel('y1')\nplt.title('y1 vs x')\nplt.subplots_adjust(top=1, bottom=0.2, left=0.5, right=0.95)\nplt.xlabel('x')\nplt.ylabel('y2')\nplt.title('y2 vs x')\n",
        "\nplt.setp(plt.gca().get_xticklabels(), visible=False)\n",
        "\nplt.xticks([])\n",
        "\nplt.xticks([3, 4], ['3', '4'])\nplt.yticks([])\n",
        "\nplt.yticks(np.arange(0, 11, 1))\nplt.grid(b=True, which='both', color='gray', linestyle='-')\nplt.show()\n",
        "\nplt.xticks([1, 2], ['1', '2'])\nplt.yticks([3, 4], ['3', '4'])\nplt.grid(True, which='both', axis='both')\n",
        "\nplt.grid(True)\nplt.show()\n",
        "\nplt.legend(loc='lower right')\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), subplot_wspace=0.05, subplot_hspace=0.05)\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\n",
        "\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n",
        "\nax.set_xlim([0, 4])\n",
        "\nplt.xlabel('X')\nplt.xlim(-0.5, 9.5)\nplt.xticks(np.arange(10))\nplt.yticks(np.arange(10))\nplt.grid(True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"green\")\nsns.distplot(tips[\"tip\"], color=\"blue\")\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"green\")\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\")\n",
        "\nfor celltype, data in df.groupby(\"celltype\"):\n    plt.bar(data[\"celltype\"], data[\"s1\"], color=matplotlib.colors.to_hex(data[\"celltype\"]))\n    plt.xlabel(\"celltype\", rotation=90)\n    plt.xticks(rotation=90)\n    plt.show()\n",
        "\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n# For data in df, make a bar plot of s1 and s1 and use celltype as the xlabel\n# Make the x-axis tick labels rotate 45 degrees\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xticks(rotation=45)\nplt.xlabel(\"celltype\")\nplt.ylabel(\"score\")\nplt.title(\"Bar plot of s1 and s2\")\nplt.legend()\nplt.show()\n",
        "\nplt.xlabel('X', color='red')\nplt.ylabel('Y', color='red')\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.xscale('log')\nplt.show()\n",
        "\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nplt.axvline(0.22058956, color='red', linestyle='--')\nplt.axvline(0.33088437, color='red', linestyle='--')\nplt.axvline(2.20589566, color='red', linestyle='--')\n",
        "\nplt.xticks(rotation=45)\nplt.yticks(rotation=-45)\n",
        "\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make two side-by-side subplots and and in each subplot, plot y over x\n# Title each subplot as \"Y\"\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title(\"Y\")\naxs[1].plot(x, y)\naxs[1].set_title(\"Y\")\nplt.show()\n",
        "\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, size=30)\nplt.show()\n",
        "\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n# create dataframe with columns a, b, and c\ndf = pd.DataFrame({'a': a, 'b': b, 'c': c})\n# create scatter plot of a over b\nplt.scatter(df['a'], df['b'])\n# annotate each data point with correspond numbers in c\nfor i in range(len(df)):\n    plt.annotate(str(df.loc[i, 'c']), xy=(df.loc[i, 'a'], df.loc[i, 'b']), xytext=(df.loc[i, 'a'] + 0.05, df.loc[i, 'b'] + 0.05), fontsize=10)\n# show plot\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend()\nplt.title(\"Line Chart\")\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", fontweight=\"bold\")\n",
        "\nplt.hist(x, bins=10, density=True, align='left', alpha=0.5)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Make two subplots. Make the first subplot three times wider than the second subplot but they should have the same height.\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[1].plot(x, y)\n# Set the width of the first subplot to three times the width of the second subplot.\naxs[0].set_xlim(0, 10)\naxs[1].set_xlim(0, 10)\naxs[0].set_ylim(0, 10)\naxs[1].set_ylim(0, 10)\n",
        "\nplt.hist2d(x, y, bins=bins, cmap='bone', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\n# Plot a grouped histograms of x and y on a single chart with matplotlib\n# Use grouped histograms so that the histograms don't overlap with each other\nfig, ax = plt.subplots(figsize=(10, 5))\nax.hist2d(x, y, bins=20, cmap='Blues', alpha=0.5)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n",
        "\nx1, y1 = a, b\nx2, y2 = c, d\nx3, y3 = c, d + (d - b) / (c - a) * (x2 - x1)\nx4, y4 = a, b + (b - a) / (c - a) * (y2 - y1)\nplt.plot([x1, x2, x3, x4], [y1, y2, y3, y4], 'r-')\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n",
        "\nx = np.random.random((10, 10))\ny = np.random.random((10, 10))\n# make two colormaps with x and y and put them into different subplots\ncmap1 = plt.get_cmap('coolwarm')\ncmap2 = plt.get_cmap('coolwarm')\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs[0, 0].imshow(x, cmap=cmap1)\naxs[0, 1].imshow(y, cmap=cmap2)\naxs[1, 0].imshow(x, cmap=cmap1)\naxs[1, 1].imshow(y, cmap=cmap2)\n# use a single colorbar for these two subplots\ncbar = axs[0, 0].figure.colorbar(axs[0, 0].imshow(x, cmap=cmap1), ax=axs)\ncbar.ax.set_ylabel('Temperature')\ncbar.ax.tick_params(labelsize=12)\ncbar.ax.set_title('Colorbar for subplot 1')\ncbar = axs[1, 0].figure.colorbar(axs[1, 0].imshow(x, cmap=cmap1), ax=axs)\ncbar.ax.set_ylabel('Temperature')\ncbar.ax.tick_params(labelsize=12)\ncbar.ax.set_title('Colorbar for subplot 2')\nplt.show()\n",
        "\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n# plot y over x and z over a in two different subplots\n# Set \"Y and Z\" as a main title above the two subplots\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title(\"Y over X\")\naxs[1].plot(z, a)\naxs[1].set_title(\"Z over A\")\nplt.show()\n",
        "\nplt.plot(points[:, 0], points[:, 1], 'ro')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xscale('log')\nplt.show()\n",
        "\nplt.rc('font', size=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.plot(x, y)\nplt.show()\n",
        "\nax.set_xticks(np.arange(1, 11))\nax.set_yticks(np.arange(1, 11))\nax.set_xlabel('X')\nax.set_ylabel('Y')\n",
        "\nfor line in lines:\n    x1, y1 = line\n    x2, y2 = line\n    plt.plot([x1, x2], [y1, y2], color=c[line])\n",
        "\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-Log Plot')\nplt.xticks(np.log10(x))\nplt.yticks(np.log10(y))\nplt.show()\n",
        "\ndf = pd.DataFrame(\n    np.random.randn(50, 4),\n    index=pd.date_range(\"1/1/2000\", periods=50),\n    columns=list(\"ABCD\"),\n)\ndf = df.cumsum()\n# make four line plots of data in the data frame\nfig, axs = plt.subplots(4, 1, figsize=(10, 5))\nfor i, ax in enumerate(axs.flatten()):\n    ax.plot(df.index, df.iloc[:, i], label=f\"Line {i+1}\")\n    ax.scatter(df.index, df.iloc[:, i], s=10, color=df.iloc[:, i])\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Value\")\n    ax.set_title(f\"Line {i+1}\")\n    ax.legend()\nplt.show()\n",
        "\nfig, ax = plt.subplots()\nax.hist(data, bins=10, normed=True)\nax.set_xlabel('Data')\nax.set_ylabel('Percentage')\nax.set_yticks([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\nax.set_yticklabels(['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\nplt.show()\n",
        "\nplt.plot(x, y, 'ro-')\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\na = np.arange(10)\nz = np.arange(10)\n# Plot y over x and a over z in two side-by-side subplots.\n# Label them \"y\" and \"a\" and make a single figure-level legend using the figlegend function\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs[0, 0].plot(x, y)\naxs[0, 0].set_title(\"y over x\")\naxs[0, 1].plot(a, z)\naxs[0, 1].set_title(\"a over z\")\naxs[1, 0].plot(x, y)\naxs[1, 0].set_title(\"y over x\")\naxs[1, 1].plot(a, z)\naxs[1, 1].set_title(\"a over z\")\n# Create a single figure-level legend\nfiglegend(axs, [\"y\", \"a\"])\n# Display the plot\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs)\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs)\naxs.set_xlabel(\"bill_length_mm\")\naxs.set_ylabel(\"bill_depth_mm\")\naxs.set_title(\"Seaborn Regression Plots\")\nplt.show()\n",
        "\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\nax.set_xlabel(\"second\")\n",
        "\nplt.plot(x, y)\nplt.legend(('Lambda',), loc='best')\nplt.show()\n",
        "\nplt.xticks(range(0, 10, 2), [2.1, 3, 7.6, *plt.xticks()])\n",
        "\nplt.xticks(rotation=60, ha=\"left\")\n",
        "\nplt.gca().invert_yaxis()\nplt.xticks(rotation=60)\nplt.yticks(rotation=60)\n",
        "\nplt.xticks(rotation=45)\nplt.setp(plt.xticks(), color='black', fontsize=8)\nplt.setp(plt.gca().get_xticklabels(), fontsize=8, rotation=45, ha='right', va='bottom')\n",
        "\nplt.xlim(0, 9)\nplt.ylim(0, 9)\n",
        "\nplt.xticks(np.arange(10), [i+1 for i in range(10)], rotation=45)\nplt.yticks(np.arange(10), [i+1 for i in range(10)])\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make a two columns and one row subplots. Plot y over x in each subplot.\n# Give the plot a global title \"Figure\"\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\naxs.plot(x, y)\naxs.set_title(\"Figure\")\nplt.show()\n",
        "\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.plot(df.index, df[\"Type A\"])\nplt.plot(df.index, df[\"Type B\"])\nplt.show()\n",
        "\nplt.scatter(x, y, marker='v', s=100, c='r', linewidths=0.5, edgecolors='k')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='v', s=100)\nplt.show()\n",
        "\nplt.scatter(x, y, marker='*', s=100)\nplt.show()\n",
        "\nplt.scatter(x, y, s=100, marker='*v')\n",
        "\nxlim = [1, 5]\nylim = [1, 4]\nplt.imshow(data, cmap='coolwarm', extent=(xlim, xlim, ylim, ylim))\nplt.show()\n",
        "\nplt.stem(x, y, orientation='horizontal')\n",
        "\nd = {\"a\": 4, \"b\": 5, \"c\": 7}\nc = {\"a\": \"red\", \"c\": \"green\", \"b\": \"blue\"}\n# Make a bar plot using data in `d`. Use the keys as x axis labels and the values as the bar heights.\n# Color each bar in the plot by looking up the color in colors\ncolors = {\"a\": \"red\", \"c\": \"green\", \"b\": \"blue\"}\nfor key, value in d.items():\n    plt.bar(key, value, color=colors[key])\nplt.show()\n",
        "\nplt.axvline(x=3, color='black', linestyle='--')\nplt.legend(['cutoff'])\n",
        "\nfig, ax = plt.subplots(figsize=(10, 10))\nax.bar(ax.get_theta(), height, label=labels)\nax.set_theta_zero_location(\"N\")\nax.set_theta_direction(-1)\nax.set_rlabel_position(270)\nplt.show()\n",
        "\nplt.pie(data, labels=l, autopct='%1.1f%%', startangle=90)\nplt.axis('off')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(color='blue', linestyle='dashed')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(True, which='minor', color='gray', linestyle='dashed')\nplt.show()\n",
        "\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\nfont = {'weight' : 'bold'}\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors, font=font)\nplt.show()\n",
        "\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\nfont = {'weight' : 'bold'}\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors, font=font)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart but use transparent marker with non-transparent edge\n# Create a new array with the same values as y but with an additional column for the edge color\nedge_color = np.ones_like(y)\nedge_color[:, 1] = 0\n# Plot y over x with the edgecolor parameter set to edge_color\nplt.plot(x, y, 'o', edgecolors=edge_color)\n",
        "\nplt.axvline(55, color=\"green\")\n",
        "\n# Create a list to store the heights of the blue bars\nblue_bar_heights = []\nfor height in blue_bar:\n    blue_bar_heights.append(height)\n# Create a list to store the heights of the orange bars\norange_bar_heights = []\nfor height in orange_bar:\n    orange_bar_heights.append(height)\n# Create a list to store the heights of the blue and orange bars\nbar_heights = blue_bar_heights + orange_bar_heights\n# Sort the list of heights in ascending order\nbar_heights.sort()\n# Create a list to store the indices of the blue and orange bars\nblue_bar_indices = []\norange_bar_indices = []\nfor i, height in enumerate(bar_heights):\n    if height == blue_bar_heights[0]:\n        blue_bar_indices.append(i)\n    elif height == orange_bar_heights[0]:\n        orange_bar_indices.append(i)\n# Create a list to store the heights of the blue and orange bars\nblue_bar_heights = [bar_heights[i] for i in blue_bar_indices]\norange_bar_heights = [bar_heights[i] for i in orange_bar_indices]\n# Plot the blue bar and the orange bar side-by-side in the same bar plot.\n# Make sure the bars don't overlap with each other.\nplt.bar(range(len(blue_bar_heights)), blue_bar_heights, color='blue', label='Blue')\nplt.bar(range(len(orange_bar_heights)), orange_bar_heights, color='orange', label='Orange')\nplt.xlabel('Index')\nplt.ylabel('Height')\nplt.title('Blue and Orange Bars')\nplt.legend()\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\na = np.arange(10)\n# Make two subplots\nfig, axs = plt.subplots(2, 1)\n# Plot y over x in the first subplot and plot z over a in the second subplot\naxs[0].plot(x, y)\naxs[1].plot(a, z)\n# Label each line chart and put them into a single legend on the first subplot\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[1].set_xlabel('a')\naxs[1].set_ylabel('z')\n# Add a legend to the first subplot\naxs[0].legend(['y', 'z'], loc='best')\n# Show the plot\nplt.show()\n",
        "\nplt.scatter(x, y, c=y)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nsns.factorplot(x=\"species\", y=\"bill_length_mm\", data=df, kind=\"bar\", height=0.8, aspect=1.5)\nplt.show()\n",
        "\nplt.scatter(0.5, 0.5, s=200, c='red')\n",
        "\nplt.plot(x, y)\nplt.title(\"Phi\", fontweight=\"bold\")\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1), borderaxespad=0.)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"best\", handlelength=0.3)\nplt.show()\n",
        "\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\nplt.legend()\nplt.show()\n",
        "\nplt.imshow(data, cmap='bone', extent=[0, 10, 0, 10])\nplt.colorbar()\nplt.show()\n",
        "\nplt.title(\"Figure 1\", fontweight=\"bold\")\n",
        "\nsns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\", legend=False)\n",
        "\nplt.plot(y, x)\nplt.xscale('reverse')\nplt.show()\n",
        "\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n",
        "\nplt.scatter(x, y, c='red', s=100, edgecolors='black')\nplt.show()\n",
        "\nplt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor i in range(2):\n    for j in range(2):\n        plt.subplot(2, 2, i * 2 + j + 1)\n        plt.plot(x, y)\n",
        "\nplt.hist(x, bins=5, range=(0, 10), rwidth=2)\n",
        "\nplt.plot(x, y)\nplt.fill_between(x, y - error, y + error, alpha=0.5)\nplt.show()\n",
        "\nplt.plot([0, 0], [0, 0], 'w-')\n",
        "\nfor i in range(4):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], color=c[i])\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n# Plot y over x and z over a in two side-by-side subplots\n# Make \"Y\" the title of the first subplot and \"Z\" the title of the second subplot\n# Raise the title of the second subplot to be higher than the first one\n# Create a figure with two subplots\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\n# Set the title of the first subplot\naxs[0].set_title(\"Y\")\n# Set the title of the second subplot\naxs[1].set_title(\"Z\", y=1.1)\n# Plot y over x in the first subplot\naxs[0].plot(x, y)\n# Plot z over a in the second subplot\naxs[1].plot(a, z)\n# Show the plot\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make 4 by 4 subplots with a figure size (5,5)\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(5,5))\n# in each subplot, plot y over x and show axis tick labels\nfor i in range(4):\n    for j in range(4):\n        axs[i,j].plot(x,y)\n        axs[i,j].set_xlabel('x')\n        axs[i,j].set_ylabel('y')\n# give enough spacing between subplots so the tick labels don't overlap\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\nplt.show()\n",
        "\nplt.matshow(d, cmap='bone', fignum=1, aspect='auto', extent=[0, 10, 0, 10])\nplt.show()\n",
        "\ntable = df.plot(kind=\"table\", bbox=[0, 0, 1, 1])\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(np.arange(10))\nplt.show()\n",
        "\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\nplt.title(\"Group: Fat\")\nplt.subplots_adjust(top=0.85)\nplt.show()\n",
        "\nsns.catplot(x=\"Exercise Time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"pulse\")\nplt.show()\n",
        "\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=2.5, aspect=1.5)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y over x')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend()\nplt.frameon = False\nplt.show()\n",
        "\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='a+b')\nplt.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"lower center\")\nplt.show()\n",
        "\nsns.set_style(\"whitegrid\")\nfig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 10))\nfor i, ax in enumerate(axs.flatten()):\n    ax.set_xticks(np.arange(1, 31, 2))\n    ax.set_xticklabels(np.arange(1, 31, 2))\n    sns.pointplot(x=\"a\", y=\"c\", data=df, ax=ax)\n    ax.set_title(f\"b={df['b'][i]}\")\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(100, 50)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line Chart\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Line Chart\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n",
        "\nx = np.random.random((10, 10))\nfrom matplotlib import gridspec\nnrow = 2\nncol = 2\nfig = plt.figure(figsize=(ncol + 1, nrow + 1))\n# Make a 2x2 subplots with fig and plot x in each subplot as an image\nax1 = fig.add_subplot(nrow, ncol, 1)\nax2 = fig.add_subplot(nrow, ncol, 2)\nax1.imshow(x)\nax2.imshow(x)\n# Remove the space between each subplot and make the subplot adjacent to each other\nfig.tight_layout()\n# Remove the axis ticks from each subplot\nax1.set_axis_off()\nax2.set_axis_off()\n"
    ],
    "Tensorflow": [
        "\nimport tensorflow as tf\nx = tf.Variable(0)\nx.assign(1)\nresult = x.read_value()\n",
        "\nimport tensorflow as tf\nx = tf.Variable(0)\nx.assign(114514)\nresult = x.read_value()\n",
        "\nresult = tf.constant(labels, dtype=tf.int32)\n",
        "\nresult = tf.constant(labels, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(tf.constant(labels), 10)\n",
        "\n    result = tf.constant([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                            [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n                            [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n                            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])\n    return result\n",
        "\nresult = tf.one_hot(tf.constant(labels), 10)\n",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [[i, i+1, i+2]]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(ds))\nprint(result)\n",
        "\n    result = []\n    for i in input:\n        result.extend([i, i+1, i+2])\n    ",
        "\nresult = tf.pad(tf.ones(8), [[0, 0], [0, lengths - 1]])\n",
        "\nresult = tf.pad(tf.ones(8), [[0, 0], [0, lengths - 1]])\n",
        "\nresult = tf.pad(tf.constant([0, 0, 0, 0, 1, 1, 1, 1]), [[0, 0], [0, 0]])\nprint(result)\n",
        "\n    result = tf.concat([tf.ones(lengths), tf.zeros(8 - lengths)], axis=0)\n    ",
        "\nresult = tf.pad(tf.ones(8), [[0, 0], [0, lengths]])\nprint(result)\n",
        "\nimport tensorflow as tf\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\nresult = tf.concat([a, b], axis=0)\nprint(result)\n",
        "\n    result = tf.concat([a,b],axis=0)\n    ",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 1, 512))\nresult = tf.reshape(a, (50, 100, 512))\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\nresult = tf.expand_dims(a, axis=-1)\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\n# Add two new dimensions to the tensor\nresult = tf.expand_dims(a, axis=0)\nresult = tf.expand_dims(result, axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\nresult = tf.reduce_sum(A, axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\nresult = tf.reduce_prod(A, axis=1)\nprint(result)\n",
        "\nimport tensorflow as tf\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\nresult = tf.reciprocal(A)\nprint(result)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.sub(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.sub(a, b)), axis=1)\n",
        "\n    result = tf.square(tf.sub(lhs, rhs))\n    ",
        "\nm = tf.gather(x, y, axis=1)\n",
        "\nm = tf.gather(x, row, axis=0)\nm = tf.gather(m, col, axis=1)\n",
        "\n    result = tf.gather(x, y, axis=1)\n    result = tf.gather(result, z, axis=1)\n    ",
        "\nnp.dot(A, B)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\nA = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\nB = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\nresult = tf.einsum('bijn,bijm->bijn', A, B)\nprint(result)\n",
        "\nimport tensorflow as tf\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\n# Convert bytes to strings\nresult = [x.decode('utf-8') for x in x]\nprint(result)\n",
        "\n    result = []\n    for xi in x:\n        result.append(tf.compat.as_bytes(xi.decode('utf-8')))\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    result = tf.reduce_sum(x, axis=2, keepdims=True) / tf.reduce_sum(tf.cast(x != 0, tf.float32), axis=2, keepdims=True)\n    return result\n",
        "\ntf.compat.v1.enable_eager_execution()\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    scores = tf.argmax(a, axis=1)\n    result = tf.reduce_sum(tf.cast(tf.equal(scores, a), tf.int32))\n    ",
        "\nresult = tf.argmin(a, axis=1)\n",
        "model.save(\"export/1\")\n",
        "\nimport tensorflow as tf\nseed_x = 10\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=4)\nprint(result)\n",
        "\nresult = tf.random.uniform(shape=[114], minval=2, maxval=5, dtype=tf.int32, seed=seed_x)\n",
        "\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform(minval=1, maxval=4, shape=[10])\n    ",
        "\nimport tensorflow as tf\n### output the version of tensorflow into variable 'result'\nresult = tf.__version__\nprint(result)\n"
    ],
    "Scipy": [
        "\nresult = scipy.optimize.minimize(scipy.optimize.least_squares, [1], args=(x, y))\n",
        "\nA, B = np.polyfit(x, y, 1)\nresult = np.array([A, B])\nprint(result)\n",
        "\ndef exponential_fit(x, y, p0):\n    A, B, C = p0\n    y_fit = A * np.exp(B * x) + C\n    return y_fit, A, B, C\nresult = scipy.optimize.curve_fit(exponential_fit, x, y, p0)\nprint(result)\n",
        "test_stat = stats.kstest(x, y, 'two-sample')\nprint(test_stat)",
        "\n# Calculate the test statistic\ntest_stat = stats.kstest(x, y, 'two-sample')\n# Calculate the p-value\np_value = test_stat.pvalue\n# Check if the p-value is less than the alpha level\nresult = p_value < alpha\n# Print the result\nprint(result)\n",
        "\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\ndef f(a, b, c):\n    return ((a + b - c) - 2)**2 + ((3 * a - b - c)**2 + sin(b) + cos(b) + 4)\nresult = optimize.minimize(f, initial_guess, args=(a, b, c))\nprint(result)\n",
        "\np_values = scipy.stats.norm.sf(z_scores)\n",
        "\np_values = scipy.stats.norm.sf(z_scores)\n",
        "z_scores = scipy.stats.norm.ppf(p_values)\nprint(z_scores)",
        "\ndist = stats.lognorm(mu, stddev)\nresult = dist.cdf(x)\nprint(result)\n",
        "\ndist = stats.lognorm(total,mu,stddev)\nexpected_value = dist.mean()\nmedian = dist.median()\n",
        "\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\nresult = sa * sb\nprint(result)\n",
        "\n    result = sA.dot(sB)\n    ",
        "import numpy as np\nimport scipy.interpolate\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30]])\ndef interpolate(points, V, request):\n    x, y, z = request\n    x_min, x_max = np.min(points[:,0]), np.max(points[:,0])\n    y_min, y_max = np.min(points[:,1]), np.max(points[:,1])\n    z_min, z_max = np.min(points[:,2]), np.max(points[:,2])\n    x_index = np.searchsorted(points[:,0], x)\n    y_index = np.searchsorted(points[:,1], y)\n    z_index = np.searchsorted(points[:,2], z)\n    x_index_left = x_index - 1\n    x_index_right = x_index + 1\n    y_index_left = y_index - 1\n    y_index_right = y_index + 1\n    z_index_left = z_index - 1\n    z_index_right = z_index + 1\n    x_left = points[x_index_left,0]\n    x_right = points[x_index_right,0]\n    y_left = points[y_index_left,1]\n    y_right = points[y_index_right,1]\n    z_left = points[z_index_left,2]\n    z_right = points[z_index_right,2]\n    x_left_index = np.searchsorted(points[:,0], x_left)\n    x_right_index = np.searchsorted(points[:,0], x_right)\n    y_left_index = np.searchsorted(points[:,1], y_left)\n    y_right_index = np.searchsorted(points[:,1], y_right)\n    z_left_index = np.searchsorted(points[:,2], z_left)\n    z_right_index = np.searchsorted(points[:,2], z_right)\n    x_left_left = points[x_left_index,0]\n    x_left_right = points[x_left_index+1,0]\n    y_left_left = points[y_left_index,1]\n    y_left_right = points[y_left_index+1,1]\n    z_left_left = points[z_left_index,2]\n    z_left_right = points[z_left_index+1,2]\n    x_right_left = points[x_right_index-1,0]\n    x_right_right = points[x_right_index,0]\n    y_right_left = points[y_right_index-1,1]\n    y_right_right = points[y_right_index,1]\n    z_right_left = points[z_right_index-1,2]\n    z_right_right = points[z_right_index,2]\n",
        "\nimport numpy as np\nimport scipy.interpolate\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n# Define a function to interpolate the value of V at a given point\ndef interpolate_V(x, y, z):\n    # Get the indices of the closest points in the points array\n    x_index, y_index, z_index = np.searchsorted(points[:, 0], x)\n    x_index_next, y_index_next, z_index_next = np.searchsorted(points[:, 0], x + 1)\n    # Calculate the weights for the two closest points\n    weight_x = (x - points[x_index]) / (points[x_index_next] - points[x_index])\n    weight_y = (y - points[y_index]) / (points[y_index_next] - points[y_index])\n    weight_z = (z - points[z_index]) / (points[z_index_next] - points[z_index])\n    # Interpolate the value of V at the given point\n    interpolated_V = weight_x * V[x_index] + weight_y * V[y_index] + weight_z * V[z_index]\n    return interpolated_V\n# Interpolate the value of V at the two requested points\nresult = []\nfor i in range(len(request)):\n    result.append(interpolate_V(request[i, 0], request[i, 1], request[i, 2]))\nprint(result)\n",
        "\nfrom scipy import misc\nfrom scipy.ndimage import rotate\nimport numpy as np\ndata_orig = misc.face()\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nangle = np.random.randint(1, 360)\nrotation_matrix = rotate(data_orig, angle, reshape=False)\nx_rot, y_rot = rotation_matrix.shape\nx_rot //= 2\ny_rot //= 2\nx_rot -= x0\ny_rot -= y0\nprint(data_rot, (x_rot, y_rot))\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\nresult = M.diagonal()\nprint(result)\n",
        "\n# Calculate the Kolmogorov-Smirnov statistic\nks_stat = stats.kstest(times, \"uniform\")\n",
        "\n    n = len(times)\n    result = stats.kstest(times, \"uniform\", args=(n,))\n    return result\n    ",
        "\n# Calculate the mean and standard deviation of the times\nmean_time = np.mean(times)\nstd_time = np.std(times)\n# Perform the Kolmogorov-Smirnov test\nresult = stats.kstest(times, \"uniform\", args=(mean_time, std_time))\n# Print the result\nprint(result)\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.vstack((c1, c2))\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.csr_matrix((c1.data, c2.data))\n#print(Feature)\n",
        "\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\nFeature = sparse.vstack((c1, c2))\n#print(Feature)\n",
        "\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\ndef distance(p1, p2):\n    x1, y1 = p1\n    x2, y2 = p2\n    return np.sqrt((x1-x2)**2 + (y1-y2)**2)\ndef assign_points(points1, points2):\n    distances = distance(points1, points2)\n    assignments = np.argmin(distances, axis=1)\n    return assignments\nresult = assign_points(points1, points2)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\ndef distance(p1, p2):\n    x1, y1 = p1\n    x2, y2 = p2\n    return np.sqrt((x1-x2)**2 + (y1-y2)**2)\ndef assign_points(points1, points2):\n    distances = distance(points1, points2)\n    assignments = np.argmin(distances, axis=1)\n    return assignments\nresult = assign_points(points1, points2)\nprint(result)\n",
        "\nb.setdiag(0)\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\ndef count_regions(img):\n    result = 0\n    for i in range(1, img.shape):\n        for j in range(1, img.shape):\n            if img[i, j] > threshold:\n                result += 1\n    return result\nresult = count_regions(img)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\ndef count_regions(img):\n    labels = ndimage.label(img)\n    regions = []\n    for label in range(1, len(np.unique(labels))):\n        region = []\n        for x in range(img.shape):\n            for y in range(img.shape):\n                if labels[x, y] == label:\n                    region.append((x, y))\n        regions.append(region)\n    return len(regions)\nresult = count_regions(img)\nprint(result)\n",
        "\ndef f(img = example_img):\n    threshold = 0.75\n    result = 0\n    for i in range(512):\n        for j in range(512):\n            if img[i, j] > threshold:\n                result += 1\n    return result\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n# Find the regions of cells which value exceeds a given threshold\nregions = []\nfor i in range(1, 512 - 1):\n    for j in range(1, 512 - 1):\n        if img[i, j] > threshold:\n            regions.append((i, j))\n# Determine the distance between the center of mass of such regions and the top left corner, which has coordinates (0,0)\nresult = []\nfor region in regions:\n    x, y = region\n    x_center = x + (512 - 1) // 2\n    y_center = y + (512 - 1) // 2\n    result.append((x_center, y_center))\nprint(result)\n",
        "\nM.make_symmetric()\n",
        "\n    sA = lil_matrix(sA)\n    sA = sA.T.conj()\n    return sA\n    ",
        "\nimport numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\ndef remove_isolated_cells(square):\n    kernel = np.ones((3, 3), dtype=bool)\n    kernel[1, 1] = False\n    kernel = kernel.astype(square.dtype)\n    return scipy.ndimage.morphology.binary_erosion(square, kernel)\nsquare = remove_isolated_cells(square)\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\nprint(square)\n",
        "\n# Create a mask to identify isolated cells\nmask = np.zeros_like(square)\nfor i in range(1, 11):\n    for j in range(1, 11):\n        mask[i-1:i+2, j-1:j+2] = square[i:i+2, j:j+2] == 0\n# Apply the mask to the original array\nsquare = square * mask\n",
        "mean = np.mean(col)\nstandard_deviation = np.std(col)\nprint(mean)\nprint(standard_deviation)",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\nMax = np.max(col.data)\nMin = np.min(col.data)\nprint(Max)\nprint(Min)\n",
        "\nMedian = np.median(col.data)\nMode = np.mode(col.data)\n",
        "\ndef fourier(x, a):\n    return a * np.cos(1 * np.pi / tau * x)\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1] * degree)\n",
        "\n# Calculate pairwise distances between all regions\ndistances = cdist(example_array, example_array, 'euclidean')\n# Create a diagonal distance matrix\ndiag_distances = np.zeros((len(example_array), len(example_array)))\nfor i in range(len(example_array)):\n    for j in range(i+1, len(example_array)):\n        diag_distances[i, j] = distances[i, j]\n        diag_distances[j, i] = distances[j, i]\n# Calculate the minimum distance separating the nearest edges of each raster patch\nmin_distances = np.min(diag_distances, axis=1)\n# Create a distance matrix with the minimum distances\nmin_dist_matrix = np.zeros((len(example_array), len(example_array)))\nfor i in range(len(example_array)):\n    for j in range(i+1, len(example_array)):\n        min_dist_matrix[i, j] = min_distances[i, j]\n        min_dist_matrix[j, i] = min_distances[j, i]\n# Print the result\nprint(min_dist_matrix)\n",
        "\n# Calculate Manhattan distances between all regions\ndistances = np.zeros((len(example_array), len(example_array)))\nfor i in range(len(example_array)):\n    for j in range(i+1, len(example_array)):\n        distances[i, j] = cdist(example_array[i], example_array[j], 'euclidean')[0, 0]\n        distances[j, i] = distances[i, j]\n",
        "\n# Calculate the distances between all regions\ndistances = cdist(example_array, example_array, 'euclidean')\n# Create a matrix to store the distances\nresult = np.zeros((len(example_array), len(example_array)))\n# Iterate over the rows and columns of the distance matrix\nfor i in range(len(example_array)):\n    for j in range(len(example_array)):\n        # Get the indices of the current region and its nearest neighbor\n        current_region = i\n        nearest_neighbor = np.argmin(distances[i, :])\n        # Store the distance between the current region and its nearest neighbor\n        result[i, j] = distances[i, nearest_neighbor]\n        result[nearest_neighbor, i] = distances[nearest_neighbor, i]\nreturn result\n",
        "result = interpolate.splev(x_val, tck, der = 0)\nprint(result)",
        "\nx1_mean = np.mean(x1)\nx2_mean = np.mean(x2)\nx3_mean = np.mean(x3)\nx4_mean = np.mean(x4)\nx1_std = np.std(x1)\nx2_std = np.std(x2)\nx3_std = np.std(x3)\nx4_std = np.std(x4)\nx1_sample = x1 - x1_mean\nx2_sample = x2 - x2_mean\nx3_sample = x3 - x3_mean\nx4_sample = x4 - x4_mean\nx1_sample_std = x1_sample / x1_std\nx2_sample_std = x2_sample / x2_std\nx3_sample_std = x3_sample / x3_std\nx4_sample_std = x4_sample / x4_std\nx1_sample_std_norm = (x1_sample_std - x1_mean) / x1_std\nx2_sample_std_norm = (x2_sample_std - x2_mean) / x2_std\nx3_sample_std_norm = (x3_sample_std - x3_mean) / x3_std\nx4_sample_std_norm = (x4_sample_std - x4_mean) / x4_std\n",
        "\nx1_mean, x1_std = ss.anderson_ksamp(x1)\nx2_mean, x2_std = ss.anderson_ksamp(x2)\nresult = ss.ks_2samp(x1_mean, x2_mean, x1_std, x2_std)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndef tau1(x):\n    y = np.array(df['A'])\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\nprint(df)\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nonzero()[0].size == 0\n",
        "\nblocks = [a[i:i+2] for i in range(0, len(a), 2)]\nresult = block_diag(*blocks)\n",
        "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\nprint(p_value)",
        "\nimport numpy as np\nfrom scipy import stats\ndef f(pre_course_scores, during_course_scores):\n    # Calculate the ranksums result\n    ranksums_result = stats.ranksums(pre_course_scores, during_course_scores)\n    # Extract the p-value from the ranksums result\n    p_value = ranksums_result.pvalue\n    return p_value\nexample_pre_course_scores = np.random.randn(10)\nexample_during_course_scores = np.random.randn(10)\nprint(f(pre_course_scores=example_pre_course_scores, during_course_scores=example_during_course_scores))\n",
        "kurtosis_result = scipy.stats.kurtosis(a, bias=False)\nprint(kurtosis_result)",
        "kurtosis_result = scipy.stats.kurtosis(a, fisher=True)\nprint(kurtosis_result)",
        "result = scipy.interpolate.interp2d(x, y, z, s, t, bounds_error=False, fill_value=0.0)\nprint(result(s, t))",
        "result = scipy.interpolate.interp2d(x, y, z, kind='cubic')(s, t)\nreturn result",
        "\nresult = []\nfor i, point in enumerate(extraPoints):\n    region = vor.point_region(point)\n    result.append(region)\nprint(result)\n",
        "\nresult = []\nfor i, point in enumerate(extraPoints):\n    region = vor.point_region(point)\n    result.append(region)\nprint(result)\n",
        "# Create a sparse matrix from the list of numpy vectors\nresult = sparse.csr_matrix(vectors)\n# Pad zeros to the end of elements for the vectors which are smaller than the maximum size\nresult = result.tocsr()\n# Print the result\nprint(result)",
        "\nimport numpy as np\nimport scipy.ndimage\na= np.zeros((5, 5))\na[1:4, 1:4] = np.arange(3*3).reshape((3, 3))\norigin = 1\nb = scipy.ndimage.median_filter(a, 3, origin=origin)\nprint(b)\n",
        "value = M.getrow(row).toarray()[column]\nprint(value)",
        "result = []\nfor i in range(len(row)):\n    result.append(M[row[i], column[i]])\nprint(result)",
        "\ninterpolator = scipy.interpolate.interp1d(x, array[:, 0, 0])\nnew_array[:, :, :] = interpolator(x_new)\n",
        "\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\nx = 2.5\nu = 1\no2 = 3\nP_inner = scipy.integrate.quad(NDfx, -o2, x)\nP_outer = 1 - P_inner\nP = P_inner + P_outer/2\nprint(prob)\n",
        "\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\ndef f(x = 2.5, u = 1, o2 = 3):\n    # Calculate the probability up to position 'x'\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev, args=(x,))\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n",
        "\nresult = sf.dctn(N)\n",
        "\nfrom scipy import sparse\nimport numpy as np\nmatrix = np.array([[3.5,   13. ,   28.5,   50. ,   77.5],\n                   [-5. ,  -23. ,  -53. ,  -95. , -149. ],\n                   [2.5,   11. ,   25.5,   46. ,   72.5]])\nresult = sparse.diags(matrix, [-1, 0, 1], (5, 5)).toarray()\nprint(result)\n",
        "M = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        M[i,j] = scipy.stats.binom.pmf(i, N, p)\nprint(result)",
        "\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\nresult = df.apply(lambda x: (x - x.mean()) / x.std(), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\nresult = df.apply(lambda x: stats.zscore(x), axis=1)\nprint(result)\n",
        "\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\nresult = pd.DataFrame(index=df.index)\nresult['data'] = df.values\nresult['zscore'] = stats.zscore(df.values)\nprint(result)\n",
        "\nimport pandas as pd\nimport io\nimport numpy as np\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n# Calculate z-scores\nzscores = stats.zscore(df)\n# Round z-scores to 3 decimal places\nzscores = np.round(zscores, 3)\n# Create a new dataframe with z-scores\nresult = pd.DataFrame({'probegenes': df.index, 'sample1': zscores.loc[:, 'sample1'], 'sample2': zscores.loc[:, 'sample2'], 'sample3': zscores.loc[:, 'sample3']})\n# Print the result\nprint(result)\n",
        "\nimport scipy\nimport scipy.optimize\nimport numpy as np\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\ndef phi(xk, alpha):\n    pk = xk + alpha * direction\n    return test_func(pk)\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction, phi)\nprint(result)\n",
        "\nmid = np.array([[0, 0], [0, 5], [5, 0], [5, 5]])\nresult = distance.cdist(scipy.dstack((y, x)), mid)\n",
        "\nmid = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nresult = distance.cdist(np.dstack((y, x)), mid)\n",
        "\n    mid = np.zeros((shape, 2))\n    mid[:, 0] = shape // 2\n    mid[:, 1] = shape // 2\n    ",
        "\nresult = scipy.ndimage.zoom(x, 2, order=1, mode='nearest')\nresult = result.reshape(shape)\n",
        "\n# Define the objective function\ndef func(x,a):\n    return np.dot(a, x**2)\n# Define the residual function\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model) **2\n# Define the initial guess for the optimization\nx0 = np.array([2, 3, 1, 4, 20])\n# Define the optimization parameters\nparams = {'x': x0}\n# Optimize the objective function\nout = scipy.optimize.minimize(residual, params, args=(a, y))\n# Print the optimized values\nprint(out)\n",
        "# Define the objective function\ndef objective_function(x, a):\n    return np.dot(a, x**2) - y\n# Define the constraints\ndef constraint_function(x, a):\n    return np.dot(a, x**2) - y\n# Define the initial guess\nx0 = np.array([2, 3, 1, 4, 20])\n# Define the bounds\nx_lower_bounds = x_true / 2\nx_upper_bounds = x_true * 2\n# Define the optimization problem\nproblem = scipy.optimize.Minimize(objective_function, x0, bounds=(x_lower_bounds, x_upper_bounds))\n# Solve the optimization problem\nout = problem.solve()\n# Print the results\nprint(out)",
        "\nt = np.linspace(time_span, time_span, 100)\ny0 = [N0]\nfun = lambda t, y: -100 * y + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun, t_span, y0)\nresult = sol.y\nprint(result)\n",
        "\nt = np.linspace(0, 2*np.pi, 100)\ny0 = N0 + 100*np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=y0)\nresult = sol.y\nprint(result)\n",
        "\nt = np.linspace(time_span, time_span, 100)\ny0 = [N0, -cos(t)]\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=y0)\nresult = sol.y\nprint(result)\n",
        "\ndef const(x):    \n    y=x[t]\n    return y\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sa.vstack(sb)\nprint(result)\n",
        "\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = sa.multiply(sb)\nprint(result)\n",
        "\nimport scipy.integrate\nc = 5\nlow = 0\nhigh = 1\nI = []\nfor n in range(len(c)):\n    # equation\n    eqn = 2*x*c[n]\n    # integrate \n    result,error = integrate.quad(lambda x: eqn,low,high)\n    I.append(result)\nprint(result)\n",
        "\n    x = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    result = x[0]\n    ",
        "\nx_nonzero = np.where(V.data != 0)\nV.data[x_nonzero] += x\n",
        "\nx_indices = V.nonzero()\nx_values = x[x_indices]\nV[x_indices] += x_values\n",
        "\nx_indices = V.nonzero()\nx_values = V.data[x_indices]\nV.data[x_indices] += x\nV.data[y_indices] += y\n",
        "\n# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    # get the column data\n    Column = sa[:,Col].data\n    # calculate the column length\n    Len = math.sqrt(sum(x**2 for x in Column))\n    # normalize the column\n    sa[:,Col] *= (1/Len)\n",
        "\nfor col in range(sa.shape[1]):\n    column = sa[:, col].data\n    norm = math.sqrt(sum([x**2 for x in column]))\n    sa[:, col] *= (1/norm)\n",
        "\nb = (a > 0).astype(int)\n",
        "\nb = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(len(a)):\n        if i != j:\n            b[i, j] = a[i, j]\nprint(b)\n",
        "\nimport numpy as np\nimport scipy.spatial\ndef closest_to_centroid(centroids, data):\n    distances = scipy.spatial.distance.cdist(data, centroids)\n    indices = np.argmin(distances, axis=1)\n    return indices\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nresult = closest_to_centroid(centroids, data)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.spatial\ndef closest_to_centroid(centroids, data):\n    distances = scipy.spatial.distance.cdist(data, centroids)\n    indices = np.argmin(distances, axis=1)\n    return data[indices]\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nresult = closest_to_centroid(centroids, data)\nprint(result)\n",
        "\nimport numpy as np\nimport scipy.spatial\ndef closest_to_centroid(centroids, data, k):\n    distances = scipy.spatial.distance.cdist(data, centroids)\n    indices = np.argsort(distances, axis=1)[:, ::-1][:, :k]\n    return indices\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nk = 3\nresult = closest_to_centroid(centroids, data, k)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\ndef fsolve_for_a(x, a, b):\n    return fsolve(eqn, x, args=(a, b))\nresult = []\nfor a in adata:\n    for b in bdata:\n        result.append(fsolve_for_a(xdata, a, b))\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\ndef f(x, a, b):\n    return eqn(x, a, b)\nresult = []\nfor x, a, b in zip(xdata, adata, adata):\n    root = fsolve(f, x, args=(a, b))\n    result.append([root, -root])\nprint(result)\n",
        "\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n# Define the probability density function\ndef pdf(x, a, m, d):\n    return bekkers(x, a, m, d)\n# Define the cumulative distribution function\ndef cdf(x, a, m, d):\n    return 0.5*(1+np.exp(-((x-m)/d)**2))\n# Define the sample data\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n# Define the range of x values\nx_range = np.linspace(range_start, range_end, 100)\n# Define the number of bins for the histogram\nnum_bins = 10\n# Define the bin edges for the histogram\nbin_edges = np.linspace(range_start, range_end, num_bins+1)\n# Define the histogram of the sample data\nhist, bin_edges = np.histogram(sample_data, bins=bin_edges, density=True)\n# Define the cumulative histogram of the sample data\ncumulative_hist = np.cumsum(hist)\n# Define the expected value of the sample data\nexpected_value = np.mean(sample_data)\n# Define the standard deviation of the sample data\nstd_dev = np.std(sample_data)\n# Define the KS test statistic\nks_statistic = (cumulative_hist - expected_value) / std_dev\n# Define the p-value of the KS test\np_value = stats.kstest(cumulative_hist, pdf, args=(a, m, d), alternative='two-sided')\n# Print the result\nprint(f'KS test statistic: {ks_statistic}')\nprint(f'p-value: {p_value}')\n",
        "\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n# Define the probability density function\ndef pdf(x, a, m, d):\n    return bekkers(x, a, m, d)\n# Define the cumulative distribution function\ndef cdf(x, a, m, d):\n    return np.cumsum(pdf(x, a, m, d))\n# Define the null hypothesis\nH0 = \"The experimental data is consistent with the fitted probability density function\"\n# Define the alternative hypothesis\nHa = \"The experimental data is not consistent with the fitted probability density function\"\n# Define the significance level\nalpha = 0.05\n# Define the sample size\nn = len(sample_data)\n# Define the test statistic\nz = stats.kstest(sample_data, pdf, cdf, args=(estimated_a, estimated_m, estimated_d), alternative=Ha)\n# Define the p-value\np_value = stats.percentile(z, alpha/2)\n# Define the result\nresult = (p_value < alpha)\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\nimport io\nfrom scipy import integrate\nstring = '''\nTime                      A\n2017-12-18-19:54:40   -50187.0\n2017-12-18-19:54:45   -60890.5\n2017-12-18-19:54:50   -28258.5\n2017-12-18-19:54:55    -8151.0\n2017-12-18-19:55:00    -9108.5\n2017-12-18-19:55:05   -12047.0\n2017-12-18-19:55:10   -19418.0\n2017-12-18-19:55:15   -50686.0\n2017-12-18-19:55:20   -57159.0\n2017-12-18-19:55:25   -42847.0\n'''\ndf = pd.read_csv(io.StringIO(string), sep = '\\s+')\n# Define the rolling window\nwindow_size = 25\nwindow_start = 0\nwindow_end = len(df) - window_size\n# Define the rolling integral function\ndef rolling_integral(df, window_size, window_start, window_end):\n    integrand = df['A'].rolling(window_size).sum()\n    return integrand.cumsum()\n# Apply the rolling integral function to the dataframe\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\n# Print the result\nprint(integral_df)\n",
        "\nimport scipy.interpolate\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\neval = [(2.7, 2.3)]\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# Vectorize the objective function\ne_vec = np.vectorize(e)\n# Define the bounds for each parameter\npmin_vec = np.array([pmin, pmin])\npmax_vec = np.array([pmax, pmax])\n# Optimize the objective function\nresult = sciopt.minimize(e_vec, pmin_vec, pmax_vec, args=(x, y))\n# Extract the optimized values\npopt = result.x\n",
        "\nimport numpy as np\nfrom scipy import signal\narr = np.array([-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,])\nn = 2\ndef find_extrema(arr, n):\n    result = []\n    for i in range(len(arr) - n + 1):\n        if arr[i] <= arr[i + n] and arr[i] <= arr[i - n]:\n            result.append(i)\n    return result\nresult = find_extrema(arr, n)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy import signal\ndef find_extrema(arr, n):\n    result = []\n    for i in range(n):\n        for j in range(n):\n            if i == 0:\n                if arr[i][j] <= arr[i+1][j] and arr[i][j] <= arr[i+1][j+1]:\n                    result.append([i, j])\n            elif i == n-1:\n                if arr[i][j] <= arr[i-1][j] and arr[i][j] <= arr[i-1][j-1]:\n                    result.append([i, j])\n            elif arr[i][j] <= arr[i+1][j] and arr[i][j] <= arr[i+1][j+1]:\n                result.append([i, j])\n    return result\narr = np.array([[-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,], [3, 0, 0, 1, 2, 4]])\nn = 2\nresult = find_extrema(arr, n)\nprint(result)\n",
        "\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\ndef remove_outliers(df):\n    numerical_columns = [col for col in df.columns if df[col].dtype == 'float64']\n    for col in numerical_columns:\n        df[col] = df[col].apply(lambda x: np.abs(stats.zscore(x)) if isinstance(x, pd.Series) else x)\n    return df\ndf = remove_outliers(df)\nprint(df)\n"
    ],
    "Sklearn": [
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_data()\ndata1 = pd.DataFrame(data)\nprint(data1)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_data()\ndata1 = pd.DataFrame(data)\nprint(data1)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_boston\nimport pandas as pd\ndata = load_data()\ndata1 = pd.DataFrame(data)\nprint(data1)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndef load_data():\n    data = load_iris()\n    return data\ndef convert_to_df(data):\n    df = pd.DataFrame(data)\n    return df\ndata = load_data()\nresult = convert_to_df(data)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndef one_hot_encode(df):\n    df_out = pd.get_dummies(df, columns=['Col3'])\n    return df_out\ndf = load_data()\ndf_out = one_hot_encode(df)\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndef convert_list_to_columns(df, column_name):\n    unique_values = df[column_name].unique()\n    new_columns = [column_name + '_' + str(i) for i in range(len(unique_values))]\n    df[new_columns] = df[column_name].apply(lambda x: [1 if v in x else 0 for v in unique_values])\n    return df\ndf_out = convert_list_to_columns(df, 'Col3')\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder\ndf = load_data()\nencoder = OneHotEncoder()\ndf_out = encoder.fit_transform(df[['Col4']].to_numpy())\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndef one_hot_encode(df, col_name):\n    df_out = pd.get_dummies(df[col_name], drop_first=True)\n    return df_out\ndf = load_data()\ndf_out = one_hot_encode(df, 'Col3')\nprint(df_out)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndef one_hot_encode(df):\n    df_out = pd.DataFrame()\n    for col in df.columns:\n        if col == 'Col4':\n            df_out[col] = df[col].apply(lambda x: [1 if i in x else 0 for i in df['Col4'].unique()])\n        else:\n            df_out[col] = df[col].apply(lambda x: [1 if i in x else 0 for i in df[col].unique()])\n    return df_out\ndf = load_data()\ndf_out = one_hot_encode(df)\nprint(df_out)\n",
        "\n# Convert decision scores to probabilities\nproba = 1 / (1 + np.exp(-predicted_test_scores))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm\nfrom sklearn.model_selection import CalibratedClassifierCV\nfrom sklearn.metrics import f1_score\nX, y, x_predict = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\nassert type(x_predict) == np.ndarray\nmodel = svm.LinearSVC()\nclf = CalibratedClassifierCV(model, cv=5)\nclf.fit(X, y)\ny_pred = clf.predict(x_predict)\ny_pred_proba = clf.predict_proba(x_predict)\nprint(f1_score(y, y_pred))\nprint(f1_score(y, y_pred_proba))\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndef merge_onehot_encoded_data(df_origin, transform_output):\n    # Convert the sparse matrix to a dense matrix\n    dense_matrix = transform_output.toarray()\n    # Add the dense matrix as a new column to the dataframe\n    df_origin['onehot_encoded'] = dense_matrix\n    # Return the updated dataframe\n    return df_origin\ndf_origin, transform_output = load_data()\nmerged_df = merge_onehot_encoded_data(df_origin, transform_output)\nprint(merged_df)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndef transform_to_csr(df):\n    # Transform the data to csr_matrix\n    transform_output = csr_matrix(df)\n    return transform_output\ndef merge_csr_with_df(df, transform_output):\n    # Merge the csr_matrix with the original df\n    merged_df = pd.concat([df, transform_output], axis=1)\n    return merged_df\ndf_origin, transform_output = load_data()\nmerged_df = merge_csr_with_df(df_origin, transform_output)\nprint(merged_df)\n",
        "\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\ndef solve(df, transform_output):\n    # Convert the sparse matrix to a dense matrix\n    dense_matrix = transform_output.toarray()\n    \n    # Merge the dense matrix with the original dataframe\n    result = pd.concat([df, pd.DataFrame(dense_matrix)], axis=1)\n    \n    return result\ndf_origin, transform_output = load_data()\nresult = solve(df_origin, transform_output)\nprint(result)\n",
        "\nsteps = clf.named_steps()\ndel steps['reduce_dim']\nclf.set_params(**steps)\nprint(len(clf.steps))\n",
        "\nsteps = clf.named_steps()\ndel steps['reduce_poly']\nclf.set_params(**steps)\nprint(len(clf.steps))\n",
        "\n# Get the list of steps\nsteps = clf.named_steps()\n# Remove the second step\ndel steps['pOly']\n# Update the list of steps\nclf.named_steps = steps\n",
        "\nsteps = clf.named_steps()\nsteps.insert(1, ('new_step', PolynomialFeatures()))\nclf = Pipeline(steps)\nprint(len(clf.steps))\n",
        "\nsteps = clf.named_steps()\nsteps.insert(1, ('new_step', PolynomialFeatures()))\nclf = Pipeline(steps)\nprint(len(clf.steps))\n",
        "\nsteps = clf.named_steps()\nsteps.insert(1, ('t1919810', PCA()))\nclf = Pipeline(steps)\nprint(clf.named_steps)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\ngridsearch, testX, testY, trainX, trainY = load_data()\nassert type(gridsearch) == sklearn.model_selection._search.GridSearchCV\nassert type(trainX) == list\nassert type(trainY) == list\nassert type(testX) == list\nassert type(testY) == list\n# Define the early stopping callback\nclass EarlyStoppingCallback(xgb.callback.Callback):\n    def __init__(self, early_stopping_rounds, eval_metric, eval_set):\n        self.early_stopping_rounds = early_stopping_rounds\n        self.eval_metric = eval_metric\n        self.eval_set = eval_set\n    def on_train(self, dtrain):\n        self.best_score = float('-inf')\n        self.best_iteration = 0\n    def on_eval(self, dtest, pred):\n        if pred.shape == (len(dtest), 1):\n            pred = pred.reshape(-1)\n        score = self.eval_metric(dtest, pred)\n        if score > self.best_score:\n            self.best_score = score\n            self.best_iteration = dtest.get_label().shape\n    def on_end(self, dtrain, dtest, y_true, y_pred):\n        if self.best_score > self.early_stopping_rounds:\n            print(f\"Early stopping at iteration {self.best_iteration} with score {self.best_score}\")\n            gridsearch.stop_search()\nfit_params = {\"early_stopping_rounds\": 42,\n              \"eval_metric\" : \"mae\",\n              \"eval_set\" : [[testX, testY]]}\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, early_stopping_callback=EarlyStoppingCallback(fit_params[\"early_stopping_rounds\"], fit_params[\"eval_metric\"], fit_params[\"eval_set\"]))\ngridsearch.fit(trainX, trainY)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\ndef load_data():\n    trainX = ...\n    trainY = ...\n    testX = ...\n    testY = ...\n    gridsearch = ...\n    return gridsearch, testX, testY, trainX, trainY\ngridsearch, testX, testY, trainX, trainY = load_data()\nassert type(gridsearch) == sklearn.model_selection._search.GridSearchCV\nassert type(trainX) == list\nassert type(trainY) == list\nassert type(testX) == list\nassert type(testY) == list\n# Define the early stopping parameters\nearly_stopping_rounds = 42\neval_metric = \"mae\"\neval_set = [[testX, testY]]\n# Create a TimeSeriesSplit object to split the data into train and validation sets\nts_splitter = TimeSeriesSplit(n_splits=3)\ntrain_indices, valid_indices = ts_splitter.get_n_splits([trainX, trainY])\n# Define the fit_params dictionary to pass to GridSearchCV\nfit_params = {\"early_stopping_rounds\": early_stopping_rounds,\n             \"eval_metric\": eval_metric,\n             \"eval_set\": eval_set}\n# Create a dictionary to pass to GridSearchCV to define the search space\nparam_grid = {\"n_estimators\": [100, 200, 300],\n              \"max_depth\": [6, 8, 10],\n              \"learning_rate\": [0.05, 0.1, 0.15]}\n# Create a GridSearchCV object to search for the best hyperparameters\ngridsearch = GridSearchCV(xgb.XGBRegressor(), param_grid, verbose=1, cv=ts_splitter, n_jobs=3, iid=False, fit_params=fit_params)\n# Fit the model to the training data\ngridsearch.fit(trainX[train_indices], trainY[train_indices])\n# Print the best hyperparameters\nprint(\"Best hyperparameters:\")\nprint(gridsearch.best_params_)\n# Print the mean absolute error on the validation set\nprint(\"Validation set mean absolute error:\")\nprint(gridsearch.cv_results_[\"mean_absolute_error\"])\n",
        "\n# predict probabilities for each fold\nproba = []\nfor train_index, test_index in cv.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n",
        "\ny_pred = logreg.predict(X)\nproba = logreg.predict_proba(X)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ninversed = scaler.inverse_transform(scaled)\nprint(inversed)\n",
        "\n    predicted_t = scaler.inverse_transform(scaled)\n    real_t = data['t']\n    score = np.mean((predicted_t - real_t)**2)\n    return score\n# Load data\ndata = load_data()\n# Scale data\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\n# Predict t'\ninversed = solve(data, scaler, scaled)\nprint(inversed)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\nprint(model_name)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\nprint(model_name)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import LinearSVC\nmodel = LinearSVC()\nmodel_name = model.__class__.__name__\nprint(model_name)\n",
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\ndata = load_data()\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\nprint(tf_idf_out)\n",
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\ndata = load_data()\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\ntf_idf_out = pipe.fit_transform(data.test)\nprint(tf_idf_out)\n",
        "\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\ndata, target = load_data()\npipe = Pipeline(steps=[\n    ('select', SelectKBest(k=2)),\n    ('clf', LogisticRegression())]\n)\nselect_out = pipe.fit(data, target).transform(data)\nprint(select_out)\n",
        "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\n",
        "\n# Convert X data to a dataframe\nX_df = pd.DataFrame(X)\n# Drop the first column of X_df\nX_df = X_df.drop(columns=X_df.columns)\n# Convert y data to a dataframe\ny_df = pd.DataFrame(y)\n# Drop the first column of y_df\ny_df = y_df.drop(columns=y_df.columns)\n# Concatenate X_df and y_df\nX_df_concat = pd.concat([X_df, y_df], axis=1)\n# Drop the first row of X_df_concat\nX_df_concat = X_df_concat.drop(index=0)\n# Convert X_df_concat to a numpy array\nX_df_concat = X_df_concat.to_numpy()\n# Convert y_df to a numpy array\ny_df = y_df.to_numpy()\n# Split X_df_concat and y_df into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_df_concat, y_df, test_size=0.2, random_state=42)\n# Convert X_train and X_test to dataframes\nX_train_df = pd.DataFrame(X_train)\nX_test_df = pd.DataFrame(X_test)\n# Drop the first column of X_train_df and X_test_df\nX_train_df = X_train_df.drop(columns=X_train_df.columns)\nX_test_df = X_test_df.drop(columns=X_test_df.columns)\n# Concatenate X_train_df and y_train\nX_train_df_concat = pd.concat([X_train_df, y_train], axis=1)\n# Drop the first row of X_train_df_concat\nX_train_df_concat = X_train_df_concat.drop(index=0)\n# Convert X_train_df_concat to a numpy array\nX_train_df_concat = X_train_df_concat.to_numpy()\n# Convert y_train to a numpy array\ny_train = y_train.to_numpy()\n# Split X_train_df_concat and y_train into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_train_df_concat, y_train, test_size=0.2, random_state=42)\n# Convert X_train and X_test to dataframes\nX_train_df = pd.DataFrame(X_train)\nX_test_df = pd.DataFrame(X_test)\n# Drop the first column of X_train_df and X_test_df\nX_train_df = X_train_df.drop(columns=X_train_df.columns)\nX_test_df = X_test_df.drop(columns=X_test_df.columns)\n# Concatenate X_train_df and y_train\nX_train_df_concat = pd.concat([X_train_df, y_train], axis=1)\n# Drop the first row of X_train_df_concat\nX_train_df_concat = X_train_df_concat.drop(index=0)\n# Convert X_train_df_concat to a numpy array\nX_train_df_concat = X_train_df_concat.to_numpy()\n# Convert y_train to a numpy array\ny_train = y_train.to_numpy()\n# Split X_train_df_concat and y_train into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_train_df_concat, y_train, test_size=0.2, random_state=42)\n",
        "X_test = X_test.reshape(1, 1)\nregressor.fit(X, y)\npredict = regressor.predict(X_test)\nprint(predict)\n",
        "preprocessor = preprocess\ntfidf = TfidfVectorizer(preprocessor=preprocessor)\nprint(tfidf.preprocessor)",
        "def prePro(text):\n    return text.lower()\ntfidf.preprocessor = prePro\nprint(tfidf.preprocessor)",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\ndf_out = pd.DataFrame(preprocessing.scale(data))\nprint(df_out)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data).toarray(), columns=data.columns)\n",
        "coef = grid.best_estimator_.coef_\nprint(coef)",
        "coef = grid.best_estimator_.coef_\nprint(coef)",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\ncolumn_names = X.columns.tolist()\nselected_columns = model.get_support()\nprint(column_names)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns.tolist()\nprint(column_names)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n# Get the column names from the original dataframe\ncolumn_names = X.columns\n# Get the selected columns from the model\nselected_columns = model.support_\n# Create a new dataframe with the selected columns and the original column names\nselected_df = pd.DataFrame(X_new[:, selected_columns], columns=column_names)\nprint(selected_df)\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\ncolumn_names = X.columns.tolist()\nselected_columns = model.get_support()\nselected_columns_names = [column_names[i] for i in selected_columns]\nprint(selected_columns_names)\n",
        "# Get the cluster centers\ncenters = km.cluster_centers_\n# Get the indices of the 50 closest samples to the p-th cluster center\np_idx = np.argmin(np.abs(centers - p))\nclosest_50_samples = X[np.abs(centers - p) < np.abs(centers - centers[p_idx])].tolist()[:50]\n# Print the 50 closest samples\nprint(closest_50_samples)",
        "\n# Calculate the distance between each sample and the cluster center\ndistances = np.sqrt(np.sum((X - p)**2, axis=1))\n# Sort the distances in descending order\nsorted_indices = np.argsort(distances)[::-1]\n# Get the indices of the 50 closest samples\nclosest_50_indices = sorted_indices[:50]\n# Get the corresponding samples from the original data\nclosest_50_samples = X[closest_50_indices]\n",
        "# Get the cluster centers\ncenters = km.cluster_centers_\n# Get the index of the p^th cluster center\np_center_index = np.where(centers == p)[0][0]\n# Get the indices of the 100 closest samples to the p^th cluster center\nclosest_100_indices = np.argsort(np.linalg.norm(X - centers[p_center_index], axis=1))[:100]\n# Get the corresponding samples from the original data\nclosest_100_samples = X[closest_100_indices]",
        "def get_samples(p, X, km):\n    cluster_centers = km.cluster_centers_\n    distances = np.sqrt(np.sum((X - cluster_centers)**2, axis=1))\n    indices = np.argsort(distances)\n    samples = X[indices[:50]]\n    return samples\n",
        "\n# Convert categorical variable to matrix\nX_train[0] = pd.get_dummies(X_train[0], drop_first=True)\n# Merge matrix with original training data\nX_train = pd.concat([X_train, X_train[0]], axis=1)\n",
        "\n# convert categorical variable to matrix\nX_train_matrix = pd.get_dummies(X_train, columns=X_train.columns)\n# merge matrix with original training data\nX_train = pd.concat([X_train, X_train_matrix], axis=1)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Define the SVR model\nmodel = SVR(kernel='gaussian')\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n# Predict the test data\npredict = model.predict(X_test)\n# Calculate the mean squared error\nmse = mean_squared_error(y_test, predict)\nprint(f'Mean Squared Error: {mse}')\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.kernel import GaussianKernel\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\n# Create a Gaussian kernel\nkernel = GaussianKernel()\n# Create an SVM regressor\nregressor = SVC(kernel=kernel)\n# Fit the regressor to the data\nregressor.fit(X, y)\n# Predict the target variable\npredict = regressor.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nX, y = load_data()\nX_poly = PolynomialFeatures(degree=2).fit_transform(X)\nclf = SVR(kernel='poly')\nclf.fit(X_poly, y)\npredict = clf.predict(X_poly)\nprint(predict)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\n# Create polynomial features\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n# Create SVR model\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X_poly, y)\n# Predict on new data\npredict = model.predict(X_poly)\n",
        "\nquery_vectors = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_vectors, tfidf.transform(documents).T)\n",
        "\nquery_vectors = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_vectors, tfidf.transform(documents).T)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef get_term_frequency_inverse_data_frequency(documents):\n    vectorizer = TfidfVectorizer()\n    matrix = vectorizer.fit_transform(documents)\n    return matrix\ndef get_tf_idf_query_similarity(documents, query):\n    tfidf = get_term_frequency_inverse_data_frequency(documents)\n    query_vector = tfidf.transform([query])\n    return query_vector\ndef cosine_similarity(query_vector, document_matrix):\n    dot_product = np.dot(query_vector, document_matrix)\n    norm_query = np.linalg.norm(query_vector)\n    norm_document = np.linalg.norm(document_matrix)\n    if norm_query == 0 or norm_document == 0:\n        return 0\n    return dot_product / (norm_query * norm_document)\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_vectors = [get_tf_idf_query_similarity(documents, query) for query in queries]\n    cosine_similarities_of_queries = [cosine_similarity(query_vector, tfidf.transform(doc)) for query_vector, doc in zip(query_vectors, documents)]\n    return cosine_similarities_of_queries\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\nnew_features = []\nfor sample in features:\n    new_row = []\n    for feature in sample:\n        new_row.append(feature)\n    new_features.append(new_row)\nnew_features = np.array(new_features)\nprint(new_features)\n",
        "new_f = np.array(f)\nprint(new_f)",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\nnew_features = np.zeros((len(features), len(features[0])))\nfor i, row in enumerate(features):\n    for j, feature in enumerate(row):\n        new_features[i, j] = 1 if feature == '1' else 0\nprint(new_features)\n",
        "new_features = np.array(features).T\nreturn new_features",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nfeatures = load_data()\nencoder = OneHotEncoder()\nencoded_features = encoder.fit_transform(features)\nnew_features = encoded_features.toarray()\nprint(new_features)\n",
        "# Convert the distance matrix to a similarity matrix\nsimilarity_matrix = 1 - data_matrix\n# Perform hierarchical clustering\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2).fit_predict(similarity_matrix)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.cluster\ndata_matrix = load_data()\nX = data_matrix.values\ny = data_matrix.columns\nagg_clustering = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\nagg_clustering.fit(X)\ncluster_labels = agg_clustering.labels_\nprint(cluster_labels)\n",
        "# Convert the similarity matrix to a distance matrix\ndistM = 1 - simM\n# Perform hierarchical clustering\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2).fit_predict(distM)\nprint(cluster_labels)",
        "# Convert the distance matrix to a similarity matrix\nsimilarity_matrix = 1 - data_matrix\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(similarity_matrix, method='ward')\nprint(cluster_labels)",
        "\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\ndata_matrix = load_data()\nlinkage = scipy.cluster.hierarchy.linkage(data_matrix, method='ward')\ndendrogram = scipy.cluster.hierarchy.dendrogram(linkage, no_plot=True)\ncluster_labels = [i for i, _ in enumerate(dendrogram)]\nprint(cluster_labels)\n",
        "# Convert the similarity matrix to a distance matrix\ndistM = 1 - simM\n# Perform hierarchical clustering\nZ = linkage(distM, method='ward')\ncluster_labels = fcluster(Z, 2, criterion='distance')\nprint(cluster_labels)",
        "\nscaler = sklearn.preprocessing.StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "# Scale the data\nscaler = sklearn.preprocessing.StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n# Print the transformed data\nprint(centered_scaled_data)",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nassert type(data) == np.ndarray\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\nbox_cox_data = pd.DataFrame(scaled_data)\n",
        "box_cox_data = sklearn.preprocessing.BoxCoxTransformer().fit_transform(data)\nprint(box_cox_data)",
        "# Yeo-Johnson transformation\nyeo_johnson_data = sklearn.preprocessing.YeoJohnson(copy=True).fit_transform(data)\nprint(yeo_johnson_data)",
        "yeo_johnson_data = sklearn.preprocessing.YeoJohnson(data)\nprint(yeo_johnson_data)",
        "vectorizer = CountVectorizer(token_pattern=r'\\w+')\ntransformed_text = vectorizer.fit_transform(text)\nprint(transformed_text)",
        "\nimport numpy as np\nimport pandas as pd\nimport random\ndataset = load_data()\n# Split the dataset into training and testing sets\ntrain_size = int(len(dataset) * 0.8)\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=train_size, random_state=42)\n# Split each set into x and y\nx_train, x_test = x_train.iloc[:, :-1], x_test.iloc[:, :-1]\ny_train, y_test = y_train.iloc[:, -1], y_test.iloc[:, -1]\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\ndata = load_data()\n# Split the data into training and testing sets\ntrain_size = int(len(data) * 0.8)\ntest_size = len(data) - train_size\ntrain_data, test_data = data[:train_size], data[train_size:]\n# Split the training data into features and target\nx_train = train_data.drop(columns=['target'])\ny_train = train_data['target']\n# Split the test data into features and target\nx_test = test_data.drop(columns=['target'])\ny_test = test_data['target']\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport random\ndataset = load_data()\n# Split dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n# Print the training and testing sets\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport random\ndef load_data():\n    dataset = pd.read_csv('example.csv', header=None, sep=',')\n    return dataset\ndef split_data(data):\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    return x, y\ndef split_dataset(data):\n    x_train, x_test, y_train, y_test = train_test_split(data, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\nif __name__ == '__main__':\n    dataset = load_data()\n    x_train, y_train, x_test, y_test = split_dataset(dataset)\n    print(x_train)\n    print(y_train)\n    print(x_test)\n    print(y_test)\n",
        "\nfrom sklearn.cluster import KMeans\ndf = load_data()\nX = df['mse'].values\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\nprint(labels)\n",
        "\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\nprint(labels)\n",
        "\n# Initialize LinearSVC with L1 penalty\nlinear_svc = LinearSVC(penalty='l1', **default_params)\n# Fit the model to the transformed data\nlinear_svc.fit(X, y)\n# Get the indices of the selected features\nselected_indices = linear_svc.coef_\n# Get the feature names from the vectorizer\nfeature_names = vectorizer.get_feature_names()\n# Create a boolean mask of the selected features\nselected_feature_mask = [True if i in selected_indices else False for i in range(len(feature_names))]\n# Create a list of the selected feature names\nselected_feature_names = [feature_names[i] for i in range(len(feature_names)) if selected_feature_mask[i]]\n",
        "\nfeature_selector = LinearSVC(penalty='l1', C=1.0)\nfeature_selector.fit(X, y)\nselected_feature_names = [vectorizer.get_feature_names()[i] for i in feature_selector.support_]\nprint(selected_feature_names)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ncorpus, y = load_data()\nassert type(corpus) == list\nassert type(y) == list\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\ndef solve(corpus, y, vectorizer, X):\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names()\n    # Get the indices of the selected features\n    selected_indices = vectorizer.get_support()\n    # Get the names of the selected features\n    selected_feature_names = [feature_names[i] for i in selected_indices]\n    # Return the names of the selected features\n    return selected_feature_names\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer.set_vocabulary(vectorizer.vocabulary)\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer.set_vocabulary(vectorizer.vocabulary)\nX = vectorizer.fit_transform(corpus)\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer.set_vocabulary(feature_names)\nX = vectorizer.fit_transform(corpus)\nprint(feature_names)\nprint(X.toarray())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef linear_regression(df, col):\n    df2 = df[~np.isnan(df[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    return m\nslopes = []\nfor col in df1.columns:\n    slopes.append(linear_regression(df1, col))\nprint(slopes)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    series = np.concatenate((series, m), axis = 0)\nprint(slopes)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef load_data():\n    data = pd.read_csv('titanic.csv')\n    return data\ndef transform_sex(data):\n    le = LabelEncoder()\n    data['Sex'] = le.fit_transform(data['Sex'])\n    return data\ndef main():\n    data = load_data()\n    transformed_df = transform_sex(data)\n    print(transformed_df)\nif __name__ == '__main__':\n    main()\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef load_data():\n    data = {'Sex': ['male', 'female', 'male', 'female', 'male', 'female']}\n    return pd.DataFrame(data)\ndef transform_sex(df):\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\nif __name__ == '__main__':\n    df = load_data()\n    transformed_df = transform_sex(df)\n    print(transformed_df)\n",
        "\n    le = LabelEncoder()\n    le.fit(df['Sex'])\n    transformed_df['Sex'] = le.transform(df['Sex'])\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import linear_model\nimport statsmodels.api as sm\nX_train, y_train, X_test, y_test = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nassert type(X_test) == np.ndarray\nassert type(y_test) == np.ndarray\n# Create a linear regression model\nlasso = linear_model.ElasticNet()\n# Fit the model to the training data\nlasso.fit(X_train, y_train)\n# Print the coefficients\nprint(lasso.coef_)\nprint(lasso.intercept_)\n# Calculate the R^2 score for the training set\ntraining_set_score = lasso.score(X_train, y_train)\n# Calculate the R^2 score for the test set\ntest_set_score = lasso.score(X_test, y_test)\n# Print the R^2 scores\nprint(\"R^2 for training set:\", training_set_score)\nprint(\"R^2 for test set:\", test_set_score)\n",
        "scaler = MinMaxScaler()\nscaled_array = scaler.fit_transform(np_array)\nprint(scaled_array)",
        "scaler = MinMaxScaler()\nscaled_array = scaler.fit_transform(np_array)\nprint(scaled_array)",
        "\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a)\n    return new_a\ntransformed = Transform(np_array)\nprint(transformed)\n",
        "\n# Create a new DataFrame with the last close price and the moving averages\nlast_close = closing[-1]\nma50 = ma_50[-1]\nma100 = ma_100[-1]\nma200 = ma_200[-1]\ndf = pd.concat([last_close, ma50, ma100, ma200], axis=1)\n# Predict the target variable using the DecisionTreeRegressor\npredict = clf.predict(df)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['asdf', '1'], ['asdf', '0']]\nclf = DecisionTreeClassifier()\n# Convert string labels to numerical labels\nnew_X = np.array(X)\nnew_X[:, 1] = new_X[:, 1].astype(int)\nclf.fit(new_X, ['2', '3'])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nX = [['asdf', '1'], ['asdf', '0']]\nclf = DecisionTreeClassifier()\n# Convert the string labels to numerical labels\nnew_X = np.array(X)\nnew_X[:, 1] = new_X[:, 1].astype(int)\nclf.fit(new_X, ['2', '3'])\n",
        "new_X = [['dsa', '2'], ['sato', '3']]\nnew_X = pd.DataFrame(new_X)\nclf.fit(new_X, ['4', '5'])\n",
        "\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n",
        "\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\nlogReg = LogisticRegression()\nlogReg.fit(X[:None],y)\npredict = logReg.predict(X)\nprint(predict)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef split_data(data, train_size):\n    train_dataframe, test_dataframe = train_test_split(data, train_size=train_size)\n    train_dataframe = train_dataframe.sort([\"date\"])\n    test_dataframe = test_dataframe.sort([\"date\"])\n    return train_dataframe, test_dataframe\nfeatures_dataframe = load_data()\ntrain_dataframe, test_dataframe = split_data(features_dataframe, train_size=0.2)\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef split_data(data, train_size):\n    train_indices = np.arange(len(data))\n    np.random.shuffle(train_indices)\n    train_data = data[train_indices[:int(train_size * len(data))]]\n    test_data = data[train_indices[int(train_size * len(data)):]]\n    return train_data, test_data\nfeatures_dataframe = load_data()\ntrain_dataframe, test_dataframe = split_data(features_dataframe, train_size)\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef split_data(features_dataframe):\n    # Sort the dataframe by date\n    features_dataframe = features_dataframe.sort([\"date\"])\n    # Split the dataframe into train and test sets\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    # Filter the train dataframe to only include data newer than the test dataframe\n    train_dataframe = train_dataframe[train_dataframe[\"date\"] > test_dataframe[\"date\"].max()]\n    return train_dataframe, test_dataframe\ntrain_dataframe, test_dataframe = split_data(features_dataframe)\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].scaler.fit_transform(df[cols])\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nmyData = pd.DataFrame({\n    'Month': [3, 3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8],\n    'A1': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],\n    'A2': [31, 13, 13, 13, 33, 33, 81, 38, 18, 38, 18, 18, 118],\n    'A3': [81, 38, 18, 38, 18, 18, 118, 31, 13, 13, 13, 33, 33],\n    'A4': [1, 1, 1, 1, 1, 1, 8, 8, 8, 8, 8, 8, 8],\n})\nscaler = MinMaxScaler()\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].scaler.fit_transform(myData[cols])\nprint(myData)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nwords = load_data()\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names())\n",
        "\ncount = CountVectorizer(lowercase = False)\ncount.fit(words)\nvocabulary = count.get_feature_names_out()\nprint(vocabulary)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\n# Create a dataframe to store the full results of GridSearchCV\nfull_results = pd.DataFrame()\n# Iterate through the candidate models and fit each one to the data\nfor i, model in enumerate(GridSearch_fitted.estimators_):\n    # Fit the model to the data\n    model.fit(X_train, y_train)\n    \n    # Get the score of the model on the validation data\n    score = model.score(X_val, y_val)\n    \n    # Add the score and the model to the dataframe\n    full_results = full_results.append({'Score': score, 'Model': model}, ignore_index=True)\n# Print the dataframe\nprint(full_results)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nGridSearch_fitted = load_data()\nassert type(GridSearch_fitted) == sklearn.model_selection._search.GridSearchCV\n# Get the full results of GridSearchCV\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n# Sort the results by mean_fit_time\nfull_results = full_results.sort_values(by='mean_fit_time')\n# Print the results\nprint(full_results)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport pickle\ndef load_data():\n    # Load the clean data\n    clean_data = pd.read_csv('clean_data.csv')\n    return clean_data\ndef save_model(model):\n    # Save the model to a file named \"sklearn_model\"\n    with open('sklearn_model.pkl', 'wb') as f:\n        pickle.dump(model, f)\ndef predict(data):\n    # Load the saved model\n    with open('sklearn_model.pkl', 'rb') as f:\n        model = pickle.load(f)\n    # Predict the outliers\n    outliers = model.predict(data)\n    return outliers\nif __name__ == '__main__':\n    # Load the clean data\n    clean_data = load_data()\n    # Train the model\n    model = IsolationForest()\n    model.fit(clean_data)\n    # Save the model\n    save_model(model)\n    # Predict the outliers\n    outliers = predict(clean_data)\n    # Print the outliers\n    print(outliers)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef cosine_similarity(vector1, vector2):\n    dot_product = np.dot(vector1, vector2)\n    norm1 = np.linalg.norm(vector1)\n    norm2 = np.linalg.norm(vector2)\n    return dot_product / (norm1 * norm2)\ndef cosine_similarity_matrix(tfidf_matrix):\n    similarity_matrix = np.zeros((len(tfidf_matrix), len(tfidf_matrix)))\n    for i in range(len(tfidf_matrix)):\n        for j in range(i+1, len(tfidf_matrix)):\n            similarity_matrix[i, j] = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\n    return similarity_matrix\ndf = load_data()\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['description'])\nsimilarity_matrix = cosine_similarity_matrix(tfidf_matrix)\nprint(similarity_matrix)\n"
    ],
    "Pytorch": [
        "optim.param_groups[0]['lr'] = 0.001",
        "\noptim.set_lr(0.001)\n",
        "optim.param_groups[0]['lr'] = 0.0005",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\noptim = torch.optim.SGD(..., lr=0.005)\ndef update_lr(loss):\n    if loss > 0.1:\n        optim.set_lr(optim.param_groups['lr'], 0.0005)\n    else:\n        optim.set_lr(optim.param_groups['lr'], 0.005)\nfor epoch in range(num_epochs):\n    train_loss = train(model, train_loader, optim, criterion)\n    test_loss = test(model, test_loader, criterion)\n    update_lr(train_loss)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\ndef load_data():\n    # Load data from a CSV file\n    df = pd.read_csv('data.csv')\n    return df.values\ndef embed_input(input_Tensor):\n    # Load pre-trained word2vec embedding\n    word2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n    # Convert input Tensor to PyTorch Tensor\n    input_Tensor = torch.from_numpy(input_Tensor).float()\n    # Get embedding weights from word2vec model\n    embedding_weights = word2vec.wv\n    # Compute embeddings for input Tensor\n    embedded_input = torch.mm(input_Tensor, embedding_weights)\n    return embedded_input\ninput_Tensor = load_data()\nembedded_input = embed_input(input_Tensor)\nprint(embedded_input)\n",
        "\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x.numpy())\nprint(px)\n",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x.numpy())\nprint(px)\n",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x.numpy())\nprint(px)\n",
        "\nA_log = torch.ByteTensor(A_log)\nC = torch.LongTensor(B.shape)\nC = C.zero_()\n",
        "C = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical] # Throws error",
        "\nA_log = torch.ByteTensor(A_log)\nB = torch.LongTensor(B)\nC = B[:, A_log]\n",
        "\nA_log = torch.ByteTensor(A_log)\nC = torch.LongTensor(B.shape)\nC = C.zero_()\nC = C.scatter_(1, A_log, B)\n",
        "\n    B_truncated = B[:, A_log]\n    C = B_truncated\n    ",
        "\nA_log = torch.ByteTensor(A_log)\nC = B[:, A_log]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef index_select(tensor, index):\n    return tensor[index]\nidx, B = load_data()\nC = index_select(B, idx)\nprint(C)\n",
        "\nx_tensor = torch.tensor(x_array.values)\n",
        "\nimport pandas as pd\nimport torch\nimport numpy as np\nx_array = load_data()\nx_tensor = torch.tensor(x_array.values)\nprint(x_tensor)\n",
        "\nimport pandas as pd\nimport torch\nimport numpy as np\nx_array = load_data()\ndef Convert(a):\n    # Convert numpy array to torch tensor\n    t = torch.from_numpy(a)\n    return t\nx_tensor = Convert(x_array)\nprint(x_tensor)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef batch_to_mask(lens):\n    mask = torch.zeros(len(lens), max(lens))\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\nlens = load_data()\nmask = batch_to_mask(lens)\nprint(mask)\n",
        "\nmask = torch.zeros(len(lens), max(lens)).long()\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmask = torch.zeros(len(lens), max(lens))\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\n    mask = torch.zeros(len(lens), max(lens))\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    ",
        "# Construct the diagonal matrix\ndiag_ele = Tensor_2D.diag()\n# Reshape the diagonal matrix to a 3D tensor\ndiag_ele = diag_ele.unsqueeze(0)\n# Repeat the diagonal matrix to match the size of the 2D tensor\nTensor_3D = diag_ele.repeat(Tensor_2D.shape, 1)\n# Print the resulting 3D tensor\nprint(Tensor_3D)",
        "\n    diag_ele = torch.diag(t)\n    result = torch.cat((torch.zeros(t.shape, t.dtype), diag_ele, torch.zeros(t.shape, t.dtype)), dim=1)\n    ",
        "\na = a.unsqueeze(0)\nb = b.unsqueeze(0)\nab = torch.cat((a, b), dim=0)\n",
        "\na = a.unsqueeze(0)\nb = b.unsqueeze(0)\nab = torch.cat((a, b), dim=0)\n",
        "\n    if a.shape == b.shape:\n        ab = torch.stack((a, b), 0)\n    else:\n        raise ValueError(\"The two tensors must have the same shape.\")\n    ",
        "\nmask = torch.zeros_like(a)\nmask[..., :lengths] = 1\na = a * mask\n",
        "a[ : , lengths : , : ]  = 2333",
        "\nmask = torch.zeros_like(a)\nmask[:, :lengths, :] = 1\na = a * mask\n",
        "\nmask = torch.zeros_like(a)\nmask[..., :lengths] = 1\na = a * mask\n",
        "tensor_of_tensors = torch.stack(list_of_tensors)\nprint(tensor_of_tensors)",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist = load_data()\nnew_tensors = torch.stack(list)\nprint(new_tensors)\n",
        "\n    tt = torch.tensor([lt])\n    return tt\nlist_of_tensors = load_data()\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n",
        "tensor_of_tensors = torch.stack(list_of_tensors)\nprint(tensor_of_tensors)",
        "\nresult = torch.index_select(t, 0, idx)\n",
        "\nresult = torch.index_select(t, 0, idx)\n",
        "\nresult = torch.index_select(t, 0, idx)\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nresult = torch.gather(x, 1, ids)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ny = torch.argmax(softmax_output, dim=1)\nprint(y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ny = torch.argmax(softmax_output, dim=1)\nprint(y)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nsoftmax_output = load_data()\ny = torch.argmin(softmax_output, dim=1)\nprint(y)\n",
        "\n    y = torch.argmax(softmax_output, dim=1)\n    ",
        "\n    y = torch.argmin(softmax_output, dim=1)\n    ",
        "\n# Define the cross-entropy loss function\ndef cross_entropy_loss(logits, labels):\n    # Calculate the softmax probabilities\n    probs = F.softmax(logits, dim=1)\n    # Calculate the cross-entropy loss\n    loss = -torch.sum(labels * torch.log(probs))\n    return loss\n",
        "cnt_equal = torch.sum(torch.eq(A, B))\nprint(cnt_equal)",
        "cnt_equal = torch.sum(torch.eq(A, B))\nprint(cnt_equal)",
        "cnt_not_equal = np.sum(np.abs(A - B) > 1e-6)\nprint(cnt_not_equal)",
        "\n    cnt_equal = np.sum(A == B)\n    ",
        "cnt_equal = torch.sum(torch.eq(A[:, -x:], B[:, -x:]))\nprint(cnt_equal)",
        "# Create a mask for the last x elements\nmask = torch.zeros(2*x, 1)\nmask[x:, 0] = 1\n# Count the number of non-equal elements in the mask\ncnt_not_equal = torch.sum(mask != mask.t())\nprint(cnt_not_equal)",
        "\ntensors_31 = []\nfor i in range(0, 31, 1):\n    tensor = a[:, :, :, i*chunk_dim:(i+1)*chunk_dim, :]\n    tensors_31.append(tensor)\n",
        "\ntensors_31 = []\nfor i in range(0, a.shape, chunk_dim):\n    tensors_31.append(a[i:i+chunk_dim, :, :, :, :])\n",
        "\noutput[mask.unsqueeze(1)] = clean_input_spectrogram[mask.unsqueeze(1)]\n",
        "\noutput[mask==0] = clean_input_spectrogram[mask==0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\n",
        "\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = torch.where(torch.abs(x) == min, sign_x, torch.where(torch.abs(y) == min, sign_y, 0))\n    ",
        "\n# Softmax function to get the confidence score in range (0-1)\noutput = torch.nn.functional.softmax(output, dim=1)\n# Get the maximum value and corresponding class label\nconf, classes = torch.max(output.reshape(1, 3), 1)\n# Convert class label to character\nclass_names = '012'\nreturn conf, class_names[classes.item()]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nresult = torch.cat((a, b), dim=1)\nresult = result.mean(dim=1)\nprint(result)\n",
        "\n    result = torch.cat((a, b), dim=1)\n    result = result.mean(dim=1)\n    return result\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    t = torch.arange(8).reshape(1,4,2).float()\n    return t\nt = load_data()\nnew = torch.tensor([[0., 0., 0., 0.]])\nresult = torch.cat([t, new], dim=0)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef load_data():\n    t = torch.arange(4).reshape(1,2,2).float()\n    return t\nt = load_data()\nnew = torch.tensor([[0., 0., 0., 0.]])\nresult = torch.cat([t, new], dim=0)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = load_data()\nnew = torch.tensor([[-1, -1, -1, -1]])\nresult = torch.cat([t, new], dim=0)\nprint(result)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nhid_dim = 32\ndata = torch.randn(10, 2, 3, hid_dim)\ndata = data.view(10, 2 * 3, hid_dim)\nW = torch.randn(hid_dim)\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n"
    ]
}