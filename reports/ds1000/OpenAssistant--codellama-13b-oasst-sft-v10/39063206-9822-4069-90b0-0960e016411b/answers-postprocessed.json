{
    "Pandas": [
        "\nresult = df.iloc[List]\n",
        "\ndf_shuffled = df.iloc[List]\nresult = df_shuffled['Type'].nunique()\n",
        "\n# Create a dictionary to map the values to be replaced\nreplace_dict = {'apple': 'other', 'egg': 'other'}\n# Apply the replace_dict to the columns Qu1 and Qu3\ndf['Qu1'] = df['Qu1'].replace(replace_dict)\ndf['Qu3'] = df['Qu3'].replace(replace_dict)\n# Create a list of values to keep\nkeep_values = ['cheese', 'potato', 'banana']\n# Apply the keep_values to the columns Qu1 and Qu3\ndf['Qu1'] = df['Qu1'].apply(lambda x: x if x in keep_values else 'other')\ndf['Qu3'] = df['Qu3'].apply(lambda x: x if x in keep_values else 'other')\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    value_counts = df[col].value_counts()\n    if (value_counts >= 3).any():\n        result[col] = 'other'\n    else:\n        result[col] = df[col]\n",
        "\n    result = df.copy()\n    for col in ['Qu1', 'Qu2', 'Qu3']:\n        value_counts = pd.value_counts(df[col])\n        for val in value_counts.index:\n            if value_counts[val] >= 2:\n                result[col].replace(val, val, inplace=True)\n            else:\n                result[col].replace(val, 'other', inplace=True)\n    ",
        "\n# Create a new column 'Qu1_new' and assign the values based on the value_counts()\ndf['Qu1_new'] = df['Qu1'].apply(lambda x: 'other' if x not in df['Qu1'].value_counts()[df['Qu1'].value_counts() >= 3].index else x)\n# Create a new column 'Qu2_new' and assign the values based on the value_counts()\ndf['Qu2_new'] = df['Qu2'].apply(lambda x: 'other' if x not in df['Qu2'].value_counts()[df['Qu2'].value_counts() >= 2].index else x)\n# Create a new column 'Qu3_new' and assign the values based on the value_counts()\ndf['Qu3_new'] = df['Qu3'].apply(lambda x: 'other' if x not in df['Qu3'].value_counts()[df['Qu3'].value_counts() >= 2].index else x)\n",
        "\n# Create a dictionary to map values to 'other'\nvalue_dict = {'potato': 'other', 'banana': 'other', 'apple': 'apple', 'egg': 'other'}\n# Replace values in Qu1 column\ndf['Qu1'] = df['Qu1'].map(value_dict)\n# Replace values in Qu2 and Qu3 column\ndf['Qu2'] = df['Qu2'].replace(value_dict)\ndf['Qu3'] = df['Qu3'].replace(value_dict)\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[result['keep_if_dup'] == 'Yes']\n",
        "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[result['drop_if_dup'] == 'No']\n",
        "\nresult = df.drop_duplicates(subset='url', keep='last')\nresult = result[result['keep_if_dup'] == 'Yes']\nresult = result.append(df[df['keep_if_dup'] == 'Yes'])\n",
        "\nresult = {}\nfor name, group in df.groupby('name'):\n    result[name] = {}\n    for v1, group2 in group.groupby('v1'):\n        result[name][v1] = {}\n        for v2, row in group2.iterrows():\n            result[name][v1][v2] = row['v3']\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n    result = df['datetime'].dt.tz_localize(None)\n    ",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n",
        "\n# Extract key value pairs from the message column\nmessage_list = df['message'].str.split(', ').apply(lambda x: [i.split(': ') for i in x])\n# Create a dictionary with the key value pairs\nmessage_dict = {k: v for d in message_list for k, v in d}\n# Add the key value pairs to the dataframe\ndf = df.join(pd.DataFrame(message_dict))\n# Rename the columns\ndf = df.rename(columns={'job': 'job', 'money': 'money', 'wife': 'wife', 'group': 'group', 'kids': 'kids'})\n# Replace 'none' with None\ndf = df.replace('none', None)\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\nfor product_range in products:\n    df.loc[df['product'].isin(product_range), 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] = (df['score'] - df['score'].min()) / (df['score'].max() - df['score'].min())\n",
        "\ndf['category'] = df.idxmax(axis=1)\n",
        "\ndf['category'] = df.idxmax(axis=1)\n",
        "\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\nresult = df\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\nresult = df\n",
        "\ndf['Date'] = df['Date'].dt.to_period(\"M\")\ndf['Date'] = df['Date'].dt.strftime(\"%d-%b-%Y\")\ndf['Date'] = df['Date'] + \" \" + df['Date'].dt.day_name()\n",
        "\ndf.loc['1980-01-01', '#1'] = df.loc['1980-01-05', '#1']\ndf.loc['1980-01-05', '#1'] = df.loc['1980-01-01', '#1']\n",
        "\ndf.loc['1980-01-01', '#1'] = df.loc['1980-01-05', '#1']\ndf.loc['1980-01-05', '#1'] = df.loc['1980-01-01', '#1']\n",
        "\ndf = df.shift(1, axis=0)\ndf.loc['1980-01-01', '#1'] = df.loc['1980-01-05', '#1']\ndf.loc['1980-01-05', '#1'] = df.loc['1980-01-01', '#1']\ndf.loc['1980-01-05', '#2'] = df.loc['1980-01-01', '#2']\ndf.loc['1980-01-01', '#2'] = df.loc['1980-01-05', '#2']\n",
        "\ndf = df.shift(1)\ndf.loc['1980-01-01', '#1'] = 72.4399\ndf = df.sort_index()\n",
        "\ndf.columns = [col + 'X' for col in df.columns]\n",
        "\ndf.columns = ['X' + col for col in df.columns]\n",
        "\ndf.columns = [col + \"X\" if not col.endswith(\"X\") else col for col in df.columns]\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"mean\" for col in df.columns if col != \"group_color\" and col != \"group\"}})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"sum\" for col in df.columns if \"val\" in col}})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", **{col: \"mean\" if col.endswith('2') else \"sum\" for col in df.columns if col != 'group' and col != 'group_color'}})\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.loc[row_list, column_list].mean(axis=0)\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\nresult = df.loc[row_list, column_list].sum(axis=0)\nresult = result.sort_values(ascending=False)\nresult = result.drop(result.index[0])\n",
        "\nresult = df.apply(lambda x: x.value_counts())\n",
        "\nresult = df.isnull().sum()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = \"\"\nfor col in df.columns:\n    if df[col].value_counts().size > 1:\n        result += \"---- \" + col + \" ----\\n\"\n        result += str(df[col].value_counts()) + \"\\n\"\n    else:\n        result += \"---- \" + col + \" ----\\n\"\n        result += \"null    3\\n\"\n        result += \"Name: \" + col + \", dtype: int64\\n\"\nprint(result)\n",
        "\nresult = df.iloc[[0,1]].combine_first(df.iloc[[1]])\n",
        "\nresult = df.iloc[[0,1]].combine_first(df.iloc[[1]])\n",
        "\nresult = df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),1)\n",
        "\nresult = df.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n",
        "\nresult = df.loc[df['value'] >= thresh]\nresult.loc['X'] = df.loc[df['value'] < thresh]['value'].sum()\nresult = result.reset_index()\n",
        "\nresult = df.loc[df['value'] >= thresh].groupby(level=0).mean()\nresult.loc['X'] = df.loc[df['value'] < thresh]['value'].mean()\nresult = result.reset_index()\n",
        "\nresult = df.loc[df['value'] < section_left]\nresult = result.append(df.loc[df['value'] > section_right])\nresult['value'] = result['value'].mean()\nresult = result.append(df.loc[(df['value'] >= section_left) & (df['value'] <= section_right)])\nresult = result.drop_duplicates(keep=False)\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    result[f\"inv_{col}\"] = 1 / df[col]\n",
        "\nresult = df.apply(lambda x: x.apply(lambda y: y**2))\nresult.columns = [f\"exp_{col}\" for col in result.columns]\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    if col != 'A':\n        result[f'inv_{col}'] = 1/df[col]\n    else:\n        result[f'inv_{col}'] = df[col]\n",
        "\nresult = df.copy()\nfor col in df.columns:\n    result[f\"sigmoid_{col}\"] = 1 / (1 + np.exp(-df[col]))\n",
        "\nresult = df.idxmax()\nresult = result[result <= df.idxmin()]\n",
        "\nresult = df.idxmax()\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\ndf_expanded = pd.DataFrame({'user': df['user'].unique(), 'dt': date_range, 'val': 0})\nresult = pd.merge(df, df_expanded, on=['user', 'dt'], how='outer')\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\ndf_expanded = df.set_index('dt').reindex(date_range).reset_index()\ndf_expanded['val'] = df_expanded['val'].fillna(0)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\ndf_expanded = pd.DataFrame({'user': df['user'].unique(), 'dt': date_range, 'val': 233})\ndf_expanded = df_expanded.merge(df, on=['user', 'dt'], how='left')\ndf_expanded = df_expanded.fillna(233)\nresult = df_expanded[['user', 'dt', 'val']]\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndf_expanded = pd.DataFrame({'dt': pd.date_range(min_date, max_date), 'user': df['user'].unique()})\ndf_expanded = df_expanded.merge(df, on=['user', 'dt'], how='left')\ndf_expanded['val'].fillna(df_expanded.groupby('user')['val'].transform('max'), inplace=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\ndate_range = pd.date_range(min_date, max_date)\ndf = df.set_index('dt')\ndf = df.reindex(date_range)\ndf = df.reset_index()\ndf['val'] = df.groupby('user')['val'].transform('max')\ndf['dt'] = df['dt'].dt.strftime('%d-%b-%Y')\nresult = df\n",
        "\ndf['name'] = df['name'].factorize()[0] + 1\n",
        "\ndf['a'] = df.groupby('name')['a'].transform(lambda x: x.rank(method='dense'))\n",
        "\n    unique_ids = {}\n    for i, name in enumerate(df['name'].unique()):\n        unique_ids[name] = i+1\n    df['name'] = df['name'].map(unique_ids)\n    ",
        "\ndf['ID'] = df.groupby('name')['a'].transform(lambda x: x.rank(method='dense'))\ndf = df.drop('name', axis=1)\n",
        "\ndf = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(['user', 'date'])\n",
        "\ndf = df.melt(id_vars=['user', '01/12/15'], var_name='others', value_name='value')\n",
        "\ndf = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(['user', 'date'])\n",
        "\nresult = df.loc[df['c'] > 0.5, columns].values\n",
        "\nresult = df.loc[df['c'] > 0.45, columns].values\n",
        "\n    locs = [df.columns.get_loc(_) for _ in columns]\n    result = df[df.c > 0.5][locs].values\n    ",
        "\n    result = df[df.c > 0.5][columns].copy()\n    result['sum'] = result['b'] + result['e']\n    ",
        "\n    result = df.loc[df['c'] > 0.5, columns]\n    ",
        "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n# Sort the dataframe by date\ndf = df.sort_values(by='date')\n# Create a new column with the difference between the current date and the previous date\ndf['diff'] = df['date'].diff()\n# Filter the dataframe to remove rows where the difference is less than X days\ndf = df[df['diff'].dt.days >= X]\n# Remove the diff column\ndf = df.drop('diff', axis=1)\n# Sort the dataframe by ID\ndf = df.sort_values(by='ID')\n# Reset the index\ndf = df.reset_index(drop=True)\n# Print the result\nresult = df\n",
        "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n# Sort the dataframe by date\ndf = df.sort_values(by='date')\n# Create a new column with the difference between the current date and the previous date\ndf['diff'] = df['date'].diff()\n# Filter the dataframe to remove rows where the difference is less than X weeks\ndf = df[df['diff'] >= pd.Timedelta(weeks=X)]\n# Remove the diff column\ndf = df.drop('diff', axis=1)\n",
        "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n# Create a new column with the difference between the current date and the previous date\ndf['diff'] = df['date'].diff()\n# Filter out rows where the difference is less than X weeks\ndf = df[df['diff'] >= pd.Timedelta(weeks=X)]\n# Remove the diff column\ndf = df.drop('diff', axis=1)\n# Convert the date column back to string format\ndf['date'] = df['date'].dt.strftime('%m-%d-%Y')\n# Sort the dataframe by date\ndf = df.sort_values(by='date')\n# Reset the index\ndf = df.reset_index(drop=True)\n# Print the result\nprint(df)\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 3).sum()\n",
        "\nresult = df.groupby(df.index // 4).sum()\n",
        "\nresult = df.iloc[::-3].mean()\n",
        "\nresult = []\nfor i in range(0, len(df), 3):\n    if i+2 < len(df):\n        result.append(df.iloc[i:i+3].sum())\n    else:\n        result.append(df.iloc[i:].sum())\n    if i+1 < len(df):\n        result.append(df.iloc[i+1:i+3].mean())\n    else:\n        result.append(df.iloc[i+1:].mean())\n",
        "\nresult = []\nfor i in range(0, len(df), 3):\n    if i + 3 <= len(df):\n        result.append(df.iloc[i:i+3]['col1'].sum())\n    else:\n        result.append(df.iloc[i:len(df)]['col1'].sum())\n    if i + 5 <= len(df):\n        result.append(df.iloc[i+3:i+5]['col1'].mean())\n    else:\n        result.append(df.iloc[i+3:len(df)]['col1'].mean())\n",
        "\ndf['A'] = df['A'].fillna(method='ffill')\n",
        "\ndf['A'] = df['A'].fillna(method='ffill')\n",
        "\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].fillna(method='bfill')\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\ndf['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n",
        "\n    df['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\n    df['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\n    df['time_days'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\n    ",
        "\ndf['number'] = df.duration.str.extract(r'(\\d+)', expand=False)\ndf['time'] = df.duration.str.extract(r'(\\w+)', expand=False)\ndf['time_day'] = df.time.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True)\ndf['time_day'] *= df['number']\n",
        "\ncheck = np.where([df1[column] != df2[column] for column in columns_check_list])\nresult = [False] * len(check[0])\nfor i in range(len(check[0])):\n    if check[0][i] != -1:\n        result[i] = True\n",
        "\ncheck = np.where([df1[column] == df2[column] for column in columns_check_list])\nresult = [True if len(check[0]) == 0 else False for check in check]\n",
        "df.index.levels[1] = pd.to_datetime(df.index.levels[1])",
        "\ndf.index.set_levels([df.index.levels[0], pd.to_datetime(df.index.levels[1])], inplace=True)\n",
        "\n    df.index = pd.to_datetime(df.index.get_level_values(1))\n    df = df.reset_index()\n    df = df[['date', 'x', 'y']]\n    df = df.values\n    ",
        "\n    df.index = pd.to_datetime(df.index.get_level_values(0))\n    df = df.swaplevel(0,1)\n    ",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'], var_name='year', value_name='var1')\n",
        "\ndf = pd.melt(df, id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'], var_name='year', value_name='var1')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(by=['Country', 'Variable', 'year'], ascending=[True, True, False])\n",
        "\nresult = df[df.filter(regex='Value_').abs().max(axis=1) >= 1]\n",
        "\nresult = df[df.filter(regex='Value_').abs().max(axis=1) <= 1]\n",
        "\nresult = df.loc[(df.filter(regex='Value_').abs() > 1).any(axis=1)]\nresult.columns = [col.replace('Value_', '') for col in result.columns]\n",
        "\ndf = df.replace('&AMP;', '&', regex=True)\n",
        "\ndf = df.replace('&LT', '<', regex=True)\n",
        "\n    df = df.replace('&AMP;', '&', regex=True)\n    ",
        "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n",
        "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['C'] = df['C'].str.replace('&AMP;', '&')\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['first_name'], df['last_name'] = zip(*df['name'].apply(lambda x: x.split(' ') if validate_single_space_name(x) else (x, None)))\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['1_name'] = df['name'].apply(lambda x: x.split()[0])\ndf['2_name'] = df['name'].apply(lambda x: x.split()[1] if len(x.split()) > 1 else None)\ndf['1_name'] = df['1_name'].apply(lambda x: validate_single_space_name(x))\ndf['2_name'] = df['2_name'].apply(lambda x: validate_single_space_name(x))\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndef split_name(name: str) -> list:\n    if validate_single_space_name(name):\n        name_list = name.split()\n        if len(name_list) == 2:\n            return [name_list[0], None, name_list[1]]\n        elif len(name_list) == 3:\n            return [name_list[0], name_list[1], name_list[2]]\n        else:\n            return [None, None, None]\n    else:\n        return [None, None, None]\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(split_name))\n",
        "\nresult = pd.merge_asof(df2, df1, on='Timestamp', direction='nearest')\n",
        "\nimport pandas as pd\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\nresult = pd.merge_asof(df1, df2, on='Timestamp', tolerance=pd.Timedelta('1s'))\nprint(result)\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] <= 50 and x['col3'] <= 50 else max(x['col1'], x['col2'], x['col3']), axis=1)\n",
        "\ndf['state'] = df.apply(lambda x: x['col1'] if x['col2'] > 50 and x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'], axis=1)\n",
        "\nerrors = []\nfor index, row in df.iterrows():\n    if not isinstance(row['Field1'], int):\n        errors.append(row['Field1'])\n",
        "\nresult = []\nfor row in df.itertuples():\n    if isinstance(row.Field1, int):\n        result.append(row.Field1)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    result = []\n    for row in df.itertuples():\n        if not isinstance(row.Field1, int):\n            result.append(row.Field1)\n    return result\n",
        "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n",
        "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\n",
        "\n    result = df.loc[test]\n    ",
        "\n# Calculate pairwise distances between cars\ndistances = df.groupby('time')['x', 'y'].apply(lambda x: pd.DataFrame(pd.DataFrame(x).values[:, None] - pd.DataFrame(x).values[None, :]).apply(lambda x: (x**2).sum(axis=1)**0.5))\n# Find nearest neighbours for each car\nnearest_neighbours = distances.groupby('time')['car'].apply(lambda x: x.sort_values().iloc[1:].reset_index(drop=True))\n# Calculate euclidean distances between cars and their nearest neighbours\neuclidean_distances = distances.groupby('time')['car'].apply(lambda x: x.sort_values().iloc[1:].reset_index(drop=True).apply(lambda x: distances.loc[x.name, x.name, x]))\n# Create final dataframe\nresult = pd.concat([df['time'], df['car'], nearest_neighbours, euclidean_distances], axis=1)\nresult.columns = ['time', 'car', 'nearest_neighbour', 'euclidean_distance']\n",
        "\n# Calculate pairwise distances between cars\ndistances = df.groupby('time')['x', 'y'].apply(lambda x: pd.DataFrame(pd.DataFrame(x).values[:, None] - pd.DataFrame(x).values[None, :]).apply(lambda x: (x**2).sum(axis=1)**0.5))\n# Find the farmost car neighbour for each car\ndf2 = pd.DataFrame({'time': df['time'].unique(), 'car': df['car'].unique()})\ndf2 = df2.merge(distances, on='time')\ndf2['farmost_neighbour'] = df2.apply(lambda x: x.sort_values(x.name).iloc[1, 0], axis=1)\ndf2['euclidean_distance'] = df2.apply(lambda x: x.sort_values(x.name).iloc[1, 1], axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \",\".join(x.dropna().tolist()), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[cols].apply(lambda x: \"-\".join(x.dropna().tolist()), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\n# Select 20% of rows randomly\nrandom_rows = df.sample(frac=0.2, random_state=0)\n# Set Quantity column of selected rows to zero\nrandom_rows['Quantity'] = 0\n",
        "\n# Select 20% of rows randomly\nrandom_rows = df.sample(frac=0.2, random_state=0)\n# Change the value of the ProductId column of these rows to zero\nrandom_rows['ProductId'] = 0\n",
        "\n# Group the DataFrame by UserId and apply the sample function to each group\nsampled_df = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n# Set the Quantity column of the sampled rows to zero\nresult = df.merge(sampled_df, on=['UserId', 'ProductId'], how='left')\nresult.loc[result['Quantity'].notnull(), 'Quantity'] = 0\n# Drop the extra columns\nresult = result.drop(['Quantity_x', 'Quantity_y'], axis=1)\n# Rename the columns\nresult = result.rename(columns={'Quantity_x': 'Quantity'})\n",
        "\ndf['index_original'] = df.groupby(['col1','col2']).cumcount()\n",
        "\ndf['index_original'] = df.groupby(['col1','col2']).transform('idxmax')\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate.index\n    ",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = df.index[duplicate_bool == True].values\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].apply(lambda x: x.nlargest(1))\nresult = result.reset_index()\nresult = df[df['count'].isin(result['count'])]\n",
        "\nresult = df.query(\"Category in @filter_list\")\n",
        "\nresult = df.query(\"Category not in @filter_list\")\n",
        "\nvalue_vars = [(df.columns[0][i], df.columns[1][j], df.columns[2][k]) for i in range(len(df.columns[0])) for j in range(len(df.columns[1])) for k in range(len(df.columns[2]))]\nresult = pd.melt(df, value_vars=value_vars)\n",
        "\nresult = pd.melt(df, value_vars=list(zip(df.columns.levels[0], df.columns.levels[1], df.columns.levels[2])))\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\ndf['cummax'] = df.groupby('id')['val'].cummax()\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].apply(lambda x: x.cumsum().clip(lower=0))\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('r')['v'].sum(skipna=False)\n",
        "\nresult = df.groupby('l')['v'].sum(skipna=False)\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if df[col1].isin(df[col2]).all():\n                result.append(f\"{col1} {col2} one-to-many\")\n            elif df[col2].isin(df[col1]).all():\n                result.append(f\"{col1} {col2} many-to-one\")\n            elif df[col1].isin(df[col2]).any() and df[col2].isin(df[col1]).any():\n                result.append(f\"{col1} {col2} many-to-many\")\n            else:\n                result.append(f\"{col1} {col2} one-to-one\")\n",
        "\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            if len(df[col1].unique()) == len(df[col2].unique()):\n                result.append(f\"{col1} {col2} one-2-many\")\n            elif len(df[col1].unique()) == 1:\n                result.append(f\"{col1} {col2} one-2-one\")\n            else:\n                result.append(f\"{col1} {col2} many-2-many\")\n",
        "\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor i in range(len(df.columns)):\n    for j in range(len(df.columns)):\n        if i == j:\n            result.iloc[i, j] = 'NaN'\n        elif df.iloc[:, i].isin(df.iloc[:, j]).all():\n            result.iloc[i, j] = 'one-to-many'\n        elif df.iloc[:, j].isin(df.iloc[:, i]).all():\n            result.iloc[i, j] = 'many-to-one'\n        elif df.iloc[:, i].isin(df.iloc[:, j]).any() and df.iloc[:, j].isin(df.iloc[:, i]).any():\n            result.iloc[i, j] = 'many-to-many'\n        else:\n            result.iloc[i, j] = 'one-to-one'\n",
        "\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor i in range(len(df.columns)):\n    for j in range(len(df.columns)):\n        if i == j:\n            result.iloc[i, j] = 'NaN'\n        else:\n            if df.iloc[:, i].isin(df.iloc[:, j]).all():\n                result.iloc[i, j] = 'one-2-many'\n            elif df.iloc[:, j].isin(df.iloc[:, i]).all():\n                result.iloc[i, j] = 'many-2-one'\n            else:\n                result.iloc[i, j] = 'many-2-many'\n",
        "\n# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n",
        "\nresult = pd.to_numeric(s.str.replace(',',''), errors='coerce')\n",
        "\nresult = df.groupby(df['SibSp'] > 0 | df['Parch'] > 0).mean()\n",
        "\nresult = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0)).mean()\n",
        "\nresult = df.groupby([(df['SibSp'] == 1) & (df['Parch'] == 1),\n                     (df['SibSp'] == 0) & (df['Parch'] == 0),\n                     (df['SibSp'] == 0) & (df['Parch'] == 1),\n                     (df['SibSp'] == 1) & (df['Parch'] == 0)]).mean()\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\ndf.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.DataFrame({'mean': df.groupby('a').b.mean(), 'std': df.groupby('a').b.apply(stdMeann)})\n",
        "\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(df.groupby('b').a.apply(stdMeann))\n",
        "\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\ndf['min-max'] = df.groupby('a')['b'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "\nresult = df.loc[df.sum(axis=1) != 0, ['A', 'B', 'D']]\n",
        "\nresult = df[(df.sum(axis=1) != 0) & (df.sum(axis=0) != 0)]\n",
        "\nresult = df.loc[df.max(axis=1) <= 2]\n",
        "\nresult = df.where(df < 2, 0)\n",
        "\nresult = s.sort_values(ascending=True)\n",
        "\ndf = pd.DataFrame({'index': s.index, '1': s.values})\ndf = df.sort_values(by=['1', 'index'])\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp','Mt']).apply(lambda x: x[x['count'] == x['count'].max()])\n",
        "\nresult = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == x['count'].min()])\n",
        "\nresult = df.groupby(['Sp','Value'])['count'].apply(lambda x: x.nlargest(1))\nresult = result.reset_index()\nresult = df[df['count'].isin(result['count'])]\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna(df['Member'])\n",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\n",
        "\n    result = df.set_index('Member').fillna(df['Member']).reset_index()\n    result['Date'] = result['Member'].map(dict)\n    ",
        "\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926').apply(lambda x: pd.to_datetime(x).strftime('%d-%b-%Y'))\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\ndf1['Count_m'] = df1.groupby('year')['count'].transform('sum')\ndf1['Count_y'] = df1.groupby('year')['count'].transform('sum')\ndf1 = df1.reset_index()\ndf1 = df1.merge(df, on=['year', 'month', 'Date'])\ndf1 = df1[['Date', 'Val', 'Count_d', 'Count_m', 'Count_y']]\nresult = df1\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\ndf1['Count_m'] = df1.groupby('year')['count'].transform('sum')\ndf1['Count_y'] = df1.groupby('year')['count'].transform('sum')\ndf1['Count_Val'] = df1.groupby(['year', 'month'])['count'].transform('sum')\ndf1 = df1.reset_index()\ndf1 = df1.merge(df, on=['year', 'month', 'Val'])\ndf1['Count_d'] = df1.groupby(['year', 'month', 'Val'])['count'].transform('sum')\ndf1 = df1.drop(columns=['count'])\ndf1 = df1.drop_duplicates()\ndf1 = df1[['Date', 'Val', 'Count_d', 'Count_m', 'Count_y', 'Count_Val']]\nresult = df1\n",
        "\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.map(df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).size())\ndf['Count_y'] = df.Date.map(df.groupby([df['Date'].dt.year.rename('year')]).size())\ndf['Count_w'] = df.Date.map(df.groupby([df['Date'].dt.weekday.rename('weekday')]).size())\ndf['Count_Val'] = df.Val.map(df.groupby('Val').size())\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n# result1: zero\nresult1 = df.groupby('Date').sum()\nresult1[result1 == 0] = 1\nresult1[result1 != 0] = 0\nresult1 = result1.groupby('Date').sum()\n# result2: non-zero\nresult2 = df.groupby('Date').sum()\nresult2[result2 != 0] = 1\nresult2[result2 == 0] = 0\nresult2 = result2.groupby('Date').sum()\nprint(result1)\nprint(result2)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n# result1: even\nresult1 = df.groupby('Date').apply(lambda x: (x % 2 == 0).sum())\n# result2: odd\nresult2 = df.groupby('Date').apply(lambda x: (x % 2 != 0).sum())\nprint(result1)\nprint(result2)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.sum)\nresult['E'] = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.max, 'E': np.min})\n",
        "\ndf = dd.from_pandas(df, npartitions=1)\nresult = df.assign(var2=df.var2.str.split(',')).explode('var2')\n",
        "\ndf = dd.from_pandas(df, npartitions=1)\nresult = df.var2.str.split(\",\", expand=True).stack().reset_index(drop=True)\nresult = result.to_frame().rename(columns={0: 'var2'})\nresult = result.join(df.var1.to_frame().rename(columns={'var1': 'var1'}))\nresult = result.reset_index(drop=True)\n",
        "\ndf = dd.from_pandas(df, npartitions=1)\nresult = df.var2.str.split('-', expand=True).stack().reset_index(drop=True)\nresult = result.to_frame().rename(columns={0: 'var2'})\nresult = result.join(df.var1.to_frame().rename(columns={'var1': 'var1'}))\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\ndf[\"new\"] = df.apply(count_special_char, axis = 0)\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\ndf[\"new\"] = df.apply(count_special_char, axis = 0)\n",
        "\ndf['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[6:]\n",
        "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n",
        "\ndf[['fips', 'medi', 'row']] = df['row'].str.split(expand=True)\n",
        "\ndf = df.replace(0, float('nan'))\ndf = df.cumsum() / df.notnull().cumsum()\n",
        "\n# Calculate cumulative average for each row from end to head\ndf['2001'] = df['2001'].cumsum() / df['2001'].notnull().cumsum()\ndf['2002'] = df['2002'].cumsum() / df['2002'].notnull().cumsum()\ndf['2003'] = df['2003'].cumsum() / df['2003'].notnull().cumsum()\ndf['2004'] = df['2004'].cumsum() / df['2004'].notnull().cumsum()\ndf['2005'] = df['2005'].cumsum() / df['2005'].notnull().cumsum()\ndf['2006'] = df['2006'].cumsum() / df['2006'].notnull().cumsum()\n",
        "\n    result = df.apply(lambda x: x.divide(x.ne(0).sum()).cumsum())\n    ",
        "\n# Calculate cumulative average for each row from end to head\ndf['2001'] = df['2001'].cumsum() / df['2001'].notnull().cumsum()\ndf['2002'] = df['2002'].cumsum() / df['2002'].notnull().cumsum()\ndf['2003'] = df['2003'].cumsum() / df['2003'].notnull().cumsum()\ndf['2004'] = df['2004'].cumsum() / df['2004'].notnull().cumsum()\ndf['2005'] = df['2005'].cumsum() / df['2005'].notnull().cumsum()\ndf['2006'] = df['2006'].cumsum() / df['2006'].notnull().cumsum()\n",
        "\ndf['Label'] = 0\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\ndf.loc[0, 'Label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\ndf.loc[0, 'label'] = 1\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else 0 if x == 0 else -1)\ndf.loc[0, 'label'] = 1\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n",
        "\ndf['Duration'] = df.departure_time.iloc[1:] - df.arrival_time.iloc[:-1]\n",
        "\ndf['Duration'] = df.departure_time.iloc[1:] - df.arrival_time.iloc[:-1]\n",
        "\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], format='%d-%b-%Y %H:%M:%S')\ndf['Duration'] = df.departure_time.iloc[i+1] - df.arrival_time.iloc[i]\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'one').sum())\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'] == 'two').sum())\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: (x['key2'].str.endswith('e')).sum())\n",
        "\nmax_result = df.index.max()\nmin_result = df.index.min()\n",
        "\nmode_result = df.index[df.value.mode().index[0]]\nmedian_result = df.index[df.value.median().index[0]]\n",
        "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n",
        "\nresult = df.groupby(\"item\", as_index=False)[\"diff\", \"otherstuff\"].apply(lambda x: x.loc[x[\"diff\"] == x[\"diff\"].min()])\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n",
        "\n    result = df['SOURCE_NAME'].str.split('_').str[-1]\n    ",
        "\n# Fill the first 50% of NaN values with '0'\ndf.loc[df['Column_x'].isnull() & (df.index < len(df)//2), 'Column_x'] = 0\n# Fill the last 50% of NaN values with '1'\ndf.loc[df['Column_x'].isnull() & (df.index >= len(df)//2), 'Column_x'] = 1\n",
        "\n# Fill the first 30% of NaN values with '0'\ndf.loc[df['Column_x'].isnull() & (df.index < len(df) * 0.3), 'Column_x'] = 0\n# Fill the middle 30% of NaN values with '0.5'\ndf.loc[df['Column_x'].isnull() & (df.index >= len(df) * 0.3) & (df.index < len(df) * 0.6), 'Column_x'] = 0.5\n# Fill the last 30% of NaN values with '1'\ndf.loc[df['Column_x'].isnull() & (df.index >= len(df) * 0.6), 'Column_x'] = 1\n",
        "\n# Count the number of NaN values in the column\nnum_nan = df['Column_x'].isnull().sum()\n# Calculate the number of zeros and ones to fill the NaN values\nnum_zeros = int(num_nan / 2)\nnum_ones = num_nan - num_zeros\n# Fill the NaN values with zeros and ones\ndf['Column_x'] = df['Column_x'].fillna(0)\ndf['Column_x'] = df['Column_x'].fillna(1, limit=num_ones)\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nresult = pd.concat([a, b], axis=1)\nresult = result.apply(lambda x: tuple(x), axis=1)\nresult = pd.DataFrame(result.values.tolist(), columns=['one', 'two'])\nprint(result)\n",
        "\nresult = pd.concat([a, b, c], axis=1).apply(lambda x: tuple(x), axis=1)\n",
        "\nresult = pd.concat([a, b], axis=1)\nresult = result.apply(lambda x: tuple(x), axis=1)\nresult = result.to_frame()\nresult.columns = ['one', 'two']\n",
        "\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack(fill_value=0)\n",
        "\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack(fill_value=0)\n",
        "\ngroups = df.groupby(['username', pd.cut(df.views, bins)])\nresult = groups.size().unstack(fill_value=0)\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\nresult = df['text'].str.cat(sep=', ')\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = df['text'].str.cat(sep=', ')\nprint(result)\n",
        "\nresult = df['text'].str.cat(sep='-')\n",
        "\ndf2 = df2.merge(df1[['id', 'city', 'district']], on='id', how='left')\n",
        "\ndf2 = df2.merge(df1[['id', 'city', 'district']], on='id', how='left')\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2['date'] = df2['date'].dt.strftime('%d-%b-%Y')\ndf2 = df2.sort_values(['id', 'date'])\nresult = pd.concat([df1, df2], axis=0)\n",
        "\ndf2 = df2.merge(df1[['id', 'city', 'district']], on='id', how='left')\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2 = df2.sort_values(['id', 'date'])\nresult = df2.groupby('id').apply(lambda x: x.reset_index(drop=True))\nresult = result.reset_index(drop=True)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['B'] = result['B_y'].fillna(result['B_x'])\nresult = result.drop('B_x', axis=1)\nresult = result.drop('B_y', axis=1)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['B'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(['B_x', 'B_y'], axis=1)\n",
        "\nresult = pd.merge(C, D, how='outer', on='A', suffixes=('_x', '_y'))\nresult['dulplicated'] = result['A'].isin(C['A']).astype(bool)\nresult = result.drop('B_x', axis=1)\nresult = result.rename(columns={'B_y': 'B'})\n",
        "\nresult = df.groupby('user').apply(lambda x: [[x['time'].iloc[i], x['amount'].iloc[i]] for i in range(len(x))])\n",
        "\nresult = df.groupby('user').apply(lambda x: list(zip(x['time'], x['amount']))).reset_index(name='amount-time-tuple')\n",
        "\nresult = df.groupby('user').apply(lambda x: list(zip(x['amount'], x['time']))).reset_index(name='amount-time-tuple')\n",
        "\ndf_concatenated = pd.DataFrame(series.values.tolist(), index=series.index)\n",
        "\ndf_concatenated = pd.DataFrame(series.values.tolist(), index=series.index, columns=range(len(series.values[0])))\ndf_concatenated.columns = ['0', '1', '2', '3']\ndf_concatenated.index.name = 'name'\n",
        "\nresult = [col for col in df.columns if s in col and s != col]\n",
        "\nresult = df.columns[df.columns.str.contains(s, case=False) & ~df.columns.str.contains(s, case=False, regex=True)]\n",
        "\nresult = []\nfor col in df.columns:\n    if s in col and s != col:\n        result.append(col)\n",
        "\nresult = pd.DataFrame(df['codes'].tolist(), index=df.index)\nresult.columns = ['code_' + str(i) for i in range(result.shape[1])]\nresult = result.fillna(pd.np.nan)\n",
        "\nresult = pd.DataFrame(df['codes'].tolist(), index=df.index)\nresult.columns = ['code_' + str(i+1) for i in range(result.shape[1])]\nresult = result.fillna(pd.np.nan)\n",
        "\nresult = pd.DataFrame(df['codes'].tolist(), index=df.index)\nresult.columns = ['code_1', 'code_2', 'code_3']\nresult = result.fillna(pd.np.nan)\n",
        "\nresult = [item for sublist in df['col1'] for item in sublist]\n",
        "\nresult = ','.join([str(x) for sublist in df['col1'] for x in sublist[::-1]])\n",
        "\nresult = ','.join([str(x) for y in df['col1'] for x in y])\n",
        "\ndf['Time'] = df['Time'].dt.floor('2min')\ndf = df.groupby('Time').mean()\n",
        "\ndf['Time'] = df['Time'].dt.floor('3T')\ndf = df.groupby('Time').sum()\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n",
        "\nresult = df.loc[filt.index, :]\n",
        "\nresult = df[df.index.get_level_values('a').isin(filt.index[filt])]\n",
        "\nresult = df.iloc[0].ne(df.iloc[8]).index[df.iloc[0].ne(df.iloc[8])].tolist()\n",
        "\nresult = df.iloc[0].equals(df.iloc[8])\n",
        "\nresult = []\nfor col in df.columns:\n    if not df.iloc[0][col] == df.iloc[8][col]:\n        result.append(col)\n",
        "\nresult = []\nfor i in range(len(df.columns)):\n    if df.iloc[0, i] != df.iloc[8, i]:\n        result.append((df.iloc[0, i], df.iloc[8, i]))\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n",
        "\nresult = df.stack().reset_index(drop=True)\n",
        "\nresult = df.stack().reset_index(drop=True)\n",
        "\ndf['dogs'] = df['dogs'].fillna(0).round(2)\n",
        "\ndf['dogs'] = df['dogs'].fillna(0).round(2)\ndf['cats'] = df['cats'].fillna(0).round(2)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n",
        "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n",
        "\nresult = df.sort_values(by=['time'], ascending=True)\n",
        "\nresult = df.sort_values(by=['VIM', 'time'], ascending=[True, False])\n",
        "\nresult = df.drop(df.loc[df.index.isin(['2020-02-17', '2020-02-18'])].index)\n",
        "\ndf['Day of the Week'] = df.index.day_name()\ndf['Date'] = df.index.strftime('%d-%b-%Y')\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\nresult = corr.loc[corr > 0.3]\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\nresult = corr.where(corr > 0.3)\nprint(result)\n",
        "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\n",
        "\ndf.rename(columns={df.columns[0]: 'Test'}, inplace=True)\n",
        "\n# Create a new column 'frequent' to store the most frequent value in each row\ndf['frequent'] = df.apply(lambda row: row.value_counts().index[0], axis=1)\n# Create a new column 'freq_count' to store the frequency count of the most frequent value in each row\ndf['freq_count'] = df.apply(lambda row: row.value_counts()[row.value_counts().index[0]], axis=1)\n",
        "\n# Create a new column 'frequent' to store the most frequent value in each row\ndf['frequent'] = df.apply(lambda row: row.value_counts().index[0], axis=1)\n# Create a new column 'freq_count' to store the frequency count of the most frequent value in each row\ndf['freq_count'] = df.apply(lambda row: row.value_counts()[row.value_counts().index[0]], axis=1)\n",
        "\ndf['frequent'] = df.apply(lambda row: row.value_counts().index[row.value_counts() == row.value_counts().max()].tolist(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n",
        "\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].agg({\"foo\": \"mean\", \"bar\": \"first\"})\n",
        "\ndf[\"bar\"] = df[\"bar\"].fillna(0)\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\n",
        "\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'a_col']]\n",
        "\nresult = pd.merge(df_a, df_b, on='EntityNum', suffixes=('', '_b'))\nresult = result.drop('a_col_b', axis=1)\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\nx = np.nan_to_num(x)\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nimport numpy as np\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nresult = x[~np.isnan(x)]\nresult = result.tolist()\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.eye(np.max(a) + 1)[a]\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.eye(np.max(a) + 1)[a]\nprint(b)\n",
        "\nimport numpy as np\na = np.array([-1, 0, 3])\nb = np.eye(a.max() - a.min() + 1, dtype=int)[a - a.min()]\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1.5, -0.4, 1.3])\nb = np.zeros((len(a), len(a)))\nb[np.arange(len(a)), np.argsort(a)] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([[1,0,3], [2,4,1]])\nb = np.zeros((a.shape[0], a.shape[1], a.max()+1), dtype=np.int8)\nb[np.arange(a.shape[0]), np.arange(a.shape[1]), a.flatten()] = 1\nb = b.reshape(a.shape[0], -1)\nprint(b)\n",
        "\nresult = np.percentile(a, p)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nncol = 2\nB = np.reshape(A, (-1, ncol))\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nnrow = 3\nB = np.reshape(A, (nrow, -1))\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (-1, ncol))\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (-1, ncol))\n",
        "\nresult = np.roll(a, shift)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nresult = np.roll(a, shift, axis=1)\n",
        "\nnp.random.seed(0)\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmin(a), a.shape)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    result = np.unravel_index(a.argmax(), a.shape)\n    ",
        "\nresult = np.unravel_index(np.argpartition(a.ravel(), -2)[-2], a.shape)\n",
        "\nz = np.isnan(a)\na = np.delete(a, np.where(z.any(axis=0))[0], axis=1)\n",
        "\na = np.delete(a, np.where(np.isnan(a))[0], axis=0)\n",
        "\nimport numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \nresult = np.array(a)\nprint(result)\n",
        "\na = a[:, permutation]\n",
        "\nresult = a[permutation]\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nresult = np.unravel_index(a.argmin(), a.shape)\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nresult = np.unravel_index(a.argmax(), a.shape)\n",
        "\nimport numpy as np\na = np.array([[1, 0], [0, 2]])\nresult = np.unravel_index(a.argmin(), a.shape)\n",
        "\nimport numpy as np\ndegree = 90\nresult = np.sin(np.deg2rad(degree))\n",
        "\nimport numpy as np\ndegree = 90\nresult = np.cos(np.deg2rad(degree))\n",
        "\nimport numpy as np\nnumber = np.random.randint(0, 360)\nresult = 0\nif np.sin(np.deg2rad(number)) > np.sin(number):\n    result = 1\nprint(result)\n",
        "\nresult = np.degrees(np.arcsin(value))\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0, 0))\n",
        "\nresult = np.pad(A, (0, length - len(A)), 'constant', constant_values=(0, 0))\n",
        "\na = a**power\n",
        "\n    result = a**power\n    ",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\nresult = np.divmod(numerator, denominator)\nprint(result)\n",
        "\n    result = np.divmod(numerator, denominator)\n    ",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\nresult = (numerator, denominator)\nif denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // denominator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, np.maximum(b, c))\n",
        "\nresult = a[::-1,::-1]\n",
        "\nresult = a[::-1,::-1].diagonal()\n",
        "\nresult = np.fliplr(a).diagonal()\n",
        "\nresult = np.diag(a, k=1)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor row in X:\n    for element in row:\n        result.append(element)\nprint(result)\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor row in X:\n    for element in row:\n        result.append(element)\nprint(result)\n",
        "\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for row in X:\n        for element in row:\n            result.append(element)\n    return result\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i,j])\nprint(result)\n",
        "\nresult = np.array([int(x) for x in mystr])\n",
        "\nresult = np.cumsum(a[:, col] * multiply_number)\n",
        "\nresult = np.cumsum(a[row] * multiply_number)\n",
        "\nresult = np.multiply(a[row] / divide_number, a[row])\n",
        "\nresult = np.linalg.matrix_rank(a)\n",
        "\nresult = a.shape[0]\n",
        "\n# Calculate the mean and standard deviation of sample 1 and sample 2\nmean_a = np.mean(a)\nstd_a = np.std(a)\nmean_b = np.mean(b)\nstd_b = np.std(b)\n# Calculate the degrees of freedom\ndf = (std_a**2/len(a) + std_b**2/len(b))**2 / ((std_a**4/len(a)**2 + std_b**4/len(b)**2)/len(a) + (std_a**4/len(a)**2 + std_b**4/len(b)**2)/len(b))\n# Calculate the t-statistic\nt_stat = (mean_a - mean_b) / np.sqrt(std_a**2/len(a) + std_b**2/len(b))\n# Calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_stat), df))\n",
        "\n# Remove nans from a and b\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n# Calculate the t-statistic\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\n# calculate the degrees of freedom\ndf = anobs + bnobs - 2\n# calculate the t-statistic\nt_stat = (amean - bmean) / np.sqrt(avar/anobs + bvar/bnobs)\n# calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_stat), df))\n",
        "\noutput = np.setdiff1d(A, B)\n",
        "\noutput = np.concatenate((np.setdiff1d(A, B, axis=0), np.setdiff1d(B, A, axis=0)))\n",
        "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nresult = b[np.argsort(np.sum(a, axis=2))]\n",
        "\na = np.delete(a, 2, axis=1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0, 3], axis=1)\n",
        "\nresult = np.delete(a, del_col, axis=1)\n",
        "\nimport numpy as np\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\na = np.insert(a, pos, element)\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\npos = 1\nelement = [3,5]\na = np.insert(a, pos, element, axis=0)\nprint(a)\n",
        "\n    a = np.insert(a, pos, element)\n    ",
        "\na = np.insert(a, pos, element, axis=0)\n",
        "\nresult = np.array([arr.copy() for arr in array_of_arrays])\n",
        "\nresult = np.all(a[0] == a, axis=0)\n",
        "\nresult = np.all(a == a[0], axis=0)\n",
        "\n    result = np.all(a == a[0])\n    ",
        "\nresult = np.trapz(np.trapz(np.cos(x)**4 + np.sin(y)**2, x, axis=0), y)\n",
        "\n    result = np.cos(x)**4 + np.sin(y)**2\n    ",
        "\n# [Missing Code]\n",
        "\necdf = np.cumsum(grades) / np.sum(grades)\nresult = np.interp(eval, ecdf, grades)\n",
        "\nsorted_grades = np.sort(grades)\necdf = np.cumsum(np.ones_like(sorted_grades)) / len(sorted_grades)\nlow = sorted_grades[np.argmax(ecdf < threshold)]\nhigh = sorted_grades[np.argmax(ecdf >= threshold)]\n",
        "\nimport numpy as np\none_ratio = 0.9\nsize = 1000\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n",
        "\nimport torch\nimport numpy as np\na = torch.ones(5)\na_np = a.numpy()\n",
        "\nimport torch\nimport numpy as np\na = np.ones(5)\na_pt = torch.from_numpy(a)\nprint(a_pt)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = tf.ones([2,3,4])\na_np = a.numpy()\nprint(a_np)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nresult = np.argsort(a)[::-1][:N]\n",
        "\nresult = np.linalg.matrix_power(A, n)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = []\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\n",
        "\nresult = np.array([a[i:i+2, j:j+2] for i in range(0, a.shape[0], 2) for j in range(0, a.shape[1], 2)])\n",
        "\nresult = np.array([a[i:i+patch_size, j:j+patch_size] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])\n",
        "\nresult = np.reshape(a, (h, w))\n",
        "\nresult = []\nfor i in range(0, a.shape[0], patch_size):\n    for j in range(0, a.shape[1], patch_size):\n        patch = a[i:i+patch_size, j:j+patch_size]\n        result.append(patch)\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 5\nresult = a[:, low:high+1]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 0\nhigh = 2\nresult = a[low:high+1]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 10\nresult = a[:, low:high]\n",
        "\na = np.fromstring(string, dtype=float, sep=' ')\na = a.reshape(2, 2)\n",
        "\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\nresult = np.logspace(np.log10(min), np.log10(max), n)\n",
        "\nresult = np.random.loguniform(min, max, n)\n",
        "\n    result = np.logspace(np.log10(min), np.log10(max), n)\n    ",
        "\nB = pd.Series(np.zeros(len(A)))\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n",
        "\nB = pd.Series(index=A.index)\nB[0] = a * A[0]\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nresult = np.ravel_multi_index(index, dims)\n",
        "\nresult = np.ravel_multi_index(index, dims, order='C')\n",
        "\nimport numpy as np\nimport pandas as pd\nindex = ['x', 'y']\ncolumns = ['a','b','c']\nvalues = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df)\n",
        "\nresult = np.add.reduceat(a, np.where(accmap)[0])\n",
        "\nresult = np.unique(index, return_counts=True)[1]\nresult = np.max(a[index == np.unique(index)[0]])\n",
        "\nresult = np.add.reduceat(a, accmap)\n",
        "\nresult = np.unique(index, return_index=True)[1]\nresult = a[result]\n",
        "\nz = np.zeros_like(x)\nfor i in range(len(x)):\n    for j in range(len(x[i])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n",
        "\nimport numpy as np\nprobabilit = [0.333, 0.333, 0.333]\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\nresult = np.random.choice(lista_elegir, samples, p=probabilit)\nprint(result)\n",
        "\nimport numpy as np\na = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\nresult = np.pad(a, ((low_index, high_index), (low_index, high_index)), mode='constant', constant_values=0)\n",
        "\nimport numpy as np\nx = np.array([-2, -1.4, -1.1, 0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2])\nresult = x[x >= 0]\nprint(result)\n",
        "\nimport numpy as np\nx = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\nresult = x[np.isreal(x)]\nprint(result)\n",
        "\nbin_data = np.array_split(data, len(data)//bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nbin_data = np.array_split(data, len(data)//bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n",
        "\nbin_data = np.array_split(data, len(data[0])//bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n",
        "\nbin_data = np.array_split(data, data.shape[1] // bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nbin_data = np.array_split(data, data.shape[1] // bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\n",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\ndef smoothclamp(x):\n    if x < x_min:\n        return x_min\n    elif x > x_max:\n        return x_max\n    else:\n        return 3*x**2 - 2*x**3\nresult = smoothclamp(x)\nprint(result)\n",
        "\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\nN = 5\ndef smoothclamp(x, N=5):\n    if x < x_min:\n        return x_min\n    elif x > x_max:\n        return x_max\n    else:\n        return np.power(x, N) / np.power(x_max, N)\nresult = smoothclamp(x, N=N)\nprint(result)\n",
        "\nresult = np.correlate(a, np.roll(b, -1), mode='wrap')\n",
        "\nresult = df.unstack().values.reshape(4,15,5)\n",
        "\nresult = df.values.reshape(15,4,5)\n",
        "\nresult = np.unpackbits(np.uint8(a), axis=1)[:, :m]\n",
        "\nresult = np.zeros((len(a), m), dtype=np.int8)\nfor i, num in enumerate(a):\n    binary = np.unpackbits(np.uint8(num))\n    binary = binary[-m:]\n    result[i] = binary\n",
        "\nresult = np.zeros((len(a), m), dtype=np.bool)\nfor i, num in enumerate(a):\n    binary = np.unpackbits(np.uint8(num))\n    binary = binary[:m]\n    result[i] = binary\n",
        "\nmu = np.mean(a)\nstd = np.std(a)\nresult = (mu - 3*std, mu + 3*std)\n",
        "\nmu = np.mean(a)\nstd = np.std(a)\nresult = (mu - 2*std, mu + 2*std)\n",
        "\n    mu = np.mean(a)\n    sigma = np.std(a)\n    result = (mu - 3*sigma, mu + 3*sigma)\n    ",
        "\nmu = np.mean(a)\nstd = np.std(a)\nresult = (a < mu - 2*std) | (a > mu + 2*std)\n",
        "\nmasked_data = ma.masked_where(DataArray < 0, DataArray)\nprob = np.percentile(masked_data, percentile)\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n",
        "\na[1,:] = 0\na[:,0] = 0\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(len(a)), np.argmax(a, axis=1)] = True\nprint(mask)\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.zeros_like(a, dtype=bool)\nmask[np.argmin(a, axis=1)] = True\nprint(mask)\n",
        "\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = np.einsum('ij,ik->ijk', X, X.T)\nprint(result)\n",
        "\nX = np.zeros((Y.shape[1], Y.shape[2]))\nfor i in range(Y.shape[0]):\n    X += Y[i, :, :].dot(Y[i, :, :].T)\n",
        "\nis_contained = number in a\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.setdiff1d(A, B)\nprint(C)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.intersect1d(A, B)\nprint(C)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,4,8])\nC = A[np.logical_or(A >= B[0], A <= B[2])]\nprint(C)\n",
        "\nresult = rankdata(a, method='min')\nresult = np.where(result == 1, len(a), result)\nresult = np.where(result == len(a), 1, result)\nresult = result[::-1]\n",
        "\nresult = rankdata(a, method='dense')\nresult = np.argsort(result)[::-1]\n",
        "\n    result = rankdata(a, method='max')\n    result = result.astype(int)\n    result = result[::-1]\n    ",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\ndists = np.dstack((x_dists, y_dists))\n",
        "\nresult = a[:, second, third]\n",
        "\narr = np.zeros((20,10,10,2))\n",
        "\nl1 = X.sum(axis=1)\nresult = X/l1.reshape(5,1)\n",
        "\nresult = X / LA.norm(X, axis=1, ord=2)[:, None]\n",
        "\nresult = X / LA.norm(X, axis=1, ord=np.inf)[:, None]\n",
        "\nconditions = [df['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\n",
        "\nresult = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(len(a)):\n        if i == j:\n            result[i][j] = 0\n        else:\n            result[i][j] = np.linalg.norm(a[i] - a[j])\n",
        "\nfrom scipy.spatial.distance import pdist\nresult = pdist(a)\n",
        "\nresult = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(i+1, a.shape[0]):\n        result[i, j] = np.linalg.norm(a[i] - a[j])\n        result[j, i] = result[i, j]\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nNA = np.asarray([float(x) if x != 'np.inf' else np.inf for x in A])\n",
        "\nresult = np.unique(a[a != 0])\n",
        "\nresult = np.array([])\nfor i in range(1, len(a)):\n    if a[i] != a[i-1] and a[i] != 0:\n        result = np.append(result, a[i])\n",
        "\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n",
        "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ",
        "\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df[['lat', 'lon', 'val']].max(axis=1)\n",
        "\nresult = np.lib.stride_tricks.as_strided(a, shape=(a.shape[0]-size[0]+1, a.shape[1]-size[1]+1, size[0], size[1]), strides=(a.strides[0], a.strides[1], a.strides[0], a.strides[1]))\n",
        "\nresult = np.lib.stride_tricks.as_strided(a, shape=(a.shape[0]-size[0]+1, a.shape[1]-size[1]+1, size[0], size[1]), strides=(a.strides[0], a.strides[1], a.strides[0], a.strides[1]))\n",
        "\n# Remove the complex infinities from the array\na = a[np.isfinite(a)]\n# Compute the mean of the remaining finite values\nresult = np.mean(a)\n",
        "\n    result = np.mean(a.real)\n    ",
        "\nresult = Z[tuple([slice(None)] + [slice(-1, None)] * (Z.ndim - 1))]\n",
        "\nresult = a[-1:, ...]\n",
        "\nresult = any(np.array_equal(c, cnt) for cnt in CNTS)\n",
        "\nresult = any(np.array_equal(c, cnt) for cnt in CNTS)\n",
        "\nf = intp.interp2d(np.arange(0, 2, 1), np.arange(0, 2, 1), a)\nresult = f(x_new, y_new)\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].apply(lambda x: np.cumsum(x))\n",
        "\ni = np.diag(i)\n",
        "\na[np.triu_indices(a.shape[0], k=1)] = 0\n",
        "\nstart_dt = pd.to_datetime(start)\nend_dt = pd.to_datetime(end)\ndelta = end_dt - start_dt\nresult = np.linspace(start_dt, end_dt, n)\n",
        "\nresult = np.where((x == a) & (y == b))[0][0]\n",
        "\nresult = np.where(x == a)[0]\nresult = np.where(y[result] == b)[0]\n",
        "\nresult = np.polyfit(x, y, 2)\n",
        "\ncoeffs = np.polyfit(x, y, degree)\nresult = coeffs[::-1]\n",
        "\ndf = df.apply(lambda x: x - a[df.index.get_loc(x.name)], axis=1)\n",
        "\nresult = np.einsum('ijk,kl->ijl', A, B)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler(feature_range=(0, 1))\nresult = scaler.fit_transform(arr)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(-1, a.shape[-1]))\nresult = result.reshape(a.shape)\n",
        "\nmask = arr < -10\nmask2 = arr >= 15\narr[mask] = 0\narr[mask2] = 30\narr[~mask2] += 5\n",
        "\nfor i in range(len(n1)):\n    mask = arr < n1[i]\n    mask2 = arr >= n2[i]\n    mask3 = mask ^ mask2\n    arr[mask] = 0\n    arr[mask3] = arr[mask3] + 5\n    arr[~mask2] = 30\n",
        "\nresult = np.nonzero(np.abs(s1 - s2) > np.finfo(float).eps)[0].shape[0]\n",
        "\nresult = np.count_nonzero(np.isnan(s1) & np.isnan(s2))\n",
        "\nresult = all(np.array_equal(a[0], x) for x in a[1:])\n",
        "\nimport numpy as np\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\nresult = all(np.isnan(arr) for arr in a)\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=(0, 0))\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=(0, 0))\n",
        "\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n",
        "\n    result = np.zeros(shape)\n    result[:arr.shape[0], :arr.shape[1]] = arr\n    ",
        "\nresult = np.zeros(shape)\nresult[:a.shape[0], :a.shape[1]] = a\n",
        "\na = a.reshape(a.shape[0]/3,3)\n",
        "\nresult = np.take_along_axis(a, b, axis=2)\n",
        "\nresult = a[b == 1]\n",
        "\nresult = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]\n",
        "\nresult = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b], axis=2)\n",
        "\nresult = np.sum(a[b])\n",
        "\nresult = np.where(df['a'] > 1, df['b'], np.nan)\n",
        "\nresult = im[np.any(im, axis=1)]\nresult = result[:, np.any(result, axis=0)]\n",
        "\nresult = A[np.nonzero(A)]\n",
        "\nresult = np.where(im != 0, im, 0)\nresult = np.where(np.sum(result, axis=0) == 0, 0, result)\nresult = np.where(np.sum(result, axis=1) == 0, 0, result)\n",
        "\nresult = im[np.any(im, axis=1)]\nresult = result[:, np.any(result, axis=0)]\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label=\"x-y\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n",
        "\n# turn on minor ticks on y axis\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\nplt.minorticks_on()\n",
        "\n# turn on minor ticks on x axis\nplt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\n# create a list of different line styles\nline_styles = ['-', '--', '-.', ':']\n# loop through the list of line styles\nfor style in line_styles:\n    # generate random y values\n    y = np.random.rand(10)\n    \n    # plot the line with the current line style\n    plt.plot(x, y, style)\n",
        "\n# create a list of different line styles\nline_styles = ['-', '--', '-.', ':']\n# loop through the list of line styles\nfor style in line_styles:\n    # generate random y values\n    y = np.random.rand(10)\n    \n    # plot the line with the current line style\n    plt.plot(x, y, style)\n",
        "\nplt.plot(x, y, marker='D', markersize=10, linestyle='none')\nplt.show()\n",
        "\nplt.plot(x, y, marker='D', markersize=10, linewidth=2)\nplt.show()\n",
        "\nax.set_ylim(0, 40)\n",
        "\nplt.axvspan(2, 4, color='red', alpha=0.5)\n",
        "\nx = np.linspace(0, 1, 100)\ny = 2 * x\nplt.plot(x, y)\nplt.show()\n",
        "\n# create a figure and axis\nfig, ax = plt.subplots()\n# create a line segment from (0,0) to (1,2)\nline = np.array([[0, 0], [1, 2]])\n# plot the line segment\nax.plot(line[:, 0], line[:, 1])\n",
        "\n# make seaborn relation plot and color by the gender field of the dataframe df\nsns.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df)\n",
        "\nsns.set_style(\"darkgrid\")\nsns.scatterplot(x, y)\nplt.show()\n",
        "\n# create a pandas dataframe\ndf = pd.DataFrame({'x': x, 'y': y})\n# plot the line plot using seaborn\nsns.lineplot(data=df, x='x', y='y')\n# show the plot\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', linewidth=7)\n",
        "\nplt.legend(fontsize=20)\n",
        "\nplt.plot(x, y)\nplt.legend(title='xyz')\nplt.title('Title', fontsize=20)\n",
        "\n# set the face color of the markers to have an alpha (transparency) of 0.2\nl.set_facecolor((1, 1, 1, 0.2))\n",
        "\nl.set_markeredgecolor('black')\nl.set_markeredgewidth(2)\n",
        "\nl.set_color('r')\nl.set_markerfacecolor('r')\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(np.arange(0, 2 * np.pi, np.pi/2))\n",
        "\nplt.legend()\n",
        "\nplt.imshow(H, cmap='viridis')\nplt.colorbar()\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.xlabel(\"X\", ha=\"right\")\n",
        "\n# get the current x axis labels\nlabels = g.get_xticklabels()\n# rotate the labels by 90 degrees\nfor label in labels:\n    label.set_rotation(90)\n",
        "\n",
        "\nplt.gca().invert_yaxis()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nfig, ax = plt.subplots()\nax.scatter(x, y, c=z, cmap='viridis')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolor='black', facecolor='blue')\n",
        "\nplt.xticks(np.arange(10))\nplt.yticks(np.arange(0, 2.1, 0.5))\n",
        "\n# set the y axis tick labels to use comma as the thousands separator\nplt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, pos: '{:,}'.format(int(x))))\n",
        "\n# Plot a dashed line at y=1\nax.axhline(y=1, linestyle='--', color='red')\n",
        "\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\nax1.plot(x, y1)\nax1.set_title('Sine')\nax2.plot(x, y2)\nax2.set_title('Cosine')\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y1)\nax1.set_title('Sine')\nax1.set_xlabel('x')\nax1.set_ylabel('y1')\nax1.set_frame_on(False)\nax2.plot(x, y2)\nax2.set_title('Cosine')\nax2.set_xlabel('x')\nax2.set_ylabel('y2')\nax2.set_frame_on(False)\n",
        "\n# remove x axis label\nplt.xlabel('')\n",
        "\n# remove x tick labels\nplt.xticks([])\n",
        "\nplt.xticks([3, 4])\nplt.grid(axis='x', which='major')\n",
        "\nplt.yticks(np.arange(3, 6))\nplt.grid(axis='y', which='major', linestyle='--')\n",
        "\nplt.yticks([3, 4])\nplt.grid(axis='y', which='major', linestyle='--')\nplt.xticks([1, 2])\nplt.grid(axis='x', which='major', linestyle='--')\n",
        "\nplt.grid(True)\n",
        "\n# add legend to the plot\nplt.legend(loc='lower right')\n",
        "\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), constrained_layout=True)\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\nplt.show()\nplt.clf()\n",
        "\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n",
        "\n# Rotate the tick labels by 45 degrees\nplt.setp(ax.get_xticklabels(), rotation=45)\n# Move the x-axis to the top of the plot\nax.xaxis.tick_top()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xticks(x, x, rotation=45, horizontalalignment='right')\nplt.subplots_adjust(bottom=0.2)\n",
        "\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().invert_yaxis()\n",
        "\nplt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.yticks(np.arange(10), np.arange(10), rotation=0, va=\"center\")\nplt.gca().invert_yaxis()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"green\")\nplt.show()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\", color=\"green\")\n",
        "\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\n",
        "\n# Create a bar plot of s1 and s2\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\n# Set the x-axis tick labels to be horizontal\nplt.xticks(rotation=90)\n# Add a legend to the plot\nplt.legend()\n# Show the plot\nplt.show()\n",
        "\nplt.bar(df[\"celltype\"], df[\"s1\"])\nplt.bar(df[\"celltype\"], df[\"s2\"])\nplt.xticks(rotation=45)\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1 and s2\")\nplt.title(\"Bar Plot of s1 and s2\")\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.xticks(color=\"red\")\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xticks(x)\nplt.yticks(y)\nplt.axhline(color='r')\n",
        "\nplt.plot(x, y)\nplt.xticks(rotation=90, fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()\n",
        "\nx_values = [0.22058956, 0.33088437, 2.20589566]\nplt.plot(x_values, [0] * len(x_values), '|', color='black')\n",
        "\nplt.imshow(rand_mat, cmap='hot', interpolation='nearest')\nplt.xticks(range(len(xlabels)), xlabels, rotation=90)\nplt.yticks(range(len(ylabels)), ylabels[::-1])\nplt.colorbar()\nplt.show()\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\n# Add legend for all three curves in the two subplots\nhandles, labels = ax.get_legend_handles_labels()\nhandles2, labels2 = ax2.get_legend_handles_labels()\nax.legend(handles + handles2, labels + labels2, loc=0)\nplt.show()\nplt.clf()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title(\"Y\")\nax2.plot(x, y)\nax2.set_title(\"Y\")\nplt.show()\n",
        "\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, s=30)\n",
        "\nplt.scatter(a, b)\nfor i in range(len(a)):\n    plt.annotate(str(c[i]), (a[i], b[i]))\nplt.xlabel('a')\nplt.ylabel('b')\nplt.title('Scatter Plot of a over b')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"y over x\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"y over x\")\nplt.legend(title=\"Legend\")\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line chart and label the line \"y over x\"\n# Show legend of the plot and give the legend box a title  \"Legend\"\n# Bold the legend title\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", loc=\"best\")\nplt.title(\"y over x\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n",
        "\nplt.hist(x, edgecolor='black', linewidth=1.2)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\nplt.hist(x, bins=bins, alpha=0.5)\nplt.hist(y, bins=bins, alpha=0.5)\nplt.show()\n",
        "\nplt.hist([x, y], bins=10, histtype='stepfilled', alpha=0.5, label=['x', 'y'])\nplt.legend()\nplt.show()\n",
        "\n# calculate the slope of the line\nslope = (d - b) / (c - a)\n# calculate the intercept of the line\nintercept = b - slope * a\n# create a line equation\nline_equation = lambda x: slope * x + intercept\n# set the xlim and ylim to be between 0 and 5\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n# plot the line\nplt.plot([0, 5], [line_equation(0), line_equation(5)])\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nim1 = ax1.imshow(x, cmap='viridis')\nim2 = ax2.imshow(y, cmap='plasma')\ncbar = fig.colorbar(im1, ax=ax1)\ncbar.ax.set_ylabel('x')\ncbar = fig.colorbar(im2, ax=ax2)\ncbar.ax.set_ylabel('y')\nax1.set_title('x')\nax2.set_title('y')\nplt.show()\n",
        "\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title('Y')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax2.plot(a, z)\nax2.set_title('Z')\nax2.set_xlabel('A')\nax2.set_ylabel('Z')\nplt.suptitle('Y and Z')\n",
        "\nx = [point[0] for point in points]\ny = [point[1] for point in points]\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Plot of y over x\", fontsize=20)\nplt.xlabel(\"x\", fontsize=18)\nplt.ylabel(\"y\", fontsize=16)\nplt.show()\n",
        "\nax.plot(x, y)\nax.set_xticks(np.arange(1, 11))\nax.set_yticks(np.arange(1, 11))\n",
        "\n# Create a figure and axis\nfig, ax = plt.subplots()\n# Loop through each line segment and color it according to the corresponding color in c\nfor i in range(len(lines)):\n    x = [line[0][0] for line in lines[i]]\n    y = [line[0][1] for line in lines[i]]\n    ax.plot(x, y, color=c[i])\n# Show the plot\nplt.show()\n",
        "\nplt.plot(x, y, 'o')\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([1, 10, 100])\nplt.yticks([1, 10, 100])\nplt.show()\n",
        "\n# create a figure and axis object\nfig, ax = plt.subplots()\n# plot the data in the dataframe\ndf.plot(ax=ax)\n# show the data points on the line plot\nax.plot(df.index, df.values, 'o')\n# set the x-axis label\nax.set_xlabel('Date')\n# set the y-axis label\nax.set_ylabel('Value')\n# set the title\nax.set_title('Cumulative Sum of Random Data')\n# show the plot\nplt.show()\n",
        "\n# Renormalize the data to sum up to 1\ndata = np.array(data) / np.sum(data)\n# Create a histogram of the data\nplt.hist(data, bins=len(data), edgecolor='black')\n# Format the y tick labels into percentage\nplt.yticks(np.arange(0, 1.1, 0.1), ['{:.0%}'.format(x) for x in np.arange(0, 1.1, 0.1)])\n# Set y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1), ['{:.0%}'.format(x) for x in np.arange(0, 1.1, 0.1)])\n",
        "\nplt.plot(x, y, marker='o', markersize=10, alpha=0.5)\nplt.show()\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y, label='y')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('y vs x')\nax2.plot(z, a, label='a')\nax2.set_xlabel('z')\nax2.set_ylabel('a')\nax2.set_title('a vs z')\nfig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\nax1.set_title(\"Bill Depth vs Bill Length\")\nax2.set_title(\"Flipper Length vs Bill Length\")\n",
        "\nax.set_xticklabels([\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\"])\n",
        "\nplt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n",
        "\nxticks = plt.xticks()[0]\nxticks = np.append(xticks, [2.1, 3, 7.6])\nplt.xticks(xticks)\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.yticks(rotation=-60)\nplt.xticks(rotation=0, va='top')\n",
        "\n# Get the current xtick labels\nxtick_labels = plt.gca().get_xticklabels()\n# Set the transparency of the xtick labels to 0.5\nfor label in xtick_labels:\n    label.set_alpha(0.5)\n",
        "\nplt.margins(x=0, y=0.1)\n",
        "\nplt.margins(x=0, y=0.1)\n",
        "\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(x, y)\naxs[0].set_title(\"Subplot 1\")\naxs[1].plot(x, y)\naxs[1].set_title(\"Subplot 2\")\nplt.suptitle(\"Figure\")\n",
        "\nplt.plot(df)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n",
        "\nplt.scatter(x, y, marker='v', hatch='/', s=100)\n",
        "\nplt.scatter(x, y, marker='o', edgecolors='none', hatch='//')\n",
        "\nplt.scatter(x, y, marker='*', hatch='/')\nplt.show()\n",
        "\nplt.scatter(x, y, s=100, marker='*', hatch='//')\n",
        "\nplt.imshow(data, cmap='hot', interpolation='nearest')\nplt.xlim(1, 5)\nplt.ylim(1, 4)\nplt.colorbar()\nplt.show()\n",
        "\nplt.stem(x, y, orientation='horizontal')\nplt.show()\n",
        "\n# Create a list of keys from the dictionary `d`\nkeys = list(d.keys())\n# Create a list of values from the dictionary `d`\nvalues = list(d.values())\n# Create a list of colors from the dictionary `c`\ncolors = [c[key] for key in keys]\n# Create a bar plot with the keys as x axis labels and the values as the bar heights\nplt.bar(keys, values, color=colors)\n# Show the plot\nplt.show()\n",
        "\nplt.plot([3, 3], [0, 1], color='black', label='cutoff')\nplt.legend()\n",
        "\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, polar=True)\nax.bar(labels, height, width=0.5)\nax.set_theta_zero_location(\"N\")\nax.set_theta_direction(-1)\nplt.show()\n",
        "\nplt.pie(data, labels=l, wedgeprops={'width': 0.4})\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.grid(True, linestyle='--', color='blue')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='--', color='gray')\nplt.grid(which='major', linestyle='-', color='white')\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n",
        "\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markerfacecolor='none', markeredgecolor='black')\nplt.show()\n",
        "\nplt.axvline(x=55, color=\"green\")\n",
        "\n# Create a list of x-axis values\nx_values = [1, 2, 3]\n# Create a list of y-axis values for blue bars\nblue_y_values = blue_bar\n# Create a list of y-axis values for orange bars\norange_y_values = orange_bar\n# Create a bar plot with blue bars\nplt.bar(x_values, blue_y_values, color='blue')\n# Create a bar plot with orange bars\nplt.bar(x_values, orange_y_values, color='orange')\n# Add labels to the x-axis and y-axis\nplt.xlabel('Bars')\nplt.ylabel('Height')\n# Add a title to the plot\nplt.title('Blue and Orange Bars')\n# Show the plot\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\na = np.arange(10)\n# Make two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2)\n# Plot y over x in the first subplot\nax1.plot(x, y, label='y')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Subplot 1')\n# Plot z over a in the second subplot\nax2.plot(a, z, label='z')\nax2.set_xlabel('a')\nax2.set_ylabel('z')\nax2.set_title('Subplot 2')\n# Label each line chart and put them into a single legend on the first subplot\nax1.legend()\nplt.show()\n",
        "\n# Create a scatter plot with x and y values\nplt.scatter(x, y, c=y, cmap='Spectral')\n# Set the x-axis label\nplt.xlabel('x')\n# Set the y-axis label\nplt.ylabel('y')\n# Set the title of the plot\nplt.title('Scatter Plot with Spectral Colormap')\n# Show the plot\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n",
        "\ng = sns.FacetGrid(df, col=\"species\", hue=\"sex\", sharey=False)\ng.map(sns.barplot, \"bill_length_mm\", \"sex\")\n",
        "\n# create a circle object\ncircle = plt.Circle((0.5, 0.5), 0.2, color='r', fill=False)\n# add the circle to the current axis\nplt.gca().add_artist(circle)\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\bf{\\phi}$', fontsize=16)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"best\", markerscale=0.1)\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\nplt.show()\n",
        "\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)\n",
        "\n# Add a legend to the plot\nplt.legend()\n# Show two markers on the line\nplt.plot(x, y, marker=\"o\", label=\"Markers\")\n",
        "\nplt.imshow(data, cmap='viridis')\nplt.colorbar()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.title(\"Figure 1\", fontweight='bold')\nplt.show()\n",
        "\nsns.pairplot(df, x_vars=[\"x\"], y_vars=[\"y\"], hue=\"id\", legend=False)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().invert_xaxis()\n",
        "\nplt.scatter(x, y)\nplt.gca().set_axisbelow(True)\nplt.gca().set_xlim(0, 10)\nplt.gca().set_ylim(0, 10)\n",
        "\nplt.scatter(x, y, c='r', edgecolor='k')\nplt.show()\n",
        "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor i in range(4):\n    axs[i//2, i%2].plot(x, y)\n",
        "\nplt.hist(x, bins=5, range=(0, 10), width=2)\nplt.xlabel('x')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.fill_between(x, y - error, y + error, alpha=0.2)\nplt.show()\n",
        "\n# add x=0 and y=0 lines to the plot\nplt.plot([0, 0], [-5, 5], color='white')\nplt.plot([-5, 5], [0, 0], color='white')\n",
        "\nax.errorbar(box_position, box_height, yerr=box_errors, color=c)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n# Plot y over x and z over a in two side-by-side subplots\n# Make \"Y\" the title of the first subplot and \"Z\" the title of the second subplot\n# Raise the title of the second subplot to be higher than the first one\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title(\"Y\", fontsize=16)\nax1.set_xlabel(\"X\")\nax1.set_ylabel(\"Y\")\nax2.plot(a, z)\nax2.set_title(\"Z\", fontsize=16)\nax2.set_xlabel(\"A\")\nax2.set_ylabel(\"Z\")\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# make 4 by 4 subplots with a figure size (5,5)\n# in each subplot, plot y over x and show axis tick labels\n# give enough spacing between subplots so the tick labels don't overlap\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(16):\n    axs[i//4, i%4].plot(x, y)\n    axs[i//4, i%4].set_xlabel('x')\n    axs[i//4, i%4].set_ylabel('y')\n    axs[i//4, i%4].tick_params(axis='both', which='major', labelsize=8)\n    axs[i//4, i%4].tick_params(axis='both', which='minor', labelsize=6)\nplt.tight_layout()\nplt.show()\n",
        "\nplt.matshow(d, cmap='viridis')\nplt.gcf().set_size_inches(8, 8)\nplt.show()\n",
        "\ntable = plt.table(cellText=df.values, colLabels=df.columns, loc='center', bbox=[0, 0, 1, 1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(12)\n",
        "\nplt.plot(x, y)\nplt.xticks(x, x, rotation=45)\nplt.yticks(y, y)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().xaxis.set_label_position('top')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(x, x, rotation=45, ha='right')\nplt.yticks(y, y)\nplt.gca().tick_params(axis='x', which='both', bottom=True, top=True)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xticks(x, x)\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True)\nplt.show()\n",
        "\n# Create a new dataframe with only the relevant columns\ndf_plot = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n# Create a catplot with \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df_plot)\n# Change the subplots titles to \"Group: Fat\" and \"Group: No Fat\"\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\")\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Pulse vs Time by Diet and Kind\", y=0.95)",
        "\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\n",
        "\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\", height=3, aspect=1.5, ylabel=False)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nfig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\nplt.show()\n",
        "\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, legend=False)\n",
        "\n# Create a seaborn FacetGrid with rows in \"b\"\ng = sns.FacetGrid(df, row=\"b\")\n# Plot seaborn pointplots of \"c\" over \"a\" in each subplot\ng.map(sns.pointplot, \"a\", \"c\", markers=\"o\")\n# Set xticks of intervals of 1\ng.set_xticks(np.arange(1, 31, 1))\n# Set xtick labels with intervals of 2\ng.set_xticklabels(np.arange(1, 31, 2))\n# Show the plot\nplt.show()\n",
        "\nfrom mpl_toolkits.mplot3d import Axes3D\nx = np.random.random(10)\ny = np.random.random(10)\nz = np.random.random(10)\n# Make a 3D scatter plot of x,y,z\n# change the view of the plot to have 100 azimuth and 50 elevation\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(100, 50)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n",
        "\ngs = gridspec.GridSpec(nrow, ncol, wspace=0, hspace=0)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ",
        "\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n",
        "\ndef my_map_func(i):\n  return [[i, i+1, i+2]]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda input: tf.data.Dataset.from_tensor_slices(my_map_func(input)))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n",
        "\n    result = []\n    for i in input:\n        result.extend(my_map_func(i))\n    ",
        "\nresult = tf.cast(tf.sequence_mask(lengths, maxlen=8), dtype=tf.int32)\n",
        "\nresult = tf.sequence_mask(lengths, maxlen=8)\n",
        "\nresult = tf.sequence_mask(lengths, maxlen=8)\n",
        "\n    mask = tf.sequence_mask(lengths, maxlen=8)\n    ",
        "\nresult = tf.constant([[1.0] * (8 - l) + [0.0] * l for l in lengths], dtype=tf.float32)\n",
        "\nresult = tf.stack(tf.meshgrid(a, b), axis=-1).reshape(-1, 2)\n",
        "\n    result = tf.stack(tf.meshgrid(a, b), axis=-1).reshape(-1, 2)\n    ",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 1, 512))\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\nresult = tf.expand_dims(a, axis=2)\n",
        "\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nresult = tf.math.reciprocal(A)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\n",
        "\n    result = tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n    ",
        "\nimport tensorflow as tf\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\nresult = tf.gather_nd(x, tf.stack([y, z], axis=-1))\n",
        "\nimport tensorflow as tf\nx = [[1,2,3],[4,5,6]]\nrow = [0,0]\ncol = [1,2]\nx = tf.constant(x)\nrow = tf.constant(row)\ncol = tf.constant(col)\nresult = tf.gather_nd(x, tf.stack([row, col], axis=-1))\n",
        "\n    result = tf.gather_nd(x, tf.stack([y,z], axis=-1))\n    ",
        "\nC = tf.einsum('ijk,klm->ijl', A, B)\n",
        "\nC = tf.einsum('ijk,ikl->ijl', A, B)\n",
        "\nresult = [tf.strings.decode_utf8(i) for i in x]\n",
        "\n    result = tf.strings.unicode_decode(x, 'UTF-8')\n    ",
        "\nnon_zero_mask = tf.not_equal(x, 0)\nnon_zero_sum = tf.reduce_sum(tf.cast(non_zero_mask, tf.float32), axis=-1)\nresult = tf.reduce_sum(x, axis=-1) / non_zero_sum\n",
        "\nnon_zero_mask = tf.not_equal(x, 0)\nnon_zero_mask = tf.cast(non_zero_mask, tf.float32)\nnon_zero_mask = tf.expand_dims(non_zero_mask, axis=-1)\nnon_zero_mask = tf.tile(non_zero_mask, [1, 1, 1, 3])\nresult = tf.reduce_sum(x * non_zero_mask, axis=(1, 2)) / tf.reduce_sum(non_zero_mask, axis=(1, 2))\nresult = tf.expand_dims(result, axis=-1)\n",
        "\n    non_zero_mask = tf.not_equal(x, 0)\n    non_zero_sum = tf.reduce_sum(tf.cast(non_zero_mask, tf.float32), axis=-1)\n    result = tf.reduce_sum(x, axis=-1) / non_zero_sum\n    ",
        "\n# [Missing Code]\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    result = tf.argmax(a, axis=1)\n    ",
        "\nimport tensorflow as tf\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\nresult = tf.argmin(a, axis=1)\nprint(result)\n",
        "\ntf.saved_model.save(model, \"export/1\")\n",
        "\nresult = tf.random.uniform([10], minval=1, maxval=5, dtype=tf.int32, seed=seed_x)\n",
        "\nimport tensorflow as tf\nseed_x = 10\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform([114], minval=2, maxval=6, dtype=tf.int32)\n",
        "\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform([10], minval=1, maxval=5, dtype=tf.int32)\n    ",
        "\nresult = tf.__version__\n"
    ],
    "Scipy": [
        "\nresult = np.polyfit(x, np.log(y), 1)\n",
        "\nresult = np.polyfit(x, np.log(y), 1)\n",
        "\ndef func(x, A, B, C):\n    return A*np.exp(B*x) + C\nresult = scipy.optimize.curve_fit(func, x, y, p0=p0)\n",
        "\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\ntest_stat, p_value = stats.ks_2samp(x, y)\nprint(test_stat, p_value)\n",
        "\ntest_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n",
        "\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\nresult = optimize.minimize(lambda x: ((x[0]+x[1]-x[2])-2)**2 + ((3*x[0]-x[1]-x[2]))**2 + sin(x[1]) + cos(x[1]) + 4, initial_guess)\nprint(result.x)\n",
        "\nimport numpy as np\nimport scipy.stats\nz_scores = np.array([-3, -2, 0, 2, 2.5])\np_values = scipy.stats.norm.sf(z_scores)\n",
        "\np_values = scipy.stats.norm.sf(z_scores, loc=mu, scale=sigma)\n",
        "\nimport numpy as np\nimport scipy.stats\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\nz_scores = [scipy.stats.norm.ppf(p) for p in p_values]\nprint(z_scores)\n",
        "\ndist = stats.lognorm(s=stddev, scale=np.exp(mu))\nresult = dist.cdf(x)\n",
        "\nexpected_value = np.exp(mu + (stddev**2/2))\nmedian = np.exp(mu + stddev**2)\n",
        "\nresult = sa.dot(sb)\n",
        "\n    result = sA.dot(sB)\n    ",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n",
        "\n# Get the center of the image\ncenter = np.array(data_orig.shape) / 2\n# Rotate the point (x0,y0) around the center\nrot_matrix = np.array([[np.cos(np.deg2rad(angle)), -np.sin(np.deg2rad(angle))],\n                       [np.sin(np.deg2rad(angle)), np.cos(np.deg2rad(angle))]])\nrotated_point = np.dot(rot_matrix, np.array([x0-center[1], y0-center[0]])) + center\nxrot, yrot = rotated_point[1] + center[1], rotated_point[0] + center[0]\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\nresult = M.diagonal()\n",
        "\nresult = stats.kstest(times, 'uniform')\n",
        "\n    # [Missing Code]\n    ",
        "\nresult = stats.kstest(times, 'uniform', args=(0, T))\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = sparse.hstack((c1, c2))\n",
        "\nFeature = sparse.vstack([c1, c2])\n",
        "\n# Define the cost function\ndef cost_function(x):\n    # x is a vector of size N, where x[i] is the index of the point in points2 that corresponds to points1[i]\n    # Calculate the total euclidean distance between the pairs of points\n    total_distance = 0\n    for i in range(N):\n        total_distance += np.linalg.norm(points1[i] - points2[int(x[i])])\n    return total_distance\n# Use the scipy.optimize.minimize function to find the optimal assignment\nresult = scipy.optimize.minimize(cost_function, np.arange(N), method='Nelder-Mead')\n",
        "\ndef cost_function(x):\n    return np.sum(np.abs(points1[x] - points2))\nresult = scipy.optimize.minimize(cost_function, np.arange(N), method='Nelder-Mead')\n",
        "\nb.setdiag(0)\nb.eliminate_zeros()\n",
        "\nresult = ndimage.measurements.label(img > threshold)\n",
        "\nresult = ndimage.measurements.label(img < threshold)\n",
        "\n    # Find the connected components in the image\n    labeled_img, num_features = ndimage.label(img > threshold)\n    # Count the number of regions\n    result = num_features\n    ",
        "\n# Find the regions of cells which value exceeds a given threshold\nregions = np.zeros_like(img)\nregions[img > threshold] = 1\nregions = ndimage.label(regions)[0]\n# Determine the distance between the center of mass of such regions and the top left corner\nresult = []\nfor i in range(1, regions.max()+1):\n    region = (regions == i)\n    y, x = np.where(region)\n    center_y = np.mean(y)\n    center_x = np.mean(x)\n    distance = np.sqrt((center_y - 0)**2 + (center_x - 0)**2)\n    result.append(distance)\n",
        "\nM = M + M.T\n",
        "\n    sA = sA + sA.T\n    ",
        "\n# Remove isolated single cells\nsquare = scipy.ndimage.binary_erosion(square, structure=np.ones((3, 3)))\nsquare = scipy.ndimage.binary_dilation(square, structure=np.ones((3, 3)))\n",
        "\n# Remove isolated single cells\nsquare = scipy.ndimage.morphology.binary_opening(square, np.ones((3, 3)))\n",
        "\nmean = np.mean(col.data)\nstandard_deviation = np.std(col.data)\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\n# [Missing Code]\n",
        "\ndef fourier_series(x, a, tau):\n    series = np.zeros_like(x)\n    for i in range(1, degree+1):\n        series += a[i-1] * np.cos(i * np.pi / tau * x)\n    return series\npopt, pcov = curve_fit(fourier_series, z, Ua, p0=np.ones(degree))\n",
        "\n# Convert the array to a list of points\npoints = example_array.reshape(-1, 1)\n# Calculate pairwise Euclidean distances\nresult = scipy.spatial.distance.cdist(points, points, metric='euclidean')\n",
        "\n# Convert the array to a list of unique regions\nunique_regions = np.unique(example_array)\n# Calculate pairwise Manhattan distances\nresult = scipy.spatial.distance.cdist(unique_regions.reshape(-1, 1), unique_regions.reshape(-1, 1), metric='cityblock')\n",
        "\n    # Convert the array to a list of points\n    points = example_array.reshape(-1, 1)\n    # Calculate pairwise Euclidean distances\n    distances = scipy.spatial.distance.cdist(points, points, metric='euclidean')\n    ",
        "\ntck = interpolate.splrep(x, y, k = 2, s = 4)\nresult = interpolate.splev(x_val, tck, der = 0)\n",
        "\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\n",
        "\nresult = ss.anderson_ksamp([x1, x2])\n",
        "\ndef tau1(x):\n    y = np.array(df['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\ndf['AC'] = pd.rolling_apply(df['C'], 3, lambda x: tau1(x))\ndf['BC'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(df['C']))\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = sa.nnz == 0\n",
        "\nresult = block_diag(*a)\n",
        "\np_value = stats.ranksums(pre_course_scores, during_course_scores)[1]\n",
        "\n    ranksums_result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = ranksums_result.pvalue\n    ",
        "\nkurtosis_result = np.sum((a - np.mean(a))**4) / np.sum((a - np.mean(a))**2)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\n",
        "\nfrom scipy.interpolate import interp2d\nf = interp2d(s, t, z)\nresult = f(s, t)\n",
        "\n    f_interp = scipy.interpolate.interp2d(x, y, z)\n    result = f_interp(s, t)\n    ",
        "\nresult = []\nfor point in extraPoints:\n    region = vor.find_simplex(point)\n    result.append(region)\n",
        "\nresult = []\nfor point in extraPoints:\n    region = vor.find_simplex(point)\n    result.append(region)\n",
        "\nresult = sparse.coo_matrix(([1]*len(vectors), (np.arange(len(vectors)), np.arange(len(vectors)))))\nfor i, vector in enumerate(vectors):\n    result = sparse.vstack([result, sparse.coo_matrix(([1], (i, i)), shape=(len(vectors)+1, len(vectors)+1))])\n    result = sparse.hstack([result, sparse.coo_matrix((vector, (i, np.arange(len(vector)))), shape=(len(vectors)+1, max_vector_size))])\n",
        "\nb = scipy.ndimage.median_filter(a, 3, origin=(1, 0))\n",
        "\nresult = M[row, column]\n",
        "\nresult = M[row, column].toarray().flatten()\n",
        "\nnew_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[:, i, j])\n        new_array[:, i, j] = f(x_new)\n",
        "\nprob = NormalDistro(u, o2, x)\n",
        "\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\ndef f(x = 2.5, u = 1, o2 = 3):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner[0]\n    P = P_inner[0] + P_outer/2\n    return P\n",
        "\nresult = sf.dct(np.eye(N), norm='ortho')\n",
        "\nresult = sparse.diags(matrix.ravel(), [-1,0,1], (5, 5)).toarray()\n",
        "\nM = np.zeros((N+1, N+1))\nfor i in range(N+1):\n   for j in range(i+1):\n      M[i,j] = scipy.stats.binom.pmf(j, i, p)\n",
        "\nresult = df.apply(lambda x: stats.zscore(x), axis=0)\n",
        "\nresult = df.apply(lambda x: stats.zscore(x))\n",
        "\nresult = pd.DataFrame(index=df.index)\nfor col in df.columns:\n    zscore = stats.zscore(df[col])\n    result[col] = df[col]\n    result['zscore_' + col] = zscore\n",
        "\n# Calculate z-scores for each column\nzscores = df.apply(lambda x: stats.zscore(x))\n# Round z-scores to 3 decimal places\nzscores = zscores.round(3)\n# Create a new dataframe with z-scores and original data\nresult = pd.concat([df, zscores], axis=1)\nresult.columns = ['data', 'zscore']\n# Rename index column\nresult.index.name = 'probegenes'\n# Round data to 3 decimal places\nresult['data'] = result['data'].round(3)\n",
        "\nresult = scipy.optimize.line_search(test_func,test_grad,starting_point,direction)\n",
        "\nmid = np.array([[shape[0]//2, shape[1]//2]])\nresult = distance.cdist(np.dstack((np.arange(shape[0]), np.arange(shape[1]))), mid)\n",
        "\nmid = np.array([[shape[0]//2, shape[1]//2]])\nresult = distance.cdist(np.dstack((np.arange(shape[0]), np.arange(shape[1]))), mid, metric='cityblock')\n",
        "\n    mid = np.array([[shape[0]/2, shape[1]/2]])\n    result = distance.cdist(np.indices(shape).reshape(2, -1).T, mid)\n    ",
        "\nresult = scipy.ndimage.zoom(x, (shape[0]/x.shape[0], shape[1]/x.shape[1]), order=1)\n",
        "\ndef objective(x):\n    return np.sum((a.dot(x ** 2) - y) ** 2)\nout = scipy.optimize.minimize(objective, x0)\n",
        "\ndef objective(x):\n    return np.sum((a.dot(x ** 2) - y) ** 2)\ndef constraint(x):\n    return x - x_lower_bounds\ncons = [{'type': 'ineq', 'fun': constraint}]\nout = scipy.optimize.minimize(objective, x0, method='L-BFGS-B', constraints=cons)\n",
        "\ndef dN1_dt_time_varying(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_time_varying, t_span=time_span, y0=[N0,])\n",
        "\ndef dN1_dt_time_varying(t, N1):\n    if 0 < t < 2*np.pi:\n        return -100 * N1 + t - np.sin(t)\n    else:\n        return -100 * N1 + 2*np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_time_varying, t_span=time_span, y0=[N0,])\n",
        "\ndef dN1_dt_sinusoid(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_sinusoid, t_span=time_span, y0=[N0,])\n",
        "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nresult = sparse.vstack((sa, sb))\n",
        "\nresult = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n",
        "\n    result = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    ",
        "\nV += x\n",
        "\nV = V + x\n",
        "\nV = V + x\nV = V + y\n",
        "\n# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    # get the column length\n    Len = math.sqrt(sum(List))\n    # normalize the column\n    sa[:,Col] = sa[:,Col] / Len\n",
        "\n# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    # get the column length\n    Len = math.sqrt(sum(List))\n    # normalize the column\n    sa[:,Col].data = np.divide(Column, Len)\n",
        "\na = np.where(a > 0, 1, 0)\n",
        "\na = np.where(a > 0, 1, 0)\n",
        "\nresult = []\nfor i in range(len(centroids)):\n    dists = scipy.spatial.distance.cdist(data, centroids[i].reshape(1, -1))\n    min_dist = np.min(dists)\n    min_ind = np.argmin(dists)\n    result.append(min_ind)\n",
        "\nresult = []\nfor i in range(len(centroids)):\n    dists = scipy.spatial.distance.cdist(data, centroids[i].reshape(1, -1))\n    min_dist = np.min(dists)\n    min_ind = np.argmin(dists)\n    result.append(data[min_ind])\n",
        "\n# Calculate the distance matrix between data and centroids\ndist_matrix = scipy.spatial.distance.cdist(data, centroids)\n# Find the indices of the k-closest elements to each centroid\nindices = np.argsort(dist_matrix, axis=1)[:, :k]\n# Extract the indices of the k-closest elements in the original data\nresult = np.take_along_axis(indices, np.arange(indices.shape[0])[:, None], axis=0)\n",
        "\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\nresult = np.zeros((4,))\nfor i in range(4):\n    result[i] = fsolve(lambda a: eqn(xdata[i], a, bdata[i]), x0=0.5)[0]\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\nresult = []\nfor i in range(len(xdata)):\n    a = adata[i]\n    x = xdata[i]\n    b = fsolve(lambda b: eqn(x, a, b), x0=0.5)\n    result.append([b[0], b[1]])\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# Calculate the theoretical distribution function\nx_range = np.linspace(range_start, range_end, 1000)\ntheoretical_dist = bekkers(x_range, estimated_a, estimated_m, estimated_d)\n# Calculate the KS test statistic and p-value\nks_stat, p_value = stats.ks_2samp(sample_data, theoretical_dist)\n# Check if the p-value is less than 0.05\nif p_value < 0.05:\n    result = True\nelse:\n    result = False\n",
        "\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(lambda x: integrate.trapz(x['A'], x.index))\n",
        "\ninterp = scipy.interpolate.LinearNDInterpolator(x, y)\nresult = interp(eval)\n",
        "\n# Calculate the total number of observations\nn = len(a['A1'])\n# Calculate the frequency of each category\nfreq = a['A1'].value_counts()\n# Calculate the log likelihood function\ndef log_likelihood(weights):\n    log_likelihood = 0\n    for i in range(len(freq)):\n        log_likelihood += freq[i] * np.log(weights[i])\n    return -log_likelihood\n# Use scipy.optimize.minimize to find the best parameters\nresult = sciopt.minimize(log_likelihood, np.ones(len(freq)))\nweights = result.x\n",
        "\npopt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\n",
        "\nresult = signal.argrelmax(arr, order=n)\n",
        "\nresult = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if arr[i][j] <= arr[i][max(0, j-n):min(arr.shape[1], j+n)+1].min() and arr[i][j] >= arr[i][max(0, j-n):min(arr.shape[1], j+n)+1].max():\n            result.append([i, j])\n",
        "\n# [Missing Code]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n    data1 = pd.DataFrame(data.data, columns=data.feature_names)\n    ",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\ndf_out = pd.concat([df[['Col1', 'Col2']], df_out], axis=1)\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\ndf_out = df_out.drop('Col3', axis=1)\ndf_out = df_out.join(df[['Col1', 'Col2']])\n",
        "\ndf_out = pd.get_dummies(df['Col4'], prefix='', prefix_sep='')\ndf_out = pd.concat([df[['Col1', 'Col2', 'Col3']], df_out], axis=1)\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\n",
        "\ndf_out = pd.get_dummies(df, columns=['Col3'])\n",
        "\nfrom sklearn.calibration import CalibratedClassifierCV\ncalibrated_svm = CalibratedClassifierCV(svmmodel, cv=5)\nproba = calibrated_svm.predict_proba(x_test)\n",
        "\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba = calibrated_model.predict_proba(x_predict)\n",
        "\n# Convert the transform_output to a pandas dataframe\ntransform_output = pd.DataFrame(transform_output.toarray())\n# Merge the transformed output with the original dataframe\ndf = pd.concat([df_origin, transform_output], axis=1)\n",
        "\ndf_transformed = pd.DataFrame.sparse.from_spmatrix(transform_output, columns=df_origin.columns)\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\n    # Convert the transform_output to a pandas dataframe\n    transform_output = pd.DataFrame(transform_output.toarray())\n    # Merge the transformed output with the original dataframe\n    result = pd.concat([df, transform_output], axis=1)\n    ",
        "\nsteps = clf.named_steps()\ndel steps['poly']\nclf.set_params(**steps)\n",
        "\nclf.steps.pop('reduce_poly')\n",
        "\nsteps = clf.named_steps()\ndel steps['pOly']\nclf.set_params(**steps)\n",
        "\nsteps = clf.named_steps()\nsteps['new_step'] = PolynomialFeatures()\nclf.set_params(**steps)\n",
        "\nclf.steps.insert(1, ('new_step', PolynomialFeatures()))\n",
        "\nsteps = clf.named_steps()\nsteps['t1919810'] = PCA()\nclf.steps = list(steps.values())\n",
        "\nparamGrid = {'max_depth': [3, 4, 5], 'learning_rate': [0.1, 0.01, 0.001]}\nverbose = 1\ncv = 5\nn_jobs = -1\niid = True\nmodel = xgb.XGBRegressor()\nGridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid, fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]}).fit(trainX,trainY)\n",
        "\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\ngridsearch.fit(trainX, trainY, **fit_params)\n",
        "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nmodel_name = type(model).__name__\n",
        "\nmodel_name = type(model).__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\nselect_out = pipe.named_steps['select'].fit_transform(data, target)\n",
        "\nclf = GridSearchCV(bc, param_grid, cv=5)\nclf.fit(X_train, y_train)\n",
        "\ny = y.reshape(-1, 1)\n",
        "\n# Reshape y data to match the shape of X data\ny = y.reshape(-1, 1)\n",
        "\ndef preprocess(s):\n    return s.upper()\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\ndef prePro(text):\n    return text.lower()\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns, index=data.index)\n",
        "\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ncoef = grid.best_estimator_.named_steps['model'].coef_\n",
        "\ncolumn_names = X.columns[clf.feature_importances_.argsort()[::-1]]\n",
        "\ncolumn_names = X.columns[clf.get_support()]\n",
        "\ncolumn_names = X.columns.values\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nselected_columns = X.columns[model.get_support()]\ncolumn_names = np.array(column_names)[model.get_support()]\n",
        "\ncolumn_names = X.columns[model.get_support()]\n",
        "\n# Find the p^th center\ncenter = km.cluster_centers_[p]\n# Calculate the distance between each sample and the p^th center\ndistances = np.linalg.norm(X - center, axis=1)\n# Sort the samples based on their distance to the p^th center\nsorted_samples = X[np.argsort(distances)]\n# Select the 50 samples closest to the p^th center\nclosest_50_samples = sorted_samples[:50]\n",
        "\n# Find the index of the p^th center\np_index = km.cluster_centers_.index(p)\n# Find the indices of the samples closest to the p^th center\nclosest_indices = np.argsort(np.linalg.norm(X - km.cluster_centers_[p_index], axis=1))[:50]\n# Get the 50 samples closest to the p^th center\nclosest_50_samples = X[closest_indices]\n",
        "\n# Find the p^th center\ncenter = km.cluster_centers_[p]\n# Calculate the distance between each sample and the p^th center\ndistances = np.linalg.norm(X - center, axis=1)\n# Sort the samples based on their distance to the p^th center\nsorted_indices = np.argsort(distances)\n# Select the 100 samples closest to the p^th center\nclosest_100_samples = X[sorted_indices[:100]]\n",
        "\n    # Get the p^th center\n    center = km.cluster_centers_[p]\n    # Calculate the distance between each sample and the center\n    distances = np.linalg.norm(X - center, axis=1)\n    # Sort the samples by distance to the center\n    sorted_indices = np.argsort(distances)\n    # Return the first 50 samples\n    samples = X[sorted_indices[:50]]\n    ",
        "\n# Convert categorical variable to matrix and merge back with original training data\nX_train = pd.get_dummies(X_train)\n",
        "\nX_train = pd.get_dummies(X_train)\n",
        "\nfrom sklearn.svm import SVR\n# fit, then predict X\nsvr = SVR(kernel='rbf')\nsvr.fit(X, y)\npredict = svr.predict(X)\n",
        "\n# create a gaussian kernel\nkernel = 1.0 * RBF(1.0)\n# create a support vector regression model\nsvr = SVR(kernel=kernel)\n# fit the model\nsvr.fit(X, y)\n# predict the output\npredict = svr.predict(X)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\n# fit, then predict X\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nsvr_rbf = SVR(kernel='poly')\nsvr_rbf.fit(X_poly, y)\npredict = svr_rbf.predict(X_poly)\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\n# fit, then predict X\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nsvr_rbf = SVR(kernel='poly')\nsvr_rbf.fit(X_poly, y)\npredict = svr_rbf.predict(X_poly)\n",
        "\ncosine_similarities_of_queries = []\nfor query in queries:\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = np.dot(query_tfidf, tfidf.transform(documents).T).toarray()\n    cosine_similarities_of_queries.append(cosine_similarities)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf, tfidf.idf_)\n",
        "\n    query_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_tfidf, tfidf.idf_).toarray()\n    ",
        "\nnew_features = np.zeros((len(features), len(set(sum(features, [])))))\nfor i, sample in enumerate(features):\n    for feature in sample:\n        new_features[i][list(set(sum(features, []))).index(feature)] = 1\n",
        "\nnew_f = np.zeros((len(f), len(set(sum(f, [])))))\nfor i, row in enumerate(f):\n    for feature in row:\n        new_f[i][list(set(sum(f, []))).index(feature)] = 1\n",
        "\nnew_features = np.zeros((len(features), len(set(sum(features, [])))))\nfor i, sample in enumerate(features):\n    for feature in sample:\n        new_features[i][list(set(sum(features, []))).index(feature)] = 1\n",
        "\n    new_features = np.zeros((len(features), len(set(sum(features, [])))))\n    for i, sample in enumerate(features):\n        for feature in sample:\n            new_features[i][list(set(sum(features, []))).index(feature)] = 1\n    ",
        "\nnew_features = np.zeros((len(features), len(set(sum(features, [])))))\nfor i, row in enumerate(features):\n    for feature in row:\n        new_features[i][list(set(sum(features, []))).index(feature)] = 1\n",
        "\n# Convert the distance matrix to a pandas dataframe\ndf = pd.DataFrame(data_matrix, index=['prof1', 'prof2', 'prof3'], columns=['prof1', 'prof2', 'prof3'])\n# Create a linkage matrix using the ward method\nlinkage_matrix = sklearn.cluster.ward_tree(df)\n# Perform hierarchical clustering using the linkage matrix\ncluster_labels = sklearn.cluster.fcluster(linkage_matrix, t=2, criterion='maxclust')\n",
        "\ncluster_labels = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='complete').fit_predict(data_matrix)\n",
        "\n# Convert the distance matrix to a condensed distance matrix\ncondensed_dist = sklearn.cluster.hierarchy.linkage(simM, method='ward')\n# Perform hierarchical clustering using the condensed distance matrix\ncluster_labels = sklearn.cluster.hierarchy.fcluster(condensed_dist, t=2, criterion='maxclust')\n",
        "\n# Perform hierarchical clustering on the data matrix\nZ = scipy.cluster.hierarchy.linkage(data_matrix, method='ward')\n# Find the optimal number of clusters using the elbow method\ndendrogram = scipy.cluster.hierarchy.dendrogram(Z)\n# Find the optimal number of clusters using the elbow method\nn_clusters = len(set(dendrogram['leaves']))\n# Perform hierarchical clustering on the data matrix with the optimal number of clusters\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, t=n_clusters, criterion='maxclust')\n",
        "\n# Perform hierarchical clustering using scipy.cluster.hierarchy\nZ = scipy.cluster.hierarchy.linkage(data_matrix, method='ward')\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, t=2, criterion='maxclust')\n",
        "\nZ = scipy.cluster.hierarchy.linkage(simM, method='ward')\ncluster_labels = scipy.cluster.hierarchy.fcluster(Z, t=2, criterion='maxclust')\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n",
        "\nbox_cox_data = PowerTransformer(method='box-cox').fit_transform(data)\n",
        "\nfrom sklearn.preprocessing import PowerTransformer\nbox_cox_data = PowerTransformer(method='box-cox').fit_transform(data)\n",
        "\nyeo_johnson_data = scipy.stats.yeojohnson(data)\n",
        "\nyeo_johnson_data = PowerTransformer(method='yeo-johnson').fit_transform(data)\n",
        "\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', stop_words='english')\ntransformed_text = vectorizer.fit_transform(text)\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the dataset into training and testing sets\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n# Split the training set into x and y\nx_train = x_train[:, :-1]\ny_train = y_train[:, -1]\n# Split the testing set into x and y\nx_test = x_test[:, :-1]\ny_test = y_test[:, -1]\n",
        "\n# Split the dataframe into training and testing sets\ntrain_size = int(len(data) * 0.8)\ntest_size = len(data) - train_size\ntrain_data, test_data = data[:train_size], data[train_size:]\n# Split the training set into x and y\nx_train = train_data.iloc[:, :-1]\ny_train = train_data.iloc[:, -1]\n# Split the testing set into x and y\nx_test = test_data.iloc[:, :-1]\ny_test = test_data.iloc[:, -1]\n",
        "\nfrom sklearn.model_selection import train_test_split\n# Split the dataset into training and testing sets\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n# Split the training set into x and y\nx_train = x_train[:, :-1]\ny_train = x_train[:, -1]\n# Split the testing set into x and y\nx_test = x_test[:, :-1]\ny_test = x_test[:, -1]\n",
        "\n    x = data.iloc[:, :-1].values\n    y = data.iloc[:, -1].values\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    ",
        "\nX = df['mse'].values.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\nX = X.reshape(-1, 1)\n",
        "\n# Use LinearSVC with penalty='l1' to perform feature selection\nmodel = LinearSVC(penalty='l1')\nmodel.fit(X, y)\nselected_feature_indices = model.coef_ != 0\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_feature_indices]\n",
        "\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n# Select the top 1000 features using SelectKBest\nselector = SelectKBest(f_classif, k=1000)\nX_selected = selector.fit_transform(X, y)\n# Get the selected feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.get_support()]\n",
        "\n    lsvc = LinearSVC(penalty='l1', dual=False).fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[lsvc.coef_ != 0]\n    ",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\n",
        "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ",
        "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array.reshape(-1, 1))\n",
        "\n    scaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a.reshape(-1, 1))\n    ",
        "\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict(b)\n",
        "\nnew_X = np.array(X).astype(str)\n",
        "\nnew_X = np.array(X).astype(str)\n",
        "\nnew_X = np.array(X).astype(str)\n",
        "\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n",
        "\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n",
        "\ntrain_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n",
        "\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n",
        "\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort([\"date\"])\n    test_dataframe = test_dataframe.sort([\"date\"])\n    ",
        "\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].transform(lambda x: scaler.fit_transform(x))\n",
        "\ncount = CountVectorizer(lowercase = False, token_pattern = r'(?u)\\b\\w+\\b')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n",
        "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\n",
        "\nimport pickle\nwith open('sklearn_model', 'wb') as f:\n    pickle.dump(fitted_model, f)\n",
        "\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = 1 - cosine_similarity(tfidf_matrix)\n"
    ],
    "Pytorch": [
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.001\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\noptim.param_groups[0]['lr'] = 0.0005\n",
        "\n# Convert gensim word2vec model to PyTorch embedding layer\nword2vec_weights = torch.tensor(word2vec.wv.vectors, dtype=torch.float32)\nembedding_layer = torch.nn.Embedding.from_pretrained(word2vec_weights)\n# Embed input data using the embedding layer\nembedded_input = embedding_layer(input_Tensor)\n",
        "\n    # Convert gensim word2vec model to PyTorch tensor\n    word2vec_tensor = torch.from_numpy(word2vec.wv.vectors)\n    # Create PyTorch embedding layer with the same size as the gensim model\n    embedding_layer = torch.nn.Embedding(word2vec.wv.vocab_size, word2vec.vector_size)\n    # Load the gensim word2vec model weights into the PyTorch embedding layer\n    embedding_layer.weight.data.copy_(word2vec_tensor)\n    ",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x.numpy())\n",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = load_data()\npx = pd.DataFrame(x.numpy())\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nC = B[:, A_log.type(torch.bool)]\n",
        "\nC = B[:, A_logical.type(torch.LongTensor)]\n",
        "\nC = B[:, A_log.bool()]\n",
        "\nC = B[:, A_log.bool()]\n",
        "\n    C = B[:, A_log.bool()]\n    ",
        "\nC = B[:, bool(A_log)]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nidx, B = load_data()\nC = B.index_select(1, idx)\nprint(C)\n",
        "\nx_tensor = torch.from_numpy(x_array)\n",
        "\nx_tensor = torch.from_numpy(x_array)\n",
        "\n    t = torch.from_numpy(a.astype(np.float32))\n    ",
        "\nmask = torch.zeros(len(lens), max(lens))\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmask = torch.zeros(len(lens), max(lens))\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nmask = torch.zeros(len(lens), max(lens))\nfor i, l in enumerate(lens):\n    mask[i, :l] = 1\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlens = load_data()\ndef get_mask(lens):\n    mask = torch.zeros(len(lens), max(lens))\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\nmask = get_mask(lens)\nprint(mask)\n",
        "\nTensor_3D = torch.diag(Tensor_2D)\n",
        "\n    diag_ele = torch.diag(t)\n    Matrix = torch.diag(diag_ele)\n    result = torch.stack([Matrix] * t.shape[0], dim=0)\n    ",
        "\nab = torch.cat((a, b), dim=0)\n",
        "\nab = torch.cat((a, b), dim=0)\n",
        "\n    ab = torch.cat((a, b), dim=0)\n    ",
        "\na[torch.arange(a.shape[0]), lengths:, :] = 0\n",
        "\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 2333\n",
        "\na[:, :lengths, :] = 0\n",
        "\nfor i in range(len(lengths)):\n    a[i, :lengths[i], :] = 2333\n",
        "\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx]\n",
        "\nresult = t[idx == 0, 1]\n",
        "\nresult = x.gather(1,ids.unsqueeze(-1).expand_as(x))\n",
        "\nresult = x.gather(1,ids.unsqueeze(-1).expand(-1, 114))\n",
        "\nresult = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1, -1, 2))\n",
        "\ny = torch.argmax(softmax_output, dim=1).unsqueeze(1)\n",
        "\ny = torch.argmax(softmax_output, dim=1)\n",
        "\ny = torch.argmin(softmax_output, dim=1)\ny = y.unsqueeze(1)\n",
        "\n    y = torch.argmax(softmax_output, dim=1).unsqueeze(1)\n    ",
        "\n    y = torch.argmin(softmax_output, dim=1)\n    ",
        "\n# [Missing Code]\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_equal = np.sum(A == B)\n",
        "\ncnt_not_equal = np.count_nonzero(A != B)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ndef Count(A, B):\n    cnt_equal = np.sum(A == B)\n    return cnt_equal\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ntensors_31 = []\nfor i in range(0, a.shape[3], chunk_dim):\n    tensor = a[:, :, :, i:i+chunk_dim, :]\n    tensors_31.append(tensor)\n",
        "\ntensors_31 = []\nfor i in range(0, a.shape[2], chunk_dim):\n    tensor = a[:, :, i:i+chunk_dim, :, :]\n    tensors_31.append(tensor)\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = sign_x * min\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_abs = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = sign_x * max_abs\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nx, y = load_data()\ndef solve(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = sign_x * min\n    return signed_min\nsigned_min = solve(x, y)\n",
        "\noutput = MyNet(input)\nconfidence_score = torch.nn.functional.softmax(output, dim=1)\n",
        "\nresult = torch.cat((a[:, :2], (a[:, -1] + b[:, 0])/2, b[:, 1:]), dim=1)\n",
        "\n    result = torch.cat((a[:, :2], (a[:, -1] + b[:, 0])/2, b[:, 1:]), dim=1)\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(8).reshape(1,4,2).float()\nprint(t)\nnew=torch.tensor([[0., 0., 0.,0.]])\nprint(new)\nr = torch.cat([t,new], dim=0)\nprint(r)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(4).reshape(1,2,2).float()\nprint(t)\nnew=torch.tensor([[0., 0., 0.,0.]])\nprint(new)\nr = torch.cat([t,new], dim=0)\nprint(r)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nt = torch.arange(8).reshape(1,4,2).float()\nprint(t)\nnew=torch.tensor([[-1, -1, -1, -1,]])\nprint(new)\nresult = torch.cat([t,new.unsqueeze(0)], dim=0)\nprint(result)\n",
        "\nresult = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\n"
    ]
}