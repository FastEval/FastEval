{
    "Pandas": [
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf.loc[df.Qu1.duplicated(keep=False), 'Qu1'] = 'other'\ndf.loc[df.Qu2.duplicated(keep=False), 'Qu2'] = 'sausage'\ndf.loc[df.Qu3.duplicated(keep=False), 'Qu3'] = 'other'\n# [Missing Code]\n",
        "\ndf.loc[df.Qu1.duplicated(keep=False), 'Qu1'] = 'other'\ndf.loc[df.Qu2.duplicated(keep=False), 'Qu2'] = 'other'\ndf.loc[df.Qu3.duplicated(keep=False), 'Qu3'] = 'other'\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\ndf.loc[df['Qu1'].apply(lambda x: len(df[x]) >= 3), 'Qu1'] = 'other'\ndf.loc[df['Qu2'].apply(lambda x: len(df[x]) >= 2), 'Qu2'] = 'other'\ndf.loc[df['Qu3'].apply(lambda x: len(df[x]) >= 2), 'Qu3'] = 'other'\n# [Missing Code]\n",
        "\ndf.loc[df['Qu1'].apply(lambda x: len(df[x]) >= 3), 'Qu1'] = 'other'\ndf.loc[df['Qu2'].apply(lambda x: len(df[x]) >= 2), 'Qu2'] = 'banana'\ndf.loc[df['Qu3'].apply(lambda x: len(df[x]) >= 2), 'Qu3'] = 'sausage'\n# [Missing Code]\n",
        "import pandas as pd\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\ndf['url'] = df['url'].apply(lambda x: x.lower() if x != 'A.com' else x)\ndf = df.drop_duplicates(subset='url', keep='first')\ndf = df.drop('keep_if_dup', axis=1)\nprint(df)",
        "import pandas as pd\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'drop_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\ndf['url'] = df['url'].apply(lambda x: x.lower() if x.lower() != x else x)\ndf = df.drop_duplicates(subset='url', keep='first')\ndf = df.drop('drop_if_dup', axis=1)\nprint(df)",
        "import pandas as pd\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\ndf = df.dropna()\nprint(df)",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\nresult = df\nprint(result)\n",
        "\n    # [Missing Code]\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf['datetime'] = df['datetime'].dt.strftime('%d-%m-%Y %H:%M:%S')\nresult = df\nprint(result)\n",
        "import pandas as pd\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf['datetime'] = df['datetime'].dt.tz_localize('UTC')\ndf['datetime'] = df['datetime'].dt.tz_convert('US/Eastern')\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing Code]\n# [Missing",
        "\n# Extract the key value pairs from the message column\ndef extract_key_value_pairs(row):\n    d = dict(zip(row['message'].split(':')[::2], row['message'].split(':')[1::2]))\n    return d\ndf['job'] = df.apply(lambda x: extract_key_value_pairs(x), axis=1)\ndf['money'] = df.apply(lambda x: extract_key_value_pairs(x), axis=1)\ndf['wife'] = df.apply(lambda x: extract_key_value_pairs(x), axis=1)\ndf['group'] = df.apply(lambda x: extract_key_value_pairs(x), axis=1)\ndf['kids'] = df.apply(lambda x: extract_key_value_pairs(x), axis=1)\n",
        "\ndf['score'] = df['score'].apply(lambda x: x * (10 if x in products else 1))\n# [Missing Code]\n",
        "\nmask = (df['product'].isin(products))\ndf.loc[mask, 'score'] = df.loc[mask, 'score'] * 10\n# [Missing Code]\n",
        "\nmask = (df['product'].isin(products[0]) | df['product'].isin(products[1]))\ndf.loc[mask, 'score'] *= 10\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf['category'] = ['A', 'B', 'C', 'D']\ndf['category'] = df['category'].apply(lambda x: x[0])\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "        Date",
        "         Date",
        "         Date",
        "\ndf['#1'] = df['#1'].shift(1)\ndf = df.iloc[[0, 1, 2, 3, 4]]\n# [Missing Code]\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf = df.iloc[[0]].append(df)\n# [Missing Code]\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf['#2'] = df['#2'].shift(-1)\n# [Missing Code]\n",
        "\ndf['#1'] = df['#1'].shift(1)\ndf.iloc[[0, -1]] = df.iloc[[0, -1]].apply(pd.Series, axis=1)\ndf = df.dropna()\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\ndef mean_advance(df, row_list, column_list, axis=0):\n    result = df.loc[row_list, column_list].mean(axis=axis)\n    return result\nmean_advance(df, row_list, column_list, axis=0)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\ndef sumAdvance(df, row_list, column_list, axis=0):\n    result = df.loc[row_list, column_list].sum(axis=axis)\n    return result\nresult = sumAdvance(df, row_list, column_list, axis=0)\nprint(result)\n",
        "\nresult = sumAdvance(df, row_list, column_list, axis=0)\n# [Missing Code]\n",
        "\ndf['id'] = df['id'].astype('category').cat.codes\ndf['temp'] = df['temp'].astype('category').cat.codes\ndf['name'] = df['name'].astype('category').cat.codes\n# [Missing Code]\nresult = df.groupby(['id', 'temp', 'name']).size().reset_index(name='value_counts').rename(columns={0: 'count'})\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = df.isnull().sum(axis=0)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n# Count the values in each column\nresult = df.apply(lambda x: x.value_counts(), axis=0)\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1.883444,0.564835,0.144654]})\n# [Missing Code]\ndf = df.drop('Unnamed: 2', axis=1)\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1.883444,0.564835,0.144654]})\n# [Missing Code]\ndf = df.drop('Unnamed: 2', axis=1)\nprint(df)\n",
        "\ndf['0'] = df['0'].apply(lambda x: x.tolist() if pd.notnull(x) else [np.nan])\ndf['1'] = df['1'].apply(lambda x: x.tolist() if pd.notnull(x) else [np.nan])\ndf['2'] = df['2'].apply(lambda x: x.tolist() if pd.notnull(x) else [np.nan])\n# [Missing Code]\n",
        "\ndf.loc[df.isnull().any(axis=1), '2'] = np.nan\n# [Missing Code]\n",
        "\ndf.loc[df.isnull().any(axis=0), :] = np.nan\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n# Group by lab and value, then sum value\ndf['value'] = df.groupby('lab')['value'].sum()\n# Remove rows that are greater than the threshold\ndf = df[df['value'] <= thresh]\nprint(df)\n",
        "\nmask = df['value'] < thresh\ndf.loc[mask] = df.loc[mask].agg({'value': 'mean'})\ndf = df.drop(columns=['value'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nsection_left = 4\nsection_right = 38\n# Group by lab and calculate the mean\ngrouped = df.groupby(['lab'])\nresult = pd.DataFrame(index=df.index)\nresult['value'] = grouped['value'].mean()\n# Replace the rows that are not in the specified range with the calculated mean\nmask = (df['lab'] >= section_left) & (df['lab'] <= section_right)\nresult.loc[mask] = df.loc[mask]['value']\nprint(result)\n",
        "\n# Create the new columns based on the existing column names with a prefix\n",
        "\ndf['exp_A'] = df['A'].apply(lambda x: np.exp(x))\ndf['exp_B'] = df['B'].apply(lambda x: np.exp(x))\n# [Missing Code]\n",
        "\n# Create the new columns based on the existing column names with a prefix\n# [Missing Code]\n# Handle the case where the value is 0\nif df.loc[df['B'] == 0].count() > 0:\n    df.loc[df['B'] == 0, 'inv_B'] = 0\n",
        "\n# [Missing Code]\n",
        "\nmask = df.idxmin() > 0\ndf['idx'] = df.index[mask]\nresult = df.drop('idx', axis=1)\n# [Missing Code]\n",
        "\nmask = df.apply(lambda x: x.idxmin() == x.idxmax(), axis=1)\nmask = mask.astype(np.bool)\n# [Missing Code]\ndf['first_max_idx'] = df[mask].index\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\ndf = df.set_index('dt')\ndf['val'] = df['val'].reshape(-1, 1)\ndf = df.reset_index()\nprint(df)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\ndf = df.set_index('dt')\ndf['val'] = df['val'].fillna(0)\ndf = df.drop('val', axis=1)\nprint(df)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\ndf = df.set_index('dt')\ndf['val'] = df['val'].reshape(-1, 1)\ndf['val'] = df['val'].fillna(233)\ndf = df.drop('val', axis=1)\nprint(df)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = pd.DataFrame(df['dt'].apply(lambda x: pd.Timestamp(x) + pd.offsets.DateOffset(1)))\ndf['val'] = df.groupby('user')['val'].apply(lambda x: x.shift()).fillna(0)\nprint(df)\n",
        "\nimport pandas as pd\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\ndf['dt'] = df['dt'].dt.strftime('%d-%m-%Y')\ndf['val'] = df.groupby('user')['val'].max().reset_index(drop=True)\nprint(df)\n",
        "\ndf['id'] = df.index\ndf['name'] = df['id'].map(lambda x: f\"ID_{x}\")\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\ndf['a'] = df['name'].astype(str) + str(df['a'])\ndf = df.drop(['name', 'a'], axis=1)\nprint(df)\n",
        "\n    # [Missing Code]\n    ",
        "\ndf['ID'] = df.name.str.extract(r'(\\w+)', expand=False).astype(str)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['b','e']\n# Get the rows where 'c' is greater than 0.5\nresult = df.loc[df['c'] > 0.5]\n# Select only the desired columns\nresult = result.loc[:, columns]\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['a','b','e']\nmask = df['c'] > 0.45\ndf_masked = df[mask]\ndf_filtered = df_masked[columns]\nresult = df_filtered.to_numpy()\nprint(result)\n",
        "\n    mask = df['c'] > 0.5\n    cols = [df.columns.get_loc(_) for _ in columns]\n    # [Missing Code]\n    selected_df = df[mask][cols]\n    ",
        "\n    mask = df['c'] > 0.5\n    cols = [df.columns.get_loc(_) for _ in columns]\n    df[mask][cols] = df[mask][cols].sum(axis=1)\n    ",
        "\n    mask = df.c > 0.5\n    cols = [df.columns.get_loc(_) for _ in columns]\n    result = df[mask][cols]\n    ",
        "",
        "",
        "\nimport pandas as pd\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\nX = 17\n# Create a list of dates to filter out\nfilter_dates = []\nfor index, row in df.iterrows():\n    if index > 0:\n        for i in range(1, X + 1):\n            filter_dates.append((df.loc[index - 1, 'date'] + pd.offsets.DateOffset(months=i)))\n# Filter out rows that match any of the dates in the list\ndf = df[~df.index.isin(filter_dates)]\nprint(df)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n# Group by 3 and sum\ndf['sum_3_rows'] = df.groupby(df.index // 3)['col1'].sum()\n# Group by 2 and get average\ndf['avg_2_rows'] = df.groupby(df.index // 2)['col1'].mean()\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n# Group by 3 and sum\ndf['col1'] = df['col1'].reshape(-1, 3)\ndf['col1'] = df['col1'].sum(axis=1)\n# Group by 2 and average\ndf['col1'] = df['col1'].reshape(-1, 2)\ndf['col1'] = df['col1'].mean(axis=1)\nprint(df)\n",
        "\ndf.A = df.A.fillna(method='ffill', inplace=True)\n# [Missing Code]\n",
        "\ndf.A = df.A.fillna(df.A.shift())\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\ndf ['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf [ 'time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\n# [Missing Code]\nresult = df\nprint(result)",
        "\n    df['number'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\n    df ['time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\n    # [Missing Code]\n    result = df\n    ",
        "\ndf['time_day'] = df.duration.apply(lambda x: x.split()[-1]).astype(int)\ndf['number'] = df.duration.apply(lambda x: x.split()[0]).astype(int)\n# [Missing Code]\ndf ['numer'] = df.duration.replace(r'\\d.*' , r'\\d', regex=True, inplace = True)\ndf [ 'time']= df.duration.replace (r'\\.w.+',r'\\w.+', regex=True, inplace = True )\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "",
        "\n    # [Missing Code]\n    ",
        "\n    # [Missing Code]\n    ",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   '2000': [12, 1, 20, 0],\n                   '2001': [15, 3, 23, 1],\n                   '2002': [18, 2, 25, 2],\n                   '2003': [17, 5, 29, 2],\n                   '2004': [23, 7, 31, 3],\n                   '2005': [29, 5, 32, 3]})\ndf = df.melt(id_vars=[\"Country\", \"Variable\"],\n            value_name=\"value\",\n            var_name=\"year\")\nresult = df\nprint(result)\n",
        "\ndf = df.set_index(['Country', 'year'])\ndf = df.drop('2000', axis=1)\ndf = df.drop('2001', axis=1)\ndf = df.drop('2002', axis=1)\ndf = df.drop('2003', axis=1)\ndf = df.drop('2004', axis=1)\ndf = df.drop('2005', axis=1)\ndf = df.reset_index()\ndf.columns = ['year', 'var1', 'var2']\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n# Create a function to get the absolute values of the columns\ndef get_absolute_values(df, columns):\n    for col in columns:\n        df[col] = abs(df[col])\n    return df\n# Apply the function to the dataframe\ndf_filtered = get_absolute_values(df, df.columns.tolist())\n# Show the filtered dataframe\nresult = df_filtered\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# Find all the rows where &AMP; is present\nmask = df['A'].str.contains('&AMP;', na=False)\n# Replace the rows with &AMP; with '&''<''>'\ndf.loc[mask, 'A'] = df.loc[mask, 'A'].str.replace('&AMP;', '&''<''>')\n# Find all the rows where &LT; is present\nmask = df['A'].str.contains('&LT;', na=False)\n# Replace the rows with &LT; with '&''<''>'\ndf.loc[mask, 'A'] = df.loc[mask, 'A'].str.replace('&LT;', '&''<''>')\n# Find all the rows where &GT; is present\nmask = df['A'].str.contains('&GT;', na=False)\n# Replace the rows with &GT; with '&''<''>'\ndf.loc[mask, 'A'] = df.loc[mask, 'A'].str.replace('&GT;', '&''<''>')\n",
        "\n# [Missing Code]\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\ndf['first_name'], df['last_name'] = df['name'].str.split(expand=True)\ndf['first_name'] = df['first_name'].apply(validate_single_space_name)\ndf['last_name'] = df['last_name'].apply(lambda x: None if x == \"\" else x)\n",
        "",
        "",
        "\nmerged_df = df1.set_index('Timestamp').merge(df2.set_index('Timestamp'), on='Timestamp').drop('data', axis=1)\n# [Missing Code]\n",
        "\nmerged_df = pd.merge(df1, df2, on='Timestamp', how='outer')\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'datetime': ['2021-04-10 01:00:00', '2021-04-10 02:00:00', '2021-04-10 03:00:00', '2021-04-10 04:00:00', '2021-04-10 05:00:00'],\n                   'col1': [25, 25, 25, 50, 100],\n                   'col2': [50, 50, 100, 50, 100],\n                   'col3': [50, 50, 50, 100, 100]})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf['state'] = np.where((df['col2'] <= 50) & (df['col3'] <= 50), df['col1'],\n                        np.max([df['col1'], df['col2'], df['col3']]))\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'datetime': ['2021-04-10 01:00:00', '2021-04-10 02:00:00', '2021-04-10 03:00:00', '2021-04-10 04:00:00', '2021-04-10 05:00:00'],\n                   'col1': [25, 25, 25, 50, 100],\n                   'col2': [50, 50, 100, 50, 100],\n                   'col3': [50, 50, 50, 100, 100]})\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf['state'] = np.where(df['col2'] > 50 & df['col3'] > 50, df['col1'],\n                        df['col1'] + df['col2'] + df['col3'])\ndf = df.drop(['col2', 'col3'], axis=1)\ndf['datetime'] = pd.to_datetime(df['datetime'])\nresult = df\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    errors = []\n    for index, row in df.iterrows():\n        value = row['Field1']\n        if not isinstance(value, int):\n            errors.append(value)\n    # [Missing Code]\n    result = []\n    for value in errors:\n        result.append(value)\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport io\ndata = io.StringIO(\"\"\"\nrs  alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP7      A/T      0    7      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP15     C/A      0   15      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n\"\"\")\ndf = pd.read_csv(data, delim_whitespace=True).set_index('rs')\ntest = ['TP3', 'TP7', 'TP18']\n# [Missing Code]\ndf = df.loc[lambda x: x.name in test]\n# [Missing Code]\nresult = df.to_dict(orient='records')\nprint(result)\n",
        "\ndf = df.loc[test]\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    result = []\n    seen = set()\n    for r in test:\n        if r in seen:\n            continue\n        seen.add(r)\n        result.append(df.loc[r])\n    ",
        "\nimport pandas as pd\nimport numpy as np\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\ndef euclidean_distance(row):\n    car = row['car']\n    x = row['x']\n    y = row['y']\n    nearest_neighbour = df[df['car'] == car]['x'].values[0]\n    euclidean_distance = np.sqrt((nearest_neighbour - x) ** 2 + (y - nearest_neighbour) ** 2)\n    return euclidean_distance\ndf['euclidean_distance'] = df.apply(euclidean_distance, axis=1)\ndf['nearest_neighbour'] = df['euclidean_distance'].apply(lambda x: df[x == np.min(x)][['car', 'x']].values[0])\ndf = df.groupby('time').agg({'car': 'count', 'nearest_neighbour': 'first', 'euclidean_distance': 'min'}).reset_index()\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nfrom scipy.spatial.distance_metrics import euclidean\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# calculate pairwise distances between cars\ndf['euclidean_distance'] = df.apply(lambda x: euclidean(x['x'], x['x']), axis=1)\n# calculate farmost neighbor\ndf['farmost_neighbour'] = df.groupby('car')['car'].apply(lambda x: x.iloc[[-1]].car)\n# calculate average distance\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\nresult = df\nprint(result)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\n# [Missing Code]\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "",
        "\ndf['Quantity'] = df['Quantity'].apply(lambda x: 0 if x > 0 else x)\n# [Missing Code]\n",
        "\nduplicate['index_original'] = duplicate.index\n# [Missing Code]\n",
        "\ndf['index_original'] = duplicate.index\n# [Missing Code]\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    # [Missing Code]\n    # Add a column referring to the index of the first duplicate (the one kept)\n    duplicate['index_original'] = duplicate.index\n    ",
        "\ndf['index_original'] = df.index\n# [Missing Code]\n",
        "\ndf['index_original'] = df.index\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf.query(\"Category in @filter_list\")\n# [Missing Code]\n",
        "\ndf.query(\"Category!='{}'\".format(','.join(filter_list)))\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n# [Missing Code]\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n# [Missing Code]\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n# [Missing Code]\n",
        "\ndf['cummax'] = df.groupby('id').apply(lambda x: x['val'].cummax())\n# [Missing Code]\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\ndf['cumsum'] = df['cumsum'].apply(lambda x: 0 if x < 0 else x)\n# [Missing Code]\n",
        "\ndf['v'] = df['v'].replace([np.nan, -np.inf, np.inf], 0)\ndf['v'] = df['v'].astype(np.float64)\n# [Missing Code]\n",
        "\ndf['v'] = df['v'].replace([np.nan, -np.inf, np.inf], 0)\ndf['v'] = df['v'].astype(np.float64)\n# [Missing Code]\n",
        "\ndf['v'] = df['v'].fillna(0)\ndf['v'] = df['v'].astype(int)\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndef get_relationship_type(column1, column2):\n    if column1.isin(column2).any():\n        return 'many-to-many'\n    elif len(column1.isin(column2)) == 1:\n        return 'one-to-many'\n    elif len(column1.isin(column2)) == 0:\n        return 'one-to-one'\ndf['relationship_type'] = df.apply(lambda x: get_relationship_type(x['Column1'], x['Column2']), axis=1)\nresult = df[df['relationship_type'] != 'one-to-one']\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndef get_relationship_type(column1, column2):\n    if column1.isin(column2).any():\n        return 'many-2-many'\n    elif len(column1.isin(column2)) == 1:\n        return 'one-2-many'\n    elif len(column1.isin(column2)) == 0:\n        return 'one-2-one'\ndf['relationship_type'] = df.apply(lambda x: get_relationship_type(x['Column1'], x['Column2']), axis=1)\nresult = df[df['relationship_type'] != 'one-2-one']\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\ndf = df.dropna(how='all', axis=1)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n# Function to get relationship between columns\ndef get_relationship(df):\n    rel_df = pd.DataFrame(index=df.index, columns=['Type'])\n    for col in df.columns:\n        if col in ['Column1', 'Column2']:\n            continue\n        for i in range(1, len(df.columns)):\n            if col + '_' + df.columns[i] in df.columns:\n                rel_df.loc[i, 'Type'] = 'one-2-many'\n                break\n        rel_df.loc[i, 'Type'] = 'many-2-one'\n    return rel_df\ndf['Type'] = get_relationship(df)\nprint(df)\n",
        "\n# [Missing Code]\n",
        "50",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "    (A, a)   (A, b)   (B, a)   (B, b)",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nnp.random.seed(123)\nbirds = np.random.choice(['African Swallow', 'Dead Parrot', 'Exploding Penguin'], size=int(5e4))\nsomeTuple = np.unique(birds, return_counts=True)\nresult = pd.DataFrame(list(someTuple[0]), columns=['birdType'])\nresult['birdCount'] = list(someTuple[1])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n# Calculate mean and standard deviation separately for each group\nmean_std = lambda x: np.mean(x)\nstd_mean = lambda x: np.std(x)\n# Calculate mean and standard deviation for each group\nmean_values = df.groupby('a')['b'].mean().reset_index(name='mean')\nstd_devices = df.groupby('a')['b'].std().reset_index(name='std')\n# Merge mean and standard deviation values\nresult = pd.merge(mean_values, std_devices, on='a')\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.DataFrame({'mean':df.groupby('b').a.mean(), 'std':df.groupby('b').a.apply(stdMeann)})\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\ndf['softmax'] = df.groupby('a')['b'].apply(lambda x: np.softmax(x, axis=0))\ndf['min-max'] = df.groupby('a')['b'].apply(lambda x: np.min(x) - np.max(x))\nresult = df\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\ndf = df.drop(df.loc[df.sum(axis=1) == 0].index)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ns = s.reset_index(drop=True)\ns = s.sort_values(['value'])\ns = s.set_index('index')\n# [Missing Code]\n",
        "\ndef is_int_or_numeric(x):\n    try:\n        float(x)\n        return True\n    except ValueError:\n        return False\ndf_filtered = df[df['A'].apply(is_int_or_numeric)]\n# [Missing Code]\n",
        "\ndef select_records_where_A_is_string(df):\n    return df[df['A'].apply(type) == str]\n# [Missing Code]\nresult = select_records_where_A_is_string(df)\n",
        "\ngrouped = df.groupby(['Sp', 'Mt'])\n# [Missing Code]\nmax_count = grouped['count'].max()\n",
        "",
        "\n# [Missing Code]\n",
        "\ngrouped = df.groupby(['Sp','Value'])\n# [Missing Code]\nmax_count = grouped['count'].max()\n",
        "",
        "",
        "\n    # [Missing Code]\n    ",
        "",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count': 'sum'}).reset_index(name='Count_m')\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count': 'sum'}).reset_index(name='Count_y')\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count':lambda x: x.astype(int)})\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count':lambda x: x.astype(int)})\ndf['Count_Val'] = df.groupby(['Date','Val']).agg({'count':lambda x: x.astype(int)})\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count': 'sum'})\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count': 'sum'}).reset_index()\ndf['Count_w'] = df.groupby([df['Date'].dt.weekday(), df['Date'].dt.month.rename('month')]).agg({'count': 'sum'}).reset_index()\ndf['Count_Val'] = df.groupby(['Val', df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count': 'sum'}).reset_index()\nresult = df\nprint(result)\n",
        "Date            \nDate            ",
        "\ndf['B'] = df['B'].apply(lambda x: 1 if x % 2 == 0 else 0)\ndf['C'] = df['C'].apply(lambda x: 1 if x % 2 == 0 else 0)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nagg_func = {'D': np.sum, 'E': np.mean}\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=agg_func)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf['var2'] = df['var2'].str.split(',')\ndf['var2'] = df['var2'].apply(pd.Series)\n# [Missing Code]\ndf = df.drop('var2', axis=1)\n",
        "\ndf['var2'] = df['var2'].str.split(',')\ndf['var2'] = df['var2'].apply(pd.Series)\ndf = df.drop('var2', axis=1)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\ndf[\"new\"]=df.apply(count_special_char, axis = 0)\n# [Missing Code]\nresult = df\nprint(result)\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\ndf[\"new\"]=df.apply(count_special_char, axis = 0)\n# [Missing Code]\nresult = df\nprint(result)\n",
        "\ndf['fips'] = df['row'].str.split(' ', expand=True).iloc[:, 0]\ndf['row'] = df['row'].str.split(' ', expand=True).iloc[:, 1:]\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf['fips'] = df['row'].str.split(' ').str[0]\ndf['medi'] = df['row'].str.split(' ').str[1]\ndf['row'] = df['row'].str.split(' ').str[2:]\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\ndf['2001'] = df['2001'].apply(lambda x: x if x != 0 else None)\ndf['2002'] = df['2002'].apply(lambda x: x if x != 0 else None)\ndf['2003'] = df['2003'].apply(lambda x: x if x != 0 else None)\ndf['2004'] = df['2004'].apply(lambda x: x if x != 0 else None)\ndf['2005'] = df['2005'].apply(lambda x: x if x != 0 else None)\ndf['2006'] = df['2006'].apply(lambda x: x if x != 0 else None)\nresult = df\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n    mask = df.ne(0)\n    df.loc[:, mask] = df.loc[:, mask].div(df.loc[:, mask], axis=0)\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 0)\n# [Missing Code]\ndf['Label'] = (df['Close'] - df['Close'].shift(1) > 1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\ndf['label'] = 1\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else 0 if x < 0 else 1)\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\ndf['label'] = 0\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1)\ndf['label'] = df['label'].fillna(0)\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%m-%Y')\nresult = df\nprint(result)\n",
        "\ndf['duration'] = df.departure_time.diff(1).apply(lambda x: x.days)\n# [Missing Code]\n",
        "\ndf['duration'] = df.departure_time.diff(1).astype('timedelta64[ns]')\n# [Missing Code]\n",
        "\ndf['duration'] = df.departure_time.diff(1).astype('timedelta64[ns]')\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n# [Missing Code]\n",
        "\ndf['key2'] = df['key2'].map({'one': 1, 'two': 0}) # add this line to create a column with 1 for 'one' and 0 for 'two'\ndf['count'] = df.groupby(['key1'])['key2'].sum() # then group by key1 and sum the values\n# [Missing Code]\n",
        "\ndf['key2'] = df['key2'].map({'two': 1, 'one': 0}) # Modify the key2 column to be a binary indicator\n# [Missing Code]\ndf = df.groupby(['key1'])['key2'].sum().reset_index() # Group by key1 and sum the key2 column\n",
        "\ndf['key2'] = df['key2'].str.endswith('e')\n# [Missing Code]\nresult = df.groupby(['key1']).size().reset_index(name='count')\n",
        "\n# [Missing Code]\n",
        "\nmode_result, median_result = df.groupby(df.index.day).agg({'value': [np.mode, np.median]}).apply(lambda x: x[0][0])\n# [Missing Code]\n",
        "import pandas as pd\nimport numpy as np\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\ndf = df[(99 <= df['closing_price'] <= 101)]\nprint(df)",
        "       closing_price",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "     SOURCE_NAME",
        "\n    # [Missing Code]\n    ",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\ndf['Column_x'] = df['Column_x'].mask(df['Column_x'].isna(),\n                                     df['Column_x'].apply(lambda x: 0 if x < 0.5 else 1),\n                                     inplace= True)\nprint(df)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\n# Create a list of dataframes\ndataframes_list = [a, b]\n# Create the dataframe a_b\na_b = pd.DataFrame([tuple(x) for x in zip(a, b)], columns=['one', 'two'])\n# You can add more dataframes in the list\n# dataframes_list.append(your_dataframe_here)\n# And then create the dataframe a_b with the new dataframes\n# a_b = pd.DataFrame([tuple(x) for x in zip(dataframes_list[0], dataframes_list[1])], columns=['one', 'two'])\nprint(a_b)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8],[9, 10]]), columns=['one', 'two'])\n# Create a list of dataframes, including 'a' and 'b'\ndataframes_list = [a, b]\n# Fill in the missing values with np.nan\nmissing_values = [np.nan] * len(dataframes_list[0])\n# Create a list of tuples, combining corresponding elements from each dataframe\ntuples_list = [tuple(zip(dataframe.one, dataframe.two)) for dataframe in dataframes_list]\n# Create the final dataframe 'a_b'\na_b = pd.DataFrame(tuples_list, columns=['one', 'two'])\n# Add the remaining missing values with np.nan\nfor i in range(len(a_b)):\n    if len(a_b.loc[i]) < len(missing_values):\n        a_b.loc[i] = list(missing_values) + a_b.loc[i]\nprint(a_b)\n",
        "\n# Create a pivot table with counts\nresult = df.pivot_table(index='username', columns='views', values='post_id', fill_value=0)\n# [Missing Code]\n# Count the frequency of each bin\nresult['(1, 10]'] = result['(1, 10]'].astype(int)\nresult['(10, 25]'] = result['(10, 25]'].astype(int)\nresult['(25, 50]'] = result['(25, 50]'].astype(int)\nresult['(50, 100]'] = result['(50, 100]'].astype(int)\n",
        "\n# [Missing Code]\n",
        "\n# Create a pivot table with counts\nresult = df.pivot_table(index='username', columns='views', values='post_id', fill_value=0)\n# [Missing Code]\n# Count the frequency of each post_id in each bin\nresult['(1, 10]'] = result['(1, 10]'].astype(int)\nfor i in range(1, len(bins)):\n    result[f'({bins[i-1]}, {bins[i]})'] = result[f'({bins[i-1]}, {bins[i]})'].astype(int)\n",
        "\ndf['text'] = df['text'].apply(','.join)\n# [Missing Code]\n",
        "\ndf['text'] = df['text'].apply(lambda x: '-'.join(x))\n# [Missing Code]\n",
        "\ndf['text'] = df['text'].apply(','.join)\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\ndf['text'] = df['text'].apply(','.join)\nresult = df.drop('text', axis=1)\nprint(result)\n",
        "\ndf['text'] = df['text'].apply(lambda x: '-'.join(x))\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf1['date'] = pd.to_datetime(df1['date'])\ndf2['date'] = pd.to_datetime(df2['date'])\n# [Missing Code]\n",
        "\nimport pandas as pd\nC = pd.DataFrame({\"A\": [\"AB\", \"CD\", \"EF\"], \"B\": [1, 2, 3]})\nD = pd.DataFrame({\"A\": [\"CD\", \"GH\"], \"B\": [4, 5]})\n# Merge the two dataframes on 'A' and set the 'B' values to be overwritten\nresult = pd.merge(C, D, on='A', how='left').set_index('A').dropna()\nprint(result)\n",
        "\nmerged_df = pd.merge(C, D, on='A', how='left')\nmerged_df['B'] = merged_df['B'].fillna(merged_df['B_y'])\n# [Missing Code]\n",
        "\nmerged_df = pd.merge(C, D, how='outer', on='A')\nmerged_df['duplicated'] = merged_df['A'].duplicated()\nmerged_df.drop_duplicates(subset=['A'], keep='first', inplace=True)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf = pd.DataFrame(index=series.index)\ndf['0'] = series.apply(lambda x: x[0])\ndf['1'] = series.apply(lambda x: x[1])\ndf['2'] = series.apply(lambda x: x[2])\ndf['3'] = series.apply(lambda x: x[3])\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n# Find the columns that contain 'spike'\ncolumns = [c for c in df.columns if s in c]\n# Rename the columns\nfor i, col in enumerate(columns):\n    df.rename(columns={col: f'spike{i+1}'}, inplace=True)\n# Drop the original columns\ndf.drop(columns=['spike-2', 'hey spke', 'spiked-in'], inplace=True)\nprint(df)\n",
        "\ndf['codes'] = df['codes'].apply(pd.Series)\n# [Missing Code]\ndf['codes'] = df['codes'].fillna(0)\n",
        "\ndf['codes'] = df['codes'].apply(pd.Series)\n# [Missing Code]\ndf['codes'] = df['codes'].fillna(0)\n",
        "\n# [Missing Code]\n",
        "\nids = [tuple(x) for x in df.loc[0:len(df), 'col1'].values]\n# [Missing Code]\nresult = [item for sublist in ids for item in sublist]\n",
        "\nids = df.loc[0, 'col1'].values.tolist()\nfor i in range(1, len(df)):\n    ids.append(df.loc[i, 'col1'].values.tolist())\n# [Missing Code]\nresult = ', '.join([' and '.join(map(str, sublist)) for sublist in ids])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\nresult = ', '.join(['{}'.format(i) for i in df.loc[0, 'col1'].values.tolist()])\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Time': ['2015-04-24 06:38:49', '2015-04-24 06:39:19', '2015-04-24 06:43:49', '2015-04-24 06:44:18',\n                            '2015-04-24 06:44:48', '2015-04-24 06:45:18', '2015-04-24 06:47:48', '2015-04-24 06:48:18',\n                            '2015-04-24 06:50:48', '2015-04-24 06:51:18', '2015-04-24 06:51:48', '2015-04-24 06:52:18',\n                            '2015-04-24 06:52:48', '2015-04-24 06:53:48', '2015-04-24 06:55:18', '2015-04-24 07:00:47',\n                            '2015-04-24 07:01:17', '2015-04-24 07:01:47'],\n                   'Value': [0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075]})\ndf['Time'] = pd.to_datetime(df['Time'])\ndf['Time'] = df['Time'].dt.floor('2min')\ndf['Value'] = df['Value'].rolling('2min', min_periods=1).mean().reset_index(drop=True)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Time': ['2015-04-24 06:38:49', '2015-04-24 06:39:19', '2015-04-24 06:43:49', '2015-04-24 06:44:18',\n                            '2015-04-24 06:44:48', '2015-04-24 06:45:18', '2015-04-24 06:47:48', '2015-04-24 06:48:18',\n                            '2015-04-24 06:50:48', '2015-04-24 06:51:18', '2015-04-24 06:51:48', '2015-04-24 06:52:18',\n                            '2015-04-24 06:52:48', '2015-04-24 06:53:48', '2015-04-24 06:55:18', '2015-04-24 07:00:47',\n                            '2015-04-24 07:01:17', '2015-04-24 07:01:47'],\n                   'Value': [0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075,\n                             0.023844, 0.019075, 0.023844, 0.019075, 0.023844, 0.019075]})\ndf['Time'] = pd.to_datetime(df['Time'])\ndf['Time'] = df['Time'].dt.floor('3min')\ndf['Value'] = df['Value'].sum()\nresult = df\nprint(result)\n",
        "\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n# [Missing Code]\n",
        "",
        "15",
        "\ndf = df.loc[filt]\n# [Missing Code]\n",
        "\ndf = df.loc[filt]\n# [Missing Code]\n",
        "def equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\ndf_equal = df.eq(df.iloc[[0, 8]])\nresult = df.loc[~df_equal].index.tolist()\nprint(result)",
        "\n# [Missing Code]\nmask = df.isna()\nresult = mask[mask.any(axis=0)]\n",
        "def equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\nresult = []\nfor i in range(len(df)):\n    same = True\n    for j in range(len(df.columns)):\n        if i == 0:\n            same = equalp(df.loc[i, j], df.loc[8, j])\n        else:\n            same = equalp(df.loc[i-1, j], df.loc[i, j]) and same\n        if not same:\n            result.append(df.loc[i, j])\nprint(result)",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\ndef different_values(row, column):\n    values = df.loc[row, column].values\n    if isinstance(values, np.nditer.NDIterator):\n        values = list(values)\n    for i in range(len(values) - 1):\n        if values[i] != values[i + 1]:\n            yield values[i], values[i + 1]\n# [Missing Code]\nresult = [list(d) for d in different_values(0, i) for i in range(1, len(df.columns))]\nprint(result)\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n",
        "\ndf = df.reset_index(drop=True)\ndf.columns = ['A_1','B_1','C_1','D_1','E_1','A_2','B_2','C_2','D_2','E_2','A_3','B_3','C_3','D_3','E_3']\n# [Missing Code]\nresult = df\nprint(result)\n",
        "\ndf = df.melt(id_vars=False)\ndf.columns = ['A_0','B_0','C_0','D_0','E_0','A_1','B_1','C_1','D_1','E_1','A_2','B_2','C_2','D_2','E_2']\n# [Missing Code]\nresult = df\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndf['Sum'] = sum(list_of_my_columns)\n# [Missing Code]\n",
        "",
        "\ndf[list_of_my_columns] = df[list_of_my_columns].apply(lambda x: np.mean(x))\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898.33, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n# Delete rows with dates '2020-02-17' and '2020-02-18'\ndf = df[~df.index.isin(['2020-02-17', '2020-02-18'])]\nprint(df)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898.33, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 273],\n                   'OHLC Avg': [2898.44, 2898.81, 2898, 2898.31, 2898.62],\n                   'HLC Avg': [2898.33, 2898.67, 2897.75, 2898.33, 2898.75],\n                   'HL Avg': [2897.88, 2898.75, 2897.75, 2898.5, 2898.75],\n                   'Delta': [-146, 168, -162, -100, -100],\n                   'HiLodiff': [11, 8, 10, 6, 6],\n                   'OCdiff': [-2, 3, 2, 1, 1],\n                   'div_Bar_Delta': [1, 2, -1, -1, -1]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n# [Missing Code]\ndf = df.drop(df.index[df['Date'].apply(lambda x: x.weekday() == 5)])\n# [Missing Code]\ndf['Date'] = df['Date'].dt.strftime('%d-%m-%Y')\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\n# only want to pass all columns in\ncols = list(corr.columns)\nresult = corr[cols[cols >= 0.3]]\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\n# only want to pass all columns in\ncols = list(corr.columns)\nresult = corr[cols[cols >= 0.3]]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Create the frequent and freq_count columns\ndf['freq_count'] = 1\ndf['frequent'] = df.apply(lambda x: x['bit1'] + x['bit2'] + x['bit3'] + x['bit4'] + x['bit5'], axis=1)\n# [Missing Code]\n",
        "\n# Create the frequent and freq_count columns\ndf['freq_count'] = 1\ndf['frequent'] = df.apply(lambda x: x['bit1'] + x['bit2'] + x['bit3'] + x['bit4'] + x['bit5'], axis=1)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "",
        "",
        ""
    ],
    "Numpy": [
        "\n# Get the dimensions of an array\nresult = a.shape\n# [Missing Code]\n",
        "\nimport numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = x[~np.isnan(x)]\nprint(x)\n",
        "\nx[np.isnan(x)] = np.inf\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nb = np.zeros((a.shape[0], 1), dtype=np.uint8)\nb[np.arange(a.shape[0]), a] = 1\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nb = np.zeros((a.shape[0], 4), dtype=np.int32)\nb[np.arange(a.shape[0]), 0] = a\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.array([1,2,3,4,5])\np = 25\npercentile = np.percentile(a, p)\nprint(percentile)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef shift_cumulative_product(a, shift):\n    result = np.concatenate((a[shift:], a[:shift]))\n    return result\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef create_same_array():\n    np.random.seed(0)  # set the seed to 0 for reproducibility\n    r = np.random.randint(3, size=(100, 2000)) - 1\n    return r\nr_old = create_same_array()\nr_new = create_same_array()\n",
        "\nindices = np.argmax(a)\n# [Missing Code]\n",
        "\nindices = np.argmin(a)\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(a.argmax(), a.shape)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(a.argmax(), a.shape)\nprint(result)\n",
        "\n    indices = np.argmax(a)\n    # [Missing Code]\n    ",
        "\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.argmax(a)\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nind = np.argmin(a)\nresult = (ind[0], ind[1])\n# [Missing Code]\n",
        "\nind = np.argmax(a)\nresult = ind\n# [Missing Code]\n",
        "\nindices = []\nmin_values = []\nfor i in range(a.shape[0]):\n    min_value = np.min(a[i])\n    min_indices = np.unravel_index(min_value, a[i])\n    indices.append(min_indices)\n    min_values.append(min_value)\n# [Missing Code]\nresult = np.array(indices)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nif np.sin(number) > 0.5:\n    result = 0\nelse:\n    result = 1\n# [Missing Code]\nprint(result)\n",
        "\nangle = np.arcsin(value)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\nresult = (np.fractions.Fraction(numerator, denominator).numerator, np.fractions.Fraction(numerator, denominator).denominator)\nprint(result)\n",
        "\nimport numpy as np\ndef f(numerator=98, denominator=42):\n    result = np.fractions.Fraction(numerator, denominator)\n    return result\n",
        "\nimport numpy as np\nnumerator = 98\ndenominator = 42\nif denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // denominator, denominator)\nprint(result)\n",
        "\n# [Missing Code]\n",
        "import numpy as np\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = np.maximum(a, b)\nprint(result)",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n# [Missing Code]\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n# [Missing Code]\n",
        "\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i, j])\n    # [Missing Code]\n    return result\n",
        "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i, j])\n# [Missing Code]\n",
        "\nresult = []\nfor i in range(len(mystr)):\n    result.append(int(mystr[i]))\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\ncol = 2\nmultiply_number = 5.2\nresult = np.cumsum(a[:, col] * multiply_number)\nprint(result)\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\nresult = np.sum(a[row] * multiply_number)\nprint(result)\n",
        "\nfor i in range(5):\n    result += (a[row, i] / divide_number)\n# [Missing Code]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nfor i in range(a.shape[0]):\n    # Do something with the row\n    print(i)\n# [Missing Code]\nprint(result)\n",
        "\n# Calculate the means of the two samples\nmean_a = np.mean(a)\nmean_b = np.mean(b)\n# Calculate the standard deviations of the two samples\nstd_dev_a = np.std(a)\nstd_dev_b = np.std(b)\n# Calculate the pooled standard deviation\npooled_std_dev = (std_dev_a + std_dev_b) / 2\n# Calculate the t-statistic\nt_stat = (mean_b - mean_a) / pooled_std_dev\n# Calculate the p-value\np_value = scipy.stats.t_test(mean_a, mean_b, t_stat, degrees_of_freedom=len(a) + len(b) - 2).pvalue\n",
        "\n# Calculate the means and standard deviations of the two samples\nmean_a = np.mean(a)\nmean_b = np.mean(b)\nstd_dev_a = np.std(a)\nstd_dev_b = np.std(b)\n# [Missing Code]\n# Calculate the pooled standard deviation\npooled_std_dev = (std_dev_a / np.sqrt(len(a))) + (std_dev_b / np.sqrt(len(b)))\n",
        "",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Delete the 3rd column\na = a[:, :3]\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Delete the 1st and 3rd column\na = a[:, 1::3]\n# [Missing Code]\n",
        "\ndef_col = np.array([1, 2, 4, 5])\n# [Missing Code]\n",
        "\na[pos] = element\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\nresult = np.all([np.array_equal(a[0], a[i]) for i in xrange(1,len(a))])\nprint(result)\n",
        "\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\nresult = np.all(np.array_equal(a[0], a[1:]))\nprint(result)\n",
        "\n    # [Missing Code]\n    ",
        "\ndef integrate_2D(func, x, y, w=1):\n    result = 0\n    for i in range(len(x)):\n        for j in range(len(y)):\n            result += func(x[i], y[j]) * w\n    return result\n# [Missing Code]\nresult = integrate_2D((cos(x)**4 + sin(y)**2), x, y)\n",
        "\n    result = (np.sum(f(x, y) * np.power(weights, 1) for f, weights in zip(example_x, example_y)))\n    # [Missing Code]\n    ",
        "\n# calculate the cumulative distribution function\nresult = np.cumsum(grades)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Calculate the normalized cumulative distribution function\ndef ecdf(x):\n  x = x / np.sum(x)\n  return np.cumsum(x)\n# [Missing Code]\n# Find the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow, high = np.argmin(np.array([i for i in range(len(grades)) if ecdf(grades[i]) < threshold]))\nprint(low, high)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ntorch.tensor(a)\n# [Missing Code]\n",
        "\ndef convert_tensor_to_numpy(tensor):\n    return np.array(tensor)\n# [Missing Code]\na_np = convert_tensor_to_numpy(a)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n",
        "\n# [Missing Code]\n",
        "\nindex = np.argsort(a)\nresult = list(index)\n# [Missing Code]\n",
        "\nindices = np.argsort(a)[-N:][:N]\n# [Missing Code]\nresult = list(indices)\n",
        "\nresult = np.power(A, n)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef from_string(string):\n    data = string.replace(' ', '').replace('[', '').replace(']', '').replace(' ', '')\n    return np.array(eval(data))\n# [Missing Code]\n",
        "\nlog_min = min\nlog_max = max\nn_samples = n\n# [Missing Code]\nlog_uniform_samples = np.random.uniform(log_min, log_max, n_samples)\n",
        "\nlog_min = np.log(min)\nlog_max = np.log(max)\n# [Missing Code]\n",
        "\n    log_space = (max - min) / n\n    u = np.random.rand(n)\n    v = np.log(u)\n    result = (v + min) / log_space\n    ",
        "\nimport numpy as np\nimport pandas as pd\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nB = A.shift().fillna(0)\nB = a * B + b * A\nprint(B)\n",
        "\nB = []\nfor i in range(1, len(A)):\n    B.append(a*A[i-1] + b*B[i-2])\n# [Missing Code]\n",
        "\nresult = np.zeros((0,))\n# [Missing Code]\n",
        "\nresult = np.zeros((3,0))\n# [Missing Code]\n",
        "\nresult = np.unravel_index(index, dims)\n# [Missing Code]\n",
        "\nresult = np.unravel_index(index, dims)\n# [Missing Code]\n",
        "\nvalues = np.zeros((2,3), dtype='int32,float32')\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nacc_result = np.zeros_like(a)\nfor i in range(len(a)):\n    acc_result[i] = np.sum(a[accmap == i])\n# [Missing Code]\n",
        "\nmin_values = a[index]\nmin_indices = index[np.argmin(min_values)]\nresult = min_values[min_indices]\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nfor i in range(samples):\n    sample = np.random.choice(lista_elegir, p=probabilit)\n    # [Missing Code]\n    print(sample)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\n# Split the data into bins\nbins = np.array_split(data, np.arange(0, len(data) + 1, bin_size))\n# Calculate the mean of each bin\nbin_data_mean = [np.mean(x) for x in bins]\nprint(bin_data_mean)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\n# Split the array into bins\nbins = np.array_split(data, np.arange(0, len(data) + 1, bin_size))\n# Calculate the mean of each bin\nbin_data_mean = [np.mean(x, axis=0) for x in bins]\nprint(bin_data_mean)\n",
        "\nimport numpy as np\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\n# Create bins\nbins = np.arange(data.shape[0] - bin_size + 1, data.shape[0], bin_size)\nbins = np.sort(bins)\n# Calculate means of each bin\nbin_data_mean = []\nfor i in range(1, len(bins) - 1):\n    bin_data = data[bins[i-1]:bins[i]]\n    bin_data_mean.append(np.mean(bin_data))\nprint(bin_data_mean)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\ndata = np.array([[4, 2, 5, 6, 7],\n[ 5, 4, 3, 5, 7]])\nbin_size = 3\n# Create bins\nbins = [[] for _ in range(bin_size)]\nfor i in range(len(data) - (bin_size - 1)):\n    bins[i // bin_size].append(data[i])\n# Calculate means\nbin_data_mean = [np.mean(x) for x in bins]\nprint(bin_data_mean)\n",
        "\ndef smoothclamp(x):\n    return np.piecewise(x, [x < x_min, x > x_max, x_min <= x <= x_max], [x_min, x_max, x])\n# [Missing Code]\n",
        "\ndef smoothclamp(x, N=N):\n    if x < x_min:\n        return x_min\n    if x > x_max:\n        return x_max\n    else:\n        return x\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('colu'))\n# [Missing Code]\nresult = df.values\n# [Missing Code]\n",
        "\nresult = df.values\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Calculate the 3rd standard deviation\nstd_dev = np.std(a)\n# Calculate the \u03bc\nmu = np.mean(a)\n# Calculate the 3\u03c3\nthree_sigma = mu - 3 * std_dev\n",
        "\n# Calculate the 2nd standard deviation\nstd_dev = np.std(a)\n# Calculate 2\u03c3\ntwo_sigma = std_dev * 2\n",
        "\n    sigma_mean = np.mean(a)\n    sigma_variance = np.var(a)\n    sigma_3 = np.std(a) * 3\n    # [Missing Code]\n    sigma_upper = sigma_mean + 3 * sigma_3\n    sigma_lower = sigma_mean - 3 * sigma_3\n    ",
        "\n# Calculate the 2nd standard deviation\nstd_dev = np.std(a)\n# Calculate the 2nd standard deviation interval\nlower_bound = std_dev - 2\nupper_bound = std_dev + 2\n",
        "\n# Mask the data array\nmask = DataArray < 0\nmasked_data = DataArray[mask]\n# [Missing Code]\n# Calculate the percentile\nprob = np.percentile(masked_data, percentile)\n",
        "\n# [Missing Code]\n",
        "\nfor row_index in zero_rows:\n    a[row_index] = 0\n# [Missing Code]\n",
        "\na[1] = 0\na[:, 0] = 0\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = np.zeros(a.shape, dtype=np.bool)\nmask[np.arange(a.shape[0]), np.argmax(a, axis=1)] = True\nprint(mask)\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmask = a.masked_array(a == np.min(a, axis=1), True)\nprint(mask)\n",
        "\n# Create a list of range objects\nranges = [50, 100, 500, 1000]\n# Calculate the count of postal codes for each distance range\ncounts = [0] * len(ranges)\nfor i in range(len(distance)):\n    for j in range(len(ranges)):\n        if distance[i] >= ranges[j][0] and distance[i] <= ranges[j][1]:\n            counts[j] += 1\n# [Missing Code]\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(post, distance)[0, 1]\nprint(result)\n",
        "\nresult = np.array([X[i].dot(X[i].T) for i in range(X.shape[1])])\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.array([9, 2, 7, 0])\nnumber = 0\ndef is_contained(a, number):\n    return np.any(np.equal(a, number))\nprint(is_contained(a, 0))\n",
        "\n# Create a boolean mask to indicate which elements in A are in B\nmask = np.in1d(A, B)\n# Remove the elements in B from A\nA = A[~mask]\n",
        "\n# [Missing Code]\n",
        "\n# Create a boolean mask by comparing elements of A with those in B\nmask = (A >= B[0]) & (A <= B[-1])\n# [Missing Code]\n# Keep only the elements that are within the specified range\nC = A[mask]\n",
        "\ndef myrank(x):\n    return np.argsort(x)[::-1]\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    ranked_a = np.argsort(a)\n    # [Missing Code]\n    ranked_a = list(reversed(ranked_a))\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\narr = np.zeros((20, 10, 10, 2))\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nchoices = ['XX']\nconditions  = [df[\"a\"].str.contains(target),\n               df[\"a\"].str.contains(target),\n               df[\"a\"].str.contains(target),\n               df[\"a\"].str.contains(target),\n               df[\"a\"].str.contains(target)]\nchoices     = [ \"blog\",\"info_pages\",\"signup\",\"completed\",\"home_page\",\"promo\"]\ndf[\"page_type\"] = np.select(conditions, choices, default=np.nan)     # set default element to np.nan\nprint(df)\n",
        "\nresult = np.zeros((a.shape[1], a.shape[1]))\nfor i in range(a.shape[1]):\n    for j in range(a.shape[1]):\n        result[i, j] = euclidean_distance(a[i], a[j])\n# [Missing Code]\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n# Calculate distances between all points\nresult = np.zeros((dim, dim))\nfor i in range(dim):\n    for j in range(dim):\n        distance = np.linalg.norm(a[:, i] - a[:, j])\n        result[i, j] = distance\n        result[j, i] = distance\nprint(result)\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n# Calculate distances between all points\nresult = np.zeros((dim, dim))\nfor i in range(dim):\n    for j in range(i+1, dim):\n        distance = np.linalg.norm(a[:,i] - a[:,j])\n        result[i,j] = distance\nprint(result)\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nfor i in range(len(lat)):\n    df.loc[i, 'lat'] = lat[i]\n    df.loc[i, 'lon'] = lon[i]\n    df.loc[i, 'val'] = val[i]\n# [Missing Code]\n",
        "\n    # Create a dataframe with the desired format\n    df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nlat = np.array([[10, 20, 30],\n                 [20, 11, 33],\n                 [21, 20, 10]])\nlon = np.array([[100, 102, 103],\n                 [105, 101, 102],\n                 [100, 102, 103]])\nval = np.array([[17, 2, 11],\n                 [86, 84, 1],\n                 [9, 5, 10]])\ndf = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\ndf['maximum'] = df.apply(lambda x: x['val'].max(), axis=1)\nprint(df)\n",
        "\ndef slide_window(arr, size):\n    window = np.zeros((size[1], size[0]))\n    for i in range(arr.shape[0] - size[1] + 1):\n        for j in range(arr.shape[1] - size[0] + 1):\n            window = np.concatenate((window, arr[i:i+size[1], j:j+size[0]]))\n    return window\n# [Missing Code]\nresult = slide_window(a, size)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nfor i in range(Z.ndim):\n    if i == Z.ndim - 1:\n        result = Z[:, :, -1]\n    else:\n        result = Z[:, :, :, -1]\n# [Missing Code]\n",
        "\nfor i in range(a.ndim):\n    if a.shape[i] == 1:\n        a = a.squeeze()\n# [Missing Code]\nresult = a\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = np.zeros((4, 4))\nfor i in range(4):\n    result[i] = intp(a, x_new[i], y_new[i], method='linear')\n# [Missing Code]\n",
        "\ndf['Q_cum'] = np.cumsum(df.Q.loc[df.D == data['D']])\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result.append(i)\n# [Missing Code]\n",
        "\nmean_x = np.mean(x)\nmean_y = np.mean(y)\n",
        "\nweights = np.zeros(degree+1)\nfor i in range(degree+1):\n    sum_x = 0\n    sum_y = 0\n    for j in range(len(x)):\n        sum_x += x[j]**i\n        sum_y += y[j]**i\n    weights[i] = (sum_y - sum_x) / (sum_y - sum_x)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask & mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n",
        "\nmask = arr < n1\nmask2 = arr >= n2\nmask3 = mask & mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n",
        "0",
        "\nresult = np.sum(np.isclose(s1, s2, rtol=1e-8, atol=1e-8))\n",
        "\nimport numpy as np\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\nresult = all(np.array_equal(a[0], a[1]))\nprint(result)\n",
        "\nimport numpy as np\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\nresult = all([np.isnan(arr).any() for arr in a])\nprint(result)\n",
        "\nfor i in range(41):\n    for j in range(13):\n        a[i, j] = 0\n# [Missing Code]\n",
        "\nfor i in range(41):\n    for j in range(12):\n        a[i, j] = 0\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\n# pad the array with zeros on the right and bottom to match the largest shape\n# first, find the padding needed on the right and bottom\nright_padding, bottom_padding = 0, 0\nfor i in range(1, len(shape[1])):\n    if shape[1][i] > a.shape[1]:\n        right_padding += 1\n    else:\n        break\nfor i in range(1, len(shape[0])):\n    if shape[0][i] > a.shape[0]:\n        bottom_padding += 1\n        break\n# pad the array with zeros\na = np.pad(a, (right_padding, bottom_padding), 'constant')\na = a.astype(np.float32)\n# fill the padded array with the given element\na = np.where(a == 0, element, a)\n# print the result\nprint(a)\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\na = a.reshape(3, 4)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nmask = (df['a'] > 1) & (df['a'] <= 4)\ndf['b'] = np.where(mask, df['b'], np.nan)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Create a boolean array to indicate if a value is zero or not.\nmask = A == 0\n# [Missing Code]\n# Use the mask to select only the non-zero values.\nresult = A[mask]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n"
    ],
    "Matplotlib": [
        "\nplt.plot(x, y, label=\"x-y\")\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\n# Create the plot\nplt.scatter(x, y)\n# Turn on minor ticks on the y-axis only\nplt.gca().yaxis.set_minor_ticks(True)\nplt.show()\n",
        "\n",
        "\nx = np.random.rand(10)\ny = np.random.rand(10)\n# Turn on minor ticks on the x-axis only\nplt.scatter(x, y)\nplt.set_xticklabels(minor=True)\nplt.show()\n",
        "\nfor line_style in ['--', '-.', '--', ':']:\n    y = np.random.rand(len(x))\n    plt.plot(x, y, line_style)\n",
        "\nfor line_style in ['--', '-.', '--', ':']:\n    y = np.random.rand(len(x))\n    plt.plot(x, y, line_style)\n",
        "\nplt.plot(x, y, marker='o', markersize=0.1)\n",
        "\nplt.plot(x, y, marker='d', markersize=10)\n",
        "\nsns.set_style(\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n# set the y axis limit to be 0 to 40\nax.yaxis.set_major_locator(plt.ylim(0, 40))\n# set the y axis label\nax.set_ylabel(\"Total Bill\")\n# set the x axis label\nax.set_xlabel(\"Day\")\n# show the plot\nplt.show()\n",
        "\nfor i in range(2, 4):\n    plt.axvspan(i, i+1, color='red', alpha=0.5)\n",
        "                   _ooOoo_\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y\n                   jkL\n                   jd8L\n                   fd8L\n                   ndr88y",
        "\nax = plt.gca()\nax.plot([0, 1], [0, 2], c='red', linewidth=1)\n",
        "\nrelplot = df.plot(kind='rel', hue='Gender', data=df)\nplt.show()\n",
        "\nsns.set(style=\"whitegrid\")\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.boxplot(x=x, y=y, hwhisk=False, ax=ax)\nax.set_title(\"Box Plot\", fontdict={\"size\": 18, \"weight\": \"bold\"})\nax.set_xticklabels(x, rotation=45, fontdict={\"size\": 14, \"weight\": \"bold\"})\n",
        "\nsns.lineplot(x=x, y=y, data=pd.DataFrame({'x': x, 'y': y}))\nplt.show()\n",
        "\nplt.plot(x, y, marker='+', markersize=7)\n",
        "\nplt.legend(loc=\"best\")\nplt.rc(\"font\", size=20)\n",
        "\nplt.legend(title='xyz', fontdict={'size': 20})\n",
        "\nmarker_face_color = plt.cm.rainbow(0.2)\nl.set_markerfacecolor(marker_face_color)\n",
        "\nfor i in range(len(l) - 1):\n    l[i].set_edgecolor('black')\n",
        "\nl.set_color('red')\nl.set_marker('o')\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xticks(rotation=45)\n",
        "\nplt.xlabel(\"x-axis\", labelpad=15)\nplt.tick_params(axis='x', labelsize=14)\nplt.tick_params(axis='y', labelsize=14)\nplt.grid(True, which='both')\n",
        "\nplt.legend(loc=\"center left\", bbox_to_anchor=(-0.15, 1.15))\n",
        "\nplt.imshow(H, cmap='terrain')\nplt.colorbar()\n",
        "\nplt.imshow(H, cmap='gray')\n",
        "\n",
        "",
        "\nmyTitle = myTitle.replace(\" \", \"\\n\")\nmyTitle = myTitle.replace(\"-\", \"\\n\")\nmyTitle = myTitle.replace(\"&\", \"\\n\")\nmyTitle = myTitle.replace(\"*\", \"\\n\")\nmyTitle = myTitle.replace(\"^\", \"\\n\")\nmyTitle = myTitle.replace(\"_\", \"\\n\")\nmyTitle = myTitle.replace(\"'\", \"\\n\")\nmyTitle = myTitle.replace(\"`\", \"\\n\")\n",
        "",
        "\nplt.xlabel('x-axis', labelpad=10)\nplt.xticks(x, [0, 1.5])\n",
        "",
        "\nplt.figure(figsize=(12, 6))\nax1 = plt.subplot(111)\nax1.plot(x, label='x')\nax2 = plt.subplot(111, sharex=ax1)\nax2.plot(y, label='y')\nax3 = plt.subplot(111, sharey=ax2)\nax3.plot(z, label='z')\n",
        "\nplt.scatter(x, y, c='blue', ec='black')\n",
        "\nax = plt.gca()\nax.set_xticks(x)\nax.set_yticks(y)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\n",
        "\ndata = {\n    \"reports\": [4, 24, 31, 2, 3],\n    \"coverage\": [35050800, 54899767, 57890789, 62890798, 70897871],\n}\ndf = pd.DataFrame(data)\n# do not use scientific notation in the y axis ticks labels\nsns.factorplot(y=\"coverage\", x=\"reports\", kind=\"bar\", data=df, label=\"Total\")\nplt.show()\n",
        "\n",
        "\nplt.subplot(121, sharex=True)\nplt.plot(x, y1)\nplt.title('y1')\nplt.subplot(122, sharex=True)\nplt.plot(x, y2)\nplt.title('y2')\n",
        "\nplt.subplot(111)\nplt.plot(x, y1)\nplt.title('y1')\nplt.xlabel('x')\nplt.ylabel('y1')\n",
        "\nax = plt.gca()\nax.set_xlabel('')\n",
        " ",
        "\n",
        "\nplt.yaxis.set_yticks([3, 4])\nplt.yaxis.set_yticklabels(['3', '4'])\n# show the plot\nplt.scatter(x, y)\n# show yticks and horizontal grid at y positions 3 and 4\n",
        "\nplt.ylabel('y-label', labelpad=15)\nplt.grid(which='major', axis='y', linewidth=0.8, color='gray', linetype='solid 4 4')\nplt.ylim(3, 4)\n",
        "\nplt.grid(True)\n",
        "\nplt.legend(loc='lower right')\n",
        "\n",
        "\nplt.legend(['Y', 'Z'])\n",
        "\n",
        "\nplt.xticks(x, ['X']) # Label the x-axis\nplt.setp(plt.gca().get_xticklabels(), ha='center', pad=20) # Set the space between the x-axis label and the x-axis\n",
        "\nplt.plot(x, y)\nplt.xlabel('')\nplt.ylabel('y')\nplt.title('y over x')\n",
        "\nax = plt.gca()\nax.yaxis.set_ticks_right()\n",
        "\nplt.ylabel('Y', labelpad=15, rotation=0)\nplt.tick_params(axis='y', which='both', labelsize=12)\nplt.xlabel('X')\n",
        "\nax = sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', color='green', lw=0.5)\nax.set(xlabel='Total Bill', ylabel='Tip')\n",
        "\nax = sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', color='green', hue='total_bill', data_color='blue')\n",
        "\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', ax=ax)\nax.set_title(\"Joint Regression of Total Bill and Tip\")\nax.set_xlabel(\"Total Bill\")\nax.set_ylabel(\"Tip\")\n",
        "\nax = df.plot.bar(x='celltype', y='s1', figsize=(12, 6))\nax.set_xticklabels(rotation=90)\n",
        "\nax = df.plot.bar(x='celltype', y='s1', figsize=(12, 6))\nplt.setp(ax.get_xticklabels(), rotation=45)\n",
        "\nplt.plot(x, y)\nplt.xlabel('X', color='red')\nplt.tick_params(axis='x', color='red')\n",
        "\nplt.plot(x, y)\nplt.xlabel('X')\nplt.title('X vs Y')\nplt.axes().get_x_axis().set_major_locator(plt.MaxNLocator(10))\nplt.axes().get_y_axis().set_major_locator(plt.MaxNLocator(10))\n",
        "\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.style.update({'axes.labelsize': 10})\n",
        "\n# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\nplt.axvline(0.22058956, color='r', linewidth=0.8)\nplt.axvline(0.33088437, color='r', linewidth=0.8)\nplt.axvline(2.20589566, color='r', linewidth=0.8)\n",
        "\nax = plt.axes()\nax.xaxis.tick_top()\nax.yaxis.tick_from(ylabels[::-1])\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 100 - 10\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\n# Update the legend and ylim for the subplots\nax.legend(loc=1)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\n# Add a shared xaxis\nax.axes.xaxis.join(ax2.axes.xaxis)\nplt.show()\nplt.clf()\n",
        "\nplt.figure(figsize=(10, 5))\nax1 = plt.subplot(111)\nax2 = plt.subplot(111, ylabel='Y')\ny1 = np.random.rand(5)\ny2 = np.random.rand(5)\nax1.plot(x, y1)\nax2.plot(x, y2)\nplt.title('Y')\nplt.show()\n",
        "\nmarkersize = 30\nplt.scatter(df[\"bill_length_mm\"], df[\"bill_depth_mm\"], markersize=markersize, s=markersize*0.1)\n",
        "\nax = plt.scatter(b, a)\nfor i, data in enumerate(zip(b, a)):\n    ax.annotate(c[i], data, xy=data)\nplt.show()\n",
        "\n",
        "\n",
        "\nplt.hist(x, linewidth=1.2)\n",
        "",
        "\nplt.figure()\nplt.hist(x, bins=bins, edgecolor='black', facecolor='0.5', label='x')\nplt.hist(y, bins=bins, edgecolor='black', facecolor='0.5', label='y')\nplt.legend()\nplt.show()\n",
        "\nplt.figure(figsize=(10, 15))\nhist_x, hist_y = np.histogram(x, bins=np.arange(0, 20), align='left', rwidth=0.8), np.histogram(y, bins=np.arange(0, 20), align='left', rwidth=0.8)\nplt.subplot(1, 2, 1)\nplt.hist(hist_x[:-1], bins=hist_x[1:], edgecolor='black')\nplt.hist(hist_y[:-1], bins=hist_y[1:], edgecolor='black', top_hats=True)\nplt.set_xlabel('X-axis')\nplt.set_ylabel('Frequency')\nplt.title('Histogram of X and Y')\nplt.subplot(1, 2, 2)\nplt.hist(hist_x[:-1], bins=hist_x[1:], edgecolor='black')\nplt.hist(hist_y[:-1], bins=hist_y[1:], edgecolor='black', top_hats=True)\nplt.set_xlabel('X-axis')\nplt.set_ylabel('Frequency')\nplt.title('Histogram of X and Y')\nplt.tight_layout()\nplt.show()\n",
        "\nplt.plot([a, c], [b, d])\nplt.xlim([0, 5])\nplt.ylim([0, 5])\n",
        "\nplt.figure()\nplt.subplot(121)\nplt.imshow(x, cmap='viridis')\nplt.colorbar()\nplt.subplot(122)\nplt.imshow(y, cmap='inferno')\nplt.colorbar()\n",
        "\nfor i in range(len(x.columns)):\n    plt.plot(x[:, i], label=x.columns.tolist()[i])\n",
        "\nplt.figure(figsize=(12, 6))\nax1 = plt.subplot(111)\nax2 = plt.subplot(111)\nax1.set_title(\"Y and Z\")\nax1.set_xlabel(\"X\")\nax1.set_ylabel(\"Y\")\nax1.set_zlabel(\"Z\")\nax2.set_title(\"Y and Z\")\nax2.set_xlabel(\"X\")\nax2.set_ylabel(\"Y\")\nax2.set_zlabel(\"Z\")\nax1.plot(x, y, label=\"Y\")\nax2.plot(x, z, label=\"Z\")\nplt.legend()\nplt.show()\n",
        "\nax = plt.gca()\nax.plot(points[:, 0], points[:, 1], '-o')\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_yscale('log')\n",
        "\nplt.figure(figsize=(12, 6))\nplt.title('Y over X', fontdict={'size': 20}, y=1.01)\nplt.xlabel('X', fontdict={'size': 18})\nplt.ylabel('Y', fontdict={'size': 16})\nplt.plot(x, y)\n",
        "\nax.set_xticks(range(1, 11))\nax.set_yticks(range(1, 11))\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\n",
        "\nfor i, line in enumerate(lines):\n    plt.plot(line[:, 0], line[:, 1], c[i, :], linewidth=1)\n",
        "\nplt.loglog(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.tick_params(axis='both', which='major', labelsize=14)\nplt.tick_params(axis='both', which='minor', labelsize=12)\nplt.grid(True, which='both')\n",
        "\nfor i in range(1, 5):\n    plt.plot(df.index, df[i], label=f\"Column {i}\")\n    plt.legend()\n",
        "\nhist, bins = np.histogram(data, bins=range(min(data), max(data) + 1000))\ndata_normalized = np.sum(hist) / np.sum(hist[1:])\n# Create a new array to store the normalized data\nnormalized_data = [i * 100 / data_normalized for i in hist[1:]]\n# Create a new array to store the y tick labels\ny_tick_labels = [f\"{i:.1f}%\" for i in normalized_data]\n# Plot the histogram with the normalized data and y tick labels\nplt.hist(normalized_data, bins=bins, edgecolor=\"black\", linewidth=0.8)\nplt.yticks(bins, y_tick_labels)\nplt.xlabel(\"Data values\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Histogram of data with normalized and formatted y tick labels\")\nplt.show()\n",
        "\nplt.plot(x, y, marker='o', markersize=10, color='black', alpha=0.5)\n",
        "\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(x, y)\nax1.set_title('y')\nax2.plot(x, a)\nax2.set_title('a')\n",
        "\nax1 = df.plot(kind=\"reg\", x=\"bill_length_mm\", y=\"bill_depth_mm\", ax=plt.gca())\nax2 = df.plot(kind=\"reg\", x=\"bill_length_mm\", y=\"flipper_length_mm\", ax=plt.gca())\n",
        "",
        "\nplt.plot(x, y)\nplt.legend(['lambda'], loc='best')\n",
        "\nextra_ticks = [2.1, 3, 7.6]\nplt.xticks(plt.get_xticklocs(minor=True), extra_ticks)\n",
        "\nplt.setp(plt.gca().get_xticklabels(), rotation=60, ha='left')\n",
        "\nplt.setp(plt.gca().get_yaxis(), rotation=(-60, 0))\nplt.setp(plt.gca().get_xaxis(), verticalalignment='top')\n",
        "\nplt.setp(plt.gca().get_xticklabels(), alpha=0.5)\n",
        "\nplt.margins(0, 1)\n",
        "\nplt.margins(0, 0.1)\n",
        "",
        "\nax = df.plot(kind='line', xlabel='X', ylabel='Y')\nax.set_title('Line Plot with Custom Labels')\nax.set_xlabel('X')\nax.set_ylabel('Y')\n",
        "\n",
        "\nplt.scatter(x, y, marker='o', c='gray', edgecolor='none', hatch='v')\n",
        "\nplt.scatter(x, y, marker='*', hatch='/')\n",
        "",
        "\nplt.imshow(data, extent=(0, 10, 0, 10), cmap='coolwarm')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Heatmap of Data')\n",
        "\nplt.stem(x, y, horizontal=True)\n",
        "\nfig, ax = plt.subplots()\nfor key in d:\n    ax.bar(key, d[key], color=c[key])\n",
        "\nplt.axvline(3, color='black', linewidth=0.8, label='cutoff')\nplt.legend()\n",
        "\nplt.figure(figsize=(12, 6))\nax = plt.axes(projection='polar')\nax.bar(labels, height)\nax.set_yticks(list(height))\nax.set_yticklabels(labels)\n",
        "\nplt.pie(data, labels=l, wedge_width=0.4)\n",
        "\nplt.plot(x, y)\nplt.grid(True, which='both', color='blue', linestyle='dashed')\n",
        "\nplt.minorticks_on()\nplt.grid(which='minor', line_style='dashed', color='gray')\nplt.show()\n",
        "\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title('Pie Chart of Activities')\nplt.ylabel('Activities')\nplt.xlabel('Percentage')\nplt.show()\n",
        "\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title('Pie Chart of Activities')\nplt.ylabel('Activities')\nplt.xlabel('Percentage')\n",
        "\nplt.plot(x, y, marker='o', markersize=10, color='rgba(0, 0, 0, 0.5)', edgecolor='black')\n",
        "\n",
        "\nplt.bar(range(3), blue_bar, width=0.8, align='center', color='blue')\nplt.bar(range(3) + 0.2, orange_bar, width=0.8, align='center', color='orange')\n",
        "\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax1.plot(x, y, label='y over x')\nax2.plot(x, z, label='z over a')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Line Charts')\nax2.set_xlabel('x')\nax2.set_ylabel('a')\nax2.set_title('Line Charts')\nax1.legend(loc='best')\n",
        "\nplt.scatter(x, y, c=y, cmap='Spectral')\n",
        "\nplt.ylabel('y-label', labelpad=1)\n",
        "",
        "\nplt.circle((0.5, 0.5), 0.2, color='red')\n",
        "\nplt.plot(x, y)\nplt.title(r'$\\phi$', fontdict={'weight': 'bold'})\n",
        "\nplt.legend(loc='best')\n",
        "\nplt.plot(x, y, label=\"Line\", handlelength=0.3)\n",
        "\nplt.legend(loc='best')\n",
        "\nplt.legend(loc=\"best\")\ny[0] = 10\ny[-1] = 10\nplt.plot(x, y, marker=\"o\", label=\"Marker 1\")\nplt.plot(x, y, marker=\"^\", label=\"Marker 2\")\n",
        "\nplt.imshow(data, aspect='auto')\n",
        "",
        "\n",
        "\nax = plt.gca()\nax.yaxis.reverse()\nplt.plot(x, y)\n",
        "\nplt.axis('off')\n",
        "\nplt.scatter(x, y, c='red', edgecolor='black')\n",
        "\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor i in range(4):\n    axs[i].plot(x, y)\n",
        "\nplt.hist(x, bins=range(0, 11, 2), align='left', rwidth=0.2)\n",
        "\nx = np.arange(10)\ny = np.arange(1, 11)\nerror = np.random.random(y.shape)\nfig, ax = plt.subplots()\nax.plot(x, y, marker='o')\nax.fill_between(x, y - 0.15, y + 0.15, color='gray', alpha=0.2)\n# Add error bars\nfor i in range(len(y) - 1):\n    ax.errorbar(x, y, y_err=error[i], xerr=None, fmt='o')\n",
        "\nplt.axvline(0, color='white')\nplt.axhline(0, color='white')\n",
        "\n",
        "\nplt.figure(figsize=(12, 6))\nax1 = plt.subplot(111)\nax2 = plt.subplot(111, sharey=ax1)\nax1.set_title(\"Y\")\nax2.set_title(\"Z\", y=1.1)\n",
        "\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(5, 5))\nfor i in range(10):\n    axs[i] = plt.plot(x, y, label=f\"subplot {i+1}\")\naxs[-1].set_xlabel(\"x-label\")\naxs[-1].set_ylabel(\"y-label\")\naxs[-1].legend()\n",
        "\nd = np.random.random((10, 10))\n# Use matshow to plot d and make the figure size (8, 8)\nplt.matshow(d, figsize=(8, 8))\n",
        "",
        "\nplt.plot(x, y)\nplt.xlabel('x-axis', top=True, bottom=True)\nplt.ylabel('y-axis', top=True, bottom=True)\nplt.title('Line Chart with Both Top and Bottom X Axis Tick Labels')\n",
        "\nplt.plot(x, y)\nplt.xlabel('X-Axis')\nplt.ylabel('Y-Axis')\nplt.title('Line Chart with X-Axis Ticks on Both Top and Bottom')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\naxes = df.pivot(\"kind\", \"time\", \"pulse\").plot.scatter(hue=\"diet\", cmap=\"viridis\")\nfor ax in axes.flatten():\n    ax.set_title(f\"Group: {ax.get_hue_name()}\")\n",
        "\naxes = []\nplt.xlabel(\"Exercise Time (min)\")\nplt.ylabel(\"Pulse (bpm)\")\nplt.title(\"Pulse by Diet and Exercise Time\")\n",
        "\naxes = df.pivot(\"kind\", \"time\", \"pulse\").plot.scatter(hue=\"diet\", cmap=\"Blues_r\")\naxes.set_ylabel(None)\naxes.set_title(\"Pulse by Time and Diet\")\nfor i, ax in enumerate(axes):\n    ax.set_xlabel(\"Time (s)\")\n    ax.set_yticks(ax.get_yticks() + 0.05)\n    ax.set_yticklabels(ax.get_yticklabels() + \"s\")\n    ax.set_title(f\"{i+1}. {ax.get_title()}\")\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n",
        "\nplt.figure(figsize=(5, 5))\nplt.plot(x, y)\nplt.savefig('fig.png', dpi=300)\n",
        "\nplt.plot(x, y, label=\"y\")\nplt.legend(loc=\"best\")\nplt.frame_on(False)\n",
        "\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='a + b')\n",
        "\nax = df.plot.stripline(x=\"sex\", y=\"bill_length_mm\", color=\"species\", ax=ax)\nax.legend_out()\n",
        "\naxes = sns.FacetGrid(df, hue='b', data=df, col='a')\naxes.set(xtick=np.arange(0, 31, 2))\naxes.map(plt.scatter, c='c')\naxes.add_legend()\n",
        "",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.tick_params(axis='both', which='major', labelsize=0)\n",
        "\ngs = gridspec.GridSpec(nrow, ncol, hspace=0, wspace=0)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\nax3 = fig.add_subplot(gs[1, 0])\nax4 = fig.add_subplot(gs[1, 1])\n# Plot x in each subplot\nax1.imshow(x, cmap='gray')\nax2.imshow(x, cmap='gray')\nax3.imshow(x, cmap='gray')\nax4.imshow(x, cmap='gray')\n# Hide x-axis ticks for each subplot\nax1.set_xticklabels([])\nax2.set_xticklabels([])\nax3.set_xticklabels([])\nax4.set_xticklabels([])\n"
    ],
    "Tensorflow": [
        "\nx.assign(1)\n# [Missing Code]\n",
        "\nx.assign(114514)\n# [Missing Code]\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\nresult = tf.where(labels == 0, tf.zeros([5, 10]), tf.ones([5, 10]))\nprint(result)\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\nresult = tf.where(labels == 0, 1, labels)\nprint(result)\n",
        "\nimport tensorflow as tf\nlabels = [0, 6, 5, 4, 2]\nresult = tf.where(tf.reduce_all(tf.equal(labels, 0)) , tf.zeros([5, 10]) , tf.ones([5, 10]) )\nprint(result)\n",
        "\n    one_hot_labels = tf.one_hot(labels, num_classes=10, dtype=tf.int32)\n    # [Missing Code]\n    ",
        "\n    one_hot_labels = tf.one_hot(labels, num_classes=10, dtype=tf.int32)\n    # [Missing Code]\n    result = tf.reduce_sum(tf.multiply(predictions, one_hot_labels), axis=1)\n    ",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.flat_map(lambda input: tf.data.Dataset.from_tensor_slices([my_map_func(i)]))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n",
        "\nmask = tf.keras.utils.to_categorical(tf.where(lengths >= 0, lengths, 0), 0, len(lengths))\n# [Missing Code]\n",
        "\nmask = tf.keras.utils.to_categorical(tf.where(lengths >= 0, lengths, 0), 0, 1)\n# [Missing Code]\n",
        "\nmask = tf.keras.utils.to_categorical(tf.where(lengths >= 0, lengths, 0), 0, len(lengths))\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nmask = tf.keras.utils.to_categorical(tf.where(lengths >= 0, lengths, 0), 0, len(lengths))\n# [Missing Code]\n",
        "\nimport tensorflow as tf\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\ndef cartesian_product(a, b):\n    result = []\n    for i in range(len(a)):\n        for j in range(len(b[i])):\n            result.append(tf.constant([a[i], b[i][j]]))\n    return tf.constant(result)\nresult = cartesian_product(a, b)\nprint(result)\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# Add a new dimension to the tensor\na = tf.expand_dims(a, axis=3)\n# [Missing Code]\n# Reshape the tensor\na = a.shape(50, 100, 1, 512)\n",
        "\n# Add two new dimensions to the tensor\na = tf.expand_dims(a, axis=0)\na = tf.expand_dims(a, axis=3)\n# [Missing Code]\n# Reshape the tensor\na = a.shape(1, 50, 100, 1, 512)\n",
        "\ntf.reduce_sum(A, axis=1)\n# [Missing Code]\n",
        "\nresult = tf.reduce_prod(input_tensor=A, axis=1)\n# [Missing Code]\n",
        "\nresult = tf.reciprocal(A)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "import tensorflow as tf\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\nm = tf.gather(x, y, axis=0)\nresult = m.numpy()\nprint(result)",
        "import tensorflow as tf\nx = [[1,2,3],[4,5,6]]\nrow = [0,0]\ncol = [1,2]\nx = tf.constant(x)\nrow = tf.constant(row)\ncol = tf.constant(col)\nm = tf.gather(x, row, axis=1)\nm = tf.gather(m, col, axis=0)\nresult = m.numpy()\nprint(result)",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nwith Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    scores = tf.reduce_max(a, axis=1)\n    # [Missing Code]\n    result = tf.reduce_mean(scores == a)\n    ",
        "\n# [Missing Code]\n",
        "\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nnetwork_layout = []\nfor i in range(3):\n    network_layout.append(8)\nmodel = Sequential()\ninputdim = 4\nactivation = 'relu'\noutputdim = 2\nopt='rmsprop'\nepochs = 50\n#Adding input layer and first hidden layer\nmodel.add(Dense(network_layout[0],\n                name=\"Input\",\n                input_dim=inputdim,\n                kernel_initializer='he_normal',\n                activation=activation))\n#Adding the rest of hidden layer\nfor numneurons in network_layout[1:]:\n    model.add(Dense(numneurons,\n                    kernel_initializer = 'he_normal',\n                    activation=activation))\n#Adding the output layer\nmodel.add(Dense(outputdim,\n                name=\"Output\",\n                kernel_initializer=\"he_normal\",\n                activation=\"relu\"))\n#Compiling the model\nmodel.compile(optimizer=opt,loss='mse',metrics=['mse','mae','mape'])\nmodel.summary()\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n#Save the model in \"export/1\"\nmodel.save(\"my_model\")\n",
        "\nimport tensorflow_probability as tfp\nimport random\ndef random_int_from_uniform_distribution(low, high):\n    return tf.random.uniform([1], low, high)\n# [Missing Code]\nresult = random_int_from_uniform_distribution(1, 4)\n",
        "\nresult = tf.random.uniform([114, 1], minval=2, maxval=5, dtype=tf.int32, seed=seed_x)\n",
        "\n    # [Missing Code]\n    ",
        "import tensorflow as tf\n### output the version of tensorflow into variable 'result'\nresult = tf.version.VERSION\nprint(result)\n# End of Missing Code"
    ],
    "Scipy": [
        "\nresult = np.array([])\nfor i in range(1, len(x)):\n    result = np.append(result, np.polyfit(x[1:i], y[1:i], 1))\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\nresult = np.polyfit(y, x, 1)\nprint(result)\n",
        "\ndef exponential_fit(p, x, y):\n    A, B, C = p\n    return A*np.exp(B*x) + C\n# [Missing Code]\nresult = scipy.optimize.curve_fit(exponential_fit, p0, y, x=x)\n",
        "\ntest_stat = stats.kstest(x, y)\n# [Missing Code]\n",
        "\ntest_stat, result = kstest(x, y, bias=True, variance_arg=False)\n# [Missing Code]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\np_values = scipy.stats.norm.ppf(z_scores)\n# [Missing Code]\n",
        "\n# Calculate the p-values using the Z-scores and the mean, standard deviation\np_values = scipy.stats.norm.ppf(z_scores)\n",
        "\nz_scores = [np.stats.norm.ppf(x) for x in p_values]\n# [Missing Code]\n",
        "\nresult = stats.lognorm.cdf(x, mu, stddev)\n# [Missing Code]\n",
        "\nexpected_value = mu + stddev * np.log(total)\n",
        "\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\nresult = sa * sb\n",
        "\n    # [Missing Code]\n    ",
        "\ndef interp_3d(points, V, request):\n    x, y, z = request\n    idx = (x-points[:,0])[:,np.newaxis]\n    idx2 = (y-points[:,1])[:,np.newaxis]\n    idx3 = (z-points[:,2])[:,np.newaxis]\n    result = V[np.arange(len(points))[:,np.newaxis]*idx]\n    return result\nresult = interp_3d(points, V, request)\nprint(result)\n# [Missing Code]\n",
        "def interpolate_3D(points, V, request):\n    x, y, z = request\n    idx = (x - points[:, 0])[:, np.newaxis]\n    idx2 = (y - points[:, 1])[:, np.newaxis]\n    idx3 = (z - points[:, 2])[:, np.newaxis]\n    idx = idx * np.ones((points.shape[1], 1))\n    idx2 = idx2 * np.ones((points.shape[2], 1))\n    idx3 = idx3 * np.ones((points.shape[3], 1))\n    result = V[np.arange(len(points))[:, None], idx]\n    return result\nresult = interpolate_3D(points, V, request)\nprint(result)",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\nresult = M.diagonal()\nprint(result)\n",
        "\nresult = stats.kstest(times, 'uniform')\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nFeature = c1 + c2\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n# Count the number of regions of cells which value exceeds a given threshold\n# Note: If two elements touch horizontally, vertically or diagnoally, they belong to one region\nresult = 0\nfor i in range(512):\n    for j in range(512):\n        if img[i, j] > threshold:\n            result += 1\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n# Count the number of regions of cells which value below a given threshold\n# Note: If two elements touch horizontally, vertically or diagnoally, they belong to one region.\nresult = 0\nfor i in range(512):\n    for j in range(512):\n        if img[i, j] < threshold:\n            result += 1\nprint(result)\n",
        "\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n# Find the regions of cells which value exceeds a given threshold, say 0.75;\n# Note: If two elements touch horizontally, vertically or diagnoally, they belong to one region.\nregions = np.zeros((img.shape[0], img.shape[1]), dtype=int)\nfor i in range(img.shape[0]):\n    for j in range(img.shape[1]):\n        if img[i, j] > threshold:\n            regions[i, j] = 1\n# Determine the distance between the center of mass of such regions and the top left corner, which has coordinates (0,0).\nresult = []\nfor i in range(regions.shape[0]):\n    for j in range(regions.shape[1]):\n        if regions[i, j] == 1:\n            x, y = i*img.shape[1] + j\n            count = 0\n            while x > 0 and y > 0 and img[x-1, y] == img[x, y-1]:\n                x -= 1\n                y -= 1\n            count += 1\n            center = (x, y)\n            distance = abs(center - (0, 0))\n            result.append(distance)\nprint(result)\n",
        "\ndef make_symmetric(sparse_matrix):\n    for i in range(len(sparse_matrix.data)):\n        for j in range(len(sparse_matrix.data[i])):\n            if sparse_matrix.data[i][j] != 0:\n                sparse_matrix.data[j][i] = sparse_matrix.data[i][j]\n    return sparse_matrix\n# [Missing Code]\n",
        "\n    for i in range(len(sA.data)):\n        for j in range(len(sA.data[i])):\n            if sA.data[i][j] > 0:\n                sA.data[i][j] = sA.data[j][i]\n    # [Missing Code]\n    return sA\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\nmean = np.mean(col)\nstandard_deviation = np.std(col)\nprint(mean)\nprint(standard_deviation)\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n# Get the max and min values in the 1st column of the sparse matrix\nMax = np.max(col)\nMin = np.min(col)\nprint(Max)\nprint(Min)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.spatial.distance\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n# Calculate pairwise Euclidean distances between all regions\nresult = np.zeros((example_array.shape[1], example_array.shape[1]))\nfor i in range(example_array.shape[1]):\n    for j in range(i+1, example_array.shape[1]):\n        distance = np.sqrt((example_array[i, j] - example_array[i, j-1])**2 + (example_array[j, i] - example_array[j-1, i])**2)\n        result[i, j] = distance\nprint(result)\n",
        "\ndistance_result = np.zeros((example_array.shape[1], example_array.shape[1]))\nfor i in range(example_array.shape[1]):\n    distance_result[i] = scipy.spatial.distance.cdist(example_array[:, i], example_array[:, i])\n# [Missing Code]\n",
        "\n    distances = np.zeros((example_array.shape[1], example_array.shape[1]))\n    for i in range(example_array.shape[1]):\n        for j in range(i+1, example_array.shape[1]):\n            distances[i, j] = np.sqrt((example_array[i, 0] - example_array[j, 0])**2 + (example_array[i, 1] - example_array[j, 1])**2)\n    # [Missing Code]\n    ",
        "\nfrom scipy import interpolate\nimport numpy as np\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85.36, 93.24],\n              [61.48, 69.31, 75.36, 81.87, 89.35],\n              [57.61, 65.75, 71.7, 79.1, 86.13],\n              [55.12, 63.34, 69.32, 77.29, 83.88],\n              [54.58, 62.54, 68.7, 76.72, 82.92],\n              [56.58, 63.87, 70.3, 77.69, 83.53],\n              [61.67, 67.79, 74.41, 80.43, 85.86],\n              [70.08, 74.62, 80.93, 85.06, 89.84]])\nplt.figure(figsize = (5.15,5.15))\nplt.subplot(111)\nfor i in range(5):\n    x_val = np.linspace(x[0, i], x[-1, i], 100)\n    x_int = np.interp(x_val, x[:, i], y[:, i])\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    y_int = interpolate.splev(x_val, tck, der = 0)\n    plt.plot(x[:, i], y[:, i], linestyle = '', marker = 'o')\n    plt.plot(x_val, y_int, linestyle = ':', linewidth = 0.25, color =  'black')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show() \n# [Missing Code]\n",
        "\n# You can now pass the created 2D array to the 'anderson_ksamp' function\nstatistic, critical_values, significance_level = ss.stats.anderson_ksamp(data)\n",
        "\ny1 = np.array([x1])\ny2 = np.array([x2])\n# [Missing Code]\nresult = ss.anderson_ksamp(y1, y2)\n",
        "\ndef tau1(x):\n    y = np.array(df['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\n# [Missing Code]\ndf['AC'] = pd.rolling_apply(df['C'], 3, lambda x: tau1(x))\ndf['BC'] = pd.rolling_apply(df['C'], 3, lambda x: tau1(x))\n",
        "\ndef is_csr_matrix_only_zeroes(my_csr_matrix):\n    return(my_csr_matrix.data == 0).all()\n# [Missing Code]\n",
        "\ndef is_lil_matrix_only_zeroes(my_lil_matrix):\n    return(my_lil_matrix.data == 0).all()\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nfrom scipy import stats\nnp.random.seed(10)\npre_course_scores = np.random.randn(10)\nduring_course_scores = np.random.randn(10)\nprint(stats.ranksums(pre_course_scores, during_course_scores).pvalue)\n",
        "\n    ranksums_result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = ranksums_result.pvalue\n    # [Missing Code]\n    return p_value\n",
        "\ndef kurtosis(a):\n    n = len(a)\n    s = np.sum(a)\n    k = 0\n    for i in range(n):\n        k += (a[i] - s)**2 / (n - 1)\n    return k\n# [Missing Code]\nkurtosis_result = kurtosis(a)\n",
        "\ndef kurtosis(a):\n    mu = np.mean(a)\n    s = np.std(a)\n    kurtosis = (np.var(a) - (mu * mu)) / s**2\n    return kurtosis\n# [Missing Code]\nkurtosis_result = kurtosis(a)\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nindices = [0]*len(vor.vertices)\nfor ep in extraPoints:\n    for r in vor.regions:\n        if r.contains(ep):\n            indices[r.index] += 1\n# [Missing Code]\n",
        "\n# Count how many points are in each Voronoi cell\nresult = []\nfor e in extraPoints:\n    region_points = []\n    for v in vor.vertices:\n        if scipy.spatial.distance.euclidean(v, e) < 1e-6:\n            region_points.append(v)\n        else:\n            continue\n    result.append(len(region_points))\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.sparse as sparse\nnp.random.seed(10)\nmax_vector_size = 1000\nvectors = [np.random.randint(100,size=900),np.random.randint(100,size=max_vector_size),np.random.randint(100,size=950)]\n# Create a sparse matrix with vectors\nresult = sparse.csr_matrix((np.hstack([v.astype(float) for v in vectors]), np.array([len(v) for v in vectors])), dtype=float)\n# Pad zeros to the end of elements for the vectors which are smaller than the maximum size\nresult = result.pad_to_max(0, axis=1)\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = 2\ncolumn = 3\nresult = M.get_entry(row, column)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = [2, 1]\ncolumn = [3, 0]\nresult = M[row, column]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nP_inner = scipy.integrate(NDfx,-dev,dev)\n# [Missing Code]\n",
        "\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n",
        "\n# [Missing Code]\n",
        "\ndiags(matrix, [-1,0,1], (5, 5)).toarray()\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.stats\nN = 3\np = 0.5\nM = scipy.stats.binom.pmf(N, p)\nprint(M)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef row_zscore(row):\n    return stats.zscore(row)\n# [Missing Code]\n",
        "\n# Calculate data and zscore for each element\ndf['data'] = df['sample1'], df['sample2'], df['sample3']\ndf['zscore'] = df['data'].apply(lambda x: stats.zscore(x))\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef get_distance_2(y, x):\n    return distance.cdist(scipy.dstack((y, x)), mid)\n# [Missing Code]\n",
        "\n    mid = np.zeros((shape[1], shape[2]))  # create a zero array of the desired shape\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "",
        "\n# Define the initial conditions for the ODE\ny0 = [N0]\nt0 = time_span[0]\nt_eval = np.linspace(t0, time_span[1], 1000)\n# [Missing Code]\n# Solve the ODE with the time-varying input\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=y0, t_eval=t_eval, args=(time_varying_input,))\n",
        "",
        "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n# calculate distances between each element and each centroid\ndistances = scipy.spatial.distance_matrix(data, centroids)\n# find the indices of the closest centroids for each element\nresult = np.argmin(distances, axis=1)\nprint(result)\n",
        "\nresult = extract_closest_points(clusters, data)\n# [Missing Code]\n",
        "\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nk = 3\n# calculate distance between each point and centroids\ndistances = scipy.spatial.distance_matrix(data, centroids)\n# find k-th closest centroids\nclosest_centroids = np.argsort(distances)[k-1]\n# find k-th closest elements in original data\nclosest_elements = data[closest_centroids]\n# print result\nprint(closest_elements)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Calculate the statistic and pvalue using KStest\nstatistic, pvalue = stats.kstest(bekkers, sample_data, args=(estimated_a, estimated_m, estimated_d))\nprint(statistic, pvalue)\n",
        "result = False",
        "\n# [Missing Code]\n",
        "\n6.9999999999999996",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nindices = []\nfor i in range(len(arr) - n + 1):\n    if arr[i] <= arr[i - n] and arr[i] <= arr[i + n]:\n        indices.append(i)\n# [Missing Code]\n",
        "\nindices = []\nfor i in range(len(arr) - n + 1):\n    min_idx = i\n    for j in range(i + 1, len(arr) - n + 1):\n        if arr[j][i] <= arr[min_idx][i]:\n            min_idx = j\n    indices.append(min_idx)\n# [Missing Code]\n",
        "\n# [Missing Code]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data)\n# [Missing Code]\n",
        "from sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndata1 = pd.DataFrame(data.data)\ndata1.columns = data.target_names\nprint(data1)",
        "\ndata1 = pd.DataFrame(data)\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\n# Create a one-hot-encoded version of the column 'Col3'\ndf['Col3'] = df['Col3'].apply(pd.Series).astype(np.uint8)\n# Drop the original 'Col3' column\ndf.drop('Col3', axis=1, inplace=True)\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\ndf = load_data()\n# Create a one-hot-encoded version of the list columns\ndf['Col3'] = df['Col3'].apply(pd.Series).stack().str.get_dummies()\n# Drop the original list column\ndf.drop('Col3', axis=1, inplace=True)\n# Print the resulting DataFrame\nprint(df)\n",
        "\n# Create a one-hot-encoded version of the column\ndf['Col3'] = df['Col3'].apply(lambda x: np.array_equal(x, ['Apple', 'Banana', 'Grape', 'Orange', 'Suica']))\ndf['Col3'] = df['Col3'].astype(np.uint8)\ndf = df.drop(['Col3'], axis=1)\n",
        "\n# [Missing Code]\n",
        "\n# Create a one-hot-encoded version of the column.\ndf_temp = df[df['Col3'].notnull()]\ndf_temp['Col3'] = df_temp['Col3'].apply(lambda x: ' '.join(x))\ndf_temp = df_temp.dropna()\ndf_out = df.join(df_temp)\n",
        "# Missing Code\nproba = 1 / (1 + np.exp(-x_test))",
        "\n# [Missing Code]\n",
        "\ndf_transformed = transform_output.toarray()\ndf_transformed = df_transformed.astype('float32')\ndf_transformed = pd.DataFrame(df_transformed, columns=df_origin.columns)\ndf = pd.concat([df_origin, df_transformed], ignore_index=True)\n",
        "\ndf_transformed = transform_output.toarray()\ndf_transformed = df_transformed.astype('float64')\ndf_transformed = pd.DataFrame(df_transformed, columns=df_origin.columns)\n# [Missing Code]\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Insert a new step\nclf.steps.insert(0, ('new_step', NewFeature()))\nprint(len(clf.steps))\n# Delete a step\nclf.steps.remove('poly')\nprint(len(clf.steps))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())]\nclf = Pipeline(estimators)\n# Insert a new step before 'sVm_233'\nnew_step = ('new_step', SomePreprocessingStep())\nclf.steps.insert(clf.steps.index('sVm_233') - 1, new_step)\n# Delete the 'dim_svm' step\nclf.steps.remove('dim_svm')\n# Print the updated pipeline\nprint(clf)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('pOly', PolynomialFeatures()), ('svdm', SVC())]\nclf = Pipeline(estimators)\n# Insert a new step before 'svm'\nnew_step = {'name': 'new_step', 'function': lambda X: X}\nclf.steps.insert(clf.steps.index('svm') - 1, new_step)\n# Delete the second step\nclf.steps.remove(clf.steps[1])\nprint(clf.named_steps())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('inserted_step', SomeClassifier()), ('svm', SVC())]\nclf = Pipeline(estimators)\n# Insert a new step before 'svm'\nclf.steps.insert(index_to_insert, 'new_step')\n# Print the updated list of steps\nprint(clf.steps)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_poly', PolynomialFeatures()), ('dim_svm', PCA()), ('sVm_233', SVC())]\nclf = Pipeline(estimators)\n# Insert a new step\nclf.steps.insert(0, ('AAA', PCA()))\n# Delete a step\nclf.steps.remove('dim_svm')\nprint(len(clf.steps))\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nestimators = [('reduce_dim', PCA()), ('t1919810', PolynomialFeatures()), ('svdm', SVC())]\nclf = Pipeline(estimators)\nprint(clf.named_steps)\n",
        "\nparam_grid = {\n    \"early_stopping_rounds\": 42,\n    \"eval_metric\": \"mae\",\n    \"eval_set\": [[testX, testY]]\n}\n# [Missing Code]\ngridsearch = GridSearchCV(\n    model=xgb.XGBRegressor(),\n    param_grid=param_grid,\n    verbose=verbose,\n    cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]),\n    n_jobs=n_jobs,\n    iid=iid\n)\n",
        "\n# [Missing Code]\n",
        "\npredictions = []\nfor i in range(len(cv)):\n    fold = cv.get_fold(i)\n    X_train, X_test, y_train, y_test = fold\n    logreg.fit(X_train, y_train)\n    proba = logreg.predict_proba(X_test)\n    predictions.append(proba)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ninversed = scaler.inverse_transform(scaled)\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "",
        "",
        "",
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\ndata = load_data()\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\nprint(tf_idf_out)\n",
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\ndata = load_data()\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\ndata_tfidf = pipe.named_steps[\"tf_idf\"].fit_transform(data)\ndata_nmf = pipe.named_steps[\"nmf\"].fit_transform(data_tfidf)\nprint(data_nmf)\n",
        "\n# [Missing Code]\n",
        "\nparam_grid['base_estimator__max_depth'] = [1, 2, 3, 4, 5]\nparam_grid['max_samples'] = [0.05, 0.1, 0.2, 0.5]\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\npreprocessor = preprocess\n# [Missing Code]\n",
        "\npreprocessor = prePro\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n# Get feature names\ncolumn_names = [i for i in clf.feature_names_]\nprint(column_names)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n# Get the feature names\ncolumn_names = [i for i in clf.feature_names_]\nprint(column_names)\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans()\ndef get_samples(p, X, km):\n    centroids = km.cluster_centers_\n    closest_indices = np.argmin(np.linalg.norm(X - centroids, axis=1))\n    samples = X[closest_indices]\n    return samples\nclosest_50_samples = get_samples(p, X, km)\nprint(closest_50_samples)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport pandas\n# load data in the example\nX_train, y_train = load_data()\nX_train[0] = ['a'] * 40 + ['b'] * 40\n# One-hot encode the categorical variable\nX_train[0] = pd.get_dummies(X_train[0])\n# Model.\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n",
        "\n# [Missing Code]\n",
        "\n# Define the SVM model with Gaussian kernel\nsvm_reg = sklearn.svm.SVC(kernel='gaussian')\n# fit the model\nsvm_reg.fit(X, y)\n# make predictions on new data\npredict = svm_reg.predict(X)\n# [Missing Code]\n",
        "\n# Define the SVM model\nsvm = sklearn.svm.SVC(kernel='gaussian')\n# fit the model\nsvm.fit(X, y)\n# make predictions\npredict = svm.predict(X)\n",
        "\n# Create a polynomial kernel with degree=2\nkernel = sklearn.svm.PolynomialFeatures(degree=2)\n# fit the model using the created kernel\nmodel = sklearn.svm.SVR(kernel=kernel)\nmodel.fit(X, y)\n# [Missing Code]\n# Make predictions using the fitted model\npredict = model.predict(X)\n",
        "\n# Define the SVM model with a polynomial kernel\nsvm_model = sklearn.svm.SVC(kernel='poly', degree=2)\n# fit the model\nsvm_model.fit(X, y)\n# make predictions on new data\npredict = svm_model.predict(X)\n",
        "\nsimilarity_scores = np.dot(tfidf.transform(queries), tfidf.transform(documents))\nsimilarity_scores = (similarity_scores / (np.linalg.norm(similarity_scores, ord=2) * np.linalg.norm(tfidf.transform(documents), ord=2)))\n# [Missing Code]\n",
        "\nsimilarity_scores = np.dot(tfidf.transform(queries), tfidf.transform(documents))\nsimilarity_scores = (similarity_scores / (np.linalg.norm(similarity_scores, ord=2) * np.linalg.norm(tfidf.transform(documents), ord=2)))\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nqueries, documents = load_data()\nassert type(queries) == list\nassert type(documents) == list\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    cosine_similarities = np.dot(tfidf.transform(queries), tfidf.inverse_transform(tfidf.transform(documents)))\n    return cosine_similarities\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "[['prof1', 'prof3'], ['prof2']]",
        "\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\n# Load data\ndata_matrix = load_data()\n# Calculate the distance matrix\ndist_matrix = np.array(data_matrix)\n# Perform hierarchical clustering\ncluster = AgglomerativeClustering(n_clusters=2, distance_metric='cosine', metric_params={'cosine_similarity': True}).fit(dist_matrix)\n# Get cluster labels\ncluster_labels = cluster.labels_\n# Print cluster labels\nprint(cluster_labels)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn.cluster\ndef hierarchical_clustering(distance_matrix):\n    agg_clust = sklearn.cluster.AgglomerativeClustering(n_clusters=2, metric='cosine')\n    agg_clust.fit(distance_matrix)\n    return agg_clust.labels_\nsimM = np.array([[0, 0.6, 0.8],\n                  [0.6, 0, 0.111],\n                  [0.8, 0.111, 0]])\ncluster_labels = hierarchical_clustering(simM)\nprint(cluster_labels)\n",
        "\n",
        "\n# Standardize the data\nstandardized_data = Standardize(data_matrix)\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.agglomerate(standardized_data, linkage='single')\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef centered_scaled_data(data):\n    # Center the data\n    data_centered = data - np.mean(data)\n    # Scale the data\n    data_scaled = (data_centered / np.std(data))\n    return data_scaled\n# [Missing Code]\n",
        "\ndef box_cox_transform(data):\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.preprocessing import BoxCox\n    \n    scaler = StandardScaler()\n    transformed_data = scaler.fit_transform(data)\n    box_cox = BoxCox()\n    box_cox_data = box_cox.fit_transform(transformed_data)\n    \n    return box_cox_data\n",
        "\ndef box_cox_transform(data):\n    data_transformed = np.power(data, 1 / (1 + data))\n    return data_transformed\n# [Missing Code]\n",
        "\ndef yeo_johnson_data(data):\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    X = data\n    y = StandardScaler().fit_transform(X)\n    reg = LinearRegression()\n    reg.fit(y, X)\n    transformed_data = reg.predict(y)\n    return transformed_data\n# [Missing Code]\n",
        "\ndef yeo_johnson_transformation(data):\n    data_std = np.std(data)\n    data_mean = np.mean(data)\n    data_skewness = np.var(data) - data_mean * data_std\n    data_kurtosis = np.kurtosis(data)\n    data_yeo_johnson = data_mean - data_kurtosis\n    return data_yeo_johnson\n# [Missing Code]\nyeo_johnson_data = yeo_johnson_transformation(data)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\n# Split the dataset into training and testing sets\nsplit_ratio = 0.8\ntrain_size = int(len(dataset) * split_ratio)\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = dataset.iloc[:train_size], dataset.iloc[train_size:]\n# Split each set into x and y\nx_train, y_train = train_dataset.drop('target_class', axis=1), train_dataset['target_class']\nx_test, y_test = test_dataset.drop('target_class', axis=1), test_dataset['target_class']\n",
        "\nimport numpy as np\nimport pandas as pd\ndata = load_data()\n# Split the data into train and test sets\nsplit_ratio = 0.8\nx_train, x_test, y_train, y_test = train_test_split(data, data['target'], test_size=split_ratio, random_state=42)\n# Create x and y variables\nx = x_train\ny = y_train\nprint(x)\nprint(y)\nprint(x_test)\nprint(y_test)\n",
        "\nimport numpy as np\nimport pandas as pd\ndataset = load_data()\n# Split the dataset into training and testing sets\nsplit_ratio = 3\ntrain_size = int(len(dataset) * split_ratio)\ntest_size = len(dataset) - train_size\ntrain_indices = list(range(train_size))\ntest_indices = list(range(test_size))\n# Shuffle the indices\nnp.random.shuffle(train_indices)\nnp.random.shuffle(test_indices)\ntrain_set = dataset.iloc[train_indices]\ntest_set = dataset.iloc[test_indices]\n# Extract x and y values\nx_train, y_train = train_set.drop('target', axis=1), train_set['target']\nx_test, y_test = test_set.drop('target', axis=1), test_set['target']\n# Print the sets\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n",
        "\n    # Split the dataset into training and testing sets\n    x_train, y_train, x_test, y_test = train_test_split(data, test_size=0.2)\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[linear_svc.get_support()]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ncorpus, y = load_data()\nassert type(corpus) == list\nassert type(y) == list\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\nselected_feature_names = []\nfor i, feature in enumerate(vectorizer.get_feature_names()):\n    if LinearSVC(penalty='l1').get_support()[i]:\n        selected_feature_names.append(feature)\nprint(selected_feature_names)\n",
        "\n    selected_features = vectorizer.get_feature_names()\n    selected_indices = np.asarray(selected_features)[featureSelector.get_support()]\n    ",
        "['.Net', 'Angular', 'Backend', 'C++', 'CSS', 'Database design',\n'Frontend', 'Full stack', 'Integration', 'Java', 'Jscript', 'Linux',\n'Mongo', 'NodeJS', 'Oracle', 'PHP', 'Photoshop', 'Python', 'SQL',\n'TeamCity', 'TypeScript', 'UI Design', 'UX', 'Web']\n",
        "['.Net', 'Angular', 'Backend', 'C++', 'CSS', 'Database design',\n'Frontend', 'Full stack', 'Integration', 'Java', 'Jscript', 'Linux',\n'Mongo', 'NodeJS', 'Oracle', 'PHP', 'Photoshop', 'Python',\n'TeamCity', 'TypeScript', 'UI Design', 'UX', 'Web']\n",
        "['.Net', 'Angular', 'Backend', 'C++', 'CSS', 'Database design',\n'Frontend', 'Full stack', 'Integration', 'Java', 'Jscript', 'Linux',\n'Mongo', 'NodeJS', 'Oracle', 'PHP', 'Photoshop', 'Python', 'SQL',\n'TeamCity', 'TypeScript', 'UI Design', 'UX', 'Web']\n",
        "['.Net', 'Angular', 'Backend', 'C++', 'CSS', 'Database design',\n'Frontend', 'Full stack', 'Integration', 'Java', 'Jscript', 'Linux',\n'Mongo', 'NodeJS', 'Oracle', 'PHP', 'Photoshop', 'Python',\n'TeamCity', 'TypeScript', 'UI Design', 'UX', 'Web']\n",
        "\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    X, Y = df2[:, 0], df2[:, 1]\n    slope = LinearRegression().fit(X, Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndf1 = load_data()\nslopes = [float('nan')] * len(df1.columns)\n# Iterate through the columns and calculate the slope\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    X, Y = np.array(df2[['Time', col]].values), np.array(df2[col].values)\n    slope = LinearRegression().fit(X, Y).coef_[0]\n    slopes[col] = slope\n# Concatenate the slopes into a single array\nconcatenated_slopes = np.concatenate(slopes, axis=0)\nprint(concatenated_slopes)\n",
        "\ndf['Sex'] = label_encoder.fit_transform(df['Sex'])\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    le = LabelEncoder()\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nnp_array = load_data()\ndef Transform(a):\n    min_val, max_val = np.min(a), np.max(a)\n    a = (a - min_val) / (max_val - min_val)\n    return a\ntransformed = Transform(np_array)\nprint(transformed)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n# Convert string to float\nnew_X = np.array([[float(i), float(j)] for i, j in X])\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\n# dataframe = df\n# Git rid of the name of the animal\n# And change the hunter/scavenger to 0/1\ndataframe = dataframe.drop([\"Name\"], axis=1)\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\n# One-hot encode the Class column\ndataframe['Class'] = pd.get_dummies(dataframe['Class'])\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(dataframe.drop('Class', axis=1), dataframe['Class'])\n# Create a model using the training set and fit it to the data\nlogReg = LogisticRegression()\nlogReg.fit(X_train, y_train)\n# Make predictions on the testing set\npredict = logReg.predict(X_test)\nprint(predict)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\ndataframe = dataframe.drop([\"Name\"], axis=1)\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\nX, y = dataframe.iloc[:, -1], dataframe.iloc[:, :-1]\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n# Train the model\nmod = MultinomialLogisticRegression()\nmod.fit(X_train, y_train)\n# Make predictions\ny_pred = mod.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "['#de' '@ji' '@na' 'a' 'bu' 'da' 'ha' 'ka' 'ke' 'kku' 'ko' 'me' 'mo' 'n'\n 'na' 'ni' 'no' 'ra' 'ri' 'ru' 'shi' 't' 'ta' 'te' 'to' 'tsu' 'u' 'wa',\n 'za']",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "import numpy as np\nimport pandas as pd\nfitted_model = load_data()\n# Save the model in the file named \"sklearn_model\"\njoblib.dump(fitted_model, 'sklearn_model')\n# To load the model\nfitted_model = joblib.load('sklearn_model')",
        "\n# [Missing Code]\n"
    ],
    "Pytorch": [
        "\noptim.lr = 0.001\n# [Missing Code]\n",
        "\nclass LearningRateScheduler(torch.optim.lr_scheduler.LRScheduler):\n    def __init__(self, lr, gamma, loss_fn, optimizer):\n        super().__init__(optimizer, lr, gamma)\n        self.loss_fn = loss_fn\n    def step(self, loss, optimizer):\n        loss_value = self.loss_fn(loss)\n        if loss_value > self.best_loss:\n            self.best_loss = loss_value\n            self.lr = self.gamma * self.best_loss / self.loss_fn(self.best_loss)\n        self.update_optimizer(loss)\n# Load data and create the optimizer\ndata = load_data()\noptimizer = torch.optim.SGD(data.parameters(), lr=0.01)\n# Create the custom scheduler\nlr_scheduler = LearningRateScheduler(0.01, 0.9, lambda x: x.mean(), optimizer)\n# Train the model\nfor epoch in range(num_epochs):\n    # Train the model\n    lr_scheduler.step(loss, optimizer)\n",
        "\noptim.lr = 0.0005\n# [Missing Code]\n",
        "\ndef update_learning_rate(optimizer, device, epoch, lr_scheduler):\n    if epoch > 0:\n        lr = lr_scheduler.get_lr(device)\n        if loss > lr:\n            lr_scheduler.set_lr(lr * 0.9)\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy(), columns=['a', 'b', 'c', 'd'])\nprint(px)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = torch.rand(6,6)\npx = pd.DataFrame(x.numpy(), columns=['a', 'b', 'c', 'd', 'e', 'f'])\nprint(px)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_logical, B = load_data()\nC = B[A_logical]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nidx, B = load_data()\nC = B[idx]\nprint(C)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\nindex_in_batch = Tensor_2D.size(1)\ndiag_ele = Tensor_2D[np.arange(index_in_batch)].squeeze()\n# [Missing Code]\nTensor_3D = torch.diag(Tensor_2D)\n",
        "\n    diag_ele = t.diagonal()\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\nab = torch.stack((a,b),0)\nprint(ab)\n",
        "\n    # [Missing Code]\n    ",
        "\nfor i in range(1, lengths.size(1) + 1):\n    a[ : , i : , : ]  = 0\n# [Missing Code]\n",
        "\nfor i in range(10):\n    a[i, lengths[i], :] = 2333\n# [Missing Code]\n",
        "\nfor i in range(10):\n    a[i, :, lengths[i], :] = 0\n# [Missing Code]\n",
        "\nfor i in range(10):\n    a[i, :, lengths[i], :] = 2333\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n    # [Missing Code]\n    ",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\nequal_count = 0\nfor i in range(len(A)):\n    if A[i] == B[i]:\n        equal_count += 1\ncnt_equal = equal_count / len(A)\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\nequal_count = 0\nfor i in range(11):\n    if A[i] == B[i]:\n        equal_count += 1\ncnt_equal = equal_count / 11\nprint(cnt_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ncnt_not_equal = 0\nfor i in range(len(A)):\n    if A[i] != B[i]:\n        cnt_not_equal += 1\nprint(cnt_not_equal)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA, B = load_data()\ndef Count(A, B):\n    cnt = 0\n    for i in range(len(A)):\n        if A[i] == B[i]:\n            cnt += 1\n    return cnt\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n",
        "\ncnt_equal = 0\nfor i in range(x):\n    if A[i][0] == B[i][0]:\n        cnt_equal += 1\n",
        "\ncnt_not_equal = 0\nfor i in range(x):\n    if A[i][0] != B[i][0]:\n        cnt_not_equal += 1\n# [Missing Code]\nprint(cnt_not_equal)\n",
        "\n    tensor_temp = a[..., i * chunk_dim:((i + 1) * chunk_dim - 1, ...)]\n    # [Missing Code]\n    # Fill in the missing code here\n    ",
        "\n        # Get the indices of the current dimension\n        indices = [slice(None) for _ in range(len(a.shape) - 1)]\n        indices[i] = j * chunk_dim\n        # Extract the chunk from the original tensor\n        chunk = a[indices]\n        # Add the chunk to the list of tensors\n        tensors_31.append(chunk)\n# Print the tensors\nfor tensor in tensors_31:\n    print(tensor)\n",
        "\nfor i in range(mask.size(1)):\n    if mask[i] == 1:\n        output[i] = clean_input_spectrogram[i]\n# [Missing Code]\n",
        "\nfor i in range(mask.size(1)):\n    if mask[i] == 0:\n        output[i] = clean_input_spectrogram[i]\n# [Missing Code]\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin = torch.min(torch.abs(x), torch.abs(y))\n# [Missing Code]\nsigned_min = torch.where(torch.abs(x) == min, sign_x, torch.where(torch.abs(y) == min, sign_y, torch.zeros_like(x)))\n",
        "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\n# [Missing Code]\nsigned_max = torch.where(torch.abs(x) == max, sign_x, torch.where(torch.abs(y) == max, sign_y, torch.zeros_like(x)))\n",
        "\n    # [Missing Code]\n    ",
        "\nconf_scores = []\nfor i in range(len(input)):\n    conf, class_name = predict_allCharacters(input[i])\n    conf_scores.append(conf)\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n    c = torch.tensor([[0, 0, 0], [0, 0, 0]])\n    c = c.unsqueeze(0)\n    # [Missing Code]\n    overlap = torch.zeros(a.size(1), b.size(1), a.size(2) + b.size(2) - 1)\n    overlap[0, :, :a.size(2)] = a\n    overlap[1, :, :b.size(2)] = b\n    overlap[2, :, :] = torch.cat((a.unsqueeze(0) * b.unsqueeze(0)), dim=2)\n    overlap = overlap.contiguous().view(-1, a.size(2) + b.size(2) - 1)\n    ",
        "\nt_new = torch.tensor([[0., 0., 0., 0.],\n                        [0., 1., 2., 0.],\n                        [0., 3., 4., 0.],\n                        [0., 5., 6., 0.],\n                        [0., 7., 8., 0.],\n                        [0., 0., 0., 0.]])\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nt_new = torch.tensor([[-1, -1, -1, -1,]])\ntorch.stack([t, t_new])\n",
        "\n# [Missing Code]\n"
    ]
}