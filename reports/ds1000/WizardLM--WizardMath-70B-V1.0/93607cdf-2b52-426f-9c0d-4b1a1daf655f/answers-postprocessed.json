{
    "Pandas": [
        "\nresult = df.iloc[List]\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\n",
        "\ndef change_values(df, columns):\n    for column in columns:\n        counts = df[column].value_counts()\n        for value in counts.index[counts < 2]:\n            df[column] = df[column].replace(value, 'others')\n",
        "\ndef change_values(df, col, threshold):\n    values = df[col].value_counts() >= threshold\n    new_values = df[col].where(values, \"others\")\n    df[col] = new_values\n",
        "\nunique_values_Qu1 = example_df.Qu1.unique()\n",
        "\ncounts = df.Qu1.value_counts()\ncounts[counts >= 3]\n",
        "\ndef change_qu1(df):\n    counts = df.Qu1.value_counts()\n    for value in counts[counts >= 3].index:\n        df.Qu1[df.Qu1 == value] = 'others'\n    return df\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n# Group the dataset by the \"url\" field\ngroups = df.groupby('url')\n# Apply the `drop_duplicates()` method to each group, keeping the first occurrence\nresult = groups.apply(lambda x: x.drop_duplicates('id', keep='first'))\n# Filter out the groups where the \"keep_if_dup\" field is \"No\" and there are no duplicates\nresult = result[result['keep_if_dup'] != 'No']\nprint(result)\n",
        "\ndef drop_duplicates_with_condition(df, subset, keep, drop_if_dup_col):\n    # Initialize a set to store the unique \"url\" values\n    unique_urls = set()\n    # Iterate through the rows of the dataset\n    for index, row in df.iterrows():\n        # Get the current \"url\" value\n        url = row[subset]\n        # Check if the current \"url\" value is already in the set of unique \"url\" values\n        if url in unique_urls:\n            # If the current \"url\" value is a duplicate and the \"drop_if_dup\" field is set to \"Yes\", drop the row\n            if row[drop_if_dup_col] == \"Yes\":\n                df.drop(index, inplace=True)\n        else:\n            # If the current \"url\" value is unique, add it to the set of unique \"url\" values\n            unique_urls.add(url)\n    # Return the modified dataset\n    return df\n",
        "\ndef remove_duplicates(df, keep_if_dup):\n    # Create a mask for the \"keep_if_dup\" field\n    mask = df['keep_if_dup'] == keep_if_dup\n    # Group the dataset by the \"url\" field\n    groups = df.groupby('url')\n    # For each group, find the last occurrence of the \"url\" field\n    last_occurence = groups.apply(lambda x: x.iloc[-1])\n    # If the \"keep_if_dup\" field is \"Yes\", keep the last occurrence of the \"url\" field\n    if keep_if_dup == 'Yes':\n        return df[mask]\n    # If the \"keep_if_dup\" field is \"No\", remove the last occurrence of the \"url\" field\n    else:\n        return df[~mask]\n",
        "\ndef to_nested_dict(df):\n    result = {}\n    for _, row in df.iterrows():\n        row_dict = row.to_dict()\n        result[row_dict.pop('name')] = {}\n        result[row_dict.pop('v1')] = {}\n        result[row_dict.pop('v1')][row_dict.pop('v2')] = row_dict.pop('v3')\n    return result\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\nresult = to_nested_dict(df)\nprint(result)\n",
        "\ndf['datetime'] = df['datetime'].dt.normalize()\n",
        "\n    df['datetime'] = df['datetime'].dt.normalize()\n    ",
        "\ndf['datetime'] = df['datetime'].dt.normalize()\n",
        "\ndf['datetime'] = df['datetime'].dt.normalize()\n",
        "\nimport ast\ndef extract_key_value_pairs(message):\n    return ast.literal_eval(message)\n",
        "\nfor product in products:\n    result.loc[result['product'] == product, 'score'] = result.loc[result['product'] == product, 'score'] * 10\n",
        "\n# Select the rows with the specific products\ndf_multiply = df.loc[df['product'].isin(products)]\n# Multiply the scores of the selected rows by 10\ndf_multiply['score'] = df_multiply['score'] * 10\n# Combine the original dataframe and the modified dataframe\nresult = df.append(df_multiply)\n",
        "\nproducts = [[1069104, 1069105], [1066489, 1066491]]\n",
        "\ndf.loc[df['product'] == 1069104, 'score'] = 1\ndf.loc[df['product'] == 1069105, 'score'] = 0\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = df\nresult['category'] = [\n    [col for col in df.columns if row[col] == 1]\n    for row in df.to_dict('records')\n]\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n",
        "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n",
        "\ndef convert_date(date):\n    return date.strftime('%d-%b-%Y')\n",
        "\nresult = df.iloc[0, 0] = df.iloc[1, 0]\n",
        "\n# Slice the dataframe into two parts\ndf_1 = df.iloc[:, 0]\ndf_2 = df.iloc[:, 1]\n",
        "\n                 #1     #2\n                 #1     #2\n                 #1     #2\n                 #1     #2\n                 #1     #2\n                 #1     #2",
        "\ndf_shifted = df.shift(1, axis=0)\n",
        "\nfor col in df.columns:\n    df.rename(columns={col: col + 'X'}, inplace=True)\n",
        "\nresult = df.add_prefix('X')\n",
        "\n# Add \"X\" to all column names that don't end with \"X\"\nfor col in df.columns:\n    if not col.endswith('X'):\n        df.rename(columns={col: col + 'X'}, inplace=True)\n",
        "\nval_cols = [col for col in df.columns if 'val' in col]\n",
        "\nval_cols = [col for col in df.columns if 'val' in col]\n",
        "\nimport re\n# Find all columns with '2' in their name\ncols_with_2 = [col for col in df.columns if re.search(r'\\d2$', col)]\n",
        "\nresult = df.loc[row_list, column_list].mean()\nprint(result)\n",
        "\nresult = df.loc[row_list, column_list].sum()\nprint(result)\n",
        "\nresult = df.loc[row_list, column_list].sum(axis=0)\n",
        "\nresult = df.id.value_counts()\nresult['temp'] = df.temp.value_counts()\nresult['name'] = df.name.value_counts()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\nresult = df.isnull().sum()\nprint(result)\n",
        "\nresult = []\nfor col in df.columns:\n    result.append(f\"---- {col} ---\\n{df[col].value_counts()}\\nName: {col}, dtype: int64\")\n",
        "\nresult = df.iloc[1].copy()\n",
        "\nresult = df.iloc[[0, 1]]\n",
        "\ndef rearrange_non_missing_values(x):\n    non_missing_values = x.notnull()\n    non_missing_values_sorted = x[non_missing_values].values.tolist()\n    non_missing_values_sorted.sort()\n    return non_missing_values_sorted + x[~non_missing_values].values.tolist()\ndf.apply(rearrange_non_missing_values, axis=1)\n",
        "\ndef rearrange_row(row):\n    non_missing_values = row[row.notnull()]\n    missing_values = row[row.isnull()]\n    return [non_missing_values, missing_values]\nresult = df.apply(rearrange_row, axis=1)\n",
        "\ndef find_all_missing_index(df):\n    return df.index[df.isnull().all(axis=1)]\n",
        "\ndef sum_smaller_than_thresh(group):\n    return group.loc[group['value'] < thresh].sum()\nresult = df.groupby(df['value'] < thresh).agg(sum_smaller_than_thresh)\nresult.index.name = 'lab'\n",
        "\nthreshold = 6\nrows_to_aggregate = df.loc[df['value'] > threshold]\n",
        "\n# Identify the rows to be averaged\nrows_to_average = df[(df.value < section_left) | (df.value > section_right)]\n# Calculate the average of the identified rows\navg_value = rows_to_average.value.sum() / rows_to_average.value.count()\n# Create a new row with the average value\nnew_row = pd.DataFrame({'lab':['X'], 'value':[avg_value]})\n# Combine the original dataframe and the new row\nresult = df.loc[['B', 'C', 'D']].append(new_row)\n",
        "\ninverse_values = {\n    \"A\": [1/1, 1/2, 1/3],\n    \"B\": [1/4, 1/5, 1/6]\n}\n",
        "\nexisting_columns = list(df.keys())\n",
        "\ninverse_values = {\n    \"A\": [1/1, 1/0, 1/3],\n    \"B\": [1/4, 1/5, 1/6]\n}\n",
        "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n",
        "\nmax_values = df.max(axis=1)\n",
        "\nmax_values = df.max(axis=1)\n",
        "\nmin_dates = df.groupby('user')['dt'].min()\nmax_dates = df.groupby('user')['dt'].max()\n",
        "\nmin_dates = df.groupby('user')['dt'].min()\nmax_dates = df.groupby('user')['dt'].max()\n",
        "\nmin_dates = df.groupby('user')['dt'].min()\nmax_dates = df.groupby('user')['dt'].max()\n",
        "\nmin_dates = df.groupby('user')['dt'].min()\nmax_dates = df.groupby('user')['dt'].max()\n",
        "\nmin_dates = df.groupby('user')['dt'].min()\nmax_dates = df.groupby('user')['dt'].max()\n",
        "\nresult = df.groupby('name').ngroup()\ndf['ID'] = result\n",
        "\nunique_values = df['a'].unique()\nprint(unique_values)\n",
        "\n    result = df.groupby('name').ngroup()\n    result.reset_index(inplace=True)\n    result.rename(columns={'ngroup': 'name'}, inplace=True)\n    ",
        "\ndf['ID'] = df.groupby(['name', 'a']).ngroup()\n",
        "\n# [Missing Code]\n",
        "\nresult = df.melt(id_vars=['user'], value_vars=['01/12/15', '02/12/15', 'someBool'])\nresult = result.sort_values('user')\n",
        "\n# [Missing Code]\n",
        "\ndf_c_greater_than_05 = df.query(\"c > 0.5\")\n",
        "\ndf_filtered = df.query(\"c > 0.45\")\n",
        "\ndf_c_gt_05 = df.loc[df.c > 0.5]\n",
        "\n    result = df.loc[df.c > 0.5, columns]\n    result['sum'] = result.b + result.e\n    return result\n",
        "\n    result = df.loc[df.c > 0.5, columns]\n    ",
        "\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n",
        "\ndf['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n",
        "\ndf['date'] = pd.to_datetime(df['date'])\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\nresult = df.groupby(df.index // 3).sum()\n",
        "\nresult = df.groupby(df.index // 4).sum()\n",
        "\nresult = df.groupby(df.index // 3).mean()\n",
        "\ndef sum_3(x):\n    return x.sum()\ndef avg_2(x):\n    return x.mean()\n",
        "\ndef sum_3(df):\n    return df.iloc[:3].sum()\ndef avg_2(df):\n    return df.iloc[-2:].mean()\n",
        "\ndf = df.ffill()\n",
        "\ndf = df.ffill()\n",
        "\nzeros_indices = [i for i, x in enumerate(df['A']) if x == 0]\n",
        "\ndf['number'] = df['duration'].str.extract(r'\\d+')\n",
        "\ndf['number'] = df['duration'].str.extract(r'\\d+')\n",
        "\nexample_df['number'] = example_df['duration'].str.extract(r'\\d+')\n",
        "\ndf['number'] = df['duration'].str.extract(r'\\d+')\n",
        "\ndef compare_dataframes(df1, df2, columns_check_list):\n    for column in columns_check_list:\n        if not df1[column].values == df2[column].values:\n            return True\n    return False\n",
        "\ndef all_true(lst):\n    return all(lst)\n",
        "\n# Convert the date level to datetime format\ndf.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n",
        "\n# Convert the datetime column to a datetime type\nindex = pd.MultiIndex.from_tuples([('abc', pd.to_datetime('3/1/1994')), ('abc', pd.to_datetime('9/1/1994')), ('abc', pd.to_datetime('3/1/1995'))],\n                                 names=('name', 'datetime'))\ndf = pd.DataFrame({'fee': [100, 90, 80], 'credits':[7, 8, 9]}, index=index)\n",
        "\n    df.index = pd.to_datetime(df.index)\n    df = df.to_numpy()\n    ",
        "\n    df.index = df.index.set_levels(df.index.get_level_values(1), level=1)\n    df.index = df.index.set_levels(pd.to_datetime(df.index.get_level_values(0)), level=0)\n    ",
        "\nvalue_vars = ['var1', 'var2']\nresult = df.melt(id_vars=['Country', 'Variable'], value_vars=value_vars, var_name='year')\n",
        "\nresult = df.melt(id_vars=['Country', 'Variable'], value_vars=['2000', '2001', '2002', '2003', '2004', '2005'], var_name='year')\n",
        "\nvalue_columns = df.filter(like='Value')\n",
        "\ndef check_value_cols(row):\n    for col in row.index:\n        if col.startswith('Value') and abs(row[col]) > 1:\n            return True\n    return False\nresult = df.apply(check_value_cols, axis=1)\nresult = df[result]\n",
        "\nvalue_columns = [col for col in df.columns if 'Value' in col]\n",
        "\nimport re\ndef replace_amp(x):\n    return re.sub(r'&AMP;', '&', x)\ndf = df.apply(replace_amp)\n",
        "\nimport re\ndef replace_lt(x):\n    return re.sub(r'&LT;', '<', x)\ndf.apply(replace_lt)\n",
        "\nimport pandas as pd\nexample_df = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\ndef f(df=example_df):\n    for col in df.columns:\n        df[col] = df[col].str.replace('&AMP;', '&')\n    return df\n",
        "\nimport re\ndef replace_amp(x):\n    return re.sub(r'&AMP;', '&''<''>', x)\ndef replace_lt(x):\n    return re.sub(r'&LT;', '&''<''>', x)\ndef replace_gt(x):\n    return re.sub(r'&GT;', '&''<''>', x)\ndf.apply(lambda x: x.apply(replace_amp))\ndf.apply(lambda x: x.apply(replace_lt))\ndf.apply(lambda x: x.apply(replace_gt))\n",
        "\ndef replace_amp(x):\n    return x.replace('&AMP;', '&')\ndf = df.apply(replace_amp)\n",
        "\ndef split_name(name: str) -> (str, str):\n    first_name, last_name = name.split(' ', 1)\n    return first_name, last_name\n",
        "\ndef split_name(name: str) -> (str, str):\n    pattern = re.compile(r'^(.+)( ){1}(.+)$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return match_obj.group(1), match_obj.group(2)\n    else:\n        return name, None\n",
        "\ndef split_name(name: str) -> list:\n    names = name.split(' ')\n    if len(names) == 1:\n        return [name, None, None]\n    if len(names) == 2:\n        return [names[0], None, names[1]]\n    return [names[0], names[1], names[2]]\n",
        "\nresult = df1.merge(df2, on='Timestamp', how='left')\n",
        "\nresult = df1.merge(df2, on='Timestamp', how='left')\n",
        "\ndef create_state_column(row):\n    if row['col2'] <= 50 and row['col3'] <= 50:\n        return row['col1']\n    else:\n        return max(row['col1'], row['col2'], row['col3'])\n",
        "\ndef create_state(row):\n    if row['col2'] < 50 and row['col3'] < 50:\n        return row['col1']\n    else:\n        return row['col1'] + row['col2'] + row['col3']\ndf['state'] = df.apply(create_state, axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\nerror_values = []\nfor index, row in df.iterrows():\n    if not isinstance(row[\"Field1\"], int):\n        error_values.append(row[\"Field1\"])\nprint(error_values)\n",
        "\ndef is_integer(value):\n    try:\n        int(value)\n        return True\n    except ValueError:\n        return False\n",
        "\n",
        "\nfor col in df.columns[1:]:\n    df[col] = df[col].div(df.sum(axis=1), axis=0).mul(100)\n",
        "\ntotal_val1 = df['val1'].sum()\ntotal_val2 = df['val2'].sum()\ntotal_val3 = df['val3'].sum()\ntotal_val4 = df['val4'].sum()\n",
        "\nimport pandas as pd\nimport io\ndata = io.StringIO(\"\"\"\nrs  alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP7      A/T      0    7      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP15     C/A      0   15      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n\"\"\")\ndf = pd.read_csv(data, delim_whitespace=True).set_index('rs')\ntest = ['TP3', 'TP7', 'TP18']\nresult = df.loc[test]\nprint(result)\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.drop(test)\nprint(result)\n",
        "\n    result = df.loc[test]\n    ",
        "\nimport pandas as pd\nfrom scipy.spatial.distance import pairwise_distance\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Step 1: Calculate the Euclidean distance between each pair of cars.\ndistances = pairwise_distance(df[['x', 'y']].values, metric='euclidean')\n# Step 2: Find the nearest neighbour for each car.\nnearest_neighbours = []\nfor i in range(len(df)):\n    min_distance_index = np.argmin(distances[i])\n    nearest_neighbour = df.iloc[min_distance_index]['car']\n    nearest_neighbours.append(nearest_neighbour)\n# Step 3: Calculate the average distance for each time point.\ndf['nearest_neighbour'] = nearest_neighbours\ndf['euclidean_distance'] = distances\ndf.groupby('time')['euclidean_distance'].mean()\n",
        "\nimport pandas as pd\nfrom scipy.spatial.distance import pairwise_distances\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n# Step 1: Calculate the Euclidean distance between each pair of cars.\ndistances = pairwise_distances(df[['x', 'y']].values, metric='euclidean')\n# Step 2: Find the farthest neighbor for each car.\ndf['farthest_neighbour'] = df['car'].apply(lambda x: np.argsort(distances[x])[-1])\ndf['euclidean_distance'] = df.apply(lambda row: distances[row['car']][row['farthest_neighbour']], axis=1)\n# Step 3: Calculate the average distance for each time point.\ndf_grouped = df.groupby('time')\nresult = df_grouped.mean()\nresult.to_frame('euclidean_distance').reset_index()\nprint(result)\n",
        "\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n",
        "\ndef concat_cols(cols):\n    return \"-\".join(cols)\n",
        "\ndef concatenate_keywords(row):\n    return \"-\".join(filter(None, [row.keywords_0, row.keywords_1, row.keywords_2, row.keywords_3]))\n",
        "\ndef concatenate_keywords(row):\n    keywords = [row[col] for col in ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3'] if row[col] is not np.nan]\n    return '-'.join(keywords)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n# Calculate the total number of rows in the DataFrame.\ntotal_rows = len(df)\n# Determine the number of rows to select for the 20% sample.\nn = int(total_rows * 0.2)\n# Randomly select the 20% of rows using the `df.sample(n)` method.\nsampled_df = df.sample(n, random_state=0)\n# Change the value of the `Quantity` column of the selected rows to zero.\nsampled_df['Quantity'] = 0\n# Keep the indexes of the altered rows.\nresult = sampled_df\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n# Calculate the total number of rows in the DataFrame.\ntotal_rows = len(df)\n# Determine the number of rows to be sampled (20% of the total rows).\nsampled_rows = int(total_rows * 0.2)\n# Randomly select the sampled rows using df.sample(n) with random_state=0.\nsampled_df = df.sample(n=sampled_rows, random_state=0)\n# Change the value of the ProductId column of the sampled rows to zero.\nsampled_df['ProductId'] = 0\n# Keep the indexes of the altered rows.\nresult = df.append(sampled_df)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'UserId': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1, 1, 4, 7, 4, 2, 1, 1, 4, 7],\n                   'Quantity': [6, 1, 3, 2, 7, 2, 6, 1, 3, 2, 7, 2, 6, 1, 3]})\n# Step 1: Calculate the number of rows to select for each user.\nn_rows_per_user = df.groupby('UserId').size() * 0.2\n# Step 2: Select the rows for each user.\nfor user_id, n_rows in n_rows_per_user.items():\n    df_user = df.loc[df['UserId'] == user_id]\n    df_user_sample = df_user.sample(n=n_rows, random_state=0)\n# Step 3: Change the value of the Quantity column for the selected rows.\ndf_user_sample['new_quantity'] = 0\n# Step 4: Combine the original DataFrame with the new column.\ndf.loc[df_user_sample.index, 'Quantity'] = df_user_sample['new_quantity']\n# Step 5: Drop the new_quantity column.\ndf.drop('new_quantity', axis=1, inplace=True)\nresult = df\nprint(result)\n",
        "\nfirst_duplicate_index = df[duplicate_bool].index[0]\nresult = duplicate.copy()\nresult['index_original'] = first_duplicate_index\n",
        "\nlast_duplicate_index = df[duplicate_bool].index.values\nresult = df.loc[duplicate_bool == True].copy()\nresult['index_original'] = last_duplicate_index\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    first_duplicate_index = df.duplicated(subset=['col1','col2'], keep='first')\n    first_duplicate_index = first_duplicate_index[first_duplicate_index == True].index\n    duplicate['index_original'] = first_duplicate_index\n    ",
        "\nimport pandas as pd\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nidx = duplicate.index.values\nduplicate['index_original'] = [idx[0]] * len(duplicate)\nprint(duplicate)\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\n",
        "\nresult = df.groupby(['Sp', 'Mt'])\n",
        "\nimport pandas as pd\n",
        "\nresult = df.groupby(['Sp', 'Mt'])\n",
        "\ndf.groupby(['Sp', 'Value'])\n",
        "\nresult = df.query(f\"Category in {filter_list}\")\n",
        "\ndf.query(\"Category not in @filter_list\")\n",
        "\ntuples = [(level1, level2, level3) for level1 in df.columns.levels[0] for level2 in df.columns.levels[1] for level3 in df.columns.levels[2]]\n",
        "\nvalue_vars = [(level1, level2, level3) for level1 in df.columns.levels[1] for level2 in df.columns.levels[2] for level3 in df.columns.levels[3]]\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\nresult = df.groupby('id')['val'].cumsum()\nresult = result.reset_index()\nresult.columns = ['id', 'val', 'cumsum']\nresult = result.sort_values('id')\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\ndf['cummax'] = df.groupby('id')['val'].cummax()\nprint(df)\n",
        "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf.loc[df['cumsum'] < 0, 'cumsum'] = 0\n",
        "\nimport pandas as pd\nimport numpy as np\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\nresult = df.groupby('l')['v'].agg(np.nansum)\nprint(result)\n",
        "\ndef nan_sum(group):\n    return group.sum(skipna=False)\nresult = df.groupby('r')['v'].apply(nan_sum)\n",
        "\nimport pandas as pd\nimport numpy as np\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\nresult = df.groupby('l')['v'].agg(sum, skipna=False)\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n# Step 1: Find the unique values for each column\nunique_values = {\n    'Column1': df['Column1'].unique(),\n    'Column2': df['Column2'].unique(),\n    'Column3': df['Column3'].unique(),\n    'Column4': df['Column4'].unique(),\n    'Column5': df['Column5'].unique()\n}\n# Step 2: Compare each pair of columns and determine their relationship type\nresult = []\nfor col1, col2 in itertools.combinations(unique_values, 2):\n    if len(col1) == len(col2):\n        if len(col1) == 1:\n            relationship = 'one-to-one'\n        else:\n            relationship = 'many-to-many'\n    else:\n        if len(col1) < len(col2):\n            relationship = 'one-to-many'\n        else:\n            relationship = 'many-to-one'\n    result.append(f'{col1} {col2} {relationship}')\n# Step 3: Output the list of relationship types\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\nresult = []\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 == col2:\n            continue\n        unique_values_col1 = df[col1].unique()\n        unique_values_col2 = df[col2].unique()\n        if len(unique_values_col1) == len(df[col1]) and len(unique_values_col2) == len(df[col2]):\n            relationship = \"one-to-one\"\n        elif len(unique_values_col1) == len(df[col1]) and len(unique_values_col2) != len(df[col2]):\n            relationship = \"one-to-many\"\n        elif len(unique_values_col1) != len(df[col1]) and len(unique_values_col2) == len(df[col2]):\n            relationship = \"many-to-one\"\n        else:\n            relationship = \"many-to-many\"\n        result.append(f\"{col1} {col2} {relationship}\")\nprint(result)\n",
        "\ndef analyze_relationship(column1, column2):\n    unique_values1 = set(column1)\n    unique_values2 = set(column2)\n    frequencies1 = Counter(column1)\n    frequencies2 = Counter(column2)\n    if len(unique_values1) == 1 and len(unique_values2) > 1:\n        return \"one-to-many\"\n    elif len(unique_values1) > 1 and len(unique_values2) == 1:\n        return \"many-to-one\"\n    elif len(unique_values1) > 1 and len(unique_values2) > 1:\n        if frequencies1.most_common(1)[0][1] == len(column1) and frequencies2.most_common(1)[0][1] == len(column2):\n            return \"one-to-one\"\n        else:\n            return \"many-to-many\"\n    else:\n        return \"one-to-one\"\n",
        "\ndef find_relationship(col1, col2):\n    counts1 = col1.value_counts()\n    counts2 = col2.value_counts()\n    if counts1.max() == 1 and counts2.max() == 1:\n        return 'one-2-one'\n    elif counts1.max() > 1 and counts2.max() > 1:\n        return 'many-2-many'\n    elif counts1.max() > 1 and counts2.max() == 1:\n        return 'one-2-many'\n    elif counts1.max() == 1 and counts2.max() > 1:\n        return 'many-2-one'\n",
        "\n# [Missing Code]\n",
        "",
        "\ndf['Group'] = np.where((df['SibSp'] > 0) | (df['Parch'] > 0), 'Has Family',\n                       np.where((df['SibSp'] == 0) & (df['Parch'] == 0), 'No Family', 'None'))\n",
        "\ndf['Group'] = np.where((df['Survived'] > 0) | (df['Parch'] > 0), 'Has Family', 'No Family')\n",
        "\ndf['Group'] = np.select([(df['SibSp'] == 1) & (df['Parch'] == 1), (df['SibSp'] == 0) & (df['Parch'] == 0), (df['SibSp'] == 0) & (df['Parch'] == 1), (df['SibSp'] == 1) & (df['Parch'] == 0)], ['Has Family', 'No Family', 'New Family', 'Old Family'], default='None')\n",
        "\nresult = df.groupby('cokey').sort_values('A', ascending=False)\n",
        "\nresult = df.groupby('cokey').sort_values('A')\n",
        "\nnew_columns = [(x[0], x[1]) for x in df.columns]\n",
        "\nnew_col_names = [('Caps', 'A'), ('Middle', 1), ('Lower', 'a'), ('Lower', 'b'), ('Caps', 'B'), ('Middle', 1), ('Lower', 'a'), ('Lower', 'b')]\nnew_col_index = pd.MultiIndex.from_tuples(new_col_names)\ndf.columns = new_col_index\n",
        "\ntuples = [('A', 'a', '1'), ('A', 'b', '2'), ('B', 'a', '1'), ('B', 'b', '1')]\nmulti_index = pd.MultiIndex.from_tuples(tuples)\n",
        "\nresult = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n",
        "\nresult = df.groupby('a').b.agg(['mean', 'std'])\n",
        "\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\nstdMeann = lambda x: np.std(np.mean(x))\nresult = pd.Series(data.groupby('b').a.apply(stdMeann))\nprint(result)\n",
        "\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x))\ndef min_max_normalization(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\nfor name, group in df.groupby('a'):\n    group['softmax'] = softmax(group['b'])\n    group['min_max'] = min_max_normalization(group['b'])\n",
        "\nfor i in range(df.shape[0]):\n    if all(df.iloc[i] == 0):\n        df.drop(df.index[i], inplace=True)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[-1,-1,0,2],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n# Step 1: Identify the rows and columns with a sum of 0.\nrow_sums = df.sum(axis=1)\ncol_sums = df.sum(axis=0)\n# Step 2: Remove the rows with a sum of 0.\nrows_to_remove = [i for i, x in enumerate(row_sums) if x == 0]\ndf = df.drop(rows_to_remove)\n# Step 3: Remove the columns with a sum of 0.\ncolumns_to_remove = [x for x in col_sums if x == 0]\ndf = df.drop(columns_to_remove, axis=1)\n# Step 4: Combine the remaining rows and columns.\nresult = df\nprint(result)\n",
        "\nrow_max = df.max(axis=1)\nrow_max\n",
        "\n# Find the maximum value in each row\nrow_max = df.max(axis=1)\n# Find the maximum value in each column\ncol_max = df.max(axis=0)\n",
        "\nsorted_indices = sorted(s.index)\n",
        "\nimport pandas as pd\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n              index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n# Sort by index first\ns.sort_index(inplace=True)\n# Sort by value\ns.sort_values(inplace=True)\n",
        "\nresult = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]\nprint(result)\n",
        "\nresult = df.loc[df['A'].apply(lambda x: isinstance(x, str))]\n",
        "\nresult = df.groupby(['Sp', 'Mt'])\n",
        "\nimport pandas as pd\n",
        "\nresult = df.groupby(['Sp', 'Mt'])\n",
        "\ngrouped_df = df.groupby(['Sp', 'Value'])\n",
        "\ndef map_date(group):\n    if group in dict:\n        return dict[group]\n    return np.nan\ndf['Date'] = df['Group'].apply(map_date)\n",
        "\ndef map_dict_to_df(df, dict, column_name):\n    for key, value in dict.items():\n        df.loc[df['Member'] == key, column_name] = value\n    return df\n",
        "\nimport pandas as pd\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\ndef f(dict=example_dict, df=example_df):\n    for key, value in example_dict.items():\n        for index, row in df.iterrows():\n            if key == row['Member']:\n                row['Date'] = value\n    return df\n",
        "\ndef convert_date(date_str):\n    return date_str.replace('/', '-').replace('-', '-').replace('-', '-')\n",
        "\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\ndf1['Count_m'] = df1.groupby('month')['count'].transform('sum')\ndf1['Count_y'] = df1.groupby('year')['count'].transform('sum')\ndf1 = df1.reset_index()\ndf2 = df.groupby('Date').agg({'count'})\ndf2 = df2.reset_index()\nresult = df2.merge(df1, on=['Date', 'year', 'month'], how='left')\nprint(result)\n",
        "\nimport pandas as pd\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').size()\ndf['Count_m'] = df.groupby([df['Date'].dt.month, df['Date'].dt.year]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_w'] = df.groupby([df['Date'].dt.weekday, df['Date'].dt.year]).size()\ndf['Count_Val'] = df.groupby(['Val', df['Date'].dt.year]).size()\n",
        "\ndef count_zero(df, col):\n    return df[df[col] == 0].groupby('Date').size()\ndef count_non_zero(df, col):\n    return df[df[col] != 0].groupby('Date').size()\n",
        "\ndef even_values(df, col):\n    return df[df[col] % 2 == 0][col].sum()\ndef odd_values(df, col):\n    return df[df[col] % 2 == 1][col].sum()\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\nprint(result)\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\nprint(result)",
        "\naggfunc = {\n    'D': np.sum,\n    'E': np.mean\n}\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=aggfunc)\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\ndf = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.max, 'E': np.min})\nprint(result)\n",
        "\nimport pandas as pd\nfrom dask.delayed import delayed\n# Define a function to split the var2 column into multiple rows\ndef split_var2(row):\n    var2_list = row['var2'].split(',')\n    return [{'id': row['id'], 'var1': row['var1'], 'var2': var2} for var2 in var2_list]\n# Create a dask dataframe\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\ndask_df = dd.from_pandas(df, npartitions=2)\n# Apply the split_var2 function to the dask dataframe\nresult = dask_df.apply(split_var2, axis=1)\n# Combine the results into a new dask dataframe\nresult = dd.concat(result, axis=0)\nprint(result)\n",
        "\n# Convert the pandas dataframe to a dask dataframe\ndask_df = from_pandas(df)\n# Apply the `str.split` method to the `var2` column\ndask_df['var2'] = dask_df['var2'].str.split(',')\n# Explode the `var2` column to create multiple rows for each unique value\nresult = dask_df.explode('var2')\n# Convert the dask dataframe back to a pandas dataframe\nresult = result.compute()\n",
        "\nimport pandas as pd\nfrom dask.delayed import delayed\n# Define a function to split the var2 column into multiple rows\ndef split_var2(row):\n    return pd.DataFrame([[row['var1'], x] for x in row['var2'].split('-')], columns=['var1', 'var2'])\n# Create a dask dataframe from the given pandas dataframe\ndf = pd.DataFrame([[\"A\", \"Z-Y\"], [\"B\", \"X\"], [\"C\", \"W-U-V\"]], index=[1, 2, 3], columns=['var1', 'var2'])\ndask_df = dd.from_pandas(df, npartitions=1)\n# Apply the split_var2 function to the dask dataframe\nresult = dask_df.apply(split_var2, axis=1)\n# Combine the results into a new dask dataframe\nresult = result.concat()\nprint(result)\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\ndf['new'] = df.apply(count_special_char, axis=1)\nprint(df)\n",
        "\n# Split the row column into two columns: fips and row\ndf['fips'], df['row'] = df['row'].str.split(expand=True)\n",
        "\n# Split the row column into two columns: fips and row\ndf['fips'] = df['row'].str.split(expand=True)[0]\ndf['row'] = df['row'].str.split(expand=True)[1]\n",
        "\ndf['fips'] = df['row'].str.split(' ', expand=True)[0]\ndf['medi'] = df['row'].str.split(' ', expand=True)[1]\ndf['row'] = df['row'].str.split(' ', expand=True)[2]\n",
        "\ndef cumulative_average_ignoring_zero(row):\n    non_zero_values = [x for x in row if x != 0]\n    if len(non_zero_values) > 0:\n        return sum(non_zero_values) / len(non_zero_values)\n    else:\n        return 0\n",
        "\ndef cumulative_average_ignoring_zero(row):\n    non_zero_values = [x for x in row if x != 0]\n    return sum(non_zero_values) / len(non_zero_values)\n",
        "\ncumulative_sum_df = example_df.copy()\nfor col in example_df.columns[1:]:\n    cumulative_sum_df[col] = example_df[col].cumsum()\n",
        "\ndef cumulative_average_ignoring_zeros(values):\n    non_zero_values = [value for value in values if value != 0]\n    return sum(non_zero_values) / len(non_zero_values)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07'],\n                   'Close': [1460, 1470, 1480, 1450]})\n# Add label column and set the first value to 1\ndf['label'] = 1\n# Calculate the difference between each row for the Close column\ndf['label'] = (df['Close'] - df['Close'].shift(1))\n# Convert the differences to [1-0] values based on positive or negative differences\ndf['label'] = np.where(df['label'] > 0, 1, 0)\nresult = df\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]: To find the time difference between the 1st row departure time and the 2nd row arrival time, we can use the following code:\ndf['Duration'] = df.departure_time.iloc[1] - df.arrival_time.iloc[0]\nresult = df\nprint(result)\n",
        "\n# [Missing Code]: To find the time difference in seconds between the 1st row departure time and the 2nd row arrival time, we can use the following code:\ndf['Duration'] = df.departure_time.iloc[1] - df.arrival_time.iloc[0]\nresult = df\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\ndef count_ones(group):\n    return group[group['key2'] == 'one'].shape[0]\n",
        "\ndef count_two(group):\n    return group[group['key2'] == 'two'].shape[0]\n",
        "\ndef ends_with_e(x):\n    return x.str.endswith('e')\nresult = df.groupby('key1')['key2'].apply(ends_with_e).astype(int).sum()\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\nmin_result = df.index.min()\nmax_result = df.index.max()\n",
        "\nmode_result = df.index.value_counts().index[0]\n",
        "\nresult = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]\n",
        "\nresult = df[~(99 <= df['closing_price'] <= 101)]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"item\": [1, 1, 1, 2, 2, 2, 2, 3, 3],\n                   \"diff\": [2, 1, 3, -1, 1, 4, -6, 0, 2],\n                   \"otherstuff\": [1, 2, 7, 0, 3, 9, 2, 0, 9]})\nresult = df.groupby(\"item\", as_index=False).agg({\"diff\": \"min\", \"otherstuff\": \"first\"})\nprint(result)\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_', n=1).str[0]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.rsplit('_', 1).str[-1]\n",
        "\n    for index, row in df.iterrows():\n        last_index = row['SOURCE_NAME'].rfind('_')\n        if last_index != -1:\n            new_source_name = row['SOURCE_NAME'][:last_index]\n        else:\n            new_source_name = row['SOURCE_NAME']\n        df.at[index, 'SOURCE_NAME'] = new_source_name\n    ",
        "\nnan_count = df['Column_x'].isna().sum()\n",
        "\nnan_count = df['Column_x'].isna().sum()\n",
        "\nnan_count = df['Column_x'].isna().sum()\n",
        "tuples = list(zip(a.values.flatten(), b.values.flatten()))\n",
        "\ndef create_tuples(dataframes):\n    return [(x, *y) for x, y in zip(dataframes[0], dataframes[1:])]\n",
        "\ndef create_tuples(df1, df2):\n    return [(df1.iloc[i, 0], df2.iloc[i, 0]), (df1.iloc[i, 1], df2.iloc[i, 1])]\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\nresult = df.groupby('username')['views'].apply(lambda x: pd.cut(x, bins))\nresult = result.value_counts()\nresult = result.unstack(fill_value=0)\nprint(result)\n",
        "\ndef get_bin(x):\n    for i in range(len(bins) - 1):\n        if bins[i] <= x < bins[i + 1]:\n            return i\n    return len(bins) - 1\n",
        "\ngroups = df.groupby('username')\nresult = groups.apply(lambda x: x.groupby(pd.cut(x.views, bins))['username'].count())\n",
        "\nresult = df.apply(lambda x: ', '.join(x['text']), axis=1)\nprint(result)\n",
        "\nresult = df.text.str.cat(sep='-')\nprint(result)\n",
        "\nresult = df['text'].apply(lambda x: ', '.join(df['text']))\nresult = pd.DataFrame({'text': [result]})\nprint(result)\n",
        "\nresult = df['text'].apply(lambda x: ', '.join(x))\nprint(result)\n",
        "\nresult = df.text.apply(lambda x: '-'.join(x))\nresult = pd.Series(result, name='text')\nprint(result)\n",
        "\nresult = df1.merge(df2, on='id', how='left')\n",
        "\nresult = df1.merge(df2, on='id', how='left')\n",
        "\nresult = df1.merge(df2, on='id', how='left')\n",
        "\nC.update(D)\n",
        "\nresult = C.copy()\nresult.update(D)\n",
        "\nresult = C.merge(D, how='left', on='A')\nresult['dulplicated'] = result.apply(lambda row: row['A'] in D['A'], axis=1)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n# Sort the DataFrame by 'time' and 'amount' in ascending order\ndf = df.sort_values(['time', 'amount'])\n# Groupby 'user' and apply the list function\nresult = df.groupby('user')[[ 'time', 'amount' ]].apply(list)\nprint(result)\n",
        "\ndf.sort_values(by=['user', 'time'], ascending=True)\n",
        "\ndf.sort_values(by=['user', 'time'])\n",
        "\nresult = series.to_frame().transpose()\n",
        "\nimport pandas as pd\nimport numpy as np\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\ndf = series.to_frame()\ndf = df.reset_index()\n",
        "\nresult = df.columns[df.columns.str.contains(s)]\n",
        "\nresult = df.loc[:, df.columns.str.contains(s)]\n",
        "\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n# Step 1: Identify the columns containing the string 'spike'\ncontains_spike = df.columns.str.contains(s)\n",
        "\ndf = df.explode('codes')\n",
        "\n# [Missing Code]\n",
        "\ndf['codes'] = df['codes'].astype(str)\n",
        "\nresult = df.col1.sum()\n",
        "\nresult = ''\nfor index, row in df.iterrows():\n    reversed_list = row['User IDs'][::-1]\n    result += ','.join(str(x) for x in reversed_list)\n",
        "\nresult = ','.join(itertools.chain.from_iterable(df['col1']))\n",
        "\n",
        "\nresult = df.groupby(df['Time'] // 3).sum()\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\n",
        "\ndf['TIME'] = pd.to_datetime(df['TIME'])\n",
        "\nresult = df.loc[filt]\n",
        "\nresult = df.loc[filt]\nprint(result)\n     c\nresult = df.loc[filt]\nprint(result)\n     c",
        "\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n",
        "\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n",
        "\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\n",
        "\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\ndf.where(np.random.randint(0,2, df.shape).astype(bool), np.nan, inplace=True)\ndef equalp(x, y):\n    return (x == y) or (math.isnan(x) and math.isnan(y))\ndifferences = []\nfor col in df.columns:\n    if not equalp(df.iloc[0][col], df.iloc[8][col]):\n        differences.append((df.iloc[0][col], df.iloc[8][col]))\nprint(differences)\n",
        "\nts = pd.Series(df['Value'].values, index=df['Date'])\n",
        "\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n# Add new columns with suffixed names\nfor col in df.columns:\n    new_col = col + '_' + str(df.index.max() + 1)\n    df[new_col] = df[col]\n# Concatenate the rows\nresult = df.iloc[[0]] + df.iloc[[1]] + df.iloc[[2]]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\ndef round_dogs(x):\n    if not pd.isna(x):\n        return round(x, 2)\n    else:\n        return x\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, .03), (.21, .18),(pd.NA, .18)],\n                  columns=['dogs', 'cats'])\ndf['dogs'] = df['dogs'].apply(round_dogs)\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\ndef round_if_not_na(value, decimals=2):\n    if not isinstance(value, pd.NA):\n        return round(value, decimals)\n    return value\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\ndf['dogs'] = df['dogs'].apply(round_if_not_na)\ndf['cats'] = df['cats'].apply(round_if_not_na)\nresult = df\nprint(result)\n",
        "\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nresult = df.sort_index(axis=0, level='time')\n",
        "\nresult = df.sort_values(['VIM', 'time'], ascending=[True, True])\n",
        "\nimport pandas as pd\nhd1_from = '2020-02-17'\nhd1_till = '2020-02-18'\nhd1_from_dt = pd.to_datetime(hd1_from)\nhd1_till_dt = pd.to_datetime(hd1_till)\ndates_to_remove = [hd1_from_dt, hd1_till_dt]\n",
        "\nimport pandas as pd\n",
        "\nfor i in range(len(corr)):\n    for j in range(len(corr)):\n        if corr.iloc[i, j] > 0.3:\n            print(f\"{i} {j} {corr.iloc[i, j]}\")\n",
        "\nresult = corr.where(corr > 0.3)\n",
        "\nresult = df.iloc[:, -1].rename('Test')\n",
        "\ndf.rename(columns={df.columns[0]: 'Test'}, inplace=True)\n",
        "\ndef find_frequent_value(row):\n    counts = [row.count(0), row.count(1)]\n    frequent_value = 0 if counts[0] > counts[1] else 1\n    freq_count = counts[0] if counts[0] > counts[1] else counts[1]\n    return frequent_value, freq_count\n",
        "\ndef find_frequent_value(row):\n    counts = [row.count(x) for x in row]\n    max_count = max(counts)\n    frequent_value = [x for x in row if row.count(x) == max_count][0]\n    return frequent_value, max_count\n",
        "\ndef find_frequent_values(row):\n    frequent_values = []\n    freq_count = {}\n    for value in row:\n        if value not in frequent_values:\n            frequent_values.append(value)\n        if value in freq_count:\n            freq_count[value] += 1\n        else:\n            freq_count[value] = 1\n    return frequent_values, freq_count\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\nresult = df.groupby([\"id1\",\"id2\"])[[\"foo\",\"bar\"]].mean()\nprint(result)\n",
        "\nimport pandas as pd\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n# Replace NULL values with 0\ndf.loc[df.bar.isnull(), 'bar'] = 0\n# Group by id1 and id2, and calculate the mean of foo and bar\nresult = df.groupby([\"id1\",\"id2\"])[[\"foo\",\"bar\"]].mean()\nprint(result)\n",
        "\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'a_col']]\n",
        "\nresult = df_a.merge(df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'b_col']]\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "\n# [Missing Code]\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\n# Identify the nan values\nnan_indices = np.isnan(x)\n# Remove the nan values from the array\nx = np.delete(x, np.where(nan_indices), axis=1)\n",
        "\nmax_value = np.max(a)\n",
        "\nmax_value = np.max(a)\n",
        "\nsmallest_element_index = np.argmin(a)\nlargest_element_index = np.argmax(a)\n",
        "\na_sorted = np.sort(a)\n",
        "\nsmallest = np.min(a)\nlargest = np.max(a)\n",
        "\nresult = a[int(p/100 * len(a))]\n",
        "\n",
        "\nnrow = 3\nncol = 6 / nrow\nB = A.reshape(nrow, ncol)\nprint(B)\n",
        "\nB = A.reshape(2, 3)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = A.reshape(2, 4)\nprint(B)\n",
        "\nresult = np.roll(a, shift)\n",
        "\nresult = np.roll(a, shift, axis=0)\n",
        "\nimport numpy as np\ndef shift_array(a, shift):\n    # Step 1: Define the function\n    def shift_row(row):\n        # Step 4: Shift the elements in each row\n        shifted_row = np.roll(row, shift[i], axis=0)\n        # Step 5: Fill the missing values\n        shifted_row[0:shift[i]] = np.nan\n        return shifted_row\n    # Step 2: Iterate through the rows\n    with np.nditer(a, flags=['multi_index']) as it:\n        for i in range(a.shape[0]):\n            # Step 3: Shift the elements in each row\n            it[i] = shift_row(it[i])\n    # Step 6: Combine the rows\n    return np.vstack(it)\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n\t\t[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = [-2, 3]\nresult = shift_array(a, shift)\nprint(result)\n",
        "\nnp.random.seed(42)\n",
        "\nlargest_value = np.max(a)\n",
        "\nsmallest_value = np.min(a)\n",
        "\nmax_value = np.max(a)\n",
        "\nmax_value = np.max(a)\n",
        "\nmax_value = np.max(a)\n",
        "\nlargest_value = np.max(a)\n",
        "\nz = any(np.isnan(a), axis=0)\n",
        "\n# [Missing Code]: To find the rows with NaN, we can use the isnan function.\n# [Missing Code]: We will create a mask that will be True for rows with NaN and False otherwise.\nmask = np.isnan(a).any(axis=1)\n# [Missing Code]: Now, we will use the mask to select the rows without NaN.\na = a[~mask]\n# [Missing Code]: Finally, we will print the resulting array.\nprint(a)\n",
        "\nresult = np.array(a)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nmin_value = np.min(a)\nindex = np.argmin(a)\nrow_index = index // a.shape[1]\ncolumn_index = index % a.shape[1]\nresult = (row_index, column_index)\nprint(result)\n",
        "\nmax_value = np.max(a)\n",
        "\nresult = np.stack((np.argmin(a, axis=0), np.argmin(a, axis=1)), axis=1)\nprint(result)\n",
        "\nimport numpy as np\ndegree = 90\nresult = np.sin(np.deg2rad(degree))\nprint(result)\n",
        "\nimport numpy as np\ndegree = 90\nRadians = degree * (\u03c0/180)\ncos_radians = np.cos(Radians)\ncos_degrees = cos_radians * (180/\u03c0)\nprint(cos_degrees)\n",
        "\ndef compare_sine_values(number):\n    # Convert the number to radians\n    radians = (number * np.pi) / 180\n    # Calculate the sine values for both degrees and radians\n    sin_degree = np.sin(number * np.pi / 180)\n    sin_radian = np.sin(radians)\n    # Compare the sine values\n    if sin_degree > sin_radian:\n        return 0  # Degree\n    else:\n        return 1  # Radian\n",
        "\nimport numpy as np\nvalue = 1.0\nresult = np.arcsin(value)\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.pad(A, (0, length - A.shape[0]))\nprint(result)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = np.pad(A, (0, length - A.shape[0]))\nprint(result)\n",
        "\na_power = np.power(a, power)\n",
        "\nimport numpy as np\ndef f(a, power):\n    return np.power(a, power)\nexample_a = np.arange(4).reshape(2, 2)\nresult = f(example_a, 2)\nprint(result)\n",
        "\nprint((7, 3))\nprint((7, 3))",
        "\n    gcd = np.gcd(numerator, denominator)\n    result = (numerator // gcd, denominator // gcd)\n    ",
        "\n    # [Missing Code]\n    ",
        "\nresult = np.average([a, b, c], axis=0)\n",
        "\nresult = np.maximum(a, b)\nresult = np.maximum(result, c)\n",
        "\ntop_right_diagonal_indices = [(i, 4 - i) for i in range(5)]\n",
        "\n# [Missing Code]\n",
        "\nshape = a.shape\n",
        "\nindices = [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n",
        "\nfor row in X:\n    for col in row:\n        result.append(col)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\ndef f(X):\n    result = []\n    for row in X:\n        for col in row:\n            result.append(col)\n    return result\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\ndef fortran_order(X):\n    result = []\n    for row_index in range(X.shape[0]):\n        for column_index in range(X.shape[1]):\n            result.append(X[row_index, column_index])\n    return result\nresult = fortran_order(X)\nprint(result)\n",
        "\nresult = np.array(list(mystr))\n",
        "\n# Multiply the col-th column of the array by the given number (5.2).\na[:, col] *= multiply_number\n# Calculate the cumulative sum of the numbers in that column.\nresult = np.cumsum(a[:, col])\n",
        "\n# Multiply the row-th row of the array by the given number (5.2).\na[row] = a[row] * multiply_number\n# Calculate the cumulative sum of the numbers in that row.\nresult = np.cumsum(a[row])\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nrow_length = a.shape[0]\n",
        "\na_mean = np.mean(a)\nb_mean = np.mean(b)\n",
        "\na_mean = np.mean(a)\nb_mean = np.mean(b)\n",
        "\npooled_variance = [(anobs - 1) * avar + (bnobs - 1) * bvar] / (anobs + bnobs - 2)\npooled_variance = [(40 - 1) * 0.954 + (50 - 1) * 11.87] / (40 + 50 - 2)\npooled_variance = [39 * 0.954 + 49 * 11.87] / 88\npooled_variance = 37.146 + 585.526 / 88\npooled_variance = 622.672 / 88\nSE = sqrt(pooled_variance)\nt = (amean - bmean) / SE\nt = (-0.0896 - 0.719) / 2.655\np_value = 2 * scipy.stats.t.cdf(-2.726, 88 - 2)\n",
        "\nA = np.asarray([[1, 1, 1], [1, 1, 2], [1, 1, 3], [1, 1, 4]])\nB = np.asarray([[0, 0, 0], [1, 0, 2], [1, 0, 3], [1, 0, 4], [1, 1, 0], [1, 1, 1], [1, 1, 4]])\n",
        "\nA_not_in_B = np.isin(A, B, invert=True)\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsums = np.sum(a, axis=2)\nprint(\"sums:\", sums)\n",
        "\na = np.delete(a, 2, 1)\n",
        "\na = np.delete(a, 2, axis=0)\n",
        "\na = np.delete(a, [0, 2], axis=1)\n",
        "\n# Find the range of valid column indices\nvalid_indices = np.arange(a.shape[1])\n# Remove out-of-bound indices from del_col\ndel_col = np.delete(del_col, np.where(np.logical_or(del_col < 0, del_col >= a.shape[1])))\n# Delete the valid indices from the array\nresult = np.delete(a, del_col, axis=1)\n",
        "\nimport numpy as np\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\na.insert(pos, element)\nprint(a)\n",
        "\nimport numpy as np\na = np.array([[1,2],[3,4]])\npos = 1\nelement = [3,5]\na.insert(pos, element)\nprint(a)\n",
        "\nnumpy.insert(array, position, values)\n",
        "\nfor i in range(len(pos)):\n    np.insert(a, pos[i], element[i], axis=0)\n",
        "\nresult = copy.deepcopy(array_of_arrays)\n",
        "\nresult = np.all(np.equal(a, a[0]), axis = 0)\n",
        "\nresult = np.all(a == a[:, 0], axis = 1)\n",
        "\n    first_row = a[0]\n    mask = np.all(a == first_row, axis=1)\n    result = np.all(mask)\n    ",
        "\ndef f(x, y):\n    return (np.cos(x))**4 + (np.sin(y))**2\n",
        "\ndef f(x, y):\n    return (np.cos(x))**4 + (np.sin(y))**2\n",
        "\nimport numpy as np\ndef ecdf(x):\n    # Sort the input array in ascending order\n    x_sorted = np.sort(x)\n    # Normalize the sorted array to sum to 1\n    x_sorted = x_sorted / np.sum(x_sorted)\n    # Calculate the cumulative sum of the normalized sorted array\n    return np.cumsum(x_sorted)\n",
        "\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n",
        "\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n",
        "\n# Generate 900 random indices for the 1s\nnp.random.choice(size, size=900, replace=False, p=None)\n# Set the selected indices to 1\nnums[np.random.choice(size, size=900, replace=False, p=None)] = 1\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = tf.ones([2,3,4])\na_np = a.numpy()\nprint(a_np)\n",
        "\nimport tensorflow as tf\nimport numpy as np\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n",
        "\na_sorted = a.sort()\n",
        "\nimport numpy as np\na = np.array([4, 1, 0, 8, 5, 2])\nresult = np.argsort(a)\n",
        "\nsorted_a = a.sort()\nbiggest_elements = sorted_a[:N]\n",
        "\nimport numpy as np\nA = np.arange(16).reshape(4, 4)\nn = 5\nresult = np.power(A, n)\nprint(result)\n",
        "\ndef extract_patches(a, patch_size):\n    patches = []\n    for i in range(0, a.shape[0] - patch_size + 1, patch_size):\n        for j in range(0, a.shape[1] - patch_size + 1, patch_size):\n            patch = np.array_slice(a, (i, j), (i + patch_size, j + patch_size))\n            patches.append(patch)\n    return np.concatenate(patches, axis=2)\n",
        "\ndef extract_patches(a, patch_size):\n    result = []\n    for i in range(0, a.shape[0] - patch_size + 1, patch_size):\n        for j in range(0, a.shape[1] - patch_size + 1, patch_size):\n            patch = np.array_slice(a, (i, j), (i + patch_size, j + patch_size))\n            result.append(patch)\n    return result\n",
        "\ndef extract_patches(a, patch_size):\n    patches = []\n    for i in range(0, a.shape[0] - patch_size + 1, patch_size):\n        for j in range(0, a.shape[1] - patch_size + 1, patch_size):\n            patch = np.array_slice(a, (i, j), (i + patch_size, j + patch_size))\n            patches.append(patch)\n    return np.concatenate(patches, axis=2)\n",
        "\n    # [Missing Code]\n    ",
        "\nresult = a.reshape(4, 12)\n",
        "\n    # [Missing Code]\n    ",
        "\nresult = a[:, low:high]\n",
        "\nimport numpy as np\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 0\nhigh = 2\nresult = a[low:high+1]\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(string)\nprint(a)\n",
        "\nresult = np.log(np.random.uniform(min, max, n))\n",
        "\n# Generate a uniform distribution between min and max\nuniform_dist = np.random.uniform(min, max, n)\n",
        "\n    result = np.random.uniform(min, max, n)\n    result = np.log(result)\n    ",
        "\ndef calculate_B(A, a, b, t):\n    if t == 0:\n        return a * A[0]\n    else:\n        return a * A[t] + b * calculate_B(A, a, b, t - 1)\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nB = [calculate_B(A, a, b, t) for t in range(len(A))]\nprint(B)\n",
        "\ndef calculate_b(a, b, c, A, t):\n    if t == 0:\n        return a * A[0]\n    elif t == 1:\n        return a * A[1] + b * calculate_b(a, b, c, A, t - 1)\n    else:\n        return a * A[t] + b * calculate_b(a, b, c, A, t - 1) + c * calculate_b(a, b, c, A, t - 2)\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nc = 4\nB = [calculate_b(a, b, c, A, t) for t in range(10)]\nprint(B)\n",
        "\nresult = np.array([])\n",
        "\nresult = np.zeros((3, 0))\n",
        "\nimport numpy as np\nimport numpy.lib.index_tricks as index_tricks\ndims = (3, 4, 2)\nindex = (1, 0, 1)\nresult = index_tricks.sub2ind(dims, index)\nprint(result)\n",
        "\nlinear_index = (subscript_1 * dimension_1) + (subscript_2 * dimension_2) + (subscript_3 * dimension_3)\nlinear_index = (1 * 3) + (0 * 4) + (1 * 2)\nlinear_index = 3 + 0 + 2\nlinear_index = 5\nprint(linear_index)\n",
        "\nvalues = np.zeros((2,3), dtype='int32,float32,float32')\n",
        "\nunique_indices = np.unique(accmap)\n",
        "\nimport numpy as np\ncounts = np.bincount(index)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.zeros(len(accmap))\nfor i in range(len(accmap)):\n    index = accmap[i]\n    if index >= 0:\n        result[i] = a[index].sum()\n    else:\n        result[i] = a[-index].sum()\nprint(result)\n",
        "\nunique_indices = np.unique(index)\n",
        "\nfor i in range(len(x)):\n    for j in range(len(x[i])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n",
        "\n# Convert the list of tuples into a numpy array\nlista_elegir_array = np.array(lista_elegir)\n# Use the np.random.choice function with the given probabilities\nresult = np.random.choice(lista_elegir_array, samples, probabilities=probabilit)\n",
        "\nresult = np.pad(a, ((low_index, high_index - a.shape[0]), (low_index, high_index - a.shape[1])))\n",
        "\n# [Missing Code]\n",
        "\n# Identify real numbers\nreal_mask = np.abs(x.imag) < 1e-6\n# Apply the mask to the array\nresult = x[real_mask]\n",
        "\n# Reshape the data array into bins of the given size\ndata_reshaped = np.reshape(data, (len(data) // bin_size, bin_size))\n# Calculate the mean of each bin\nbin_data_mean = np.mean(data_reshaped, axis=1)\n",
        "\nbin_data = np.array_split(data, len(data) // bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n",
        "\n# [Missing Code]\n",
        "\nbin_data = [data[-(i * bin_size) - 1:-(i * bin_size)] for i in range(number_of_bins)]\nbin_data_mean = [np.mean(bin) for bin in bin_data]\nprint(bin_data_mean)\nprint(bin_data_mean)\n[3.3333333333333335, 4.666666666666664, 5.333333333333334, 6.666666666666668]\nprint(bin_data_mean)",
        "\n# Find the length of the data array\ndata_length = data.shape[0]\n# Calculate the number of bins\nnum_bins = int(data_length / bin_size)\n# Create the bins\nbin_data = np.array([data[-(i * bin_size):-(i * bin_size) + bin_size] for i in range(num_bins)])\n# Calculate the mean of each bin\nbin_data_mean = np.array([np.mean(bin, axis=0) for bin in bin_data])\n",
        "\ndata_shape = data.shape\nnum_bins_row = np.ceil(data_shape[1] / bin_size).astype(int)\n",
        "\nimport numpy as np\ndef smoothclamp(x, x_min, x_max):\n    if x < x_min:\n        return x_min\n    if x > x_max:\n        return x_max\n    else:\n        return (4 * x**3 - 6 * x**2 * x_min - 2 * x**2 * x_max) / (6 * (x_max - x_min))\nx = 0.25\nx_min = 0\nx_max = 1\nresult = smoothclamp(x, x_min, x_max)\nprint(result)\n",
        "\nimport numpy as np\ndef smoothclamp(x, N=5):\n    if x_min <= x <= x_max:\n        return x\n    elif x < x_min:\n        return x**N\n    else:\n        return (1 - x)**N\nx = 0.25\nx_min = 0\nx_max = 1\nN = 5\nresult = smoothclamp(x, N=N)\nprint(result)\n",
        "\ndef circular_correlation(a, b):\n    a_len = len(a)\n    b_len = len(b)\n    result = np.zeros(a_len + b_len - 1)\n    for i in range(a_len + b_len - 1):\n        for j in range(a_len):\n            result[i] += a[j] * b[i - j]\n    return result\n",
        "\ndf_flat = df.unstack()\n",
        "\nresult = df.unstack('major').unstack('timestamp').unstack('columns')\n",
        "\ndef int_to_binary_array(num, m):\n    binary_array = np.zeros(m, dtype=np.uint8)\n    for i in range(m):\n        if num & (1 << i):\n            binary_array[i] = 1\n    return binary_array\n",
        "\ndef int_to_binary_array(num, m):\n    binary_array = np.unpackbits(np.uint8(num))\n    return np.pad(binary_array, (0, m - len(binary_array)), 'constant')\n",
        "\ndef convert_to_binary_array(num, m):\n    binary_string = bin(num)[2:].zfill(m)\n    return np.array(list(binary_string), dtype=np.uint8)\nresult = np.array([convert_to_binary_array(num, m) for num in a])\n",
        "\nmean = np.mean(a)\n",
        "\nmean = np.mean(a)\n",
        "\nmean = np.mean(a)\n",
        "\nmean = np.mean(a)\nprint(\"Mean (\u03bc):\", mean)\n",
        "\nimport numpy as np\nDataArray = np.arange(-5.5, 10.5)\nmasked_data = np.ma.masked_where(DataArray < 0, DataArray)\n",
        "\n# [Missing Code]\n",
        "\nfor row in zero_rows:\n    a[row] = 0\nfor col in zero_cols:\n    a[:, col] = 0\n",
        "\na[1, :] = 0\na[:, 0] = 0\n",
        "\n# For each row in the array, compare each element to the maximum value along axis 1.\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        if a[i, j] == max_values[i]:\n            mask[i, j] = True\n",
        "\nmin_values = np.min(a, axis=1)\nprint(min_values)\n",
        "\nimport numpy as np\npost = np.array([2, 5, 6, 10])\ndistance = np.array([50, 100, 500, 1000])\n",
        "\nX = np.random.randint(2, 10, (5, 6))\n",
        "\nxi = np.sqrt(Y[:, :, i])\nX = np.concatenate(xi, axis=2)\nprint(X)\nprint(X)",
        "\nimport numpy as np\na = np.array([9, 2, 7, 0])\nnumber = 0\nis_contained = number in a\nprint(is_contained)\n",
        "\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n",
        "\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n",
        "\n# Find the indices of the elements in A that are in the range (1, 4) or (4, 8)\nidx1 = np.logical_and(A >= 1, A < 4)\nidx2 = np.logical_and(A >= 4, A < 8)\n",
        "\nindices = np.argsort(a)[::-1]\nresult = rankdata(a, method='min')[indices]\n",
        "\nimport numpy as np\nfrom scipy.stats import rankdata\na = [1,2,3,4,3,2,3,4]\n",
        "\n    ranks = rankdata(a)\n    max_rank = len(a)\n    result = max_rank - ranks\n    ",
        "\n[[ 0, -1, -2],\n [ 1,  0, -1],\n [ 2,  1,  0]]\n[[ 0,  1, -2],\n [-1,  0,  1],\n [-2,  1,  0]]\ndists = np.empty((3, 3, 2))\ndists = np.dstack((x_dists, y_dists))\nprint(dists)\n[[[ 0,  0], [-1,  1], [-2, -2]],\n [ [ 1, -1], [ 0,  0], [-1,  1]],\n [ [ 2, -2], [ 1,  1], [ 0,  0]]]\ndists = np.dstack((x_dists, y_dists))\nprint(dists)\nprint(dists)\n[[[ 0,  0], [-1,  1], [-2, -2]],\n [ [ 1, -1], [ 0,  0], [-1,  1]],\n [ [ 2, -2], [ 1,  1], [ 0,  0]]]",
        "\nimport numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\nx_dists_1D = x_dists.reshape(-1)\ny_dists_1D = y_dists.reshape(-1)\ndists = np.repeat(x_dists_1D, 2, axis=1)\ndists = np.repeat(dists, 2, axis=0)\ndists = np.concatenate((dists, y_dists), axis=1)\nprint(dists)\n",
        "\nresult = np.zeros((5, len(second), len(third)))\nfor i in range(5):\n    for j in range(len(second)):\n        for k in range(len(third)):\n            result[i, j, k] = A[i][second[j]][third[k]]\n",
        "\narr = np.zeros((20, 10, 10, 2))\n",
        "\nx = np.array([LA.norm(v,ord=1) for v in X])\n",
        "\n# [Missing Code]\n",
        "\n# Normalize each row with L\u221e norm\nx = np.array([LA.norm(v, ord=np.inf) for v in X])\nresult = X / np.tile(x, (5, 1)).T\n",
        "\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 'foo', 'bar']})\ntarget = 'f'\nchoices = ['XX']\nconditions = [df['a'].str.contains(target)]\nchoices = [choices]\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n",
        "\nresult = pdist(a, metric='euclidean')\n",
        "\ndists = pdist(a)\nresult = squareform(dists)\n",
        "\ndistances = pdist(a)\nresult = np.reshape(distances, (a.shape[0], a.shape[0]))\n",
        "\nNA = np.asarray([float(x) for x in A])\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nB = [x if x != 'inf' else 0 for x in A]\nAVG = np.mean(B, axis=0)\nprint(AVG)\n",
        "\nA_converted = []\nfor i in A:\n    A_converted.append(float(i))\nA = A_converted\n",
        "\ndef remove_adjacent_duplicates(arr):\n    for i in range(1, len(arr) - 1):\n        if arr[i] == arr[i - 1] and arr[i] != 0:\n            arr[i] = 0\n    return np.nonzero(arr)[0]\n",
        "\ndef remove_adjacent_duplicates(a):\n    for i in range(1, len(a) - 1):\n        if a[i - 1] == a[i] and a[i] != 0:\n            a[i] = 0\n        if a[i] == a[i + 1] and a[i] != 0:\n            a[i] = 0\n    return np.delete(a, np.where(a == 0))\n",
        "\nimport numpy as np\nlat = np.array([[10, 20, 30],\n               [20, 11, 33],\n               [21, 20, 10]])\nlon = np.array([[100, 102, 103],\n               [105, 101, 102],\n               [100, 102, 103]])\nval = np.array([[17, 2, 11],\n               [86, 84, 1],\n               [9, 5, 10]])\nconcatenated_array = np.hstack((lat, lon, val))\n",
        "\n",
        "\nlat_df = pd.DataFrame(lat, columns=['lat'])\nlon_df = pd.DataFrame(lon, columns=['lon'])\nval_df = pd.DataFrame(val, columns=['val'])\n",
        "\n    # [Missing Code]\n    ",
        "\n    # [Missing Code]\n    ",
        "\n# Remove the infinity value from the array\na_without_inf = np.delete(a, np.argwhere(np.isinf(a)))\n# Compute the mean of the remaining values\nmean_without_inf = np.mean(a_without_inf)\n# Add the infinity value back to the result\nresult = np.array([mean_without_inf, np.inf + 0j])\n",
        "\n    infinity_index = np.argmax(np.abs(a))\n    ",
        "\nresult = Z[:, :, :-1]\n",
        "\nresult = a.squeeze(axis=-1)\n",
        "\ndef is_member(c, CNTS):\n    for i in range(len(CNTS)):\n        if np.all(c == CNTS[i]):\n            return True\n    return False\n",
        "\ndef array_equal(a, b):\n    return np.isnan(a) and np.isnan(b) or np.array_equal(a, b)\n",
        "\ndef f(x, y):\n    return x + y\n",
        "\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n",
        "\ni = np.diag(i.A.flatten())\n",
        "\nimport numpy as np\na = np.array([[1,0,2,3],[0,5,3,4],[2,3,2,10],[3,4, 10, 7]])\n# Step 1: Identify the non-diagonal elements\n# Step 2: Create a mask for the non-diagonal elements\n# Step 3: Apply the mask to set the non-diagonal elements to 0\na[~np.eye(a.shape[0]).reshape(a.shape)] = 0\nprint(a)\n",
        "\nimport numpy as np\nimport pandas as pd\nstart = \"23-FEB-2015 23:09:19.445506\"\nend = \"24-FEB-2015 01:09:22.404973\"\nn = 50\n",
        "\nimport numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nresult = -1\nfor i in range(len(x)):\n    if x[i] == a and y[i] == b:\n        result = i\n        break\nprint(result)\n",
        "\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\n",
        "\ndef f(x, a, b, c):\n    return a * x ** 2 + b * x + c\n",
        "\nx = np.array([-1, 2, 5, 100])\ny = np.array([123, 456, 789, 1255])\ncoefficients = np.array([a, b, c, d])\ny = np.dot(x, coefficients)\ncoefficients = np.linalg.solve(x, y)\nprint(coefficients)\ncoefficients = np.array([a, b, c, d])\ny = np.dot(x, coefficients)\ncoefficients = np.linalg.solve(x, y)\nprint(coefficients)",
        "\nimport numpy as np\nimport pandas as pd\ntemp_arr = [0, 1, 2, 3]\na = np.arange(4)\ndf = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\ndef subtract_from_row(row, index):\n    return row - temp_arr[index]\ndf.apply(subtract_from_row, axis=1, args=(a,))\n",
        "\nresult = np.einsum('ijk,jl->ilk', A, B)\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n",
        "\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n",
        "\nscaler = MinMaxScaler()\n",
        "\nmask1 = arr < -10\n",
        "\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\n",
        "\ndifference = np.abs(s1 - s2)\n",
        "\ndifference = np.abs(s1 - s2)\n",
        "\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\n",
        "\ndef check_for_nan(arrays):\n    for array in arrays:\n        if not np.isnan(array).any():\n            return False\n    return True\n",
        "\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), 'constant', constant_values=0)\n",
        "\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), 'constant', constant_values=0)\n",
        "\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), 'constant', constant_values=element)\n",
        "\n    result = np.pad(arr, ((0, 93 - arr.shape[0]), (0, 13 - arr.shape[1])), 'constant', constant_values=0)\n    ",
        "\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), 'constant', constant_values=0)\n",
        "\na = np.arange(12)\n",
        "\nimport numpy as np\na = np.array( \n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\nb = np.array( \n    [[0, 1, 1],\n    [1, 0, 1],\n    [1, 1, 0]]\n)\n",
        "\nb_reshaped = b.reshape(3, 3, 1)\nresult = a[np.arange(3), np.arange(3), b_reshaped]\n",
        "\nresult = np.take_along_axis(a, b, axis=2)\n",
        "\nimport numpy as np\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\nresult = 0\nfor i in range(b.shape[0]):\n    for j in range(b.shape[1]):\n        result += a[i, j, b[i, j]]\nprint(result)\n",
        "\ndef sum_unindexed(a, b):\n    sum = 0\n    for i in range(a.shape[0]):\n        for j in range(a.shape[1]):\n            for t in range(a.shape[2]):\n                if b[i, j] != t:\n                    sum += a[i, j, t]\n    return sum\n",
        "\n# Create a list of booleans by applying the comparison operation to each element in the array x\nbools = [1 < x <= 4 for x in df['a'].values]\n# Create a list of corresponding values from the array y (column b) using the list of booleans\nresult = [y if bool else np.nan for y, bool in zip(df['b'].values, bools)]\n",
        "\nrow_starts = np.nonzero(im)[0]\nrow_ends = np.nonzero(im[::-1])[0][::-1]\n",
        "\n# [Missing Code]\n",
        "\n",
        "\nnon_zero_columns = []\nfor col in range(im.shape[1]):\n    if np.any(im[:, col]):\n        non_zero_columns.append(col)\n"
    ],
    "Matplotlib": [
        "\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(['x-y'])\n",
        "\n# Add minor ticks to the y axis\nplt.gca().yaxis.set_minor_locator(mticker.AutoMinorLocator())\n",
        "\n# Set the minor locator for the x-axis\nplt.xaxis.set_minor_locator(mticker.AutoMinorLocator())\n# Set the minor locator for the y-axis\nplt.yaxis.set_minor_locator(mticker.AutoMinorLocator())\n",
        "\n# Add minor ticks to the x-axis\nplt.gca().xaxis.set_minor_locator(mticker.AutoMinorLocator())\n",
        "\n",
        "\n",
        "\nplt.plot(x, y, marker='d', markersize=2, linewidth=1)\n",
        "\nplt.plot(x, y, marker='d', markersize=10, linewidth=3)\n",
        "\nsns.set_style(\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n# set the y axis limit to be 0 to 40\nax.set_ylim(0, 40)\n",
        "\nplt.axvspan(2, 4, color='r', alpha=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n# Create a pandas DataFrame with x and y values\ndf = pd.DataFrame({'x': x, 'y': y})\n# Plot the line graph using seaborn\nsns.lineplot(data=df, x='x', y='y')\n",
        "\nplt.plot(x, y, marker='+', linewidth=7)\n",
        "\nplt.legend(fontsize=20)\nplt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20)\n",
        "\nplt.legend(title=\"xyz\")\nplt.title(\"title\", fontsize=20)\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30)\n# make the border of the markers solid black\nl.set_markeredgecolor(\"black\")\nl.set_markeredgewidth(2)\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30, color='r')\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels clockwise by 45 degrees\nplt.xticks(rotation=45)\n",
        "\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n# rotate the x axis labels counter clockwise by 45 degrees\nplt.xticks(rotation=45)\n",
        "\nplt.xticks([0, 2, 4, 6, 8])\n",
        "\nx = np.random.randn(10)\ny = np.random.randn(10)\nsns.distplot(x, label=\"a\", color=\"0.25\")\nsns.distplot(y, label=\"b\", color=\"0.25\")\n# add legends\nplt.legend()\n",
        "\nplt.imshow(H)\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.xlabel('X')\nplt.xlabel('X', rotation=90, ha='right')\n",
        "\ndf = sns.load_dataset(\"planets\")\ng = sns.boxplot(x=\"method\", y=\"orbital_period\", data=df)\n# rotate the x axis labels by 90 degrees\ng.axes.tick_params(axis='x', rotation=90)\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nplt.plot(x, y)\n# Break the long title into multiple lines\nmyTitle = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\nmyTitle = [line + \"\\n\" for line in myTitle.split(\"-\")]\n# Set the title using plt.suptitle()\nplt.suptitle(\"\".join(myTitle), fontsize=14)\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n# make the y axis go upside down\nplt.gca().invert_yaxis()\nplt.plot(x, y)\nplt.show()\n",
        "\nplt.xticks([0, 1.5])\n",
        "\nplt.yticks([-1, 1])\n",
        "\nplt.scatter(x, x, alpha=0.7, label='x')\nplt.scatter(y, y, alpha=0.5, label='y')\nplt.scatter(z, z, alpha=0.3, label='z')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n",
        "\nplt.scatter(x, y, edgecolor='black', facecolor='blue')\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n# make all axes ticks integers\nplt.xticks(np.arange(10))\nplt.yticks(np.arange(2, 4, 1))\nplt.plot(x, y)\nplt.show()\n",
        "\nax = plt.gca()\nax.ticklabel_format(style='plain', axis='y')\n",
        "\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nax = sns.lineplot(x=x, y=y, dashes=True)\n",
        "\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.plot(x, y1)\nplt.xlabel('x')\nplt.ylabel('y1')\nplt.title('x vs y1')\nplt.subplot(122)\nplt.plot(x, y2)\nplt.xlabel('x')\nplt.ylabel('y2')\nplt.title('x vs y2')\nplt.tight_layout()\nplt.show()\n",
        "\n",
        "\nplt.gca().set_xlabel('')\n",
        "\n",
        "\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n# show xticks and vertical grid at x positions 3 and 4\nplt.xticks([3, 4])\nplt.grid(True, which='major', axis='x', linestyle='-')\n",
        "\nplt.yticks([3, 4])\nplt.grid(True, axis='y', linestyle='-', linewidth=0.5, color='gray')\n",
        "\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n# show yticks and horizontal grid at y positions 3 and 4\nplt.yticks([3, 4])\nplt.grid(True, axis='y')\n# show xticks and vertical grid at x positions 1 and 2\nplt.xticks([1, 2])\nplt.grid(True, axis='x')\n",
        "\n",
        "\nplt.legend(loc='lower right')\n",
        "\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "\nx = np.arange(10)\ny = np.arange(10, 20)\nz = np.arange(10)\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\nplt.show()\n",
        "\n# Add a new axis on the top of the plot\nax_top = fig.add_axes([0, 0, 1, 1], frameon=False)\n# Set the new axis as the x-axis\nax_top.xaxis.tick_top()\nax_top.xaxis.set_label_position(\"top\")\n# Set the original x-axis as the bottom axis\nax.xaxis.tick_bottom()\nax.xaxis.set_label_position(\"bottom\")\n# Swap the row and column labels\nax_top.set_xticklabels(row_labels, rotation=90)\nax.set_yticklabels(row_labels, rotation=0)\n",
        "\nplt.plot(x, y)\nplt.xlabel('X', x=20)\n",
        "\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x\nplt.plot(x, y)\n# move the y axis ticks to the right\nplt.gca().tick_params(axis='y', which='both', right=True)\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and label y axis \"Y\"\nplt.plot(x, y)\nplt.ylabel(\"Y\")\n# Show y axis ticks on the left and y axis label on the right\nplt.tick_params(axis='y', labelleft='off')\nplt.ylabel_right(\"Y\")\nplt.show()\n",
        "\n",
        "\n",
        "\n# Create a linear regression model for the relationship between total_bill and tip\nmodel = sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', size=8)\n# Add a regression line to the plot\nmodel.plot_joint(sns.lineplot, x='total_bill', y='tip', data=tips, estimator=np.mean, ci=None, color='r', linewidth=2)\n# Add a 95% confidence interval for the regression line\nmodel.plot_joint(sns.lineplot, x='total_bill', y='tip', data=tips, estimator=np.mean, ci=95, color='r', linewidth=2)\n# Add a 95% prediction interval for the regression line\nmodel.plot_joint(sns.lineplot, x='total_bill', y='tip', data=tips, estimator=np.mean, ci=None, color='r', linewidth=2, scatter_kws={'alpha': 0.2})\n",
        "\n# Plot the data using a bar plot\nplt.bar(df['celltype'], df['s1'])\nplt.bar(df['celltype'], df['s2'], color='red')\n# Use the 'celltype' column as the x-axis labels\nplt.xlabel('celltype')\n# Make the x-axis tick labels horizontal\nplt.setp(plt.gca().get_xticklabels(), rotation=90)\n",
        "\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n",
        "\nplt.show()\n",
        "\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.tick_params(axis='both', which='major', labelsize=10)\nplt.xticks(rotation=90)\n",
        "\n# x-values where the vertical lines should be drawn\nx_values = [0.22058956, 0.33088437, 2.20589566]\n# y-values for the vertical lines (since they are vertical, the y-values don't matter)\ny_values = [0, 1, 2]\n# draw the vertical lines\nplt.vlines(x_values, y_values[0], y_values[1])\n# show the plot\nplt.show()\n",
        "\nplt.show()\n",
        "\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n",
        "\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y')\nplt.subplot(1, 2, 2)\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y')\nplt.tight_layout()\nplt.show()\n",
        "\nsns.scatterplot(\n    data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30,\n)\n",
        "\nplt.scatter(b, a)\nplt.xlabel('b')\nplt.ylabel('a')\nfor i in range(len(a)):\n    plt.annotate(c[i], (b[i], a[i]))\nplt.show()\n",
        "\nplt.plot(x, y)\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x')\nplt.legend(['y over x'], loc='upper left')\nplt.legend.set_title('Legend')\nplt.legend.set_title('Legend', prop={'weight': 'bold'})\nplt.show()\n",
        "\n",
        "\n# First, we need to define the width of the second subplot. Let's call it \"w\".\nw = 1\n# Now, we can calculate the width of the first subplot, which is three times the width of the second subplot.\nfirst_subplot_width = 3 * w\n# Next, we need to find the ratio of the widths of the two subplots.\nratio = first_subplot_width / w\n# Now, we can create the two subplots with the given widths and the same height.\nplt.subplot(1, 2, 1, width=first_subplot_width, height=w)\nplt.subplot(1, 2, 2, width=w, height=w)\n",
        "\nplt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n",
        "\n# First, we need to define the bins for the histograms. Let's use 10 bins for both x and y.\nbins = np.linspace(0, 1, 10)\n# Now, we will create the histograms for x and y using the `hist` function.\nhist_x = plt.hist(x, bins=bins, label='x')\nhist_y = plt.hist(y, bins=bins, label='y')\n# To make the histograms grouped, we need to set the `stacked` parameter to True.\nplt.hist(x, bins=bins, label='x', stacked=True)\nplt.hist(y, bins=bins, label='y', stacked=True)\n# Finally, we will add a title and a legend to the plot.\nplt.title('Grouped Histograms of x and y')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\n",
        "\nplt.show()\n",
        "\n# Create two colormaps using x and y arrays\ncmap_x = plt.cm.get_cmap('viridis', 10)\ncmap_y = plt.cm.get_cmap('plasma', 10)\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n# Plot the colormaps in each subplot\nim1 = ax1.imshow(x, cmap=cmap_x)\nim2 = ax2.imshow(y, cmap=cmap_y)\n# Add a single colorbar to the figure\ncbar = fig.colorbar(im1, ax=ax1, ax2=ax2)\ncbar.set_label('Colorbar for x and y')\n",
        "\nplt.figure()\nplt.plot(x[:, 0], label=\"a\")\nplt.plot(x[:, 1], label=\"b\")\nplt.xlabel(\"x-axis\")\nplt.ylabel(\"y-axis\")\nplt.legend()\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\n",
        "\npoints = [(3, 5), (5, 10), (10, 150)]\ndf = pd.DataFrame(points, columns=['x', 'y'])\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# plot y over x\nplt.plot(x, y)\n# set font size for title, xlabel, and ylabel\nplt.title('My Plot', fontsize=20)\nplt.xlabel('X Axis', fontsize=18)\nplt.ylabel('Y Axis', fontsize=16)\n# show the plot\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nf = plt.figure()\nax = f.add_subplot(111)\n# plot y over x\nax.plot(x, y)\n# set the tick labels for the x-axis\nax.set_xticks(np.arange(1, 11))\nax.set_xticklabels(np.arange(1, 11))\n# set the tick labels for the y-axis\nax.set_yticks(np.arange(1, 11))\nax.set_yticklabels(np.arange(1, 11))\nplt.show()\n",
        "\nfor line, color in zip(lines, c):\n    plt.plot(*line, color=color)\nplt.show()\n",
        "\nplt.loglog(x, y)\n",
        "\n",
        "\ndata = [1000, 1000, 5000, 3000, 4000, 16000, 2000]\n# Make a histogram of data\nn, bins, patches = plt.hist(data, 5, range=(min(data), max(data)))\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x in a line plot\nplt.plot(x, y)\n# Show markers on the line plot\nplt.scatter(x, y, alpha=0.5)\n# Make the markers have a 0.5 transparency\nplt.scatter(x, y, alpha=0.5)\n# Keep the lines solid\nplt.plot(x, y)\nplt.show()\n",
        "\n",
        "\n# Load the penguins dataset\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n# Create two subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=False)\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=ax1)\nax1.set_title(\"Bill Depth vs Bill Length\")\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=ax2)\nax2.set_title(\"Flipper Length vs Bill Length\")\n",
        "\n# get the current x axis tick labels\ncurrent_labels = plt.xticks()[1]\n# change the second label to \"second\"\ncurrent_labels[1] = \"second\"\n# set the new tick labels\nplt.xticks(range(1, 10), current_labels)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x\n# Show legend and use the greek letter lambda as the legend label\nplt.plot(x, y, label='\u03bb')\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.title('Y over X')\nplt.legend()\nplt.show()\n",
        "\nplt.xticks(np.arange(0, 10, 2), [2.1, 3, 7.6])\n",
        "\nplt.xticks(rotation=-60, ha='left')\n",
        "\nplt.tick_params(axis='y', labelrotation=-60)\nplt.tick_params(axis='x', labeltop=True)\n",
        "\nx = np.arange(2010, 2020)\ny = np.arange(10)\nplt.plot(x, y)\n# Set the transparency of xtick labels to be 0.5\nplt.xticks(alpha=0.5)\n",
        "\n# Set the xaxis tick labels to the range of x values\nplt.xticks(x)\n# Set the yaxis tick labels to the range of y values\nplt.yticks(y)\n",
        "\n# Set the yaxis tick labels to be the same as the y values\nplt.yticks(np.arange(10))\n# Set the xaxis tick labels to be the same as the x values\nplt.xticks(np.arange(10))\n",
        "\nplt.figure(figsize=(10, 5))\nplt.subplot(2, 1, 1)\nplt.plot(x, y)\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.title('Subplot 1')\nplt.subplot(2, 1, 2)\nplt.plot(x, y)\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.title('Subplot 2')\nplt.suptitle('Figure')\nplt.show()\n",
        "\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Make a scatter plot with x and y\nplt.scatter(x, y)\n# Use vertical line hatch for the marker\nplt.scatter(x, y, marker='|')\n# Make the hatch dense\nplt.scatter(x, y, marker='|', s=100)\n",
        "\nplt.scatter(x, y)\n",
        "\nplt.scatter(x, y, marker='*', hatch='*')\n",
        "\nplt.scatter(x, y, s=100, marker='*', hatch='/')\n",
        "\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n",
        "\nx = np.linspace(0.1, 2 * np.pi, 41)\ny = np.exp(np.sin(x))\n# make a stem plot of y over x and set the orientation to be horizontal\nplt.stem(x, y, use_line_collection=True, line_kwargs={'color': 'black'}, marker_kwargs={'color': 'black'})\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Stem Plot of y over x with Horizontal Orientation')\nplt.show()\n",
        "\n# Step 1: Make a bar plot using data in `d`.\n# We need to create a list of the keys (x-axis labels) and a list of the values (bar heights).\nkeys = list(d.keys())\nvalues = list(d.values())\n# Step 2: Use the keys as x axis labels and the values as the bar heights.\n# We can use the `plt.bar()` function to create the bar plot.\nplt.bar(keys, values)\n# Step 3: Color each bar in the plot by looking up the color in `c`.\n# We need to create a list of the colors for each bar.\ncolors = [c[key] for key in keys]\n# Now, we can set the colors for each bar using the `plt.bar()` function.\nplt.bar(keys, values, color=colors)\n# Finally, we can show the plot.\nplt.show()\n",
        "\n# First, create a figure and an axes object\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\n# Next, draw the vertical line at x=3\nax.axvline(3, color='black', linestyle='-', label='cutoff')\n# Finally, show the legend\nplt.legend()\nplt.show()\n",
        "\n# Create a polar plot\nplt.figure(figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\nax = plt.subplot(111, projection='polar')\n# Plot the bars\nax.bar(labels, height, color='r', linewidth=0)\n# Add the labels to the bars\nax.set_thetagrids(np.arange(0, 2, 1))\nax.set_rgrids(np.arange(0, 5, 1))\nax.set_title('Polar Bar Plot')\nax.set_ylabel('Height')\nax.set_xlabel('Labels')\n",
        "\nplt.pie(data, labels=l, wedgeprops={'width': 0.4})\nplt.show()\n",
        "\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x\nplt.plot(x, y)\n# Show blue dashed grid lines\nplt.grid(color='blue', linestyle='dashed')\n# Show the plot\nplt.show()\n",
        "\n# Plot y over x\nplt.plot(x, y)\n# Turn minor ticks on and show gray dashed minor grid lines\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='dashed', color='gray')\n# Do not show any major grid lines\nplt.grid(which='major', linestyle='', color='')\n",
        "\n",
        "\n",
        "\n",
        "\nax = plt.gca()\nax.axvline(55, color=\"green\")\n",
        "\n# Create a list of x-axis positions for the bars\nx_positions = np.arange(len(blue_bar))\n# Plot the blue bars\nplt.bar(x_positions, blue_bar, align='edge', color='blue')\n# Plot the orange bars\nplt.bar(x_positions + 0.5, orange_bar, align='edge', color='orange')\n",
        "\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.subplot(122)\n",
        "\nplt.scatter(x, y, cmap='Spectral', vmin=0, vmax=1)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter Plot with Spectral Colormap')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(np.arange(10), np.arange(10))\n",
        "\n# Create a list of species to use as the x-axis labels for each subplot\nspecies_list = df[\"species\"].unique()\n# Create a list of subplot titles using the species names\nsubplot_titles = [\"Bill Length by Sex for \" + species for species in species_list]\n# Use factorplot to create the barplots with the specified parameters\nsns.factorplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, kind=\"bar\", col=species_list, sharey=False, subplot_titles=subplot_titles)\n",
        "\nplt.circle((0.5, 0.5), radius=0.2)\n",
        "\n",
        "\n# Plot y over x with a legend of \"Line\"\nplt.plot(x, y)\nplt.legend([\"Line\"])\n# Adjust the spacing between legend markers and labels to be 0.1\nplt.gca().legend.markerscale = 0.1\n",
        "\nplt.plot(x, y)\n",
        "\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y, label=\"Line\")\nplt.plot(y, x, label=\"Flipped\")\n# Show a two columns legend of this plot\nplt.legend(ncol=2)\n",
        "\nplt.legend()\n",
        "\ndata = np.random.random((10, 10))\n# plot the 2d matrix data with a colorbar\nplt.imshow(data, cmap='viridis', interpolation='none')\nplt.colorbar()\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('2D Matrix Plot')\nplt.show()\n",
        "\nplt.plot(x, y)\n",
        "\n",
        "\nplt.plot(y, x)\nplt.gca().invert_xaxis()\n",
        "\n",
        "\n",
        "\n",
        "\nhist_x = np.histogram(x, bins=5, range=(0, 10))\n",
        "\nplt.plot(x, y)\nplt.fill_between(x, y - error, y + error, alpha=0.2)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot with Shaded Error Region')\nplt.show()\n",
        "\n# draw x=0 axis (y-axis)\nplt.axvline(x=0, color='white')\n# draw y=0 axis (x-axis)\nplt.axhline(y=0, color='white')\n",
        "\nbox_position, box_height, box_errors = np.arange(4), np.ones(4), np.arange(1, 5)\nc = [\"r\", \"r\", \"b\", \"b\"]\nfig, ax = plt.subplots()\nax.bar(box_position, box_height, color=\"yellow\")\n# Plot error bars with errors specified in box_errors. Use colors in c to color the error bars\nfor i in range(len(box_position)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], color=c[i])\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\n",
        "\nplt.figure(figsize=(5, 5))\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "\nplt.show()\n",
        "\n",
        "\n",
        "\nplt.show()\n",
        "\nplt.plot(x, y)\n",
        "\ndf = sns.load_dataset(\"exercise\")\n# Make catplots of scatter plots by using \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\n",
        "\n",
        "\n",
        "\nplt.show()\n",
        "\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.title('Plot of Y over X')\nplt.show()\n",
        "\nplt.plot(x, y, label='y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n",
        "\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.xlabel('Time (t)')\nplt.ylabel('Amplitude')\nplt.title('Sine and Cosine Waves')\nplt.legend()\nplt.show()\n",
        "\n",
        "\n",
        "\nplt.show()\n",
        "\nplt.plot(x, y)\n",
        "\nx = np.random.random((10, 10))\nfrom matplotlib import gridspec\nnrow = 2\nncol = 2\nfig = plt.figure(figsize=(ncol + 1, nrow + 1))\n# Make a 2x2 subplots with fig and plot x in each subplot as an image\ngs = gridspec.GridSpec(nrow, ncol)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x)\n# Remove the space between each subplot and make the subplot adjacent to each other\nplt.subplots_adjust(wspace=0, hspace=0)\n# Remove the axis ticks from each subplot\nfor ax in fig.axes:\n    ax.xaxis.set_ticks_position('none')\n    ax.yaxis.set_ticks_position('none')\nplt.show()\n"
    ],
    "Tensorflow": [
        "\nimport tensorflow as tf\nx = tf.Variable(0)\nx = 1\nresult = x\n",
        "\nx.assign(114514)\n",
        "\ndef create_tensor(labels, num_classes):\n    result = tf.zeros((len(labels), num_classes), dtype=tf.int32)\n    for i, label in enumerate(labels):\n        result[i, label] = 1\n    return result\n",
        "\nimport tensorflow as tf\ndef create_tensor(labels):\n    tensor_shape = (len(labels), 10)\n    result = tf.zeros(tensor_shape, dtype=tf.int32)\n    for i, label in enumerate(labels):\n        position = i * 10 + label\n        result[i, position] = 1\n    return result\nlabels = [0, 6, 5, 4, 2]\nresult = create_tensor(labels)\nprint(result)\n",
        "\nimport tensorflow as tf\ndef create_tensor(labels):\n    empty_tensor = tf.zeros((len(labels), 10), dtype=tf.int32)\n    for i, label in enumerate(labels):\n        new_tensor = tf.zeros((1, 10), dtype=tf.int32)\n        new_tensor[0, label] = 1\n        empty_tensor = tf.add(empty_tensor, new_tensor)\n    return empty_tensor\nlabels = [0, 6, 5, 4, 2]\nresult = create_tensor(labels)\nprint(result)\n",
        "\ndef f(labels):\n    result = tf.zeros(labels.shape, dtype=tf.int32)\n",
        "\nimport tensorflow as tf\ndef create_tensor(labels):\n    tensor_shape = (len(labels), 10)\n    result = tf.zeros(tensor_shape, dtype=tf.int32)\n    for label in labels:\n        one_hot_label = tf.one_hot(label, 10)\n        result = result + tf.multiply(one_hot_label, result)\n    return result\nlabels = [0, 6, 5, 4, 2]\nresult = create_tensor(labels)\nprint(result)\n",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [i, i+1, i+2]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n",
        "\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ninput = [10, 20, 30]\ndef my_map_func(i):\n  return [i, i+1, i+2]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n",
        "\nones = tf.ones([8], dtype=tf.int32)\n",
        "\nones = tf.ones([8, 8])\n",
        "\nlengths = [4, 3, 5, 2]\nmax_length = 8\n",
        "\nimport tensorflow as tf\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    result = tf.sequence_mask(lengths, maxlen=8)\n    return result\n",
        "\nlengths = tf.constant([4, 3, 5, 2])\nresult = tf.zeros((4, 8))\n",
        "\nimport tensorflow as tf\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\nresult = tf.expand_dims(a, -1) * tf.expand_dims(b, 0)\nprint(result)\n",
        "\nimport tensorflow as tf\nexample_a = tf.constant([1,2,3])\nexample_b = tf.constant([4,5,6,7])\ndef f(a=example_a,b=example_b):\n    result = tf.cartesian_product(a, b)\n    return result\n",
        "\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\nresult = a[:, :, :, tf.newaxis]\n",
        "\nresult = tf.expand_dims(a, 0)\nresult = tf.expand_dims(result, -1)\nprint(result)\nresult = tf.expand_dims(a, 0)\nresult = tf.expand_dims(result, -1)\nprint(result)",
        "\nresult = tf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nimport tensorflow as tf\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\nresult = tf.math.reciprocal(A)\nprint(result)\n",
        "\n# Subtract the corresponding elements of the two tensors (A and B).\ndiff = tf.subtract(a, b)\nprint(diff)\n# Square the result of the subtraction.\nsquared_diff = tf.square(diff)\nprint(squared_diff)\n# Sum the squared results along the rows.\nresult = tf.reduce_sum(squared_diff, axis=1)\nprint(result)\n",
        "\n# Subtract the corresponding elements of the two tensors (A and B).\ndiff = tf.subtract(a, b)\nprint(diff)\n# Square the result of the subtraction.\nsquared_diff = tf.square(diff)\nprint(squared_diff)\n# Sum the squared results along the rows.\nrow_sums = tf.reduce_sum(squared_diff, axis=1)\nprint(row_sums)\n# Take the square root of the sum.\nresult = tf.sqrt(row_sums)\nprint(result)\n",
        "\n    difference = tf.subtract(A, B)\n    squared_difference = tf.square(difference)\n    result = tf.reduce_sum(squared_difference, axis=1)\n    ",
        "\nresult = tf.gather_nd(x, tf.stack([y, z], axis=1))\n",
        "\nresult = tf.gather(x, indices=row, axis=0)\nresult = tf.gather(result, indices=col, axis=1)\n",
        "\n    indices = tf.stack([example_y, example_z], axis=1)\n    result = tf.gather_nd(x, indices)\n    ",
        "\nresult = tf.einsum('bns,bns->bbn', A, B)\n",
        "\nresult = tf.einsum('bns,bnk->bnn', A, B)\n",
        "\n# [Missing Code]: To convert the list of bytes into a list of strings using Tensorflow, follow these steps:\nx_tensor = tf.constant(x)\ndecoded_tensor = tf.io.decode_raw(x_tensor, tf.uint8)\nwith tf.compat.v1.Session() as sess:\n    result = sess.run(decoded_tensor)\nprint(result)\n",
        "\nimport tensorflow as tf\nexample_x=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\ndef f(x=example_x):\n    result = [tf.decode_raw(x_i, out_type=tf.string) for x_i in x]\n    return result\n",
        "\nimport tensorflow as tf\nx = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nx = tf.convert_to_tensor(x, dtype=tf.float32)\n# Create a mask for the non-zero entries in the features dimension.\nmask = tf.not_equal(x, 0)\n# Calculate the average of the non-zero entries.\nresult = tf.reduce_sum(x * mask) / tf.reduce_sum(mask)\nprint(result)\n",
        "\nresult = tf.math.reduce_variance(x, axis=2, keep_dims=True)\n",
        "\n    non_zero_count = tf.count_nonzero(x, axis=3)\n    sum_of_features = tf.reduce_sum(x, axis=3)\n    result = tf.divide(sum_of_features, non_zero_count)\n    ",
        "\n# [Missing Code]\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(scores, axis=1)\n",
        "\nmin_values = tf.math.reduce_min(a, axis=0)\n",
        "\nmodel.save(\"export/1\")\n",
        "\nimport tensorflow as tf\n",
        "\nimport tensorflow as tf\n",
        "\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform_int(shape=(10), minval=1, maxval=4, dtype=tf.int32)\n    ",
        "\nimport tensorflow as tf\nresult = tf.__version__\nprint(result)\n"
    ],
    "Scipy": [
        "\ny = A * log(x) + B\ny = A * (log(x) - log(1)) + B\nresult = np.polyfit(x, y, 1)\ny = A * log(x) + B\ny = A * (log(x) - log(1)) + B\nresult = np.polyfit(x, y, 1)",
        "\nresult = np.polyfit(x, y, 1)\nresult = np.polyfit(x, y, 1)",
        "\nimport numpy as np\nimport scipy.optimize\ndef exp_func(x, A, B, C):\n    return A * np.exp(B * x) + C\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\nresult = scipy.optimize.curve_fit(exp_func, x, y, p0)\nprint(result)\n",
        "\nstatistic, p_value = stats.kstest(x, y)\n",
        "\ntest_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\n",
        "\ninitial_guess = [-1, 0, -3]\nresult = optimize.minimize(f, initial_guess, bounds=None, method='SLSQP', tol=1e-6)\nprint(result)\n",
        "\np_values = [scipy.stats.zprob(z) for z in z_scores]\n",
        "\np_values = [scipy.stats.norm.sf(z) for z in z_scores]\n",
        "\nimport numpy as np\nimport scipy.stats\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\nz_scores = []\nfor p_value in p_values:\n    complement = 1 - p_value\n    z_score = scipy.stats.zscore(complement)\n    z_scores.append(z_score)\n",
        "\nimport numpy as np\nfrom scipy import stats\nx = 25\nmu = 1.744\nstddev = 2.0785\nresult = stats.lognorm.cdf(x, mu, stddev)\nprint(result)\n",
        "\nM = exp(mu)\nmu = 1.744\nstddev = 2.0785\nM = exp(1.744)\nprint(E(X), M)\nprint(6.382, 5.735)\n",
        "\n# Convert sparse matrices to dense matrices\na = sa.toarray()\nb = sb.toarray()\n# Perform matrix multiplication\nresult = a * b\n# Convert the result back to a sparse matrix\nresult = sparse.csr_matrix(result)\n",
        "\n    sB_csr = sparse.csr_matrix(sB)\n    result = sA * sB_csr\n    ",
        "\n# Create the interpolator\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n# Interpolate the new point\nresult = interpolator(request)\n# Output the result\nprint(result)\n",
        "\n# Create the x, y, z, and V arrays\nx = points[:, 0]\ny = points[:, 1]\nz = points[:, 2]\nV = V\n# Create the LinearNDInterpolator",
        "\nangle = np.random.randint(1, 360)\nangle_radians = np.deg2rad(angle)\n",
        "\nresult = M.diagonal()\n",
        "\nimport numpy as np\nfrom scipy import stats\n",
        "\nimport numpy as np\nfrom scipy import stats\n",
        "\nimport numpy as np\nfrom scipy import stats\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\nFeature = [c1, c2]\nFeature = sparse.hstack(Feature)\n",
        "\nFeature = sparse.hstack([c1, c2])\n",
        "\ndef euclidean_distance(p1, p2):\n    return np.linalg.norm(p1 - p2)\ndistance_matrix = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        distance_matrix[i, j] = euclidean_distance(points1[i], points2[j])\n",
        "\ndef euclidean_distance(p1, p2):\n    return np.linalg.norm(p1 - p2)\ndistance_matrix = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        distance_matrix[i, j] = euclidean_distance(points1[i], points2[j])\n",
        "\nimport numpy as np\n# Create a mask to select the non-diagonal elements\nmask = np.zeros((b.shape[0], b.shape[1]), dtype=np.bool)\nmask[np.arange(b.shape[0]), np.arange(b.shape[1])] = False\n# Create the new sparse matrix with the non-diagonal elements\nnew_b = sparse.coo_matrix((b.data[mask], (b.row[mask], b.col[mask])), shape=b.shape)\n",
        "\nthreshold = 0.75\n",
        "\nfrom scipy import ndimage\n# Set up the label function\nlabel_func = ndimage.label(img < threshold, structure=np.ones((3, 3)))\n",
        "\ndef f(img = example_img):\n    threshold = 0.75\n    binary_img = np.zeros(img.shape, dtype=np.uint8)\n    binary_img[img > threshold] = 1\n",
        "\nimport numpy as np\nfrom scipy import ndimage\n# Step 1: Find the regions of cells which value exceeds a given threshold (0.75).\nthresholded_img = img > threshold\n# Step 2: Dilate the regions to ensure they are connected.\ndilated_img = ndimage.morphology.binary_dilation(thresholded_img)\n# Step 3: Calculate the center of mass for each region.\ncom_coords = ndimage.measurements.center_of_mass(dilated_img)\n# Step 4: Calculate the distance between the center of mass of each region and the top left corner (0,0).\ndistances = [np.sqrt((x - 0)**2 + (y - 0)**2) for x, y in com_coords]\n# Step 5: Output the distances as a list.\nresult = distances\nprint(result)\n",
        "\nfor i in range(M.shape[0]):\n    for j in range(M.shape[1]):\n        if M[i, j] and not M[j, i]:\n            M[j, i] = M[i, j]\n",
        "\n    for i in range(sA.shape[0]):\n        for j in range(sA.shape[1]):\n            if sA[i, j] is not None and sA[j, i] is None:\n                sA[j, i] = sA[i, j]\n    ",
        "\ndef find_connected_components(binary_array):\n    labels, num_labels = scipy.ndimage.label(binary_array)\n    return num_labels\n",
        "\n    # [Missing Code]\n    ",
        "\nmean = 0\nnum_nonzero = 0\nfor i in range(col.shape[0]):\n    if col[i] != 0:\n        mean += col[i]\n        num_nonzero += 1\nmean = mean / num_nonzero\n",
        "\nMax = col.max()\nMin = col.min()\n",
        "\nnon_zero_values = [x for x in col.nonzero()]\n",
        "\ndef fourier_series(degree, x, a):\n    f = 0\n    for i in range(1, degree + 1):\n        f += a[i - 1] * np.cos(1 * np.pi / tau * i * x)\n    return f\n",
        "\nimport numpy as np\nimport scipy.spatial.distance\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\nexample_",
        "\ndef manhattan_distance(x1, y1, x2, y2):\n    return abs(x1 - x2) + abs(y1 - y2)\n",
        "\n    # Convert the raster array to a list of coordinates\n    coords = [(x, y) for x in range(example_array.shape[0]) for y in range(example_array.shape[1]) if example_array[x, y] != 0]\n    # Calculate the Euclidean distances between all pairs of regions\n    distances = scipy.spatial.distance.cdist(coords, coords, metric='euclidean')\n    # Convert the distances back to metres (assuming raster resolution is 1)\n    distances *= 1\n    # Output the result as a N*N array\n    return distances\n# Example usage\nresult = f(example_arr)\nprint(result)\n",
        "\n",
        "\ncombined_datasets = [x1, x2, x3, x4]\n",
        "\nimport numpy as np\nimport scipy.stats as ss\n",
        "\ndef tau1(x):\n    y = np.array(df['A'])  # Keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n",
        "\nresult = len(sa.nonzero()[0]) == 0\n",
        "\ndef is_lil_matrix_empty(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n",
        "\nimport numpy as np\nfrom scipy.linalg import block_diag\nnp.random.seed(10)\na = np.random.rand(100, 2, 2)\nresult = block_diag(*a, axis=0)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy import stats\nnp.random.seed(10)\npre_course_scores = np.random.randn(10)\nduring_course_scores = np.random.randn(10)\nranksum_result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = ranksum_result.pvalue\nprint(p_value)\n",
        "\nimport numpy as np\nfrom scipy import stats\nexample_pre_course_scores = np.random.randn(10)\nexample_during_course_scores = np.random.randn(10)\ndef f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n    ranksum_result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = ranksum_result.pvalue\n    return p_value\n",
        "\nimport numpy as np\ndef kurtosis_without_bias_correction(a):\n    n = len(a)\n    mean = np.mean(a)\n    deviations = a - mean\n    squared_deviations = deviations**2\n    average_squared_deviations = np.mean(squared_deviations)\n    deviations_of_squared_deviations = squared_deviations - average_squared_deviations\n    squared_deviations_of_squared_deviations = deviations_of_squared_deviations**2\n    average_squared_deviations_of_squared_deviations = np.mean(squared_deviations_of_squared_deviations)\n    kurtosis = average_squared_deviations_of_squared_deviations / average_squared_deviations\n    kurtosis *= (n - 1) / (n - 2)\n    return kurtosis\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\nkurtosis_result = kurtosis_without_bias_correction(a)\nprint(kurtosis_result)\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a)\nprint(kurtosis_result)\n",
        "\nimport numpy as np\nimport scipy.interpolate\n# Define the function to be interpolated\ndef fuel_consumption(x, y):\n    return (x + y) * np.exp(-6.0 * (x * x + y * y))\n# Create the data points\ns = np.linspace(-1, 1, 50)\nt = np.linspace(-2, 0, 50)\nx, y = np.ogrid[-1:1:10j, -2:0:10j]\n# Calculate the z values\nz = fuel_consumption(x, y)\n# Interpolate the data\ninterpolator = scipy.interpolate.interp2d(x, y, z)\n# Define the interpolation function\ndef interpolate_fuel_consumption(s, t):\n    return interpolator(s, t)\n# Apply the interpolation function to the given data points\nresult = interpolate_fuel_consumption(s, t)\n# Output the result\nprint(result)\n",
        "\n    result = scipy.interpolate.interp2d(example_s, example_t, z, kind='cubic')\n    ",
        "\nridge_points = vor.ridge_points\n",
        "\nresult = []\nfor point in extraPoints:\n    region_index = vor.point_region(point)\n    result.append(region_index)\n",
        "\ndef pad_vector(vector, max_size):\n    new_vector = np.zeros(max_size)\n    new_vector[:len(vector)] = vector\n    return new_vector\n",
        "\norigin = 1.0\nb = nd.median_filter(a, 3, origin=origin)\n",
        "\nresult = M[row, column]\n",
        "\nresult = []\nfor i in range(len(row)):\n    row_matrix = M.getrow(row[i])\n    result.append(row_matrix[column[i]])\n",
        "\nnew_array = np.zeros((1000, 10, 10))\nfor i in range(1000):\n    f = interp1d(x, array[:, i, np.newaxis], axis=0)\n    new_array[i, :, :] = f(x_new)\n",
        "\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\n",
        "\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\n",
        "\ndef dct_matrix(N):\n    dct_mat = sf.dctn(N, 1)\n    sum_squares = np.sum(np.square(dct_mat))\n    normalization_factor = np.sqrt(sum_squares)\n    return dct_mat / normalization_factor\n",
        "\nresult = sparse.diags(matrix, [1, 0, -1], (5, 5)).toarray()\n",
        "\n# [Missing Code]\n",
        "\nmeans = df.mean(axis=0)\nstd_devs = df.std(axis=0)\n",
        "\n# Calculate the mean and standard deviation for each column\nmeans = df.mean()\nstds = df.std()\n# Calculate the z-score for each column\nzscores = [stats.zscore(df[col], mean=means[col], std=stds[col]) for col in df.columns]\n# Create a new DataFrame with the z-scores\nresult = pd.DataFrame(zscores)\n# Combine the original index with the new z-score DataFrame\nresult.index = df.index\n",
        "\nmeans = df.mean()\nstd_devs = df.std()\n",
        "\nmeans = df.mean(axis=0)\nstd_devs = df.std(axis=0)\n",
        "\nstarting_point = np.array(starting_point)\ndirection = np.array(direction)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\n# Step 1: Define the center point\ncenter = (3, 3)\n# Step 2: Calculate the offsets\ny_offsets = np.arange(0, 6)\nx_offsets = np.arange(0, 6)\ny_offsets, x_offsets = np.meshgrid(y_offsets, x_offsets)\n# Subtract the center point coordinates from the offsets\ny_offsets = y_offsets - center[0]\nx_offsets = x_offsets - center[1]\n# Step 3: Create the distance matrix\nmid = np.dstack((y_offsets, x_offsets))\nresult = distance.cdist(mid, mid)\n# Step 4: Output the result\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ncenter = (3, 3)\nshape = (6, 6)\nresult = np.zeros(shape)\n",
        "\n",
        "\nx_factor = (6 - 1) / 3\ny_factor = (8 - 1) / 3\nresult = scipy.ndimage.zoom(x, (x_factor, y_factor), order=1)\n",
        "\nx0 = np.array([2, 3, 1, 4, 20])\nbounds = [(0, None) for _ in range(x0.shape[0])]\nconstraints = ({'type': 'ineq', 'fun': lambda x: -x},)\nproblem = {'x0': x0, 'bounds': bounds, 'constraints': constraints}\nprint(out.x)\nbounds = [(0, None) for _ in range(x0.shape[0])]\nconstraints = ({'type': 'ineq', 'fun': lambda x: -x},)\nproblem = {'x0': x0, 'bounds': bounds, 'constraints': constraints}",
        "\n# Define the objective function\ndef func(x, a):\n    return np.dot(a, x**2)\n# Define the residual function\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x, a)\n    return (y - model)**2\n# Define the bounds for x\nbounds = [(x_lower_bounds[i], None) for i in range(len(x_true))]\n# Set up the optimization problem\nproblem = {'x': (x0, bounds), 'f': residual, 'args': (a, y)}\n# Minimize the function using L-BFGS-B\nout = scipy.optimize.minimize(problem, method='L-BFGS-B')\n",
        "\ndef dN1_dt_sinusoid(t, N1):\n    return -100 * N1 + np.sin(t)\n",
        "\ndef dN1_dt_sinusoidal(t, N1):\n    return -100 * N1 + 2 * np.pi * np.sin(t)\n",
        "\ndef dN1_dt_sinusoidal(t, N1):\n    return -100 * N1 - cos(t)\n",
        "\ndef non_negativity_constraint(x):\n    for t in range(len(x)):\n        if x[t] < 0:\n            return -1000\n    return 0\n",
        "\nresult = sparse.concatenate((sa, sb), axis=0)\n",
        "\nresult = sparse.hstack((sa, sb))\n",
        "\n# [Missing Code]\n",
        "\nx^2\n(1^2) - (0^2)\n1 - 0\n1\n",
        "\n# Convert the dok_matrix to a CSR matrix\nV_csr = V.tocsr()\n# Add the scalar x to the non-zero values of the CSR matrix\nV_csr.add_scalar(x)\n",
        "\nnew_V = sparse.COO(V.row, V.col, V.data * x)\n",
        "\nimport numpy as np\n# Step 1: Initialize the sparse matrix V and scalar values x and y\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\ny = 99\n# Step 2: Add the scalar value x to the sparse matrix V\n# Iterate through the COO format and multiply the value of each non-zero element by x\nfor row, col, value in V.data:\n    V.data[row, col] = value * x\n# Step 3: Add the scalar value y to the result\n# Iterate through the COO format and add y to each non-zero element\nfor row, col, value in V.data:\n    V.data[row, col] = value + y\n# Step 4: Print the result\nprint(V)\n",
        "\ndef normalize_column(column):\n    length = math.sqrt(sum([x**2 for x in column]))\n    return [x / length for x in column]\n",
        "\ndef normalize_column(column):\n    length = math.sqrt(sum([x**2 for x in column]))\n    return [x / length for x in column]\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\n# Calculate the Euclidean distance between each data point and each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n",
        "\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\ndistances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n",
        "\n# Calculate the distance between each data point and the centroids\ndistances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n",
        "\ndef eqn(a, x, b):\n    return x + 2*a - b**2\n",
        "\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n",
        "\ndef bekkers_cdf(x, a, m, d):\n    return integrate.quad(lambda x: bekkers(x, a, m, d), range_start, range_end)[0]\n",
        "\ndef continuous_distribution(x, a, m, d):\n    return integrate.cumulative_distribution(bekkers(x, a, m, d), range_start, x)\n",
        "\ndf['Time'] = pd.to_datetime(df['Time'])\n",
        "\nx_arr = np.array(x)\ny_arr = np.array(y)\neval_points = [(2.7, 2.3)]\nresult = scipy.interpolate.griddata(x_arr, y_arr, eval_points)\nprint(result)\n",
        "\nweights = np.array([0.001, 0.1, 0.2, 0.12, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\na_0 = a['A1'].value_counts()[0]\na_1 = a['A1'].value_counts()[1]\na_2 = a['A1'].value_counts()[2]\na_3 = a['A1'].value_counts()[3]\na_4 = a['A1'].value_counts()[4]\na_5 = a['A1'].value_counts()[5]\na_6 = a['A1'].value_counts()[6]\na_7 = a['A1'].value_counts()[7]\na_8 = a['A1'].value_counts()[8]\na_9 = a['A1'].value_counts()[9]\na_10 = a['A1'].value_counts()[10]\na_11 = a['A1'].value_counts()[11]\ndef likelihood(weights):\n    return np.prod(weights**(np.array([a_0, a_1, a_2, a_3, a_4, a_5, a_6, a_7, a_8, a_9, a_10, a_11])))\nresult = sciopt.maximize(likelihood, weights)\nprint(result.x)\nweights = np.array([0.001, 0.1, 0.2, 0.12, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\na_0 = a['A1'].value_counts()[0]\na_1 = a['A1'].value_counts()[1]\na_2 = a['A1'].value_counts()[2]\na_3 = a['A1'].value_counts()[3]\na_4 = a['A1'].value_counts()[4]\na_5 = a['A1'].value_counts()[5]\na_6 = a['A1'].value_counts()[6]\na_7 = a['A1'].value_counts()[7]\na_8 = a['A1'].value_counts()[8]\na_9 = a['A1'].value_counts()[9]\na_10 = a['A1'].value_counts()[10]\na_11 = a['A1'].value_counts()[11]",
        "\nfp   = lambda p, x: p[0]*x[0]+p[1]*x[1]\n",
        "\n    # [Missing Code]\n    ",
        "\n    # [Missing Code]\n    ",
        "\nnumerical_columns = [col for col in df.columns if df[col].dtype in [np.float64, np.int64]]\n"
    ],
    "Sklearn": [
        "\n# Convert the data to a Pandas DataFrame\ndata1 = pd.DataFrame(data['data'], columns=data['target_names'])\ndata1['target'] = data['target']\n",
        "\n# [Missing Code]\n",
        "\ndata = load_boston()\ndata_array = data['data']\n",
        "\n    data_df = pd.DataFrame(data.data, columns=data.target_names)\n    target_df = pd.DataFrame(data.target)\n    ",
        "\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(df['Col3'])\ndf_out = encoder.transform(df['Col3']).toarray()\ndf_out = pd.DataFrame(df_out, columns=encoder.get_feature_names(df['Col3']))\ndf_out = pd.concat([df, df_out], axis=1)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\ndef one_hot_encode(df, column_name):\n    unique_elements = df[column_name].unique()\n    df_out = df.copy()\n    for element in unique_elements:\n        df_out[element] = df[column_name].apply(lambda x: 1 if element in x else 0)\n    return df_out\n",
        "\ndef list_to_one_hot_array(list_to_encode):\n    unique_elements = set(list_to_encode)\n    num_unique_elements = len(unique_elements)\n    one_hot_array = np.zeros(num_unique_elements)\n    for element in list_to_encode:\n        one_hot_array[unique_elements.index(element)] = 1\n    return one_hot_array\n",
        "\nimport numpy as np\ndef logistic(x):\n    return 1 / (1 + np.exp(-x))\nproba = np.array([logistic(x) for x in predicted_test_scores])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm, calibration\n",
        "\n# Load the data\ndf_origin, transform_output = load_data()\n# Convert the sparse matrix to a dense matrix\ndense_matrix = transform_output.toarray()\n# Convert the dense matrix to a pandas DataFrame\ndf_transformed = pd.DataFrame(dense_matrix)\n# Merge the two DataFrames\ndf = pd.concat([df_origin, df_transformed], axis=1)\n",
        "\ntransform_output_array = transform_output.toarray()\n",
        "\n    # Convert the sparse matrix to a dense matrix\n    dense_matrix = transform_output.toarray()\n    # Convert the dense matrix to a pandas DataFrame\n    new_df = pd.DataFrame(dense_matrix, columns=df.columns, index=df.index)\n    # Merge the new DataFrame with the original DataFrame\n    result = pd.concat([df, new_df], axis=1, join='outer')\n    ",
        "\nsteps = clf.named_steps()\ndel steps[1]\nclf.steps = steps\n",
        "\ndel clf.steps[1]\n",
        "\ndel clf.steps[1]\n",
        "\n# Insert the PolynomialFeatures step before the PCA step\nclf.insert(1, 'poly', PolynomialFeatures())\n",
        "\nnew_step = ('new_step', PolynomialFeatures())\nclf.insert(1, new_step)\n",
        "\n# Insert ('t1919810', PCA()) right before 'svdm'\nclf.insert(2, 't1919810', PCA())\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba = logreg.predict_proba(X_test)\n    proba_list.append(proba)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba_test = logreg.predict_proba(X_test)\n    proba.append(proba_test)\nproba = np.concatenate(proba, axis=0)\nprint(proba)\n",
        "\n# [Missing Code]\n",
        "\n    inversed = scaler.inverse_transform(scaled)\n    ",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel_name = model.name\nprint(model_name)\n",
        "\n# [Missing Code]\n",
        "\nmodel_name = model.__class__.__name__\nprint(model_name)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n",
        "\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n",
        "\n# [Missing Code]\n",
        "\nclf = GridSearchCV(bc, param_grid, cv=5)\nclf.fit(X_train, y_train)\nbest_estimator = clf.best_estimator_\nproba = best_estimator.predict_proba(X_test)\nprint(proba)\n",
        "\n",
        "\n# [Missing Code]\n",
        "\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\ndef prePro(x):\n    return x.lower()\n",
        "\ndata_array = data.values\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\ndf_out = data.apply(preprocessing.scale)\nprint(df_out)\n",
        "\ngrid.fit(X, y)\nmodel = grid.named_steps[\"model\"]\ncoef = model.coef_\nprint(coef)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n",
        "\nimport numpy as np\n# Assume that X and y are already loaded, and clf is fitted.\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n# Find the column names of the selected features.\nselected_features = np.where(model.get_support())[0]\ncolumn_names = X.columns[selected_features].tolist()\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\n# Find the indices of the top k features\nk = 5  # Number of features to select\ntop_k_indices = np.argsort(clf.feature_importances_)[-k:]\n# Find the corresponding column names\ncolumn_names = [X.columns[i] for i in top_k_indices]\nprint(column_names)\n",
        "\n# [Missing Code]\n",
        "\np_index = np.where(np.array_equal(km.cluster_centers_, p))\np_index = p_index[0][0]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\np, X = load_data()\nassert type(X) == np.ndarray\nkm = KMeans(n_clusters=p).fit(X)\npth_cluster_center = km.cluster_centers_[p - 1]\ndistances = np.linalg.norm(X - pth_cluster_center, axis=1)\nclosest_50_samples = np.argsort(distances)[:50]\nprint(X[closest_50_samples])\n",
        "\n# [Missing Code]\n",
        "\n    # [Missing Code]\n    ",
        "\n# Import the get_dummies function\nfrom pandas.get_dummies import get_dummies\n# Convert the categorical variable into numerical representations\nX_train_dummies = get_dummies(X_train[0])\n# Merge the numerical representations back into the original training data\nX_train_new = X_train.drop(0, axis=1)\nX_train_new = X_train_new.join(X_train_dummies)\n",
        "\n# One Hot Encoding for categorical variable\nX_train_dummies = pd.get_dummies(X_train, columns=[0])\n# Remove the original categorical variable column\nX_train_dummies.drop(columns=[0], inplace=True)\n# Merge the dummies back with the original training data\nX_train_new = pd.concat([X_train, X_train_dummies], axis=1)\n# Replace the original training data with the new one\nX_train = X_train_new\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\n",
        "\nfrom sklearn.svm import SVR\n# ",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\n",
        "\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\n# ",
        "\n# Convert the queries into a tf-idf matrix\nqueries_tfidf = tfidf.transform(queries)\n# Calculate the cosine similarity between the queries and the documents\ncosine_similarities_of_queries = cosine_similarity(queries_tfidf, tfidf.transform(documents))\n# Create a 3x5 matrix of the similarities\nsimilarities_matrix = np.zeros((3, 5))\n# Fill in the matrix with the cosine similarities\nfor i in range(3):\n    for j in range(5):\n        similarities_matrix[i, j] = cosine_similarities_of_queries[i, j]\nprint(similarities_matrix)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef get_cosine_similarity(tfidf, queries):\n    queries_tfidf = tfidf.transform(queries)\n    return cosine_similarity(queries_tfidf, tfidf.idf_)\n",
        "\n    queries_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = queries_tfidf * tfidf.idf_\n    cosine_similarities_of_queries = np.divide(cosine_similarities_of_queries, np.linalg.norm(cosine_similarities_of_queries, axis=1)[:, np.newaxis])\n    cosine_similarities_of_queries = np.dot(cosine_similarities_of_queries, tfidf.idf_)\n    cosine_similarities_of_queries = np.divide(cosine_similarities_of_queries, np.linalg.norm(tfidf.idf_, axis=1)[:, np.newaxis])\n    return cosine_similarities_of_queries\n    queries_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = queries_tfidf * tfidf.idf_\n    cosine_similarities_of_queries = np.divide(cosine_similarities_of_queries, np.linalg.norm(cosine_similarities_of_queries, axis=1)[:, np.newaxis])\n    cosine_similarities_of_queries = np.dot(cosine_similarities_of_queries, tfidf.idf_)\n    cosine_similarities_of_queries = np.divide(cosine_similarities_of_queries, np.linalg.norm(tfidf.idf_, axis=1)[:, np.newaxis])",
        "\nimport pandas as pd\n# Load the data\nfeatures = [\n    ['f1', 'f2', 'f3'],\n    ['f2', 'f4', 'f5', 'f6'],\n    ['f1', 'f2']\n]\n# Create a DataFrame with the features as columns and the samples as rows\ndf = pd.DataFrame(features)\n",
        "\ndef convert_list_to_2d_array(features):\n    # Step 3: Create a dictionary mapping features to their indices\n    feature_indices = {f: i for i, f in enumerate(features[0])}\n    # Step 4: Create the 2D-array\n    num_rows = len(features)\n    num_columns = len(feature_indices)\n    new_f = np.zeros((num_rows, num_columns))\n    # Step 5: Fill the 2D-array with the correct values\n    for i, sample in enumerate(features):\n        for feature in sample:\n            new_f[i, feature_indices[feature]] = 1\n    # Step 6: Return the 2D-array\n    return new_f\n",
        "\ndef convert_features_to_2d_array(features):\n    # Step 3: Initialize the 2D-array\n    num_samples = len(features)\n    num_features = len(set(chain.from_iterable(features)))\n    new_features = np.zeros((num_samples, num_features), dtype=np.int)\n    # Step 4: Iterate through the features list\n    for i, sample in enumerate(features):\n        # Step 5: Fill the 2D-array\n        for feature in sample:\n            index = new_features.shape[1] - 1 - len(sample) + sample.index(feature)\n            new_features[i, index] = 1\n    # Step 6: Return the 2D-array\n    return new_features\n",
        "\ndef solve(features):\n    new_features = np.zeros((len(features), 6))\n    for i, sample in enumerate(features):\n        for j, feature in enumerate(sample):\n            new_features[i, j] = 1\n    return new_features\n",
        "\nimport collections\nimport numpy as np\ndef convert_features_to_2d_array(features):\n    # Step 3: Create a dictionary to map feature names to their indices\n    feature_names = [f for s in features for f in s]\n    feature_names_counter = collections.Counter(feature_names)\n    feature_names_sorted = sorted(feature_names_counter, key=feature_names_counter.get, reverse=True)\n    feature_names_to_indices = {name: index for index, name in enumerate(feature_names_sorted)}\n    # Step 4: Initialize the 2D-array\n    num_samples = len(features)\n    num_features = len(feature_names_sorted)\n    new_features = np.zeros((num_samples, num_features), dtype=np.uint8)\n    # Step 5: Fill the 2D-array\n    for sample_index, sample in enumerate(features):\n        for feature in sample:\n            new_features[sample_index, feature_names_to_indices[feature]] = 1\n    # Step 6: Return the 2D-array\n    return new_features\n",
        "\ndata_frame = pd.DataFrame(data_matrix, index=['prof1', 'prof2', 'prof3'], columns=['prof1', 'prof2', 'prof3'])\n",
        "\nfrom sklearn.cluster import AgglomerativeClustering\n# Initialize the AgglomerativeClustering object\nhierarchical_clustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n",
        "\ndef convert_distance_to_similarity(simM):\n    for i in range(len(simM)):\n        min_value = np.min(simM[i])\n        simM[i] = np.subtract(simM[i], min_value)\n        simM[i] = np.divide(simM[i], np.max(simM[i]))\n    return simM\n",
        "\ndata_matrix = np.array([[0, 0.8, 0.9],\n                       [0.8, 0, 0.2],\n                       [0.9, 0.2, 0]])\nsimilarity_matrix = 1 - data_matrix\n",
        "\ndata_matrix = np.array([[0, 0.8, 0.9],\n                       [0.8, 0, 0.2],\n                       [0.9, 0.2, 0]])\nsimilarity_matrix = 1 - data_matrix\n",
        "\nsimM = np.array(simM)\ndistM = 1 - simM\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nfrom scipy.stats import skew\ndef box_cox_transform(data, lambdas):\n    transformed_data = []\n    for lambda_ in lambdas:\n        transformed_data.append(((data**lambda_) - 1) / lambda_)\n    return transformed_data\ndef find_optimal_lambda(data, lambdas):\n    skewness_values = []\n    for transformed_data in box_cox_transform(data, lambdas):\n        skewness_values.append(skew(transformed_data))\n    return lambdas[np.argmin(np.abs(skewness_values))]\ndef box_cox_transform_data(data, optimal_lambda):\n    return box_cox_transform(data, [optimal_lambda])[0]\n",
        "\ndef yeo_johnson_transformation(data, lambda_value):\n    transformed_data = []\n    for x in data:\n        transformed_data.append((x**lambda_value - 1) / lambda_value)\n    return transformed_data\n",
        "\nimport scipy.stats as st\n",
        "\nvectorizer = CountVectorizer(strip_punctuation=False)\ntransformed_text = vectorizer.fit_transform(text)\n",
        "\nimport numpy as np\nimport pandas as pd\n",
        "\nimport numpy as np\nimport pandas as pd\n",
        "\nimport numpy as np\nimport pandas as pd\n",
        "\nimport numpy as np\nimport pandas as pd\n",
        "\nfrom sklearn.cluster import KMeans\nimport pandas as pd\ndf = pd.read_csv(\"generate_csv/all_data_device.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nkmeans = KMeans(n_clusters=2).fit(f1)\nlabels = kmeans.predict(f1)\nprint(labels)\n",
        "\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\nprint(labels)\n",
        "\nlinear_svc = LinearSVC(penalty='l1', random_state=42)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile\n",
        "\nclf = LinearSVC(penalty='l1', dual=False)\n",
        "\nvocabulary = ['Jscript', 'Java', 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', '.Net', 'TypeScript', 'SQL']\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary=['Jscript', '.Net', 'TypeScript', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX'])\n",
        "\nvocabulary = {'Jscript', '.Net', 'TypeScript', 'SQL', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'Integration', 'Database design', 'UX', 'Web', 'UI Design'}\n",
        "\nvocabulary = ['Jscript', '.Net', 'TypeScript', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'Integration', 'Database design', 'UX', 'UI Design', 'Web']\n",
        "\ndef linear_regression_slope(df, col):\n    df2 = df[~np.isnan(df[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:, 0], npMatrix[:, 1]\n    slope = LinearRegression().fit(X, Y).coef_[0]\n    return slope\n",
        "\ndef linear_regression_slope(df, col):\n    df2 = df[~np.isnan(df[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:, 0], npMatrix[:, 1]\n    slope = LinearRegression().fit(X, Y)\n    return slope.coef_[0]\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n# Load the data\ndf = load_data()\n# Create a LabelEncoder object\nle = LabelEncoder()\n# Fit the LabelEncoder to the 'Sex' column\nle.fit(df['Sex'])\n# Transform the 'Sex' column using the LabelEncoder\ndf['Sex'] = le.transform(df['Sex'])\n# Print the transformed data\nprint(df)\n",
        "\ndef load_data():\n    df = pd.DataFrame({'Sex': ['male', 'female', 'male', 'female']})\n    return df\ndf = load_data()\nlabel_encoder = LabelEncoder()\ndf['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndf = load_data()\n# Create a LabelEncoder object\nle = LabelEncoder()\n# Fit the LabelEncoder to the 'Sex' column\nle.fit(df['Sex'])\n# Transform the 'Sex' column using the created mapping\ndf['Sex'] = le.transform(df['Sex'])\nprint(df)\n",
        "\nElasticNet = linear_model.ElasticNet()\n",
        "\n# Flatten the numpy array\nflat_np_array = np_array.flatten()\n# Apply MinMaxScaler fit_transform\nscaler = MinMaxScaler()\ntransformed = scaler.fit_transform(flat_np_array)\n# Reshape the transformed array\nreshape_transformed = transformed.reshape(np_array.shape)\n",
        "\n# [Missing Code]\n",
        "\ndef Transform(a):\n    # Flatten the array\n    a_flat = a.flatten()\n    # Fit the scaler to the flattened array\n    scaler = MinMaxScaler()\n    scaler.fit(a_flat)\n    # Transform the flattened array\n    new_a_flat = scaler.transform(a_flat)\n    # Reshape the transformed array back to the original shape\n    new_a = new_a_flat.reshape(a.shape)\n    return new_a\n",
        "\n# Step 1: Create the new data frame (b)\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n# Step 2: Predict the buy/sell signal using the trained decision tree regressor (clf)\npredict = clf.predict(b)\n",
        "\nfrom sklearn.preprocessing import LabelEncoder\n",
        "\nimport pandas as pd\ndf = pd.DataFrame(X)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\n",
        "\n# [Missing Code]\n",
        "\nX = dataframe.iloc[:-1].astype(float)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef split_data(features_dataframe):\n    total_rows = features_dataframe.shape[0]\n    train_rows = int(total_rows * (20/100))\n    test_rows = int(total_rows * (80/100))\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_rows)\n    train_dataframe.sort_values(by=\"date\", inplace=True)\n    test_dataframe.sort_values(by=\"date\", inplace=True)\n    return train_dataframe, test_dataframe\nfeatures_dataframe = load_data()\ntrain_dataframe, test_dataframe = split_data(features_dataframe)\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef split_by_date(features_dataframe, train_size, date_column):\n    # Calculate the number of rows for each set\n    train_rows = int(features_dataframe.shape[0] * train_size)\n    test_rows = features_dataframe.shape[0] - train_rows\n    # Create the masks for each set\n    train_mask = np.zeros(features_dataframe.shape[0], dtype=np.bool)\n    train_mask[:train_rows] = True\n    test_mask = np.zeros(features_dataframe.shape[0], dtype=np.bool)\n    test_mask[train_rows:] = True\n    # Apply the masks to the original dataframe\n    train_dataframe = features_dataframe[train_mask]\n    test_dataframe = features_dataframe[test_mask]\n    # Sort the train and test sets by date\n    train_dataframe.sort_values(by=date_column, inplace=True)\n    test_dataframe.sort_values(by=date_column, inplace=True)\n    return train_dataframe, test_dataframe\nfeatures_dataframe = load_data()\ntrain_size = 0.8\ndate_column = \"date\"\ntrain_dataframe, test_dataframe = split_by_date(features_dataframe, train_size, date_column)\nprint(train_dataframe)\nprint(test_dataframe)\n",
        "\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    ",
        "\nscaler = MinMaxScaler()\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].transform(scaler.fit_transform)\n",
        "\n# [Missing Code]\n",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names())\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n# Load the data and fit the GridSearchCV model\nGridSearch_fitted = load_data()\n# Extract the cv_results_ attribute\nfull_results = GridSearch_fitted.cv_results_\n# Convert the dictionary to a pandas DataFrame\nfull_results_df = pd.DataFrame(full_results)\n# Print the full results\nprint(full_results_df)\n",
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n# Load the GridSearchCV object\nGridSearch_fitted = load_data()\n# Extract the scores and fit times\nscores = GridSearch_fitted.cv_results_['mean_test_score']\nfit_times = GridSearch_fitted.cv_results_['mean_fit_time']\n# Create a pandas DataFrame\nfull_results = pd.DataFrame({'Score': scores, 'Fit Time (s)': fit_times})\n# Sort the DataFrame by fit times in ascending order\nfull_results = full_results.sort_values('Fit Time (s)')\n# Print the sorted DataFrame\nprint(full_results)\n",
        "\nimport pickle\n",
        "\ndef load_data():\n    return pd.DataFrame({'items': ['1fgg', '2hhj', '3jkl'], 'description': ['abcd ty', 'abc r', 'r df']})\ndef cosine_similarity_matrix(tfidf_matrix):\n    return np.array([[1 - np.dot(tfidf_matrix.iloc[:, i], tfidf_matrix.iloc[:, j]) / (np.linalg.norm(tfidf_matrix.iloc[:, i]) * np.linalg.norm(tfidf_matrix.iloc[:, j])) for j in range(tfidf_matrix.shape[1])] for i in range(tfidf_matrix.shape[1])])\n"
    ],
    "Pytorch": [
        "\nstate_dict = optim.state_dict()\nstate_dict['param_groups'][0]['lr'] = 0.001\noptim.load_state_dict(state_dict)\n",
        "\n# [Missing Code]\n",
        "\noptim = torch.optim.SGD(..., lr=0.005)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n# Load the word2vec model\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n# Convert the word2vec model to a numpy array\nword2vec_weights = np.array(word2vec.wv.syn0)\n# Initialize the PyTorch embedding layer\nembedding_layer = torch.nn.Embedding(word2vec.vocab_size, word2vec.vector_size)\n# Load the numpy array into the PyTorch embedding layer\nembedding_layer.weight = torch.from_numpy(word2vec_weights)\n",
        "\nx_numpy = x.numpy()\npx = pd.DataFrame(x_numpy)\n",
        "\nimport numpy as np\nimport torch\nimport pandas as pd\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\nprint(px)\n",
        "\npx = pd.DataFrame(x.numpy())\n",
        "\nC = B[:, A_log]\nC = B[:, torch.nonzero(A_log)]\nprint(C)\nC = B[:, torch.nonzero(A_log)]\nprint(C)\n",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\nA_log_long = A_log.to(torch.long)\nC = B[:, A_log_long]\nprint(C)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\nA_log_long = torch.nonzero(A_log)\nC = B[:, A_log_long]\nprint(C)\n",
        "\nC = B[:, A_log] # Throws error\n",
        "\nimport torch\nA_log_fixed = torch.ByteTensor([0, 0, 1])\nC = B[:, A_log_fixed]\nprint(C)\n",
        "\nB = torch.LongTensor([[2, 1, 3], [5, 4, 6]])\n# 2 1 3\n# 5 4 6\nidx = torch.LongTensor([1, 2])\nC = B.index_select(1, idx)\nprint(C)\nC = torch.LongTensor([[1, 3], [4, 6]])\n# 1 3\n# 4 6\nC = torch.LongTensor([[1, 3], [4, 6]])\n# 1 3\n# 4 6.",
        "\nx_tensor = torch.from_numpy(x_array)\n",
        "\nx_list = [x_array[0], x_array[1], x_array[2], x_array[3], x_array[4], x_array[5], x_array[6], x_array[7], x_array[8]]\n",
        "\ndef Convert(a):\n    return torch.from_numpy(a)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef batch_sentence_lengths_to_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, len in enumerate(lens):\n        mask[i, :len] = torch.arange(len).long()\n    return mask\nlens = [3, 5, 4]\nmask = batch_sentence_lengths_to_mask(lens)\nprint(mask)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef sentence_lengths_to_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len, dtype=torch.long)\n    for i, len in enumerate(lens):\n        mask[i, :len] = torch.repeat_interleave(torch.tensor([1]), len)\n    return torch.cat(mask, dim=1)\nlens = [1, 9, 3, 5]\nmask = sentence_lengths_to_mask(lens)\nprint(mask)\n",
        "\nimport torch\ndef batch_sentence_mask(lens):\n    mask = torch.LongTensor(sum(lens))\n    for i, length in enumerate(lens):\n        new_tensor = torch.LongTensor(length)\n        new_tensor.fill_(1)\n        mask = torch.cat((mask, new_tensor), dim=0)\n        if i < len(lens) - 1:\n            mask[-(length + 1):] = 0\n    return mask\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    for i, len_ in enumerate(lens):\n        mask[i, :len_] = torch.arange(len_, dtype=torch.long)\n    return mask\nlens = [3, 5, 4]\nmask = get_mask(lens)\nprint(mask)\n",
        "\n# [Missing Code]\n",
        "\n    matrix_size = t.shape[1]\n    Tensor_3D = torch.zeros(t.shape[0], matrix_size, matrix_size)\n    Tensor_3D = torch.repeat(t, matrix_size, 1)\n    ",
        "\nab = torch.cat((a, b), dim=0)\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na, b = load_data()\n# Pad tensor b with 90 rows to match the size of tensor a\nb_padded = torch.nn.functional.pad(b, (0, 0, 0, 90))\n# Combine the padded tensor b with tensor a\nab = torch.stack((a, b_padded), 0)\nprint(ab)\n",
        "\n    ab = torch.cat((a, b), dim=0)\n    ",
        "\n# [Missing Code]\n",
        "\nimport numpy as np\nimport torch\na = torch.rand((10, 1000, 96))\nlengths = torch.randint(1000, (10,))\n# Create the mask\nmask = torch.zeros_like(a)\nfor i in range(10):\n    length = lengths[i]\n    mask[i, :length, :] = torch.tensor([[1] * length + [0] * (1000 - length), ]).repeat(96, 1)\n",
        "\n# [Missing Code]\n",
        "\n# [Missing Code]\n",
        "\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nimport torch\nlist = [ torch.randn(3), torch.randn(3), torch.randn(3)]\nnew_tensors = torch.stack(list)\n",
        "\n    tt = torch.stack(lt)\n    ",
        "\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nidx_tensor = torch.from_numpy(idx)\n",
        "\nidx_tensor = torch.from_numpy(idx)\n",
        "\nidx_tensor = torch.from_numpy(idx)\n",
        "\nresult = torch.index_select(x, 1, ids)\n",
        "\nresult = x.gather(1, ids)\n",
        "\ndef gather_selected_slices(ids, x):\n    mask = np.zeros(x.shape, dtype=np.uint8)\n    mask[np.arange(x.shape[0]), ids] = 1\n    selected_slices = x * mask\n    result = np.sum(selected_slices, axis=1)\n    return result\n",
        "\n    # [Missing Code]\n    ",
        "\ndef get_highest_probability_class(softmax_output):\n    # Find the highest probability in each row\n    max_probabilities = torch.max(softmax_output, dim=1)\n    # Find the index of the highest probability in each row\n    class_indices = torch.argmax(softmax_output, dim=1)\n    # Create a tensor with the class labels based on the indices\n    y = torch.zeros(softmax_output.shape[0], dtype=torch.long)\n    y[torch.arange(y.shape[0]), class_indices] = 1\n    return y\n",
        "\n    # [Missing Code]\n    ",
        "\ndef solve(softmax_output):\n    # Find the highest probability for each input\n    _, max_indices = torch.max(softmax_output, dim=1)\n    # Create the tensor\n    y = torch.LongTensor(max_indices)\n    # Return the tensor\n    return y\n",
        "\ndef solve(softmax_output):\n    lowest_probability = torch.min(softmax_output, dim=1)\n    class_indices = torch.argmin(softmax_output, dim=1)\n    return torch.LongTensor([i for i in range(3) for _ in range(lowest_probability.shape[0])])[class_indices]\n",
        "\ndef cross_entropy2d(images, labels):\n    # images: (batch, channels, height, width)\n    # labels: (batch, height, width)\n    n, c, h, w = images.size()\n    log_p = F.log_softmax(images, dim=1)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)\n    mask = labels >= 0\n    labels = labels[mask]\n    log_p = log_p[labels.view(n, h, w, 1) >= 0]\n    loss = F.nll_loss(log_p, labels.view(-1), size_average=False)\n    return loss / c\n",
        "\nA_numpy = A.numpy()\nB_numpy = B.numpy()\n",
        "\nA_np = A.numpy()\nB_np = B.numpy()\n",
        "\nA_np = A.numpy()\nB_np = B.numpy()\n",
        "\nA_np = A.numpy()\nB_np = B.numpy()\n",
        "\nA = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nB = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nA_last_5 = torch.tensor([6, 7, 8, 9, 10])\nB_last_5 = torch.tensor([6, 7, 8, 9, 10])\nequal_elements = torch.eq(A_last_5, B_last_5)\ncnt_equal = equal_elements.sum()\nprint(cnt_equal)\ncnt_equal = 5\n",
        "\nA = torch.tensor([[1, 2], [3, 4]])\nB = torch.tensor([[1, 2], [3, 5]])\nA = [[1, 2], [3, 4]]\nB = [[1, 2], [3, 5]]\nA[0][1] = 2\nB[0][1] = 2\nA[1][1] = 4\nB[1][1] = 5\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\na = load_data()\nassert a.shape == (1, 3, 10, 40, 1)\nchunk_dim = 10\na_split = torch.chunk(a, 4, dim=3)\nfor chunk in a_split:\n    smaller_tensors = torch.chunk(chunk, 10, dim=3)\ntensors_31 = [tensor for chunk in a_split for tensor in torch.chunk(chunk, 10, dim=3)]\nfor tensor in tensors_31:\n    print(tensor)\n",
        "\nnumber_of_steps = 40 // 10\na_split = torch.chunk(a, number_of_steps, dim=2)\n",
        "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\n# [Missing Code]\n",
        "\nabs_x = torch.abs(x)\nabs_y = torch.abs(y)\n",
        "\nabs_x = torch.abs(x)\nabs_y = torch.abs(y)\n",
        "\n    # [Missing Code]\n    ",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\nMyNet = torch.nn.Sequential(torch.nn.Linear(4, 15),\n                            torch.nn.Sigmoid(),\n                            torch.nn.Linear(15, 3),\n                            )\nMyNet.load_state_dict(torch.load(\"my_model.pt\"))\ninput = load_data()\nassert type(input) == torch.Tensor\n# Apply softmax to the output of the model\noutput = MyNet(input)\noutput = torch.nn.functional.softmax(output, dim=1)\n# Find the maximum confidence score and the corresponding class\nconf, classes = torch.max(output.reshape(1, 3), 1)\nclass_names = '012'\n# Print the confidence score and the class name\nprint(conf, class_names[classes.item()])\n",
        "\na = [[1, 2, 3], [1, 2, 3]]\nb = [[5, 6, 7], [5, 6, 7]]\nAverage = ([3, 3] + [5, 5]) / 2\nAverage = [8, 8] / 2\nAverage = [4, 4]\nresult = [[1, 2, 3, 4, 6],\n          [1, 2, 3, 4, 6]]\nresult = [[1, 2, 3, 4, 6],\n          [1, 2, 3, 4, 6]]\n",
        "\na = [[1, 2, 3], [1, 2, 3]]\nb = [[5, 6, 7], [5, 6, 7]]\nresult = [[1, 2, 4, 6, 7],\n          [1, 2, 4, 6, 7]]\nresult = [[1, 2, 4, 6, 7],\n          [1, 2, 4, 6, 7]]\n",
        "\nnew = torch.zeros(6, 4)\n",
        "\nimport torch.nn.functional as F\n# Pad the matrix with zeros\npadded_tensor = F.pad(t, (1, 1, 1, 1))\n",
        "\nnew = torch.tensor([[-1, -1, -1, -1],\n                    [-1, 1, 2, -1],\n                    [-1, 3, 4, -1],\n                    [-1, 5, 6, -1],\n                    [-1, 7, 8, -1],\n                    [-1, -1, -1, -1]])\nresult = torch.cat((new, t, new), dim=0)\nprint(result)\n",
        "\nresult = torch.matmul(data, W)\n"
    ]
}