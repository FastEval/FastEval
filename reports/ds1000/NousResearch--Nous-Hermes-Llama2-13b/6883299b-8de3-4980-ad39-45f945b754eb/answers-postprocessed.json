{
    "Pandas": [
        "\ndf = df.iloc[List]\nresult = df.reset_index(drop=True)\n",
        "\ndf = df.iloc[List]\nresult = df.groupby('Type').agg({'Col1': 'count', 'Col2': 'count', 'Col3': 'count'})\n",
        "\nresult = df.copy()\nresult['Qu1'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(x) >= 2 else x)\nresult['Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(x) >= 2 else x)\nresult['Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(x) >= 2 else x)\n# [Missing Code]\n",
        "\nresult = df.copy()\nresult['Qu1'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(x).sum() >= 3 else x)\nresult['Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(x).sum() >= 3 else x)\nresult['Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(x).sum() >= 3 else x)\n# [Missing Code]\n",
        "\n    result = df.copy()\n    result['Qu1'] = df['Qu1'].replace(['apple', 'potato', 'cheese', 'banana'], 'others')\n    result['Qu2'] = df['Qu2'].copy()\n    result['Qu3'] = df['Qu3'].copy().replace(['apple', 'potato', 'sausage'], 'others')\n    # [Missing Code]\n",
        "\nresult = df.copy()\nresult['Qu1'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(x).sum() >= 3 else x)\nresult['Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(x).sum() >= 2 else x)\nresult['Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(x).sum() >= 2 else x)\n# [Missing Code]\n",
        "\nresult = df.copy()\nresult['Qu1'] = result['Qu1'].replace('apple', 'other')\nresult.loc[result['Qu1'].str.contains('cheese', case=False), 'Qu1'] = 'other'\nresult.loc[result['Qu2'].str.contains('banana', case=False), 'Qu2'] = 'other'\nresult.loc[result['Qu3'].str.contains('potato', case=False), 'Qu3'] = 'other'\n# [Missing Code]\n",
        "\ndf = df.drop_duplicates(subset=['url'], keep='first')\ndf['keep_if_dup'] = df['keep_if_dup'].apply(lambda x: 'Yes' if x == 'Yes' else 'No')\ndf = df.drop_duplicates(subset=['url', 'keep_if_dup'], keep='first')\n# [Missing Code]\n",
        "\ndf = df.loc[df['drop_if_dup'] == 'No', 'drop_if_dup'] = 'keep'\ndf = df.drop_duplicates(subset=['url'], keep=lambda x: x.drop_if_dup == 'keep')\n",
        "\ndf = df.loc[df['keep_if_dup'] == 'Yes', 'url'] = df['url'].drop_duplicates()\ndf = df.drop_duplicates(subset=['url'])\n# [Missing Code]\n",
        "\nresult = {}\nfor index, row in df.iterrows():\n    name = row['name']\n    if name not in result:\n        result[name] = {}\n    v1 = row['v1']\n    if v1 not in result[name]:\n        result[name][v1] = {}\n    v2 = row['v2']\n    result[name][v1][v2] = row['v3']\nprint(result)\n",
        "\ndf['datetime'] = df['datetime'].dt.tz_localize('UTC').dt.strftime('%Y-%m-%d %H:%M:%S')\n# [Missing Code]\n",
        "\n    df['datetime'] = df['datetime'].dt.tz_localize('UTC')\n    df['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    # [Missing Code]\n",
        "\n# Convert datetime column to UTC timezone\ndf['datetime'] = df['datetime'].dt.tz_localize('UTC')\n# Convert datetime column to desired timezone\ndf['datetime'] = df['datetime'].dt.tz_convert('America/New_York')\n# Remove UTC offset from datetime column\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n# Sort datetime column in ascending order\ndf = df.sort_values(by=['datetime'])\n# Format datetime column to look like this format: 19-May-2016 13:50:00\ndf['datetime'] = pd.to_datetime(df['datetime']).strftime('%d-%B-%Y %H:%M:%S')\n",
        "\n# Convert the datetime column to UTC\ndf['datetime'] = df['datetime'].dt.tz_localize('UTC')\n# Convert the datetime column to the desired timezone\ndf['datetime'] = df['datetime'].dt.tz_convert('America/New_York')\n# Remove the UTC offset from the datetime column\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\nresult = df.apply(lambda row: {\n    'job': row['message'].split(',')[0].strip(),\n    'money': row['message'].split(',')[1].strip(),\n    'wife': row['message'].split(',')[2].strip(),\n    'group': row['message'].split(',')[3].strip(),\n    'kids': row['message'].split(',')[4].strip()\n}, axis=1)\n",
        "\nresult = df.loc[df['product'].isin(products), 'score'] * 10\nresult = result.reset_index()\nresult.index += 1\nresult.columns = ['product', 'score']\nresult = result[['product', 'score']]\nprint(result)\n",
        "\nfor i, product in enumerate(products):\n    if product in df['product'].values:\n        df.loc[df['product'] == product, 'score'] = 10\n    else:\n        df.loc[df['product'] == product, 'score'] *= 10\n",
        "\ndf.loc[df['product'].isin(products), 'score'] *= 10\n",
        "\nresult = df.loc[df['product'].isin(products), 'score']\nresult[result == 0] = 1\nresult[result == 1] = 0\nprint(result)\n",
        "\n# Convert binary columns to categorical column\nresult = pd.get_dummies(df, columns=['A', 'B', 'C', 'D'])\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\n\n# Convert binary columns to categorical column\nresult = pd.get_dummies(df, columns=['A', 'B', 'C', 'D'])\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 1, 1],\n                   'C': [1, 1, 1, 0, 1, 0],\n                   'D': [0, 0, 0, 1, 1, 0]})\n\n# Convert binary columns to categorical column of lists\nresult = df.apply(lambda x: [i for i in x if i == 1], axis=1)\n\n# Rename the new column\nresult.columns += '_category'\n\n# Print the result\nprint(result)\n",
        "\ndf['Date'] = df['Date'].dt.strftime(\"%B-%Y\")\n",
        "\ndf['Date'] = df['Date'].dt.strftime(\"%d-%B-%Y\")\n# [Missing Code]\n",
        "\nresult = df[df['Date'].dt.between(List[0], List[1])].apply(lambda x: x.strftime('%B %Y %A'))\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the last row of the first column up 1 row\nresult = df.shift(1, axis=0)\n\n# Shift the first row of the first column to the last row, first column\nresult.loc['1980-01-01'] = df.iloc[0, 0]\n\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the first row of the first column down 1 row\ndf.loc['1980-01-01', '#1'] = df.loc['1980-01-02', '#1']\n\n# Shift the last row of the first column to the first row, first column\ndf.loc['1980-01-01', '#1'] = 72.4399\ndf.loc['1980-01-05', '#1'] = 11.6985\n\n# Rearrange the Dataframe to minimize the R^2 values of the first and second columns\ndf = df[['#1', '#2']].reindex(df[['#1', '#2']].min(axis=1))\n\nresult = df\nprint(result)\n",
        "\ndf.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX'}, inplace=True)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\n# Rename columns with \"X\"\nresult = df.rename(columns={'HeaderA': 'XHeaderA', 'HeaderB': 'XHeaderB', 'HeaderC': 'XHeaderC'})\nprint(result)\n",
        "\ndf.columns = [f'X{i}' for i in range(4)]\ndf.loc[0, 'HeaderAX'] = '476'\ndf.loc[0, 'HeaderBX'] = '4365'\ndf.loc[0, 'HeaderCX'] = '457'\ndf.loc[0, 'HeaderX'] = '345'\n",
        "\n# To get the grouped mean for all the other columns, we can use the following code:\nresult = df.groupby('group').agg({'group_color': 'first', 'val1': 'mean', 'val2': 'mean', 'val3': 'mean'})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"sum\", \"val3\": \"sum\"})\n",
        "\nresult = df.groupby('group').agg({\"group_color\": \"first\", \"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"mean\"})\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.mean(axis=0, row_index=row_list, columns=column_list)\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = df.sum(axis=0, columns=column_list, subset=row_list)\nprint(result)\n",
        "\nresult = df.loc[row_list, column_list].sum()\n",
        "\nresult = df.value_counts()\n# [Missing Code]\n",
        "\nresult = df.isnull().sum()\n# [Missing Code]\n",
        "\nresult = df.value_counts()\n# [Missing Code]\n",
        "\n# Merge the first and second row\nresult = df.iloc[[0]].merge(df.iloc[[1]])\n",
        "\n# Merge the first and second row\nresult = df.iloc[[0]].merge(df.iloc[[1]])\n",
        "\ndf.fillna(df.mean(), inplace=True)\n# [Missing Code]\n",
        "\ndf.fillna(df.mean(), inplace=True)\n# [Missing Code]\n",
        "\ndf.apply(lambda x : (x[x.isnull()].values.tolist()+x[x.notnull()].values.tolist()),0)\n",
        "\nresult = df.loc[df['value'] < thresh].sum().reset_index()\nresult.index += 1\nresult.lab = result.index.map(df.index)\nresult.value = result.value.sum()\n",
        "\n# Select the rows smaller than the threshold\nmask = df['value'] < thresh\n# Group the selected rows and compute the average\nresult = df.loc[mask].groupby('lab')['value'].mean().reset_index()\n",
        "\n# Find the index of the rows to be substituted\nrows_to_substitute = df.index[df['value'] >= section_left]\n# Calculate the average of the substituted rows\naverage = df.loc[rows_to_substitute]['value'].mean()\n# Substitute the rows with the average value\nresult = df.loc[df.index[~rows_to_substitute], 'value'].mean()\n",
        "\ninv_cols = ['inv_' + col for col in df.columns]\ndf[inv_cols] = df.apply(lambda x: 1/x, axis=1)\n# [Missing Code]\n",
        "\nexponents = {'A': 1, 'B': 0}\nresult = df.copy()\nfor col, exp in exponents.items():\n    result[f'exp_{col}'] = np.exp(result[col])\n",
        "\ninv_dict = {}\nfor col in df.columns:\n    inv_dict[col] = 1/df[col].iloc[0]\n# [Missing Code]\nresult = df.copy()\nresult['inv_A'] = inv_dict['A']\nresult['inv_B'] = inv_dict['B']\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n# Calculate the sigmoids of each existing column\ndf_sigmoid = df.apply(lambda x: [1/(1+e^(-1)), 1/(1+e^(-2)), 1/(1+e^(-3))], axis=1)\n\n# Rename the columns based on existing column names with a prefix\ndf_sigmoid.columns = ['sigmoid_' + str(i) for i in range(len(df_sigmoid.columns))]\n\n# Concatenate the original dataframe with the new columns\nresult = pd.concat([df, df_sigmoid], axis=1)\n\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\nidx = pd.date_range('2017', periods=a.shape[0])\ndf = pd.DataFrame(a, index=idx, columns=list('abc'))\n\n# Find the index of the minimum for each column\nmin_idx = df.idxmin()\n\n# Create a boolean mask to select rows where the maximum occurs after the minimum\nmax_mask = df.shift(min_idx) > df.iloc[min_idx]\n\n# Use the mask to select the index of the last occurrence of the column-wise maximum, up to the location of the minimum\nresult = df.loc[max_mask & (df.index > min_idx)]\n\nprint(result)\n",
        "\nresult = df.loc[df.idxmax().groupby(df.index).idxmin()].reset_index(drop=True)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date, freq='D')})\nresult['user'] = df['user'].repeat(result.index+1)\nresult['val'] = 0\nresult = result.fillna(result.val)\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date, freq='D')})\nresult['user'] = df['user'].repeat(result.index+1)\nresult['val'] = 0\nresult = result.merge(df[['user', 'val']], on='user')\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date, freq='D')})\nresult['user'] = df['user'].repeat(result.index+1)\nresult['val'] = 233\n",
        "\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_dt, max_dt, freq='D')})\nresult['user'] = df['user'].repeat(result.index+1)\nresult['val'] = df['val'].max()\n",
        "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\nresult = pd.DataFrame({'dt': pd.date_range(min_date, max_date, freq='D')})\nresult['user'] = df['user'].repeat(result.index+1)\nresult['val'] = df['val'].max()\nresult = result.fillna(result['val'])\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a dictionary to map names to unique IDs\nname_map = {}\nfor name in df['name']:\n    if name not in name_map:\n        name_map[name] = 1\n    else:\n        name_map[name] += 1\n\n# Replace names with unique IDs\ndf['ID'] = df['name'].map(name_map)\n\n# Rename column to \"name\"\ndf.rename(columns={'ID': 'name'})\n\n# Print the result\nprint(df)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a dictionary to map each a value to a unique ID\nid_map = {value: i for i, value in enumerate(df['a'].values)}\n\n# Replace each a value with its corresponding unique ID\ndf['a'] = df['a'].map(id_map)\n\n# Print the resulting dataframe\nprint(df)\n",
        "\n    # Create a dictionary to map names to unique IDs\n    name_dict = {}\n    for name in df['name']:\n        if name not in name_dict:\n            name_dict[name] = 1\n        else:\n            name_dict[name] += 1\n    # [Missing Code]\n    df['id'] = df['name'].map(name_dict)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a dictionary to store the unique IDs\nid_dict = {}\n\n# Loop through each row in the dataframe\nfor index, row in df.iterrows():\n    # Check if the name already exists in the dictionary\n    if row['name'] in id_dict:\n        # If it does, add 1 to the existing ID and update the dictionary\n        id_dict[row['name']] = id_dict[row['name']] + 1\n    else:\n        # If it doesn't, add the name to the dictionary with a value of 1\n        id_dict[row['name']] = 1\n\n# Create a new dataframe with the unique IDs and columns b and c\nresult = pd.DataFrame({'ID': list(id_dict.values()),\n                       'b': df['b'].values,\n                       'c': df['c'].values})\n\n# Print the result\nprint(result)\n",
        "\nresult = df.pivot_table(index='user', columns=['01/12/15', '02/12/15'], values='someBool', aggfunc=lambda x: x.apply(lambda x: x if x == True else -1))\n",
        "\nresult = df.pivot_table(index='user', columns=['01/12/15', '02/12/15'], values=['someBool'], aggfunc=lambda x: x)\nresult.columns = ['others', 'value']\nresult.fillna(inplace=True)\n",
        "\nresult = df.pivot_table(index='user', columns=['01/12/15', '02/12/15'], values='someBool', aggfunc=lambda x: x.apply(lambda x: x if x is not None else 0))\n",
        "\nresult = df[df['c'] > 0.5][columns].values\n",
        "\nresult = df[df['c'] > 0.45][columns].values\n",
        "\n    filtered_df = df[df['c'] > 0.5]\n    result = filtered_df[columns].to_numpy()\n",
        "\n    result = df.loc[df['c'] > 0.5, columns].sum(axis=1)\n",
        "\n    locs = df.columns.get_loc(columns)\n    result = df[df['c'] > 0.5].iloc[:, locs]\n",
        "\nfilter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((row['date'] + timedelta(days=i)))\nresult = df[~df.index.isin(filter_dates)]\n",
        "\nfilter_dates = []\nfor index, row in df.iterrows():\n     if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(months=i)))\ndf = df[~df.index.isin(filter_dates)]\n",
        "\nfilter_dates = []\nfor index, row in df.iterrows():\n    if observation_time == 'D':\n        for i in range(1, observation_period):\n            filter_dates.append((index.date() + timedelta(months=i)))\ndf = df[~df.index.isin(filter_dates)]\nresult = df.drop_duplicates(subset=['ID', 'date', 'close'])\nresult = result.sort_values(by=['ID', 'date'])\nresult.reset_index(inplace=True)\nresult.index += 1\nresult.index.name = 'ID'\nresult.columns = ['date', 'close']\nresult = result.sort_values(by=['date'])\nresult.date = pd.to_datetime(result.date)\nresult.date = result.date.dt.date\nresult.drop('date', axis=1, inplace=True)\nprint(result)\n",
        "\ndef bin_dataframe(df):\n    result = df.groupby(df.index // 3)['col1'].mean().reset_index()\n    return result\n# [Missing Code]\n",
        "\nresult = df.groupby(df.index // 3).agg({'col1':['first', 'mean', 'last']})\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1, 4]})\n\nresult = df.groupby(df.index // 4)[df.col1].sum().reset_index(drop=True)\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\nresult = df.groupby(df.index // 3)['col1'].mean().reset_index()\nresult.index += (result.index // 3) * 3\nresult.index += 1\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\nresult = df.groupby(df.index // 3)['col1'].sum().reset_index()\nresult['col1'] = result['col1'] / (result.index.size - 1)\nresult = result.drop(columns=['index'])\n\nprint(result)\n",
        "\n# [Missing Code]\n",
        "\nimport pandas as pd\n\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Fill the zeros with the previous non-zero value\ndf.fillna(method='ffill', inplace=True)\n\nresult = df\nprint(result)\n",
        "\ndf.fillna(df.mode()[0], inplace=True)\n",
        "\nimport pandas as pd\n\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\n# Fill the zeros with the maximun between previous and posterior non-zero value\ndf.fillna(df.groupby(df.index).max(), inplace=True)\n\nresult = df\nprint(result)\n",
        "\ndf['number'] = df['duration'].str.extract('(\\d+)', expand=False)\ndf['time'] = df['duration'].str.extract('(\\w+)', expand=False)\ndf['time_days'] = pd.to_timedelta(df['time'], unit='D')\nresult = df.drop(['duration'], axis=1)\nprint(result)\n",
        "\ndf['time'] = pd.to_timedelta(df['duration'], unit='D')\ndf['number'] = df['duration'].str.extract('(\\d+)', expand=False)\ndf['time_day'] = df['time'].dt.days\n",
        "\n    df['number'] = df['duration'].str.extract('(\\d+)', expand=False)\n    df['time'] = df['duration'].str.extract('(year|month|week|day)', expand=False)\n    df['time_days'] = pd.to_timedelta(df['time'], unit='D')\n",
        "\ndf['time'] = df['duration'].str.extract('(\\d+)', expand=False)\ndf['number'] = df['duration'].str.extract('(\\w+)', expand=False)\ndf['time_day'] = pd.to_timedelta(df['time'], unit='d')\ndf = df.join(pd.DataFrame({'time_day': [x.days for x in df['time_day']]}, index=df.index))\n",
        "\ncheck = np.where([df1[column] != df2[column] for column in columns_check_list])\nresult = [any(check[i]) for i in range(len(check))]\n",
        "\ncheck = np.where([df1[column] == df2[column] for column in columns_check_list])\nresult = [df1.loc[~check].Postset == df2.loc[~check].Preset for column in columns_check_list]\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n# [Missing Code]\ndf.index = pd.DatetimeIndex(df.index)\n",
        "\ndf.index.levels[1] = pd.to_datetime(df.index.levels[1])\n# [Missing Code]\ndf.index.names = ['name', 'date']\n",
        "\nimport pandas as pd\nimport datetime\ndef f(df):\n    # Convert date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Convert index to datetime\n    df.set_index('date', inplace=True)\n    # Convert index to datetime\n    df.index = pd.to_datetime(df.index)\n    # Reset index\n    df.reset_index(inplace=True)\n    return df\n",
        "\ndf = pd.DataFrame(data)\ndf.index = pd.to_datetime(df.index)\ndf = df.swaplevel(0, 1)\nreturn df\n",
        "\n# Create a new dataframe with the reshaped data\nresult = pd.DataFrame(result.pivot_table(index='Country', columns='Variable', values='2000:2005'), \n                      columns=['year'], \n                      aggfunc=lambda x: x)\n",
        "\nresult = df.melt(id_vars=['Country'], value_name='Var', var_name='year')\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\n# Filter the data frame\nresult = df[abs(df['Value_A']) < 1]\nprint(result)\n",
        "\nresult = df[abs(df['Value_A']) > 1]\n",
        "\nresult = df[abs(df['Value_B']) > 1] & abs(df['Value_C']) > 1 & abs(df['Value_D']) > 1\nresult = result.drop(['Value_B', 'Value_C', 'Value_D'], axis=1)\nprint(result)\n",
        "\nresult = df.apply(lambda x: x.replace('&AMP;', '&'))\n",
        "\nresult = df.apply(lambda x: x.replace('&LT;', '<'))\n",
        "\n    df.loc[df['A'].str.contains('&AMP;'), 'A'] = df['A'].str.replace('&AMP;', '&')\n    df.loc[df['A'].str.contains('&AMP;'), 'C'] = df['A'].str.replace('&AMP;', '&')\n",
        "\nresult = df.apply(lambda x: x.replace('&AMP;', '&''<''>').replace('&LT;', '&''<''>').replace('&GT;', '&''<''>'), axis=1)\n",
        "\nresult = df.apply(lambda x: x.replace('&AMP;', '&'))\n",
        "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name.split(' ')[0]\n    else:\n        return name\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if x == df['first_name'] else x)\n",
        "\ndf['1_name'] = df['name'].apply(lambda x: validate_single_space_name(x) if validate_single_space_name(x) else x)\ndf['2_name'] = df['name'].apply(lambda x: validate_single_space_name(x) if validate_single_space_name(x) else x)\ndf.drop(['name'], axis=1, inplace=True)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\n\n# Split the name column into first_name, middle_name and last_name IF there is more than one space in the name\ndf['first_name'] = df['name'].str.split(' ', n=1, expand=True)\ndf['middle_name'] = df['name'].str.split(' ', n=2, expand=True)\ndf['last_name'] = df['name'].str.split(' ', n=3, expand=True)\n\n# Remove rows where the name cannot be split into first and last name\ndf = df.loc[df['first_name'].notnull() & df['last_name'].notnull()]\n\n# Fill in missing values with the first name\ndf.loc[df['middle_name'].isnull() & df['last_name'].notnull(), 'middle_name'] = df['first_name']\n\n# Fill in missing values with the last name\ndf.loc[df['first_name'].isnull() & df['middle_name'].notnull(), 'first_name'] = df['last_name']\n\n# Fill in missing values with the first name\ndf.loc[df['first_name'].isnull() & df['middle_name'].isnull() & df['last_name'].notnull(), 'first_name'] = df['last_name']\n\n# Fill in missing values with the last name\ndf.loc[df['first_name'].isnull() & df['middle_name'].isnull() & df['last_name'].isnull(), 'last_name'] = df['name']\n\n# Rename columns\ndf = df.rename(columns={'first_name': 'first name', 'middle_name': 'middle_name', 'last_name': 'last_name'})\n\n# Print the final DataFrame\nprint(df)\n",
        "\n# We can use the merge function in pandas to join the two dataframes based on the timestamp\nresult = df1.merge(df2, on='Timestamp', how='outer')\n",
        "\n# We can use the merge function in pandas to join the two dataframes based on the timestamp\nresult = df1.merge(df2, on='Timestamp', how='outer')\n",
        "\nresult = df.assign(state=lambda x: x['col1'] if x['col2'] <= 50 and x['col3'] <= 50 else max(x['col1'], x['col2'], x['col3']))\nprint(result)\n",
        "\nresult = df.assign(state=lambda x: x['col1'] if x['col2'] + x['col3'] > 50 else x['col1'] + x['col2'] + x['col3'])\n",
        "\nerrors = []\nfor index, row in df.iterrows():\n    if not isinstance(row[\"Field1\"], int):\n        errors.append(row[\"Field1\"])\n",
        "\nint_list = []\nfor index, row in df.iterrows():\n    if isinstance(row[\"Field1\"], int):\n        int_list.append(row[\"Field1\"])\n    else:\n        print(\"Error: Non-integer value found in row {}.\".format(index))\n",
        "\nimport pandas as pd\n\nexample_df = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\ndef f(df=example_df):\n    errors = []\n    for index, row in df.iterrows():\n        if not isinstance(row[\"Field1\"], int):\n            errors.append(row[\"Field1\"])\n    return errors\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Calculate the percentage of each value for each category\nresult = df.apply(lambda x: {\n    'val1': (x['val1'] / x['val1'].sum()) * 100,\n    'val2': (x['val2'] / x['val2'].sum()) * 100,\n    'val3': (x['val3'] / x['val3'].sum()) * 100,\n    'val4': (x['val4'] / x['val4'].sum()) * 100\n}.items(), axis=1)\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\n# Calculate the percentage of each category\nresult = df.apply(lambda x: (x - 1) / 2, axis=1)\nprint(result)\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\nresult = df.loc[test]\n",
        "\n    result = df.loc[test]\n",
        "\n# Calculate the Euclidean distance between each car and all other cars\ndf['euclidean_distance'] = df.apply(lambda x: ((x['x'] - x['car'])**2 + (x['y'] - x['car'])**2)**0.5, axis=1)\n# Get the nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('car')['euclidean_distance'].idxmin()\n# Calculate the average distance for each frame\ndf['avg_distance'] = df.groupby('time')['euclidean_distance'].mean()\n",
        "\n# Calculate the farthest neighbor for each car\ndf['farmost_neighbour'] = df.groupby('car')['car'].transform('max')\n# Calculate the Euclidean distance between each car and its farthest neighbor\ndf['euclidean_distance'] = df.groupby('car')['car'].apply(lambda x: ((x - df['car'].iloc[df['farmost_neighbour'] == x]['car']).pow(2).sum() ** 0.5))\n# Calculate the average distance for each time point\nresult = df.groupby('time')['euclidean_distance'].mean()\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \",\".join(cols), axis=1)\n",
        "\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\ncols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n\nprint(df)\n",
        "\n# randomly select 20% of rows\nsampled_df = df.sample(int(0.2 * len(df)))\n# change the value of the Quantity column of these rows to zero\nsampled_df['Quantity'] = sampled_df['Quantity'].where(sampled_df['Quantity'] != 0, 0)\n# keep the indexes of the altered rows\nresult = sampled_df.reset_index(drop=True)\n",
        "\n# randomly select 20% of rows\nsampled_df = df.sample(int(0.2 * len(df)))\n# change the ProductId of the selected rows to zero\nsampled_df.loc[sampled_df['ProductId'].isin(sampled_df['ProductId'].dropna()), 'ProductId'] = 0\n# keep the indexes of the altered rows\nsampled_df = sampled_df.reset_index(drop=True)\n",
        "\nresult = df.sample(int(len(df) * 0.2))\nresult['Quantity'] = result['Quantity'].fillna(0)\nresult = result.reset_index(drop=True)\nprint(result)\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\n# [Missing Code]\n# Add a column referring to the index of the first duplicate (the one kept)\nduplicate['index_original'] = duplicate.index.get_level_values('index').copy()\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\n# [Missing Code]\n# Add a column referring to the index of the last duplicate (the one kept)\nduplicate['index_original'] = duplicate.index.get_level_values('index').duplicated(keep='last').reset_index(level=0, drop=True)\n",
        "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = df.index[df.duplicated(subset=['col1','col2'], keep='first') == True]\n    # [Missing Code]\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\n# [Missing Code]\n",
        "\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\n# [Missing Code]\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# Group the DataFrame by Sp and Mt columns\ngrouped = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Get the index of the grouped DataFrame\nindex = grouped.index\n\n# Select the rows from the original DataFrame whose index is in the index of the grouped DataFrame\nresult = df.loc[index]\n\nprint(result)\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max()\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# Group the DataFrame by Sp and Mt columns\ngrouped = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Get all rows in the original DataFrame which have the min value for count column\nresult = df[df['count'].isin(grouped)]\n\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n\n# Group the DataFrame by ['Sp','Value']\ngrouped = df.groupby(['Sp','Value'])['count'].max()\n\n# Get all rows in the original DataFrame which have the max value for count column, after grouping by ['Sp','Value']\nresult = df.loc[grouped.index]\n\n# Print the result\nprint(result)\n",
        "\ndf.query(\"Category\".isin(filter_list))\n",
        "\ndf.query(\"Category not in filter_list\")\n",
        "\n# Define a function to get the column levels\ndef get_column_levels(df):\n    levels = []\n    for i in range(len(df.columns)):\n        levels.append((df.columns[i][0], df.columns[i][1], df.columns[i][2]))\n    return levels\n# [Missing Code]\nresult = pd.melt(df, value_vars=get_column_levels(df))\n",
        "\n# Define a function to melt the DataFrame\ndef melt_df(df):\n    # Get the column names\n    col_names = df.columns.values.tolist()\n    \n    # Get the list of tuples for value_vars\n    value_vars = []\n    for i in range(len(col_names)):\n        for j in range(i+1, len(col_names)):\n            for k in range(j+1, len(col_names)):\n                value_vars.append((col_names[i], col_names[j], col_names[k]))\n    \n    # Melt the DataFrame\n    result = pd.melt(df, id_vars=col_names[:len(value_vars)], value_vars=value_vars)\n    \n    # Rename the columns\n    result.columns = [list('EABCDEFGHIJ')]\n    \n    return result\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n",
        "\nresult = df.groupby('id').cumsum('val')\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n# [Missing Code]\n",
        "\ndf['cummax'] = df.groupby('id').cummax(['val'])\n",
        "\ndf['cumsum'] = df.groupby('id').cumsum(df['val'])\ndf.loc[df['val'] < 0, 'val'] = 0\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('l')['v'].apply(lambda x: np.nan if np.isnan(x).any() else x.sum())\nresult = result.fillna(0)\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('r')['v'].sum()\nresult['right'] = np.where(result['right'].isnull(), np.nan, result['right'])\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\nresult = df.groupby('l')['v'].apply(lambda x: np.where(np.isnan(x), np.nan, np.sum(x)))['right']\nprint(result)\n",
        "\ndef get_relationships(df):\n    relationships = []\n    for i in range(len(df.columns)):\n        for j in range(len(df.columns)):\n            if i != j:\n                if df[i].apply(lambda x: x == df[j]).any():\n                    relationships.append(f\"{df.columns[i]} {df.columns[j]} many-to-many\")\n                else:\n                    if df[j].apply(lambda x: x == df[i]).any():\n                        relationships.append(f\"{df.columns[i]} {df.columns[j]} many-to-one\")\n                    else:\n                        relationships.append(f\"{df.columns[i]} {df.columns[j]} one-to-one\")\n    return relationships\n\nresult = get_relationships(df)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# Create a dictionary to store the relationships\nrelationships = {}\n\n# Loop through each pair of columns\nfor i in range(len(df)):\n    for j in range(i+1, len(df)):\n        # Check if the relationship is one-to-one, one-to-many, many-to-one, or many-to-many\n        if df[i].isin(df[j]).any() and df[j].isin(df[i]).any():\n            relationships[df[i] + ' ' + df[j]] = 'many-2-many'\n        elif df[i].isin(df[j]).all() and df[j].isin(df[i]).all():\n            relationships[df[i] + ' ' + df[j]] = 'one-2-one'\n        elif df[j].isin(df[i]).any() and df[i].isin(df[j]).all():\n            relationships[df[i] + ' ' + df[j]] = 'one-2-many'\n        elif df[i].isin(df[j]).all() and df[j].isin(df[i]).any():\n            relationships[df[i] + ' ' + df[j]] = 'many-2-many'\n        else:\n            relationships[df[i] + ' ' + df[j]] = 'none'\n\n# Print the relationships\nprint(relationships)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# Create a dictionary to store the relationships\nrelationships = {'Column1': 'one-to-many',\n                 'Column2': 'many-to-one',\n                 'Column3': 'many-to-many',\n                 'Column4': 'one-to-one',\n                 'Column5': 'many-to-many'}\n\n# Create a new column to store the relationships\ndf['Relationship'] = [relationships['Column1']] * len(df)\n\n# Update the relationships for the remaining columns\nfor i in range(1, len(df)):\n    if df.iloc[i].isin(df.iloc[i-1]).any():\n        df.at[i, 'Relationship'] = relationships['Column2']\n    else:\n        df.at[i, 'Relationship'] = relationships['Column3']\n\n# Print the updated DataFrame\nprint(df)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# Create a dictionary to store the relationships\nrelationships = {\n    'Column1': 'one-2-many',\n    'Column2': 'many-2-one',\n    'Column3': 'many-2-many',\n    'Column4': 'one-2-many',\n    'Column5': 'many-2-many'\n}\n\n# Create a new column in the dataframe to store the relationships\ndf['Relationship'] = [relationships[col] for col in df.columns]\n\n# Print the dataframe with the new column\nprint(df)\n",
        "\n# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\nresult = df.loc[uniq_indx]\n# print the result\nprint(result)\n",
        "\nresult = pd.to_numeric(s.astype(str).str.replace(',',''), errors='coerce')\n# [Missing Code]\n",
        "\nresult = df.groupby(df.apply(lambda x: (x['SibSp'] > 0) | (x['Parch'] > 0)).reset_index()['SibSp'] > 0 + df.apply(lambda x: (x['SibSp'] == 0) & (x['Parch'] == 0)).reset_index()['SibSp'] == 0).mean()\n",
        "\nresult = df.groupby((df['Survived'] > 0) | (df['Parch'] > 0)).mean()\nresult = result.groupby((df['Survived'] == 0) & (df['Parch'] == 0)).mean()\nprint(result)\n",
        "\nresult = df.groupby(by=['SibSp', 'Parch'])['Survived'].mean().reset_index()\n",
        "\nresult = df.groupby('cokey').sort_values(by=['A'])\n",
        "\nresult = df.groupby('cokey').sort_values(by=['A'])\n",
        "\nresult = df.stack().reset_index().rename(columns={'level_1':'Caps', 'level_2':'Lower'}).reset_index()\nresult.columns = ['Caps', 'Lower', 'A', 'B']\nprint(result)\n",
        "\nresult = df.stack().reset_index().rename(columns={'level_1':'Caps', 'level_2':'Middle', 'level_3':'Lower'})\nresult = result.reset_index()\nresult.index += 1\nresult.columns = ['Caps', 'A', 'B']\nprint(result)\n",
        "\nresult = df.stack().reset_index(level=1).rename(columns={0: 'Caps'}).reset_index(drop=True)\nresult = result.iloc[:, :-1]\nresult.columns = ['Caps', 'A', 'B']\nresult.index = result.index.droplevel(0)\nprint(result)\n",
        "\nresult = pd.DataFrame(someTuple[0], columns=['birdType'])\nresult['birdCount'] = someTuple[1]\nresult.sort_values(by=['birdType'], inplace=True)\n",
        "\nresult = df.groupby('a').b.apply(lambda x: (np.mean(x), np.std(x)))\n",
        "\nresult = df.groupby('b').a.apply(lambda x: (x.mean(), x.std()))\n",
        "\nresult = df.groupby('a').apply(lambda x: (x['b'] - x['b'].min()) / (x['b'].max() - x['b'].min())).reset_index()\nresult['b_softmax'] = result['b'] / result['b'].sum(axis=0)\nresult = result.drop(['b'], axis=1)\nresult = result.reset_index(inplace=True)\nresult.index += 1\nprint(result)\n",
        "\ndf.loc[(df['A'] == 0) & (df['B'] == 0), 'A'] = None\ndf.loc[(df['A'] == 0) & (df['B'] == 0), 'B'] = None\ndf.loc[(df['A'] == 0) & (df['B'] == 0), 'C'] = None\ndf.loc[(df['A'] == 0) & (df['B'] == 0), 'D'] = None\n# [Missing Code]\ndf.dropna(inplace=True)\n",
        "\ndf.loc[(df['A']==0) & (df['B']==0), 'A'] = None\ndf.loc[(df['A']==0) & (df['B']==0), 'B'] = None\ndf.loc[(df['A']==0) & (df['B']==0), 'C'] = None\ndf.loc[(df['A']==0) & (df['B']==0), 'D'] = None\n# [Missing Code]\n",
        "\ndf = df.drop(df.idxmax())\n# [Missing Code]\n",
        "\nresult = df.loc[(df.max() == 2), :]\n",
        "\ns = s.sort_values(by=['index', 'value'])\nresult = s.to_series()\n",
        "\nimport pandas as pd\n\n\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n              index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n\n# Sort the Series by value and index\nresult = s.sort_values(by=['index', '1'])\n\n# Create a dataframe\ndf = pd.DataFrame({'index': result.index, '1': result['1']})\n\nprint(df)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\n# Convert column A to numeric\ndf['A'] = pd.to_numeric(df['A'])\n\n# Select records where A value are integer or numeric\nresult = df[df['A'].astype(int) | df['A'].astype(float)]\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\n# Convert A column to string\ndf['A'] = df['A'].astype(str)\n\n# Select records where A value is string\nresult = df[df['A'].notnull()]\n\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# Group the DataFrame by Sp and Mt columns\ngrouped = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Get the index of the grouped DataFrame\nindex = grouped.index\n\n# Create a new DataFrame with the original DataFrame and the max count values\nresult = pd.concat([df, grouped], axis=1, keys=['Original', 'Max Count'])\n\n# Rename the columns of the result DataFrame\nresult.columns = df.columns\n\n# Drop the 'Max Count' column from the result DataFrame\nresult.drop('Max Count', axis=1, inplace=True)\n\nprint(result)\n",
        "\nresult = df.groupby(['Sp','Mt'])['count'].max()\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# Group the DataFrame by Sp and Mt columns\ngrouped = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Get all rows in the original DataFrame which have the min value for count column\nresult = df[df['count'].isin(grouped)]\n\n# Print the result\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n\nresult = df.groupby(['Sp','Value'])['count'].max()\n",
        "\nresult = df.apply(lambda x: dict[x['Member']] if x['Member'] in dict else np.nan, axis=1)\n",
        "\nresult = df.apply(lambda x: dict[x['Member']] if x['Member'] in dict else np.nan, axis=1)\n",
        "\n    df['Date'] = df['Member'].map(dict)\n",
        "\nresult = df.apply(lambda x: dict[x['Member']] if x['Member'] in dict else np.nan, axis=1)\nresult['Date'] = pd.to_datetime(result['Date'])\nresult = result.fillna(df['Member'])\nresult = result.apply(lambda x: x['Date'].strftime('%d-%B-%Y'), axis=1)\nprint(result)\n",
        "\ndf1 = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).agg({'count'})\nprint(df1)\n# [Missing Code]\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['month'] = df['Date'].dt.month\ndf['year'] = df['Date'].dt.year\ndf['Count_m'] = df.groupby([df['month'], df['year']]).size()\ndf['Count_Val'] = df.groupby([df['Val'], df['month'], df['year']]).size().reset_index(name='count')\n# [Missing Code]\ndf = df.drop(['Date'], axis=1)\n",
        "\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Val'].count()\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')])['Val'].count().reset_index(name='count')\ndf['Count_w'] = df.groupby([df['Date'].dt.dayofweek.rename('dayofweek'), df['Date'].dt.month.rename('month')])['Val'].count()\ndf['Count_Val'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month'), df['Date'].dt.dayofweek.rename('dayofweek')])['Val'].count()\n",
        "\nresult1 = df.loc[df['B'] == 0, 'B'].sum()\nresult2 = df.loc[df['B'] != 0, 'B'].sum()\nresult1 = df.loc[df['C'] == 0, 'C'].sum()\nresult2 = df.loc[df['C'] != 0, 'C'].sum()\n",
        "\ndef count_even_odd(df, column):\n    even = df[df[column] % 2 == 0].sum()\n    odd = df[df[column] % 2 != 0].sum()\n    return even, odd\n\nresult1, result2 = count_even_odd(df, 'B')\nprint(result1)\nprint(result2)\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=[np.sum, np.mean])\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
        "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=lambda x: max(x) if x.name == 'D' else min(x))\n",
        "\ndf = df.apply(lambda x: pd.Series(x[1].split(','))).stack().reset_index(level=1, drop=True)\n# [Missing Code]\n",
        "\ndf = df.apply(lambda x: pd.Series(x[1].split(','))).reset_index()\n# [Missing Code]\ndf.columns = ['var1', 'var2']\ndf.index += 1\n",
        "\nresult = df.apply(lambda x: pd.Series(x.split('-')), axis=1)\n# [Missing Code]\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n# [Missing Code]\ndf[\"new\"] = df.apply(count_special_char, axis = 0)\n",
        "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n# [Missing Code]\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALABAMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n\n# Split the 'row' column into two columns: 'fips' and 'row'\nresult = df.str.split(' ', expand=True)\nresult.columns = ['fips', 'row']\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'row': ['114 AAAAAA', '514 ENENEN',\n                           '1926 HAHAHA', '0817 O-O,O-O',\n                           '998244353 TTTTTT']})\n\n# Split the 'row' column into two columns named 'fips' and 'row'\nresult = df.str.split(' ', expand=True)\nresult.columns = ['fips', 'row']\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALAB AMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n\n# Split the 'row' column into three new columns: 'fips', 'medi', and 'row'\nresult = df.row.str.split(' ', n=3, expand=True)\nresult.columns = ['fips', 'medi', 'row']\n",
        "\nresult = df.groupby('Name').apply(lambda x: x.fillna(0).cumsum().dropna().astype(float)/len(x))\n",
        "\nresult = df.groupby('Name').apply(lambda x: x.iloc[-1] if x.iloc[-1] != 0 else x.iloc[-2])\n",
        "\n    # [Missing Code]\n",
        "\nresult = df.groupby('Name').apply(lambda x: x.iloc[::-1].mean().dropna())\n",
        "\ndf['Label'] = (df['Close'].sub(df['Close'].shift(1)) > 1)\ndf['Label'] = df['Label'].astype(int)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\n\n# Create a new column called \"label\"\nresult = df.assign(label=lambda x: [1 if (x['Close'] - x['Close'].shift()) > 0 else 0 if (x['Close'] - x['Close'].shift()) == 0 else -1 for x in df.iterrows()])\n\nprint(result)\n",
        "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x == 0 or x == 1 else -1).add(1)\n# [Missing Code]\n",
        "\n# Calculate the time difference between 1st row departure time and 2nd row arrival time\ndf['Duration'] = df.departure_time.iloc[0] - df.arrival_time.iloc[1]\n\n",
        "\n# Calculate the time difference between 1st row departure time and 2nd row arrival time\ndf['Duration'] = df.departure_time.iloc[0] - df.arrival_time.iloc[1]\n",
        "\nimport pandas as pd\n\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\n# Convert arrival_time and departure_time to datetime format\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\n# Calculate duration between 1st row departure time and 2nd row arrival time\ndf['Duration'] = df['departure_time'].iloc[1] - df['arrival_time'].iloc[0]\n\n# Convert duration to second\ndf['Duration'] = df['Duration'].dt.total_seconds()\n\n# Convert arrival_time and departure_time to desired format\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%B-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%B-%Y %H:%M:%S')\n\nresult = df\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x['key2'].eq('one').sum())\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df.groupby(['key1']).apply(lambda x: x['key2'].eq('two').sum())\n\nprint(result)\n",
        "\nresult = df.groupby(['key1']).apply(lambda x: x.value_counts() if 'e' in x['key2'].unique() else 0).reset_index()\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'value':[10000,2000,2014-03-27,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\nmax_result, min_result = df.index[0], df.index[-1]\nprint(max_result, min_result)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'value':[10000,2000,2014-03-27,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\n# Get the mode date\nmode_date = df.groupby(df.index).agg('mode')['value'].iat[0]\nprint(mode_date)\n\n# Get the median date\nmedian_date = df.groupby(df.index).agg('median')['value'].iat[0]\nprint(median_date)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\n\n# Filter the DataFrame to only contain rows where the closing_price is between 99 and 101\nresult = df[(99 <= df['closing_price'] <= 101)]\n\nprint(result)\n",
        "\ndf = df[df['closing_price'] != 99]\ndf = df[df['closing_price'] != 101]\nresult = df.dropna()\n",
        "\nresult = df.groupby(\"item\", as_index=False)[\"diff\"].min()[\"otherstuff\"]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[1:]\n",
        "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_', expand=True)[-1]\n# [Missing Code]\n",
        "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_', expand=True)[-1]\n",
        "\n# Calculate the number of NaN values\nn_nan = df['Column_x'].isnull().sum()\n# Calculate the number of NaN values in each half\nn_nan_half = int(n_nan / 2)\n# Fill the first half of NaN values with 0 and the second half with 1\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = np.where(df.index < n_nan_half, 0, 1)\n",
        "\n# Fill NaN values with '0' for the first 30% (round down) of NaN values, '0.5' for the middle 30% (round down) of NaN values, and '1' for the last 30% (round down) of NaN values.\ndf['Column_x'] = np.where(df['Column_x'].isnull(), \n                          df['Column_x'].fillna(0) if df.apply(lambda x: x == 0).sum() < 0.3 else \n                          df['Column_x'].fillna(0.5) if df.apply(lambda x: x == 0.5).sum() < 0.3 else \n                          df['Column_x'].fillna(1), \n                          df['Column_x'])\n",
        "\n# Fill NaN values with \"0\" or \"1\" so that the number of \"0\" is 50%(round down) and the number of \"1\" is 50%(round down).\n# First, calculate the total number of NaN values\nn_null = df['Column_x'].isnull().sum()\n# Then, fill in all zeros first\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = 0\n# Calculate the number of zeros and ones after filling in zeros\nzero_count = df[df['Column_x'] == 0].shape[0]\none_count = df[df['Column_x'] == 1].shape[0]\n# If the number of zeros is less than 50%, fill in ones\nif zero_count < int(0.5 * n_null):\n    df.loc[df['Column_x'].isnull(), 'Column_x'] = 1\n# If the number of ones is less than 50%, fill in zeros\nif one_count < int(0.5 * n_null):\n    df.loc[df['Column_x'].isnull(), 'Column_x'] = 0\n",
        "\nresult = pd.DataFrame([[(row['one'][0], row['one'][1]) for row in a.iterrows()], [(row['one'][0], row['one'][1]) for row in b.iterrows()]], columns=['one', 'two'])\n",
        "\nresult = pd.concat([a.apply(lambda x: (x[0], x[1], c.loc[c.index, 'one'])),\n                    b.apply(lambda x: (x[0], x[1], c.loc[c.index, 'two']))],\n                   axis=1)\n",
        "\nresult = pd.DataFrame([[(row['one'][i], col['one'][i]) for i in range(len(row))] for row in a.iterrows() for col in b.iterrows()], columns=['one', 'two'])\n",
        "\nresult = df.groupby(pd.cut(df.views, bins)).groupby('username').views.count().reset_index()\n",
        "\nresult = df.groupby(pd.cut(df.views, bins)).username.count().reset_index()\nresult.columns = ['views', 'username']\nresult['views'] = pd.cut(result.views, bins)\n",
        "\nresult = df.groupby(pd.cut(df.views, bins)).groupby('username').views.count().reset_index()\n",
        "\nresult = df['text'].apply(lambda x: ', '.join(x))\n",
        "\nresult = df['text'].apply(lambda x: '-'.join(x))\n",
        "\nresult = df['text'].agg(lambda x: ', '.join(x))\n",
        "\nresult = df.apply(lambda x: ', '.join(x['text']), axis=1)\n",
        "\nresult = df['text'].str.cat(sep='-')\n# [Missing Code]\n",
        "\nresult = pd.concat([df1, df2], axis=0)\nresult['date'] = pd.to_datetime(result['date'])\nresult.dropna(inplace=True)\nresult.drop(['date'], axis=1, inplace=True)\n",
        "\n# Concatenate df1 and df2 on 'id' and fill missing values in 'city', 'district', and 'date'\nresult = pd.concat([df1, df2], axis=0).fillna({'city': df1.city, 'district': df1.district, 'date': df1.date})\n",
        "\n# Concatenate df1 and df2 on 'id' and fill missing values in 'city', 'district', and 'date' with df1 values\nresult = pd.concat([df1, df2], axis=0).fillna(df1)\n",
        "\nresult = C.merge(D, how='outer', on='A')\nresult.loc[result.B_x.isna(), 'B'] = result.B_y\nresult = result.drop(['B_x', 'B_y'], axis=1)\n",
        "\nresult = C.merge(D, how='outer', on='A')\nresult.loc[result.B.isnull(), 'B'] = result.B_y\nresult = result.drop(['B_y'], axis=1)\n",
        "\nresult = C.merge(D, how='outer', on='A')\nresult['dulplicated'] = result.apply(lambda x: True if x['A_x'] == x['A'] else False, axis=1)\nresult = result.drop(['A_x', 'A_y'], axis=1)\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\nresult = df.groupby('user').apply(lambda x: x.sort_values(by=['time', 'amount']).tolist())\nprint(result)\n",
        "\nresult = df.groupby('user').agg(lambda x: [x['time'], x['amount']])\n# [Missing Code]\n",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\nresult = df.groupby('user').agg(lambda x: [x['time'], x['amount']])\nresult = result.sort_values(by=['amount', 'time'])\nresult = result.reset_index()\nresult.columns = ['amount-time-tuple']\nprint(result)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\n# Create a dataframe with the concatenated series\ndf = pd.concat([series.reset_index()], axis=1)\n\n# Rename the columns\ndf.columns = ['0', '1', '2', '3']\n\n# Print the result\nprint(df)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\n# Create a dataframe with the concatenated series\ndf = pd.concat([series.reset_index()], keys=series.index, axis=1)\n\n# Rename the columns\ndf.columns = ['name', '0', '1', '2', '3']\n\n# Print the result\nprint(df)\n",
        "\nresult = []\nfor col in df.columns:\n    if s in col and ' ' not in col:\n        result.append(col)\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\n# Find the column names that contain the string 'spike' but do not exactly match it\nresult = [col for col in df.columns if s in col and not col == s]\n\nprint(result)\n",
        "\nimport pandas as pd\n\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\n# Find all column names containing the substring 'spike'\ncolumn_names = [col for col in df.columns if s in col]\n\n# Rename the columns with a number at the end\nresult = df.rename(column_names)\n\nprint(result)\n",
        "\nresult = df.apply(lambda x: [y for y in x if y is not None], axis=1)\n",
        "\nresult = df.apply(lambda x: [y for y in x if y is not None], axis=1)\n",
        "\nresult = df.apply(lambda x: pd.Series(x) if isinstance(x, list) else pd.Series(x), axis=1)\n# [Missing Code]\n",
        "\nids = df.col1.apply(lambda x: list(x)).flatten()\n# [Missing Code]\n",
        "\nids = df.col1.apply(lambda x: ','.join(str(i) for i in reversed(x)))\n# [Missing Code]\n",
        "\nids = df.col1.apply(lambda x: ','.join(str(i) for i in x))\n# [Missing Code]\n",
        "\n# First, we need to create a new column called \"bin_time\" that will store the time of the beginning of each bin.\ndf['bin_time'] = pd.to_datetime(df['Time'].dt.floor('2min'))\n# Then, we need to groupby the \"bin_time\" column and calculate the mean of the \"Value\" column.\nresult = df.groupby('bin_time').mean()\n",
        "\n# First, we need to create a new column called \"bin_time\" that will store the start and end times of each bin.\ndf['bin_time'] = pd.to_datetime(df['Time'].dt.floor('3T'))\n# Then, we can groupby the \"bin_time\" column and sum the \"Value\" column.\nresult = df.groupby('bin_time').sum()\n",
        "[Missing Code]\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)",
        "[Missing Code]\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)",
        "\nresult = df.apply(lambda x: pd.to_datetime(x['TIME']).strftime('%d-%B-%Y %H:%M:%S'))\nresult['RANK'] = result.groupby('ID')['TIME'].rank(ascending=False)\nprint(result)\n",
        "\nresult = df[filt.apply(lambda x: x == df.index.get_level_values('a'))]\n",
        "\nresult = df[filt.apply(lambda x: x == df.index.get_level_values('a')) & filt.apply(lambda x: x == df.index.get_level_values('b'))]\n",
        "\nresult = df.loc[0].dropna().ne(df.loc[8].dropna().ne()).reset_index(drop=True)\n",
        "\nresult = df.iloc[0].equals(df.iloc[8])\n",
        "\nresult = []\nfor i in range(len(df)):\n    diff = df.iloc[0].diff(df.iloc[i])\n    result.extend([col for col in diff.columns if not np.isnan(diff.loc[col])])\nprint(result)\n",
        "\nresult = []\nfor i in range(len(df)):\n    for j in range(len(df[i])):\n        if df.iloc[i, j] != df.iloc[8, j]:\n            result.append((df.iloc[i, j], df.iloc[8, j]))\nprint(result)\n",
        "\nts = pd.Series(df['Value'], index=df['Date'])\n# [Missing Code]\n",
        "\ndf = df.stack().reset_index(level=0, drop=True)\n# [Missing Code]\ndf.columns = ['A_1', 'B_1', 'C_1', 'D_1', 'E_1', 'A_2', 'B_2', 'C_2', 'D_2', 'E_2', 'A_3', 'B_3', 'C_3', 'D_3', 'E_3']\n",
        "\nresult = df.reset_index(drop=True)\nprint(result)\n",
        "[Missing Code]\ndf['dogs'] = df['dogs'].apply(lambda x: round(x if not pd.isnull(x) else 0))\ndf['cats'] = df['cats'].apply(lambda x: round(x if not pd.isnull(x) else 0))\nresult = df\nprint(result)",
        "\nimport pandas as pd\n\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\n\n# Check if the value is null or pd.NA\ndef check_null(value):\n    if value is None or value == pd.NA:\n        return pd.NA\n    else:\n        return value\n\n# Apply the check_null function to each column\ndf['dogs'] = df['dogs'].apply(check_null)\ndf['cats'] = df['cats'].apply(check_null)\n\n# Round each value to 2 decimal places\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n\n# Fill in the null values with 0\ndf.fillna(0, inplace=True)\n\nresult = df\nprint(result)\n",
        "\ndf['Sum'] = sum(list_of_my_columns)\n",
        "\nimport pandas as pd\nimport numpy as np\n\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\n# Calculate the average of the selected columns for each row\nresult = df.apply(lambda row: np.average(row[list_of_my_columns]), axis=1)\nprint(result)\n",
        "\n# Calculate the average of the list of columns\ndef avg_list_columns(df, list_of_my_columns):\n    return df[list_of_my_columns].mean(axis=1)\n\nresult = avg_list_columns(df, list_of_my_columns)\nprint(result)\n",
        "\ndf.sort_index(level=2, ascending=True)\n",
        "\ndf.sort_index(level=1, axis=1, inplace=True)\n",
        "\n# Delete the row with the date '2020-02-17'\ndf = df.drop(df.index['2020-02-17'])\n# Delete the row with the date '2020-02-18'\ndf = df.drop(df.index['2020-02-18'])\n",
        "\n# Delete rows with dates '2020-02-17' and '2020-02-18'\ndf = df[(df.index < '2020-02-17') & (df.index > '2020-02-18')]\n# [Missing Code]\n",
        "\nresult = corr[corr > 0.3].reset_index()\nresult.index += 1\nresult.columns = ['Pearson Correlation Coefficient']\nresult.index.name = 'Col'\nresult.columns.name = 'Col'\nprint(result)\n",
        "\nresult = corr[corr > 0.3].iloc[:, :-1]\n",
        "\ndf.loc[:, 'A'] = df.loc[:, 'A']\ndf.loc[:, 'B'] = df.loc[:, 'B']\ndf.loc[:, 'C'] = df.loc[:, 'C']\ndf.loc[:, 'A'] = df.loc[:, 'A'] + 1\n# [Missing Code]\ndf.loc[:, 'B'] = df.loc[:, 'B'] + 1\n",
        "\ndf.rename(columns={df.columns[0]: 'Test'}, inplace=True)\n# [Missing Code]\ndf.columns = ['A', 'B', 'C']\n",
        "\nresult = df.groupby('bit1').agg({'bit2': 'first', 'bit3': 'first', 'bit4': 'first', 'bit5': 'first'})\nresult['frequent'] = result['bit2'].eq(result['bit3']) & result['bit2'].eq(result['bit4']) & result['bit2'].eq(result['bit5'])\nresult['freq_count'] = result.groupby('bit1').size()\nprint(result)\n",
        "\nresult = df.groupby('bit1').agg({'bit2': 'first', 'bit3': 'first', 'bit4': 'first', 'bit5': 'first'})\nresult['frequent'] = result['bit2'].eq(result['bit3']) & result['bit2'].eq(result['bit4']) & result['bit2'].eq(result['bit5'])\nresult['freq_count'] = result.groupby('bit1').agg({'bit2': ['count', 'sum'], 'bit3': ['count', 'sum'], 'bit4': ['count', 'sum'], 'bit5': ['count', 'sum']})\n",
        "\nresult = df.apply(lambda x: [val for val in set(x) if x.count(val) > 1], axis=1)\nresult['frequent'] = result.apply(lambda x: ','.join(x) if len(x) > 1 else None, axis=1)\nresult['freq_count'] = result.apply(lambda x: len(x) if x is not None else 0, axis=1)\n",
        "\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\n# [Missing Code]\n",
        "\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nresult[\"bar\"] = result[\"bar\"].fillna(0)\nresult = result.dropna()\nresult = result.reset_index()\nresult.columns = [\"id1\", \"id2\", \"foo\", \"bar\"]\n# [Missing Code]\n",
        "\n# We need to specify which columns we want to merge on\ndf_c = pd.merge(df_a, df_b, on=['EntityNum', 'a_col'])\n# We also need to specify which columns we want to keep\nresult = df_c[['EntityNum', 'foo', 'a_col']]\n",
        "\n# Merge the two dataframes on the 'EntityNum' column and select the desired columns\nresult = pd.merge(df_a, df_b, on='EntityNum')\nresult = result[['EntityNum', 'foo', 'b_col']]\n"
    ],
    "Numpy": [
        "\nresult = a.shape\n",
        "[Missing Code]\nx = x.dropna()\nprint(x)\n",
        "\nx[np.isnan(x)] = np.inf\n",
        "\nimport numpy as np\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nresult = [item for sublist in x for item in sublist if not np.isnan(item)]\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.zeros((len(a)+1, len(a)+1))\nfor i in range(len(a)+1):\n    for j in range(len(a)+1):\n        if a[j-1] == i:\n            b[i, j] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([1, 0, 3])\nb = np.zeros((len(a)+1, len(a)+1))\nfor i in range(len(a)+1):\n    for j in range(len(a)+1):\n        if a[j-1] == i:\n            b[i, j] = 1\nprint(b)\n",
        "\nimport numpy as np\na = np.array([-1, 0, 3])\nb = np.zeros((5,))\nb[np.argsort(a)] = np.arange(len(a))\nprint(b)\n",
        "\nb = np.zeros((len(a), 3))\nfor i in range(len(a)):\n    index = np.argmax(a[:i+1])\n    b[i, index] = 1\n",
        "\nb = np.zeros((len(a), len(a[0])))\nfor i in range(len(a)):\n    for j in range(len(a[0])):\n        b[i, j] = (a[i, j] - a[0, 0] + 1) // (a[1, 0] - a[0, 0] + 1)\n",
        "\nresult = np.percentile(a, p)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nncol = 2\nB = np.reshape(A, (ncol, -1))\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6])\nnrow = 3\nB = np.reshape(A, (nrow, nrow))\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (ncol, -1))\nprint(B)\n",
        "\nimport numpy as np\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = np.reshape(A, (ncol, -1))\nprint(B)\n",
        "\nresult = np.roll(a, shift)\n",
        "\nresult = np.roll(a, shift, axis=0)\n",
        "\nresult = np.roll(a, shift, axis=0)\n",
        "\n# Create a function to generate a random array\ndef generate_array():\n    return np.random.randint(3, size=(100, 2000)) - 1\n# [Missing Code]\nr_old = generate_array()\nr_new = generate_array()\n",
        "\nmax_val = np.amax(a, axis=0)\nresult = np.unravel_index(max_val, a.shape[0])\n",
        "\nmin_val = np.min(a, axis=0)\nresult = np.where(a == min_val)[0]\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\nresult = np.unravel_index(np.argmax(a), a.shape)\n",
        "\n    max_val = np.amax(a, axis=0)\n    indices = np.unravel_index(max_val, a.shape[0])\n    # [Missing Code]\n",
        "\n# Find the second largest value in each row\nrow_max = np.amax(a, axis=1)\nrow_inds = np.where(a == row_max)\n# Find the second largest value in the entire array\nall_max = np.amax(a)\nall_inds = np.where(a == all_max)\n# Find the indices of the second largest value in each row\nrow_inds = row_inds[row_inds[1] != all_inds[1]]\n# Find the indices of the second largest value in the entire array\nall_inds = all_inds[all_inds[1] != row_inds[1]]\nresult = np.concatenate((row_inds[0], all_inds[0]))\n",
        "\n# Create a boolean array indicating which columns to delete\ndelete_cols = np.logical_not(np.isnan(a))\n# Delete the columns specified by delete_cols\na = a[:, delete_cols]\n",
        "\na = a[~np.isnan(a)]\n# [Missing Code]\n",
        "[Missing Code]\nresult = np.array(a)\n",
        "\na = np.transpose(a, permutation)\n",
        "\nresult = np.moveaxis(a, permutation, axis=1)\n",
        "\nimport numpy as np\na = np.array([[1, 2], [3, 0]])\nmin_val = np.min(a)\nrow_idx = np.where(a == min_val)[0][0]\ncol_idx = np.where(a == min_val)[1][0]\nresult = (row_idx, col_idx)\nprint(result)\n",
        "\nresult = a.argmax()\n",
        "\nimport numpy as np\na = np.array([[1, 0], [0, 2]])\nmin_val, min_idx = np.unravel_index(np.min(a), a.shape)\nresult = np.where(a==min_val)[0]\nprint(result)\n",
        "\nresult = np.sin(np.deg2rad(degree))\n",
        "\nresult = np.cos(np.deg2rad(degree))\n",
        "\nresult = 0\nif np.sin(np.deg2rad(number)) > 0:\n    result = 1\n",
        "\ndef deg_to_rad(deg):\n    return deg * (np.pi/180)\n",
        "\nresult = np.pad(A, (length - len(A)), 'constant', constant_values=0)\n",
        "\nresult = np.pad(A, (length - len(A)), 'constant', constant_values=0)\n",
        "\na = np.power(a, power)\n",
        "\n    result = a ** power\n",
        "\nresult = np.fractions_to_decimal(numerator, denominator)\n",
        "\n    result = numerator / denominator\n    if numerator > denominator:\n        result = numerator / denominator + 1\n    return (result, denominator / numerator)\n",
        "[Missing Code]\nresult = np.fractions_to_decimal(numerator, denominator)\n",
        "\nresult = (a + b + c) / 3\n",
        "\nresult = np.maximum(a, b)\nresult = np.maximum(result, c)\n",
        "\n# Get the shape of the array\nshape = a.shape\n# Calculate the number of rows and columns\nnum_rows = shape[0]\nnum_cols = shape[1]\n# Calculate the diagonal indices starting from the top right\ndiagonal_indices = np.diag_indices(num_cols+1)\n",
        "\n# Get the shape of the array\nshape = a.shape\n# Calculate the number of rows and columns\nnum_rows = shape[0]\nnum_cols = shape[1]\n# Calculate the diagonal indices starting from the top right\ndiagonal_indices = np.indices((num_rows-1, num_cols-1))[0].reshape(-1,1)\n",
        "\n# Get the transpose of the array\na_transpose = a.T\n# Get the diagonal indices starting from the top right\ndiagonal_indices = np.diag_indices(a_transpose.shape[0])\n",
        "\n# Get the shape of the array\nshape = a.shape\n# Get the number of rows and columns\nnum_rows = shape[0]\nnum_cols = shape[1]\n# Get the diagonal indices starting from the bottom left\nbottom_left_diagonal_indices = np.indices((num_rows-1, num_cols-1))\n",
        "\nresult = []\nfor i in range(r):\n    for j in range(c):\n        result.append(X[i,j])\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = []\nfor i in range(len(X)):\n    for j in range(len(X[i])):\n        result.append(X[i][j])\nprint(result)\n",
        "\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    result = []\n    for i in range(len(X)):\n        for j in range(len(X[0])):\n            result.append(X[i][j])\n    return result\n",
        "\nresult = []\nfor i in range(r):\n    for j in range(c):\n        result.append(X[i,j])\nprint(result)\n",
        "\nresult = np.array([int(x) for x in mystr])\n",
        "\nresult = np.cumsum(a[:, col] * multiply_number)\n",
        "\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\nresult = a[row] * multiply_number\nresult = np.cumsum(result)\nprint(result)\n",
        "\nrow_result = a[row] / divide_number\nrow_product = np.product(row_result)\nresult = row_product\n",
        "\ndef find_max_lin_ind(a):\n    rows = len(a)\n    cols = len(a[0])\n    max_lin_ind = np.zeros((rows, cols))\n    for i in range(rows):\n        for j in range(cols):\n            if a[i][j] != 0:\n                max_lin_ind[i][j] = 1\n    return max_lin_ind\n",
        "\nrow_size = a.shape[0]\nresult = row_size\n",
        "\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nn1 = len(a)\nn2 = len(b)\nt_stat, p_value = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
        "\nimport numpy as np\nimport scipy.stats\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\nt_stat = (amean - bmean) / np.sqrt((avar/anobs) + (bvar/bnobs))\np_value = scipy.stats.t.sf(abs(t_stat), anobs + bnobs)\nprint(p_value)\n",
        "\n# Use set operations to find the complement of B in A\nA_without_B = np.setdiff1d(A, B, assume_unique=True)\n",
        "\nC = np.logical_or(np.logical_not(np.isin(A, B)), np.logical_not(np.isin(B, A)))\n# [Missing Code]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n",
        "\nresult = np.argsort(b, axis=1)\nresult = result.reshape(3, 3, 3)\nresult = result[::-1]\n",
        "\n# Delete the 3rd column\na[:, 2] = 0\n",
        "\n# Delete the 3rd row\na = a[:2, :]\n",
        "\na = a[:, 1::2]\n",
        "\nresult = a[:, np.delete(np.arange(3), del_col)]\n",
        "[Missing Code]\na.insert(pos, element)\n",
        "[Missing Code]\na = np.insert(a, pos, element, axis=0)\nprint(a)\n",
        "\n    a_list = a.tolist()\n    a_list.insert(pos-1, element)\n    a = np.asarray(a_list)\n    # [Missing Code]\n",
        "\na.insert(pos[0], element)\na.insert(pos[1], element)\n",
        "\nresult = array_of_arrays.copy()\n",
        "\nnp.all(np.equal.outer(a, a))\n",
        "\nnp.all(np.equal.outer(a, a))\n",
        "\nnp.all(np.equal.outer(a, a))\n",
        "\n# We can use the Simpson rule to approximate the integral\ndef simpson_rule(func, x, y, nx, ny):\n    dx = (x[1] - x[0]) / nx\n    dy = (y[1] - y[0]) / ny\n    result = 0\n    for i in range(nx):\n        for j in range(ny):\n            result += func(x[i] + dx * j, y[j])\n    return result\n# [Missing Code]\n",
        "\n    result = 0\n    for i in range(len(x)):\n        result += (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 - x[i]) * (1 - y[i]) * (1 -",
        "\nimport numpy as np\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n# Calculate the cumulative distribution function\nresult = np.cumsum(grades)\n# Normalize the result so that the sum is 1\nresult = result / np.sum(result)\nprint(result)\n",
        "\nresult = np.interp(eval, grades, ecdf(grades))\n",
        "\nlow, high = grades[np.searchsorted(ecdf(grades), threshold)][0], grades[np.searchsorted(ecdf(grades), threshold)[-1]]\n",
        "\nnums = np.random.choice(2, size=size, p=one_ratio)\n",
        "\na_np = a.numpy()\n",
        "\na_pt = torch.from_numpy(a)\n# [Missing Code]\n",
        "\na_np = np.array(a)\n",
        "\na_tf = tf.convert_to_tensor(a)\n",
        "\nresult = np.argsort(a)[::-1]\n",
        "\nresult = np.argsort(a)\n",
        "\nidx = np.argsort(a)[::-1][:N]\nresult = np.zeros(N, dtype=int)\nresult[idx] = np.arange(N)\nprint(result)\n",
        "\nresult = np.power(A, n)\n",
        "\nresult = np.reshape(a, (2,2,2,2))[np.newaxis, np.newaxis, np.newaxis, np.newaxis]\n",
        "\nresult = np.moveaxis(a, 1, 0).reshape(-1, 2, 2)\n",
        "\nresult = np.array([[a[i:i+2], a[j:j+2]] for i in range(0, len(a), 2) for j in range(i+1, len(a), 2)])\n",
        "\nresult = np.empty((len(a) // patch_size, patch_size, patch_size))\nfor i in range(len(a) // patch_size):\n    for j in range(patch_size):\n        result[i, j, 0] = a[i * patch_size + j][0]\n        result[i, j, 1] = a[i * patch_size + j][1]\n",
        "\nresult = np.reshape(a, (h, w))\n",
        "\nresult = np.empty((len(a) // patch_size, patch_size, patch_size))\nfor i in range(len(a) // patch_size):\n    result[i] = a[i * patch_size:(i + 1) * patch_size]\n    result[i] = result[i].reshape(patch_size, patch_size)\n",
        "\nresult = a[:, low:high+1]\n",
        "\nresult = a[low:high+1]\n",
        "\nresult = a[:, low:high+1]\n",
        "[Missing Code]\na = np.array(string.split('], [')[1].split('[')[0].split(', ')])\n",
        "\nimport numpy as np\n\nmin = 1\nmax = np.e\nn = 10000\n\nresult = np.random.loguniform(min, max, n)\nprint(result)\n",
        "\nimport numpy as np\n\nmin = 0\nmax = 1\nn = 10000\n\nresult = np.random.loguniform(min, max, n)\nprint(result)\n",
        "\n    result = np.random.uniform(np.log(min), np.log(max), n)\n",
        "\nB = A[0] * a\nfor t in range(1, len(A)):\n    B[t] = A[t] * a + B[t-1] * b\n",
        "\nB = A.copy()\nB[1] = a * A[0] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n",
        "\nresult = np.empty((0,))\n",
        "\nresult = np.empty((3,0))\n",
        "\nresult = np.ravel_multi_index(index, dims)\n",
        "\nimport numpy as np\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = np.ravel_multi_index(index, dims)\nprint(result)\n",
        "\nvalues = np.zeros((2,3), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = np.sum(np.bincount(accmap.ravel()[::-1], weights=a))\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\nresult = a[np.sum(index, axis=1) // index]\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = np.sum(a[accmap])\nprint(result)\n",
        "\nimport numpy as np\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\nresult = a[np.where(np.abs(index) == np.abs(index.astype(int)))]\nprint(result)\n",
        "\nz = np.empty_like(x)\nfor i in range(len(x)):\n    for j in range(len(x[0])):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n",
        "\nresult = np.random.choice(lista_elegir, samples, probabilit, replace=True)\n",
        "\nresult = np.pad(a, [(0, low_index), (0, high_index - low_index)], mode='edge')\n",
        "\nresult = x[x >= 0]\n",
        "\nreal_nums = np.real(x)\nresult = x[~np.isin(x, real_nums)]\n",
        "\nbin_data = np.split(data, np.round(len(data) / bin_size))\n",
        "\nbin_data = np.split(data, np.round(len(data) / bin_size))\nbin_data_max = np.amax(bin_data, axis=0)\n",
        "\nbin_data = np.split(data, np.array_split(np.arange(len(data)), len(data)//bin_size))\nbin_data_mean = [np.mean(row, axis=0) for row in bin_data]\n",
        "\nbin_data = np.roll(data, -int(len(data) / bin_size))\nbin_data_mean = np.mean(bin_data, axis=0)\n",
        "\nbin_data = np.split(data, np.cumsum(np.diff(data, axis=1) >= bin_size))[::-1]\nbin_data_mean = np.mean(np.array(bin_data), axis=1)\n",
        "\nbin_data = np.reshape(data, (-1,))\nbin_data_mean = np.mean(bin_data[::-1::-1], axis=0)\n",
        "\ndef smoothclamp(x):\n    return np.interp(x, [x_min, x_max], [0, 1]) * (1 - (x - x_min)**2 / (x_max - x_min)**2) + (3 * (x - x_min)**2 - 2 * (x - x_min)**3) / (x_max - x_min)**2\n",
        "\ndef smoothclamp(x, N=5):\n    if x < x_min:\n        return x_min\n    elif x > x_max:\n        return x_max\n    else:\n        t = (x - x_min) / (x_max - x_min)\n        return np.interp(t, np.arange(0, N, 1), np.linspace(0, 1, N))\n",
        "\nresult = np.correlate(a, b, mode='full')\n",
        "\n# Convert the DataFrame to a NumPy array with the desired shape\nresult = df.values.reshape(4, 15, 5)\n# [Missing Code]\n",
        "\nresult = df.to_numpy()\n",
        "\nresult = np.zeros((len(a), m))\nfor i in range(len(a)):\n    result[i] = np.unpackbits(np.uint8(a[i]))\n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 6\ndef convert_to_binary(a, m):\n    result = np.zeros((len(a), m))\n    for i in range(len(a)):\n        num = a[i]\n        if num < 0:\n            num += 2**m\n        binary = np.unpackbits(np.uint8(num))\n        result[i] = binary[:m]\n    return result\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = convert_to_binary(a, m)\nprint(result)\n",
        "\nimport numpy as np\na = np.array([1, 2, 3, 4, 5])\nm = 6\n# Convert each element to binary array of length m\na_binary = np.apply_along_axis(lambda x: np.unpackbits(np.uint8(x)), 1, a)\n# Compute exclusive OR of all the rows\nresult = np.exclusive_or.reduceat(a_binary, np.arange(m), axis=0)\nprint(result)\n",
        "\n# Calculate the mean and standard deviation of the array\nmean = np.mean(a)\nstd_dev = np.std(a)\n# Calculate the 3rd standard deviation\nthree_sigma = std_dev * 3\n# Calculate the start and end of the 3rd standard deviation interval\nstart = mean - three_sigma\nend = mean + three_sigma\n# Return the result as a tuple\nresult = (start, end)\n",
        "\n# Calculate the mean and variance of the array\nmean = np.mean(a)\nvar = np.var(a)\n# Calculate the standard deviation\nstd_dev = np.sqrt(var)\n# Calculate the 2nd standard deviation\ntwo_std_dev = 2 * std_dev\n# Calculate the start and end of the 2nd standard deviation interval\nstart = mean - two_std_dev\nend = mean + two_std_dev\n# Print the result\nresult = (start, end)\n",
        "\nimport numpy as np\nexample_a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\ndef f(a = example_a):\n    # Calculate the mean and standard deviation of the array\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    \n    # Calculate the 3rd standard deviation\n    three_sigma = mean + 3 * std_dev\n    \n    # Calculate the start and end of the 3rd standard deviation interval\n    start = mean - 3 * std_dev\n    end = mean + 3 * std_dev\n    \n    # Return a tuple containing the start and end of the 3rd standard deviation interval\n    return (start, end)\n",
        "\nimport numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n# Calculate the mean and standard deviation of the array\nmean = np.mean(a)\nstd_dev = np.std(a)\n# Calculate the 2nd standard deviation\ntwo_std_dev = std_dev * 2\n# Create a boolean array indicating whether each element is an outlier\nresult = (a < mean - two_std_dev) | (a > mean + two_std_dev)\nprint(result)\n",
        "\nprob = np.percentile(DataArray, percentile)\n",
        "\na[zero_rows, zero_cols] = 0\na[:, zero_cols] = 0\na[zero_rows, :] = 0\n",
        "\na[np.in1d(np.arange(a.shape[0]), zero_rows, invert=True)] = 0\na[:, np.in1d(np.arange(a.shape[1]), zero_cols, invert=True)] = 0\n",
        "[Missing Code]\na[:, 1] = 0\na[:, 0] = 0\nprint(a)",
        "\nmask = np.zeros_like(a)\nmax_val = np.amax(a, axis=1)\nmask[np.arange(len(max_val)), np.argmax(max_val)] = True\n",
        "\nimport numpy as np\na = np.array([[0, 1], [2, 1], [4, 8]])\nmin_val = np.min(a, axis=1)\nmask = np.where(a == min_val, True, False)\nprint(mask)\n",
        "\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(post, distance)[0, 1]\n",
        "\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\nresult = np.empty((X.shape[1], X.shape[0], X.shape[0]), dtype=object)\nfor i in range(X.shape[1]):\n    result[i] = X[i, :, :].dot(X[i, :, :].T)\nprint(result)\n",
        "\nX = Y.reshape(Y.shape[0], Y.shape[1])\n",
        "\nis_contained = a.any()\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.array([x for x in A if x not in B])\nprint(C)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = np.array([x for x in A if x in B])\nprint(C)\n",
        "\nimport numpy as np\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,4,8])\nC = np.array([])\nfor i in range(len(A)):\n    if A[i] in B[0:1] or A[i] in B[1:2] or A[i] in B[2:3]:\n        C = C + [A[i]]\nprint(C)\n",
        "\nsorted_a = np.sort(a, reverse=True)\nresult = rankdata(sorted_a)\n",
        "\nsorted_a = np.sort(a)\nranks = np.argsort(sorted_a)\nresult = np.empty(len(a))\nresult[ranks] = np.arange(len(a))\n",
        "\n    sorted_a = sorted(a, reverse=True)\n    result = np.empty(len(sorted_a), dtype=int)\n    for i in range(len(sorted_a)):\n        result[i] = rankdata(sorted_a)[i]\n",
        "\ndists = np.dstack((x_dists.T, y_dists.T))\n# [Missing Code]\n",
        "\ndists = np.dstack((x_dists, y_dists))\n# [Missing Code]\n",
        "\nresult = a[:, second, third]\n",
        "\narr = np.zeros((20, 20, 10, 10))\n# [Missing Code]\n",
        "\nresult = X/l1.reshape(5,1)\nprint(result)\n",
        "\nresult = np.array([LA.norm(row, ord=2) for row in X])\nprint(result)\n",
        "\nresult = np.array([LA.norm(row, ord=np.inf) for row in X])\nprint(result)\n",
        "\nconditions = [df['a'].str.contains(f'*{target}*')]\nresult = np.select(conditions, choices, df['a'])\n",
        "\nresult = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(len(a)):\n        if i != j:\n            result[i][j] = np.linalg.norm(a[i] - a[j])\n",
        "\nimport numpy as np\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\ndistance_matrix = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i != j:\n            distance_matrix[i][j] = np.linalg.norm(a[i] - a[j])\nprint(distance_matrix)\n",
        "\nresult = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    for j in range(i, len(a)):\n        result[i][j] = np.linalg.norm(a[i] - a[j])\n",
        "\nimport numpy as np\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\nAVG = np.mean(NA, axis=0)\nprint(AVG)\n",
        "\nAVG = np.mean(NA, axis=0)\n",
        "\n# Convert A to a list of floats\nA = [float(x) for x in A]\n",
        "\nresult = np.delete(a, np.where(a != 0)[1] + 1)\n",
        "\nresult = a.copy()\nfor i in range(1, len(result)):\n    if result[i-1] == result[i]:\n        result[i-1] = 0\n        result[i] = 0\n",
        "\ndf = pd.DataFrame(np.column_stack((lat.flatten(), lon.flatten(), val.flatten())), \n                  columns=['lat', 'lon', 'val'])\n",
        "\n    df = pd.DataFrame(np.column_stack((lat.flatten(), lon.flatten(), val.flatten())), \n                        columns=['lat', 'lon', 'val'])\n    df = df.sort_values(by=['lat', 'lon', 'val'])\n",
        "\ndf = pd.DataFrame(np.column_stack((lat.flatten(), lon.flatten(), val.flatten())), \n                  columns=['lat', 'lon', 'val'])\ndf['maximum'] = df.apply(lambda row: max(row.values), axis=1)\n",
        "\nresult = np.empty((len(a), len(size)), dtype=object)\nfor i in range(len(a)):\n    window = np.roll(a[:size[0], i] + a[i, :size[1]], -1, axis=0) + np.roll(a[i, -1, :size[1]] + a[-1, :size[1]], -1, axis=1)\n    result[i] = window.reshape(size)\n",
        "\nresult = np.empty((len(a), len(size)), dtype=object)\nfor i in range(len(a)):\n    window = np.roll(a[:size[0], i] + a[i, :size[1]], -1, axis=0) + np.roll(a[i, -1, :size[1]] + a[-1, :size[1]], -1, axis=1)\n    result[i] = window.reshape(size)\n",
        "\nresult = np.mean(a)\n",
        "\n    result = np.mean(a, out=None, dtype=float)\n",
        "\n# To slice an array of unknown dimension, we can use the following syntax:\n# result = Z[:, :, -1]\n# This will select the last element of the last axis of the array Z.\n# [Missing Code]\n",
        "\n# To slice an array of unknown dimension, we can use the `:` operator to select all elements after a certain index.\n# For example, to select all elements after the first dimension, we can use `a[1:, :]`.\n# To select all elements after the second dimension, we can use `a[:, 1:, :]`.\n# To select all elements after the third dimension, we can use `a[:, :, 1:]`.\n# We can also use negative indices to select elements before a certain index.\n# For example, to select all elements before the first dimension, we can use `a[:, -1:, :]`.\n# To select all elements before the second dimension, we can use `a[1:, :, -1:]`.\n# To select all elements before the third dimension, we can use `a[1:, :, :]`.\n",
        "\ndef is_member(arr, cnts):\n    return any(np.array_equal(arr, cnt)) for cnt in cnts\n",
        "\ndef is_member(arr, cnts):\n    return np.any(np.isin(arr, cnts))\n# [Missing Code]\nresult = is_member(c, CNTS)\n",
        "[Missing Code]\n```python\n# Create a 2D grid of the same size as a\nx_new = np.linspace(0, 2, 4)\ny_new = np.linspace(0, 2, 4)\n# Create a grid of the same size as a\nx_new = np.repeat(x_new, 2)\ny_new = np.repeat(y_new, 2)\n# Create a 2D grid of the same size as a\nx_new = np.tile(x_new, (2, 2))\ny_new = np.tile(y_new, (2, 2))\n# Create a 2D grid of the same size as a\nx_new = np.broadcast_to(x_new, a.shape)\ny_new = np.broadcast_to(y_new, a.shape)\n# Create a 2D grid of the same size as a\nx_new = np.repeat(x_new, 2)\ny_new = np.repeat(y_new, 2)\n# Create a 2D grid of the same size as a\nx_new = np.tile(x_new, (2, 2))\ny_new = np.tile(y_new, (2, 2))\n# Create a 2D grid of the same size as a\nx_new = np.broadcast_to(x_new, a.shape)\ny_new = np.broadcast_to(y_new, a.shape)\n# Create a 2D grid of the same size as a\nx_new = np.repeat(x_new, 2)\ny_new = np.repeat(y_new, 2)\n# Create a 2D grid of the same size as a\nx_new = np.tile(x_new, (2, 2))\ny_new = np.tile(y_new, (2, 2))\n# Create a 2D grid of the same size as a\nx_new = np.broadcast_to(x_new, a.shape)\ny_new = np.broadcast_to(y_new, a.shape)\n# Create a 2D grid of the same size as a\nx_new = np.repeat(x_new, 2)\ny_new = np.repeat(y_new, 2)\n# Create a 2D grid of the same size as a\nx_new = np.tile(x_new, (2, 2))\ny_new = np.tile(y_new, (2, 2))\n# Create a 2D grid of the same size as a\nx_new = np.broadcast_to(x_new, a.shape)\ny_new = np.broadcast_to(y_new, a.shape)\n# Create a 2D grid of the same size as a\nx_new = np.repeat(x_new, 2)\ny_new = np.repeat(y_new, 2)\n# Create a 2D grid of the same size as a\nx_new = np.tile(x_new, (2, 2))\ny_new = np.tile(y_new, (2, 2))\n# Create a 2D grid of the same size as a\nx_new = np.broadcast_to(x_new, a.shape)\ny_new = np.broadcast_to(y_new, a.shape)\n# Create a 2D grid of the same size as a\nx_new = np.repeat(x_new, 2)\ny_new = np.repeat(y_new, 2)\n# Create a 2D grid of the same size as a\nx_new = np.tile(x_new, (2, 2))\ny_new = np.tile(y_new, (2, 2))\n# Create a 2D grid of the same size as a\nx_new = np.broadcast_to(x_new, a.shape)\ny_new = np.broadcast_to(y_new, a.shape)\n# Create a 2D grid of the same size as a\nx_new = np.repeat(x_new, 2)\ny",
        "\ndf[name] = np.cumsum(df.loc[df['D']==df['D'].max(), 'Q'])\n",
        "\ni = np.diag(i.flat)\n",
        "\na[np.ix_(range(len(a)), range(len(a))) != 0] = 0\n",
        "\n# Convert start and end to datetime objects\nstart_dt = pd.to_datetime(start)\nend_dt = pd.to_datetime(end)\n# Calculate the frequency\nfreq = (end_dt - start_dt) / n\n# Create a list of datetime objects with equal spacing\nresult = [start_dt + freq * i for i in range(n)]\n# Convert the list to a pandas DatetimeIndex\nresult = pd.DatetimeIndex(result)\n",
        "\nresult = np.where(x == a, y.ravel(), -1)\n",
        "\nresult = np.where(x == a, np.nonzero(y == b))[0]\n",
        "[Missing Code]\nresult = np.linalg.lstsq(x, y, rcond=None)[0]\n",
        "[Missing Code]\nresult = np.polyfit(x, y, degree)\n",
        "\ndef subtract_arr(x):\n    return x - a\n# [Missing Code]\ndf.apply(subtract_arr, axis=0)\n",
        "\nC = np.multiply(B, A)\n# [Missing Code]\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a.reshape(-1, a.shape[1]))\n",
        "\nresult = MinMaxScaler(feature_range=(0, 1))\narr = result.fit_transform(arr.reshape(-1, arr.shape[1]))\n",
        "\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n",
        "\nmask = arr < -10\narr[mask] = 0\nmask2 = arr >= 15\narr[mask2] = arr[mask2] + 5\narr[~mask2] = 30\n",
        "\nimport numpy as np\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\narr_temp = arr.copy()\nmask = arr_temp < n1\nmask2 = arr_temp >= n2\narr[mask] = 0\narr[mask2] = arr[mask2] + 5\nprint(arr)\n",
        "\nresult = np.nonzero(np.abs(s1 - s2) > np.finfo(np.float64).eps)\n",
        "\nresult = np.nonzero(np.diff(np.append(s1, np.nan)))[0].shape[0]\n",
        "\nresult = np.array_equal(a[0], a[1]) and np.array_equal(a[1], a[2])\n",
        "\nnp.all(np.isnan(a))\n",
        "\nresult = np.pad(a, ((0, 0), (0, 0)), mode='constant', constant_values=0)\n",
        "\nresult = np.pad(a, ((0, 0), (0, 1)), mode='constant', constant_values=0)\n",
        "\nresult = np.pad(a, ((0, 0), (0, 1)), mode='constant', constant_values=element)\n",
        "\nimport numpy as np\ndef f(arr = None, shape=(93,13)):\n    if arr is None:\n        arr = np.ones((41,13))\n    padded_arr = np.pad(arr, ((0,0), (0,0)), mode='edge')\n    padded_arr = np.resize(padded_arr, shape)\n    return padded_arr\n",
        "\nresult = np.pad(a, ((0, 0), (0, 0), (0, 93-41), (0, 13-12)), mode='edge')\n",
        "\n# To reshape the array into a (4,3) matrix, we can use the following code:\nb = a.reshape(3,4)\n",
        "\nresult = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        result[i][j] = a[i][j][b[i][j]]\n",
        "\n# select the elements in a according to b\nresult = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        result[i][j] = a[i][j][b[i][j]]\n",
        "\nresult = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        result[i, j] = a[i, j, b[i, j]]\n",
        "\nresult = np.sum(a[np.newaxis, :, np.newaxis] * b, axis=2)\n",
        "[Missing Code]\nresult = np.sum(a, axis=2, keepdims=True) + np.sum(b, axis=1)\nprint(result)",
        "\nresult = df[df['a'].between(1, 4, inclusive=False)]['b'].values\n",
        "\nresult = im.T.tolist()\nresult = [list(filter(None, row)) for row in result]\nresult = [[max(row) for row in col] for col in zip(*result)]\n",
        "\n# Find the nonzero values in the array\nnonzero_indices = np.where(np.logical_not(np.isnan(A)))\n# Create a new array with only the nonzero values\nresult = A[nonzero_indices]\n",
        "\n# Define a function to remove peripheral non-zeros\ndef remove_peripheral_zeros(arr):\n    m, n = arr.shape\n    result = np.zeros_like(arr)\n    for i in range(m):\n        for j in range(n):\n            if i == 0 or i == m - 1 or j == 0 or j == n - 1:\n                result[i, j] = arr[i, j]\n            elif arr[i, j] == 0:\n                result[i, j] = 0\n            else:\n                result[i, j] = min(result[i-1, j], result[i+1, j], result[i, j-1], result[i, j+1]) + 1\n    return result\n",
        "\n# Define a function to remove peripheral zeros\ndef remove_zeros(arr):\n    result = []\n    for row in arr:\n        for col in row:\n            if col != 0:\n                result.append(col)\n                break\n    return np.array(result)\n# [Missing Code]\nresult = remove_zeros(im)\n"
    ],
    "Matplotlib": [
        "\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter Plot of X vs Y')\nplt.legend(['X', 'Y'])\nplt.show()\n",
        "\nplt.minorticks_on()\nplt.ylabel('')\n",
        "\nplt.minorticks_on()\n\n",
        "\nplt.xlabel('X-axis')\nplt.xticks(rotation=45)\nplt.minorticks_on()\n",
        "\nlines = [np.random.rand(10, 1)] * 10\n",
        "\nlines = [np.random.rand(10, 1)] * 10\n",
        "\nplt.plot(x, y, 'o-')\nplt.show()\n",
        "\nplt.plot(x, y, 'o-', linewidth=2)\nplt.show()\n",
        "\nax.set_ylim(0, 40)\n",
        "\nplt.axvspan(2, 4, color='red', alpha=0.3)\n\n",
        "\nline = plt.plot([0, 1], [0, 2], linewidth=2, color='black')\n",
        "\n\n# draw a line segment from (0,0) to (1,2)\nx = np.linspace(0, 1, 100)\ny = 2 * x\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Segment from (0,0) to (1,2)')\nplt.show()\n",
        "\nsns.set(style=\"ticks\")\nsns.scatterplot(\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    data=df,\n)\nplt.title(\"Height vs Weight by Gender\")\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Weight (kg)\")\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# draw a regular matplotlib style plot using seaborn\nsns.set(style=\"whitegrid\")\nplt.plot(x, y, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"X-axis label\")\nplt.ylabel(\"Y-axis label\")\nplt.title(\"Random Data Plot\")\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.sin(x)\n\n# draw a line plot of x vs y using seaborn and pandas\ndf = pd.DataFrame({'x': x, 'y': y})\nax = sns.lineplot(data=df)\nplt.show()\n",
        "\nplt.plot(x, y, 'o-', markerfacecolor='black', markeredgecolor='black', markersize=7)\n",
        "\nplt.legend(fontsize=20)\n\n",
        "\nplt.legend(['x', 'y', 'z'], titlefont=dict(size=20))\n",
        "\nl.set_markerfacecolor('r')\nl.set_alpha(0.2)\n\n",
        "\nl.set_markerfacecolor('none')\nl.set_markersize(30)\nl.set_linestyle('solid')\nl.set_color('black')\n",
        "\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30)\n\n# set both line and marker colors to be solid red\nl.set_color(\"red\")\n",
        "\nplt.xrotater(45)\n",
        "\nplt.xrotater(45)\n",
        "\nplt.xticks(np.arange(0, 2 * np.pi, 2))\n",
        "\nplt.legend(loc=\"upper right\")\n",
        "\nplt.imshow(H, cmap='viridis', interpolation='nearest')\nplt.colorbar()\nplt.show()\n",
        "\nplt.imshow(H, cmap='gray')\nplt.show()\n",
        "\nplt.xlabel('X')\nplt.xticks(rotation=90)\n\n",
        "\ng.set(xrot=45)\n",
        "\nmyTitle = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\n# split the title into multiple lines\ntitle_lines = myTitle.split(\" - \")\n# create a figure and axis\nfig, ax = plt.subplots()\n# set the title\nax.set_title(title_lines[0])\n# add the second line to the title\nax.set_title(ax.get_title() + \" \" + title_lines[1])\n# plot the data\nplt.plot(x, y)\n# show the plot\nplt.show()\n",
        "\nplt.gca().invert_yaxis()\n",
        "\nplt.xlim(0, 1.5)\n",
        "\nplt.ylim(-1, 1)\n",
        "\n\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n\n# plot x, then y then z, but so that x covers y and y covers z\nfig, ax = plt.subplots()\nax.scatter(x, 0, c='r')\nax.scatter(0, y, c='g')\nax.scatter(0, 0, c='b')\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nplt.show()\n",
        "\nplt.scatter(x, y, c='b', s=50, edgecolor='k')\n",
        "\nplt.xticks(x, [int(x) for x in x])\nplt.yticks(y, [int(y) for y in y])\n",
        "\nplt.yscale.set_characteristic('log')\n",
        "\n\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n\n# Plot a solid line\nax = sns.lineplot(x=x, y=y)\n\n# Plot a dashed line\ndashed_y = 2 * np.random.rand(10)\ndashed_x = np.arange(10)\nax2 = sns.lineplot(x=dashed_x, y=dashed_y, linestyle=\"dashed\")\n\n# Show both lines in the same plot\nax.plot(dashed_x, dashed_y, linestyle=\"dashed\")\n\nplt.show()\n",
        "\n# plot x vs y1 and x vs y2 in two subplots, sharing the x axis\nfig, ax1 = plt.subplots()\nax1.plot(x, y1)\n\nax2 = ax1.twinx()\nax2.plot(x, y2)\n\nax1.set_ylabel('y1')\nax2.set_ylabel('y2')\nax1.set_xlabel('x')\n\nplt.show()\n",
        "\n\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# plot x vs y1 and x vs y2 in two subplots\n# remove the frames from the subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\nax1.plot(x, y1, label='sin(x)')\nax2.plot(x, y2, label='cos(x)')\n\nax1.set_xlabel('x')\nax1.set_ylabel('y1')\nax2.set_xlabel('x')\nax2.set_ylabel('y2')\n\nax1.legend()\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n",
        "\nax = sns.lineplot(x=\"x\", y=\"y\", data=df).axes\nax.set_xlabel('')\n",
        "\nplt.xticks([], [])\n",
        "\nplt.xticks([3, 4], ['3', '4'])\nplt.grid(which='major', axis='y')\n\n",
        "\nplt.yticks([3, 4])\nplt.ygrid(which='major', linestyle='--')\n",
        "\nplt.yticks([3, 4])\nplt.xaxis.set_ticks([1, 2])\nplt.grid(which='major', axis='y', linestyle='--')\nplt.grid(which='minor', axis='x', linestyle='-')\n",
        "\nplt.grid(True)\n",
        "\nplt.legend(loc=\"lower right\")\n\n",
        "\n\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\n# Create a new figure with the same size as the previous one\nfig2, axes2 = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes2 = axes2.flatten()\n\n# Copy the previous plot to the new figure\nfor i in range(len(axes2)):\n    axes2[i].plot_frame(axes.flat[i])\n\n# Adjust the subplot padding to have enough space to display axis labels\nfor ax in axes2:\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n    ax.set_yticks([])\n    ax.set_xticks([])\n    ax.set_title(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$ vs $\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\nplt.clf()\n",
        "\n\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\n\nplt.legend()\n",
        "\n# Set the x-ticklabels and x-ticklabels positions\nax.set_xticklabels(row_labels, rotation=90)\nax.set_xticklabels(row_labels, top=1.1)\n\n# Set the y-ticklabels and y-ticklabels positions\nax.set_yticklabels(column_labels)\nax.set_yticklabels(column_labels, left=1.1)\n\n# Show the heatmap\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x\n# Label the x-axis as \"X\"\n# Set the space between the x-axis label and the x-axis to be 20\nplt.plot(x, y)\nplt.xlabel('X')\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nplt.plot(x, y, marker='o')\nplt.xticks([], [''])\nplt.show()\n",
        "\nplt.gca().set_yticklabels(y)\nplt.gca().reversethedata()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and label y axis \"Y\"\n# Show y axis ticks on the left and y axis label on the right\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n",
        "\n\ntips = sns.load_dataset(\"tips\")\n\n# Make a seaborn joint regression plot (kind='reg') of 'total_bill' and 'tip' in the tips dataframe\n# change the line and scatter plot color to green but keep the distribution plot in blue\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips, color=\"green\")\nplt.show()\n",
        "\n\ntips = sns.load_dataset(\"tips\")\n\n# Make a seaborn joint regression plot (kind='reg') of 'total_bill' and 'tip' in the tips dataframe\n# change the line color in the regression to green and the histograms to blue\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips, color=\"green\")\nplt.show()\n",
        "\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\")\n",
        "\n# Create a new figure and axis for the x-axis tick labels\nfig, ax = plt.subplots()\n\n# Set the x-axis labels\nax.set_xlabel('Celltype')\n\n# Set the x-axis tick labels\nax.set_xticklabels(df['celltype'])\n\n# Make the x-axis tick labels horizontal\nax.xaxis.set_label_position('top')\n\n# Make a bar plot of s1 and s2\nax.bar(df['celltype'], df['s1'], label='s1')\nax.bar(df['celltype'], df['s2'], label='s2')\n\n# Add a legend\nax.legend()\n\n# Show the plot\nplt.show()\n\n",
        "\n# Create a new figure and axis for the plot\nfig, ax = plt.subplots()\n\n# Set the xlabel to celltype\nax.set_xlabel('celltype')\n\n# Rotate the x-axis tick labels\nax.set_xticklabels(df['celltype'], rotation=45)\n\n# Make a bar plot of s1 and s2\nax.bar(df['celltype'], df['s1'], label='s1')\nax.bar(df['celltype'], df['s2'], label='s2')\n\n# Add a legend\nax.legend()\n\n# Show the plot\nplt.show()\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and label the x axis as \"X\"\n# Make both the x axis ticks and the axis label red\nplt.plot(x, y)\nplt.xlabel('X', color='red')\nplt.xticks(x, ['X' + str(i) for i in range(10)], color='red')\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and label the x axis as \"X\"\n# Make the line of the x axis red\nplt.plot(x, y)\nplt.xlabel('X')\nplt.axis('off')\nplt.plot(x, x, color='red')\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# plot y over x with tick font size 10 and make the x tick labels vertical\nplt.figure(figsize=(10, 10))\nax = plt.gca(projection='3d')\nax.plot_surface(x, y, np.zeros_like(x))\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zticklabels([])\nax.set_xlim3d(0, 10)\nax.set_ylim3d(0, 10)\nax.set_zticks([])\nax.set_zticklabels([])\nax.set_xlabel(labelsize=10)\nax.set_ylabel(labelsize=10)\nplt.show()\n",
        "\nplt.plot([0.22058956, 0.33088437, 2.20589566])\nplt.axvline(x=0.22058956, color='r')\nplt.axvline(x=0.33088437, color='r')\nplt.axvline(x=2.20589566, color='r')\n",
        "\n\nxlabels = list(\"ABCD\")\nylabels = list(\"CDEF\")\nrand_mat = numpy.random.rand(4, 4)\n\n# Plot of heatmap with data in rand_mat and use xlabels for x-axis labels and ylabels as the y-axis labels\n# Make the x-axis tick labels appear on top of the heatmap and invert the order or the y-axis labels (C to F from top to bottom)\nfig, ax = plt.subplots()\nim = ax.imshow(rand_mat, cmap='YlGnBu')\nfig.colorbar(im)\nax.set_xticklabels(xlabels, top=True)\nax.set_yticklabels(ylabels[::-1], top=True)\nplt.show()\n",
        "\nfrom matplotlib import rc\n\nrc(\"mathtext\", default=\"regular\")\n\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\n\nfig = plt.figure()\nax = fig.add_subplot(121, projection='polar')\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.legend(loc=\"best\")\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\n\nax2 = fig.add_subplot(122, projection='polar')\nax2.plot(time, Rn, \"-\", label=\"Rn\")\nax2.legend(loc=\"best\")\nax2.set_xlabel(\"Time (h)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\n\nplt.show()\nplt.clf()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# make two side-by-side subplots and and in each subplot, plot y over x\n# Title each subplot as \"Y\"\nfig, axs = plt.subplots(2, 1, sharex=x, figsize=(10, 5))\n\n# Set the title for each subplot\naxs[0].set_title('Y1')\naxs[1].set_title('Y2')\n\n# Plot y over x for each subplot\naxs[0].plot(x, y)\naxs[1].plot(x, y)\n\n# Show the plot\nplt.show()\n",
        "\nsns.scatterplot(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    marker_size=30\n)\n",
        "\n\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n\n# create a dataframe with columns a, b, and c\ndf = pd.DataFrame({'a': a, 'b': b, 'c': c})\n\n# plot scatter plot of a over b\nplt.scatter(df['a'], df['b'])\n\n# annotate each data point with correspond numbers in c\nfor i, txt in zip(df['a'], df['c']):\n    plt.annotate(str(txt), (i, df['b'][i]))\n\n# show the plot\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x in a line chart and label the line \"y over x\"\n# Show legend of the plot and give the legend box a title\nplt.plot(x, y, label='y over x')\nplt.legend()\nplt.title('Legend Title')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x in a line chart and label the line \"y over x\"\n# Show legend of the plot and give the legend box a title  \"Legend\"\n# Bold the legend title\n\nplt.plot(x, y, label='y over x')\nplt.legend(loc=\"upper right\", title=\"Legend\", title_fontweight=\"bold\")\nplt.show()\n",
        "\n\nx = np.random.rand(10)\ny = np.random.rand(10)\n\n# Make a histogram of x and show outline of each bar in the histogram\n# Make the outline of each bar has a line width of 1.2\nplt.hist(x, bins=10, edgecolor='black', linewidth=1.2)\nplt.show()\n",
        "\nfig, ax1 = plt.subplots(1, 2, figsize=(10, 5))\n\n# Set the aspect ratio of the first subplot to be three times wider than the second subplot.\nax1.set_aspect('equal')\n\n# Plot something in the first subplot.\nx1 = np.arange(0, 10, 1)\ny1 = np.sin(x1)\nax1.plot(x1, y1)\n\n# Plot something in the second subplot.\nx2 = np.arange(0, 10, 0.5)\ny2 = np.cos(x2)\nax2 = ax1.twinx()\nax2.plot(x2, y2)\n\n# Show the plot.\nplt.show()\n",
        "\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.hist(x, bins=bins, alpha=0.5, edgecolor='black', color='b')\nplt.title('Histogram of x')\nplt.xlabel('x')\nplt.ylabel('Frequency')\nplt.subplot(122)\nplt.hist(y, bins=bins, alpha=0.5, edgecolor='black', color='r')\nplt.title('Histogram of y')\nplt.xlabel('y')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n",
        "\n\nx = np.random.rand(10)\ny = np.random.rand(10)\n\n# Plot a grouped histograms of x and y on a single chart with matplotlib\n# Use grouped histograms so that the histograms don't overlap with each other\nplt.figure(figsize=(10, 6))\n\n# Create a pivot table of x and y\npivot = pd.pivot_table(pd.DataFrame({'x': x, 'y': y}), values='x', index='y', aggfunc=np.sum)\n\n# Plot the histograms\nplt.subplot(121)\npivot.plot(kind='group', by='x', color=['blue', 'red'], fontsize=8)\nplt.title('Histogram of x and y')\nplt.xlabel('x')\nplt.ylabel('Frequency')\n\nplt.subplot(122)\npivot.plot(kind='group', by='y', color=['blue', 'red'], fontsize=8)\nplt.title('Histogram of x and y')\nplt.xlabel('x')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n",
        "\nplt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n",
        "\nfrom matplotlib.colors import ListedColormap\n\nx = np.random.random((10, 10))\ny = np.random.random((10, 10))\n\n# create two colormaps with x and y\nx_cmap = ListedColormap(x)\ny_cmap = ListedColormap(y)\n\n# plot the two colormaps in different subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\ncmap1 = ax1.pcolormesh(x, cmap=x_cmap)\ncmap2 = ax2.pcolormesh(y, cmap=y_cmap)\n\n# create a single colorbar for both subplots\ncbar = plt.colorbar(cmap1, ax=ax1)\ncbar.set_label('')\ncbar.update(cmap=cmap2)\n\n# show the figure\nplt.show()\n",
        "\n# Plot each column in x as an individual line and label them as \"a\" and \"b\"\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.xlabel('')\nplt.ylabel('')\nplt.legend()\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n\n# plot y over x and z over a in two different subplots\n# Set \"Y and Z\" as a main title above the two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\nax1.plot(x, y)\nax2.plot(a, z)\nax1.set_title('Y over X')\nax2.set_title('Z over A')\nplt.show()\n",
        "\nplt.plot(points[:, 0], points[:, 1], 'o')\nplt.yscale('log')\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# plot y over x\n# use font size 20 for title, font size 18 for xlabel and font size 16 for ylabel\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, linewidth=2)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nax.set_yticklabels(y)\n",
        "\n\nlines = [[(0, 1), (1, 1)], [(2, 3), (3, 3)], [(1, 2), (1, 3)]]\nc = np.array([(1, 0, 0, 1), (0, 1, 0, 1), (0, 0, 1, 1)])\n\n# Plot line segments according to the positions specified in lines\n# Use the colors specified in c to color each line segment\nfor i, line in enumerate(lines):\n    plt.plot(*line, color=c[i])\n\nplt.show()\n",
        "\n\nx = np.arange(0, 1000, 50)\ny = np.arange(0, 1000, 50)\n\n# plot y over x on a log-log plot\n# mark the axes with numbers like 1, 10, 100. do not use scientific notation\nplt.loglog(x, y, 'k.')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([10, 100, 1000], ['1', '10', '100'])\nplt.yticks([1, 10, 100], ['1', '10', '100'])\nplt.show()\n",
        "\n# make four line plots of data in the data frame\n# show the data points  on the line plot\nplt.figure(figsize=(10, 10))\n\n# plot A\nax1 = plt.subplot(4, 1, 1)\nax1.plot(df.index, df['A'], marker='o')\nax1.set_title('Plot A')\n\n# plot B\nax2 = plt.subplot(4, 1, 2)\nax2.plot(df.index, df['B'], marker='o')\nax2.set_title('Plot B')\n\n# plot C\nax3 = plt.subplot(4, 1, 3)\nax3.plot(df.index, df['C'], marker='o')\nax3.set_title('Plot C')\n\n# plot D\nax4 = plt.subplot(4, 1, 4)\nax4.plot(df.index, df['D'], marker='o')\nax4.set_title('Plot D')\n\n# show all plots\nplt.tight_layout()\nplt.show()\n",
        "\n\ndata = [1000, 1000, 5000, 3000, 4000, 16000, 2000]\n\n# Make a histogram of data and renormalize the data to sum up to 1\n# Format the y tick labels into percentage and set y tick labels as 10%, 20%, etc.\nhist, bins = np.histogram(data, bins=np.arange(min(data), max(data)+100, 100))\nhist = hist.astype('float64')\nhist /= hist.sum()\n\nplt.figure()\nplt.bar(bins.tolist(), hist.tolist())\nplt.xlabel('Value')\nplt.ylabel('Frequency (%)')\nplt.xticks(rotation=45)\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x in a line plot\n# Show marker on the line plot. Make the marker have a 0.5 transparency but keep the lines solid.\nplt.plot(x, y, markerfacecolor='none', marker='o', markersize=10, linewidth=2, alpha=0.5)\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\na = np.arange(10)\nz = np.arange(10)\n\n# Plot y over x and a over z in two side-by-side subplots.\n# Label them \"y\" and \"a\" and make a single figure-level legend using the figlegend function\nfig, ax1 = plt.subplots(1, 1, figsize=(10, 5))\nax1.plot(x, y, label='y')\nax1.plot(z, a, label='a')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Plot y over x and a over z')\nax1.legend()\n\nplt.show()\n",
        "\n# Create two subplots with different y-axes\nfig, ax1 = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nax1[0].set_xlabel(\"bill_length_mm\")\nax1[0].set_ylabel(\"bill_depth_mm\")\nax1[0].plot(df[\"bill_length_mm\"], df[\"bill_depth_mm\"], color=\"blue\")\n\n# Plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nax1[1].set_xlabel(\"bill_length_mm\")\nax1[1].set_ylabel(\"flipper_length_mm\")\nax1[1].plot(df[\"bill_length_mm\"], df[\"flipper_length_mm\"], color=\"red\")\n\n# Show the plot\nplt.show()\n\n",
        "\nax.set_xticklabels(range(1, 10))\nax.set_xticklabels([num_format(i) for i in range(1, 10)], minor=True)\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x\n# Show legend and use the greek letter lambda as the legend label\nplt.plot(x, y, label='y over x')\nplt.legend(handles=['y over x'], labels=['y over x'])\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(y, x)\nplt.xticks(range(0, 10, 2))\n\n# Add extra ticks [2.1, 3, 7.6] to existing xticks\nxticks = plt.gca().get_xticks() + [2.1, 3, 7.6]\nplt.gca().set_xticks(xticks)\nplt.gca().set_xticklabels(xticks)\n",
        "\nplt.xticks(rotation=-60)\nplt.xticks(rotation=45, ha='left')\n\n",
        "\nplt.gca().set_yticklabels(np.rot90(y, k=45, axes=1))\nplt.gca().set_xticklabelposition('top')\n",
        "\nplt.xticks(x, y, alpha=0.5)\n\n",
        "\nplt.ylim(0, 10)\n",
        "\nplt.ylim(0, y[-1])\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# make a two columns and one row subplots. Plot y over x in each subplot.\n# Give the plot a global title \"Figure\"\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# Set the title for the figure\nfig.suptitle(\"Figure\")\n\n# Plot y over x in each subplot\nax[0].plot(x, y)\nax[1].plot(x, y)\n\n# Show the plot\nplt.show()\n",
        "\n\nvalues = [[1, 2], [3, 4]]\ndf = pd.DataFrame(values, columns=[\"Type A\", \"Type B\"], index=[\"Index 1\", \"Index 2\"])\n\n# Plot values in df with line chart\n# label the x axis and y axis in this plot as \"X\" and \"Y\"\ndf.plot(kind='line', x='Type A', y='Type B', rot=0, ax=plt.gca())\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='v', linestyle='None', linewidth=0, hatch='//')\n",
        "\nplt.scatter(x, y, marker='o', linestyle='None', linewidth=0)\nplt.gca().get_yaxis().set_tick_params(which='both', direction='in')\nplt.gca().get_xaxis().set_tick_params(which='both', direction='in')\nplt.show()\n",
        "\nplt.scatter(x, y, marker='o', c='black', s=50, hatch='o')\n",
        "\nplt.scatter(x, y, s=100, c='black', marker='o', hatch='///')\n",
        "\nfig, ax = plt.subplots()\nim = ax.imshow(data, cmap='coolwarm', extent=[0, 5, 1, 4])\nfig.colorbar(im, ax=ax)\nax.set_xlim(0, 5)\nax.set_ylim(1, 4)\nplt.show()\n",
        "\nplt.stem(x, y, 'r', basefmt='k')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\n\nd = {\"a\": 4, \"b\": 5, \"c\": 7}\nc = {\"a\": \"red\", \"c\": \"green\", \"b\": \"blue\"}\n\n# Make a bar plot using data in `d`. Use the keys as x axis labels and the values as the bar heights.\n# Color each bar in the plot by looking up the color in colors\nfig, ax = plt.subplots()\nax.bar(d.keys(), d.values(), color=c.values())\nax.set_xlabel('Key')\nax.set_ylabel('Value')\nax.set_title('Bar Plot')\nplt.show()\n",
        "\nplt.plot([0, 5], label=\"data\")\nplt.axvline(x=3, color='k', linestyle='-')\nplt.legend()\n",
        "\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.set_facecolor('white')\nax.set_theta_direction(-1)\n\nax.bar(labels, height, alpha=0.5, color='b', width=0.35, align='edge', ec='k')\n\nax.set_theta_label('')\nax.set_rticks([])\nax.set_rlimits([0, 0])\nax.set_rticklabels([])\n\nax.set_title('Bar Plot in Polar Coordinates', fontsize=18)\n\nplt.show()\n",
        "\n\nl = [\"a\", \"b\", \"c\"]\ndata = [225, 90, 50]\n\n# Make a donut plot of using `data` and use `l` for the pie labels\n# Set the wedge width to be 0.4\nplt.pie(data, labels=l, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and show blue dashed grid lines\nplt.plot(x, y, 'k-')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='dashed', color='gray')\nplt.grid(which='major', linestyle='none', color='white')\n",
        "\n\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.show()\n",
        "\n\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n\n# Make a pie chart with data in `sizes` and use `labels` as the pie labels and `colors` as the pie color.\n# Bold the pie labels\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.show()\n",
        "\n\nplt.plot(x, y, marker='o', markersize=1, linestyle='--', color='black')\n\n",
        "\nplt.axvline(55, color='green')\n",
        "\n\n# Specify the values of blue bars (height)\nblue_bar = (23, 25, 17)\n# Specify the values of orange bars (height)\norange_bar = (19, 18, 14)\n\n# Create two separate bar plots for blue and orange bars\nplt.bar(range(len(blue_bar)), blue_bar, color='blue', label='Blue')\nplt.bar(range(len(orange_bar)), orange_bar, color='orange', label='Orange')\n\n# Set the x-axis labels and title\nplt.xlabel('Values')\nplt.title('Bar Plot of Blue and Orange Bars')\n\n# Add a legend to identify the colors of the bars\nplt.legend()\n\n# Show the plot\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\na = np.arange(10)\n\n# Make two subplots\n# Plot y over x in the first subplot and plot z over a in the second subplot\n# Label each line chart and put them into a single legend on the first subplot\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# First subplot: plot y over x\naxs[0].plot(x, y, label='y')\n\n# Second subplot: plot z over a\naxs[1].plot(a, z, label='z')\n\n# Set the title and legend\naxs[0].set_title('Plot 1')\naxs[0].legend()\n\n# Show the figures\nplt.show()\n",
        "\n# Plot y over x with a scatter plot\n# Use the \"Spectral\" colormap and color each data point based on the y-value\n\n# Create a scatter plot with y as the color map\nplt.scatter(x, y, c=y, cmap='Spectral')\n\n# Set the title and axis labels\nplt.title('Scatter Plot with Color Map')\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Show the plot\nplt.show()\n",
        "\nplt.plot(x, y, 'o')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x[::2], [x[::2]])\nplt.yticks(y[::2], [y[::2]])\nplt.show()\n",
        "\n# Create a pivot table with \"species\" as index and \"bill_length_mm\" as values\npivot_table = df.pivot_table(index=\"species\", columns=\"sex\", values=\"bill_length_mm\")\n\n# Plot the pivot table using seaborn factorpot\nsns.factorplot(data=pivot_table, x=\"sex\", y=\"bill_length_mm\", hue=\"species\", aspect=1.5, palette=\"Set1\")\n\n# Set subplot sharex parameter to False to avoid sharing y axis across subplots\nsns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.2, rc={\"figure.subplot.sharex\": False})\n\n# Show the plot\nplt.show()\n",
        "\nplt.circle(0.5, 0.5, 0.2)\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x and use the greek letter phi for title. Bold the title and make sure phi is bold.\nplt.plot(x, y)\nplt.title('\u03a6', fontdict=dict(weight='bold'))\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handles=[plt.gca().get_line_artist()], loc=\"best\", scatterpoints=1, numpoints=1, fontsize=12, fancybox=True, framealpha=0.5, handlelength=1.5, markerfirst=True, markerfacecolor='None', markeredgecolor='w', markevery=0.1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Plot Y over X')\nplt.show()\n",
        "\nplt.plot(x, y, label=\"Line\")\nplt.legend(handles=[], loc=\"best\", fancybox=True, prop={'size': 12})\nplt.title(\"Plot of y over x\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n",
        "\nhandles, labels = plt.gca().get_legend_handles_labels()\nplt.legend(handles[:len(labels)//2], labels[:len(labels)//2],\n          handles[len(labels)//2:], labels[len(labels)//2:],\n          loc=\"upper right\", bbox_to_anchor=(1.05, 1.05),\n          ncol=2, fancybox=True, shadow=True, columnspacing=0.5)\n",
        "\nhandles, labels = plt.gca().get_legend_handles_labels()\nplt.legend(handles, labels, loc=\"upper right\")\nplt.plot(x, y, marker=\"o\", label=\"Line\")\n",
        "\nfig, ax = plt.subplots()\nim = ax.imshow(data, cmap='viridis')\nfig.colorbar(im)\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x. Give the plot a title \"Figure 1\" with bold font for the word \"Figure\" but not for the number \"1\"\nplt.plot(x, y)\nplt.title(\"Figure 1\")\nplt.title(fontweight='bold', size=14)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\nsns.pairplot(\n    data=df,\n    hue=\"id\",\n    x_vars=[\"x\"],\n    y_vars=[\"y\"],\n    hue_knots=[2],\n    palette=\"Set1\",\n    diag_kind=\"kde\",\n    aspect=1,\n    ci=0.8,\n)\n\n",
        "\n# Plot y over x and invert the x axis\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.invert_xaxis()\nplt.show()\n",
        "\n\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot a scatter plot with values in x and y\n# Plot the data points to have red inside and have black border\nplt.scatter(x, y, c='red', ec='black')\n\n# Show the plot\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# plot y over x on a 2 by 2 subplots with a figure size of (15, 15)\n# repeat the plot in each subplot\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor i in range(2):\n    for j in range(2):\n        axs[i, j].plot(x, y)\n\nplt.show()\n",
        "\n\nx = np.random.rand(100) * 10\n\n# Make a histogram of x\n# Make the histogram range from 0 to 10\n# Make bar width 2 for each bar in the histogram and have 5 bars in total\nplt.hist(x, bins=5, range=(0, 10), width=2)\nplt.show()\n",
        "\nfrom matplotlib import pyplot as plt\n\nx = np.arange(10)\ny = np.arange(1, 11)\nerror = np.random.random(y.shape)\n\n# Plot y over x and show the error according to `error`\n# Plot the error as a shaded region rather than error bars\nplt.errorbar(x, y, yerr=error, fmt='o-')\nplt.fill_between(x, y-error, y+error, alpha=0.2)\nplt.show()\n",
        "\nplt.plot([0, 0], [0, 0], 'k', linewidth=0)\nplt.plot([0, 0], [-5, 5], 'k', linewidth=0)\nplt.plot([-5, 5], [0, 0], 'k', linewidth=0)\n",
        "\nax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n\n# Plot y over x and z over a in two side-by-side subplots\n# Make \"Y\" the title of the first subplot and \"Z\" the title of the second subplot\n# Raise the title of the second subplot to be higher than the first one\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(10, 4))\n\nax1.plot(x, y, label='Y')\nax1.set_title('Y')\nax2.plot(z, a, label='Z')\nax2.set_title('Z', yoffset=0.5)\n\nax1.legend()\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# make 4 by 4 subplots with a figure size (5,5)\n# in each subplot, plot y over x and show axis tick labels\n# give enough spacing between subplots so the tick labels don't overlap\nfig, axs = plt.subplots(4, 4, figsize=(5, 5), tight_layout=True)\n\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xlabel('x')\n        axs[i, j].set_ylabel('y')\n\nplt.show()\n",
        "\nplt.matshow(d)\nplt.gca().set_size_inches(8, 8)\nplt.show()\n",
        "\nplt.boxplot(df.values)\nplt.xticks(rotation=45)\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().set_xticklabels([])\nplt.gca().set_yticklabels([])\nplt.gca().spines['top'].set_position(('outward', 10))\nplt.gca().spines['bottom'].set_position(('outward', -10))\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.gca().set_xticks([])\nplt.gca().set_yticks([])\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks_position('left')\nplt.show()\n",
        "\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n",
        "\n# Create a new dataframe with only the rows that have \"Fat\" in the \"diet\" column\nfat_df = df[df['diet'] == 'Fat']\n# Create a new dataframe with only the rows that have \"No Fat\" in the \"diet\" column\nno_fat_df = df[df['diet'] == 'No Fat']\n# Create a new dataframe by combining the two dataframes above\ncombined_df = pd.concat([fat_df, no_fat_df])\n# Create a new dataframe by grouping the data by \"kind\" and \"diet\"\ngrouped_df = combined_df.groupby(['kind', 'diet'])\n# Create a new dataframe by ungrouping the data\nungrouped_df = grouped_df.ungroup()\n# Create a new dataframe by selecting only the columns \"pulse\" and \"time\"\ncatplot_df = ungrouped_df[['pulse', 'time']]\n# Create a catplot of scatter plots with \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\ncatplot_df.plot(kind='scatter', x='time', y='pulse', hue='kind', col='diet', alpha=0.5)\n# Set the title of the plot to \"Group: Fat\"\nplt.title('Group: Fat')\n",
        "\n# Create a new dataframe with \"Exercise Time\" as x and \"Pulse\" as y\nexercise_time_pulse = pd.DataFrame({'Exercise Time': df['time'], 'Pulse': df['pulse']})\n\n# Set the xlabel to \"Exercise Time\" and ylabel to \"Pulse\"\nexercise_time_pulse.set_xlabel('Exercise Time')\nexercise_time_pulse.set_ylabel('Pulse')\n\n# Create a new dataframe with \"Exercise Time\" as x and \"Diet\" as y\nexercise_time_diet = pd.DataFrame({'Exercise Time': df['time'], 'Diet': df['diet']})\n\n# Set the xlabel to \"Exercise Time\" and ylabel to \"Diet\"\nexercise_time_diet.set_xlabel('Exercise Time')\nexercise_time_diet.set_ylabel('Diet')\n\n# Create a new dataframe with \"Pulse\" as x and \"Diet\" as y\npulse_diet = pd.DataFrame({'Pulse': df['pulse'], 'Diet': df['diet']})\n\n# Set the xlabel to \"Pulse\" and ylabel to \"Diet\"\npulse_diet.set_xlabel('Pulse')\npulse_diet.set_ylabel('Diet')\n\n# Create a new dataframe with \"Pulse\" as x and \"Kind\" as y\npulse_kind = pd.DataFrame({'Pulse': df['pulse'], 'Kind': df['kind']})\n\n# Set the xlabel to \"Pulse\" and ylabel to \"Kind\"\npulse_kind.set_xlabel('Pulse')\npulse_kind.set_ylabel('Kind')\n\n# Create a new dataframe with \"Kind\" as x and \"Diet\" as y\nkind_diet = pd.DataFrame({'Kind': df['kind'], 'Diet': df['diet']})\n\n# Set the xlabel to \"Kind\" and ylabel to \"Diet\"\nkind_diet.set_xlabel('Kind')\nkind_diet.set_ylabel('Diet')\n\n# Create a new dataframe with \"Kind\" as x and \"Pulse\" as y\nkind_pulse = pd.DataFrame({'Kind': df['kind'], 'Pulse': df['pulse']})\n\n# Set the xlabel to \"Kind\" and ylabel to \"Pulse\"\nkind_pulse.set_xlabel('Kind')\nkind_pulse.set_ylabel('Pulse')\n\n# Create a new dataframe with \"Diet\" as x and \"Pulse\" as y\ndiet_pulse = pd.DataFrame({'Diet': df['diet'], 'Pulse': df['pulse']})\n\n# Set the xlabel to \"Diet\" and ylabel to \"Pulse\"\ndiet_pulse.set_xlabel('Diet')\ndiet_pulse.set_ylabel('Pulse')\n\n# Create a new dataframe with \"Diet\" as x and \"Kind\" as y\ndiet_kind = pd.DataFrame({'Diet': df['diet'], 'Kind': df['kind']})\n\n# Set the xlabel to \"Diet\" and ylabel to \"Kind\"\ndiet_kind.set_xlabel('Diet')\ndiet_kind.set_ylabel('Kind')\n\n# Create a new dataframe with \"Kind\" as x and \"Exercise Time\" as y\nkind_time = pd.DataFrame({'Kind': df['kind'], 'Exercise Time': df['time']})\n\n# Set the xlabel to \"Kind\" and ylabel to \"Exercise Time\"\nkind_time.set_xlabel('Kind')\nkind_time.set_ylabel('Exercise Time')\n\n# Create a new dataframe with \"Exercise Time\" as x and \"Diet\" as y\ntime_diet = pd.DataFrame({'Exercise Time': df['time'], 'Diet': df['diet']})\n\n# Set the xlabel to \"Exercise Time\" and ylabel to \"Diet\"\ntime_diet.set_xlabel('Exercise Time')\ntime_diet.set_ylabel('Diet')\n\n# Create",
        "\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\", pcolor=True, ci=None, dodge=True, alpha=0.5, palette=\"Set1\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Scatter Plot of Pulse vs Time by Diet and Kind\")\nplt.show()\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# plot y over x with label \"y\"\n# make the legend fontsize 8\nplt.plot(x, y, label='y')\nplt.legend(fontsize=8)\nplt.show()\n",
        "\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x with label \"y\" and show legend\n# Remove the border of frame of legend\nplt.plot(x, y, label='y')\nplt.legend(loc=\"best\", frameon=False)\nplt.show()\n",
        "\nfrom numpy import *\n\nt = linspace(0, 2 * math.pi, 400)\na = sin(t)\nb = cos(t)\nc = a + b\n\n# Plot a, b, c in the same figure\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.xlabel('t')\nplt.ylabel('Function values')\nplt.legend()\nplt.show()\n",
        "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, legend=False)\n",
        "\nsns.set(style=\"whitegrid\")\nfig, ax = plt.subplots(nrows=3, ncols=10, figsize=(12, 12))\n\nfor i, col in enumerate(df[\"b\"]):\n    ax = fig.axes[i]\n    xticks = np.arange(0, 31, 1)\n    xticklabels = np.arange(0, 31, 2)\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xticklabels)\n    sns.pointplot(x=\"a\", y=\"c\", data=df[df[\"b\"] == col], ax=ax)\n\nplt.show()\n",
        "\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_3d_properties(azimuth=100, elevation=50)\n",
        "\n\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot y over x in a line chart and name axis with labels (\"x\" and \"y\")\n# Hide tick labels but keep axis labels\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([], [''])\nplt.yticks([], [''])\nplt.show()\n",
        "\n\n# Create a 2x2 grid of subplots\ngs = gridspec.GridSpec(nrow=2, ncol=2, figure=fig)\n\n# Plot x in each subplot\nfor i in range(2):\n    for j in range(2):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x[i*2:(i+1)*2, j*2:(j+1)*2], cmap='Greys')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title('')\n\n# Remove the space between each subplot\nfor i in range(2):\n    for j in range(2):\n        ax = fig.add_subplot(gs[i, j])\n        ax.set_title('')\n\n# Make the subplots adjacent to each other\nfor i in range(2):\n    for j in range(2):\n        ax = fig.add_subplot(gs[i, j])\n        ax.set_title('')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n"
    ],
    "Tensorflow": [
        "\n# Change the value of x to 1\nx.assign(1)\n# [Missing Code]\n# Get the value of x as an integer\nresult = int(x.numpy())\n",
        "\nx.assign(114514)\n",
        "\nresult = tf.constant(labels, dtype=tf.int32)\n# [Missing Code]\n",
        "\none_hot_labels = tf.one_hot(labels, depth=10)\nresult = tf.math.argmax(one_hot_labels, axis=-1)\n# [Missing Code]\n",
        "\nlabels_tensor = tf.constant(labels, dtype=tf.int32)\nresult = tf.argmax(labels_tensor, axis=-1)\n",
        "\n    result = tf.constant(labels, dtype=tf.int32)\n    # [Missing Code]\n",
        "\nlabels_tensor = tf.constant(labels, dtype=tf.int32)\n# [Missing Code]\nresult = tf.argmax(labels_tensor, axis=-1)\n",
        "\ndef my_map_func(i):\n  return [i, i+1, i+2]\n# [Missing Code]\n",
        "\n    result = []\n    for i in input:\n        result.extend([i, i+1, i+2])\n    return result\n",
        "\nresult = tf.ones((8, 8))\nresult = result[:, tf.range(8)]\nresult = result[:, tf.cast(tf.less(tf.range(8), lengths), tf.int32)]\nresult = result[:, tf.newaxis]\nresult = result[:, tf.tile(tf.reshape(tf.range(8), (1, 8)), (8, 1))]\n",
        "\nresult = tf.ones((8, 8))\nmask = tf.zeros((8, 8))\nfor i in range(len(lengths)):\n    mask[i*8:(i+1)*8] = tf.cast(tf.less(tf.range(8), lengths[i]), tf.int32)\nresult = result * tf.cast(mask, tf.float32)\n",
        "\nimport tensorflow as tf\n\n\nlengths = [4, 3, 5, 2]\n\n# Create a tensor of zeros with the desired length\nzeros = tf.zeros((8,))\n\n# Pad the lengths tensor with zeros to the front\npadded_lengths = tf.pad(lengths, [[0, 4]])\n\n# Convert the padded lengths to a binary mask\nmask = tf.cast(padded_lengths >= lengths, tf.int32)\n\n# Print the resulting mask\nprint(mask)\n",
        "\n    result = []\n    for length in lengths:\n        result.append([1]*(length+1)[:8])\n    return result\n",
        "\nresult = tf.ones((8, 8))\nresult = result[tf.newaxis::, tf.newaxis:].astype(tf.int32)\nresult[tf.range(8), lengths] = 0\nresult = result.reshape(4, 2, 2)\n",
        "\nresult = tf.cartesian_product(a, b)\n",
        "\n    result = tf.cartesian_product(a, b)\n    # [Missing Code]\n",
        "\n# Reshape the tensor to have shape (50, 100, 512)\nresult = tf.reshape(a, (50, 100, 512))\n",
        "\n# Reshape the tensor to have shape (50, 100, 1, 512)\nresult = tf.reshape(a, (50, 100, 1, 512))\n",
        "\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n",
        "\ntf.reduce_sum(A, axis=1)\n",
        "\nresult = tf.reduce_prod(A, axis=1)\n",
        "\nreciprocal = tf.reciprocal(A)\nresult = reciprocal\n",
        "\nimport tensorflow as tf\n\n\na = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nb = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\n\n# Calculate the squared difference between A and B\ndiff = tf.sub(a, b)\nsquared_diff = tf.square(diff)\n\n# Calculate the L2 distance element-wise\nresult = tf.reduce_sum(squared_diff, axis=1)\n\nprint(result)\n",
        "\nimport tensorflow as tf\n\na = tf.constant([\n  [1,1,1],\n  [0,1,1]\n])\nb = tf.constant([\n  [0,0,1],\n  [1,1,1]\n])\n\n# Calculate the L2 distance between A and B column-wise\ndistance = tf.sqrt(tf.reduce_sum(tf.square(a - b), axis=1))\n\n# Get the result of the L2 distance calculation\nresult = tf.reduce_sum(distance)\n\nprint(result)\n",
        "\n    # Calculate the squared difference between A and B\n    diff = tf.square(tf.subtract(A, B))\n    # Calculate the L2 distance element-wise\n    distance = tf.reduce_sum(tf.square(diff), axis=1) ** 0.5\n    # Return the result\n    return distance\n",
        "\nresult = x[tf.cast(y, tf.int32), tf.cast(z, tf.int32)]\n",
        "\nm = x[tf.cast(row, tf.int32)][tf.cast(col, tf.int32)]\nresult = m.numpy()\n",
        "\n    result = tf.gather(x, tf.cast(z, tf.int32))\n",
        "\nC = tf.matmul(A, B)\nresult = tf.reshape(C, [10, 10, 1])\n",
        "\nC = tf.einsum('BNI,BNI->BNN', A, B)\nresult = tf.reduce_sum(C, axis=1)\n",
        "[Missing Code]\nresult = []\nfor byte_string in x:\n    result.append(byte_string.decode('utf-8'))\n\nprint(result)",
        "[Missing Code]\nresult = []\nfor x in example_x:\n    result.append(x.decode('utf-8'))\nreturn result",
        "\nresult = tf.reduce_mean(x, axis=1, keep_dims=True) / tf.math.reduce_sum(tf.math.cast(tf.not_equal(x, 0), tf.float32))\n",
        "\n# Compute the mean of the second to last dimension (features) of x.\nmean_values = tf.reduce_mean(x, axis=1, keep_dims=True)\n# Compute the variance of the second to last dimension (features) of x.\nvariance_values = tf.reduce_variance(x, axis=1, keep_dims=True)\n# Compute the output tensor y by dividing the variance values by the square of the mean values.\nresult = tf.divide(variance_values, tf.square(mean_values))\n",
        "\n    # Compute the number of non-zero entries in the second to last dimension\n    num_non_zero = tf.math.reduce_sum(tf.math.cast(tf.math.not_equal(x, 0), tf.float32))\n    \n    # Compute the sum of the second to last dimension of x\n    sum_second_to_last_dim = tf.math.reduce_sum(x, axis=-2)\n    \n    # Compute the average of the second to last dimension of x\n    result = sum_second_to_last_dim / num_non_zero\n    \n",
        "\n# We don't need the Session class in Tensorflow 2, so we can remove it and use the Eager execution instead.\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n    scores = tf.argmax(a, axis=1)\n    result = tf.where(scores == tf.range(100), 1, 0)\n",
        "\nresult = tf.argmax(a, axis=1)\n",
        "\n#Save the model in \"export/1\"\nmodel.save('export/1')\n",
        "\nimport tensorflow as tf\n\nseed_x = 10\n### return the tensor as variable 'result'\nresult = tf.random.uniform([10], minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n",
        "\nimport tensorflow as tf\n\nseed_x = 10\n### return the tensor as variable 'result'\nresult = tf.random.uniform([114], minval=2, maxval=5, dtype=tf.int32, seed=seed_x)\nprint(result)\n",
        "\n    # Use the uniform_unit_scalar distribution to generate 10 random integers from {1, 2, 3, 4}.\n    result = tf.random.uniform_unit_scalar(10)\n    # Set the random seed to 10.\n    tf.random.set_seed(seed_x)\n",
        "\nimport tensorflow as tf\n\n# Get the version of TensorFlow\nresult = tf.__version__\n\n# Print the version\nprint(result)\n"
    ],
    "Scipy": [
        "\nresult = scipy.optimize.curve_fit(lambda x, A, B: A*np.log(x) + B, x, y)\n",
        "\nresult = scipy.optimize.curve_fit(lambda x, A, B: A + B*np.log(x), x, y)\n",
        "[Missing Code]\nresult = scipy.optimize.curve_fit(lambda x, A, B, C: A*np.exp(B*x) + C, x, y, p0=p0)\n",
        "\n# Perform two-sample KS test\nstatistic, p_value = stats.ks_2samp(x, y)\n",
        "\nresult = stats.ks_twosample(x, y, 'norm')\n",
        "[Missing Code]\nresult = optimize.minimize(f, initial_guess, method='SLSQP', bounds=[[-10, 10], [-10, 10], [-10, 10]])\n",
        "\np_values = scipy.stats.norm.sf(z_scores)\n",
        "\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\np_values = scipy.stats.norm.ppf(z_scores)\n",
        "\nz_scores = np.zeros(len(p_values))\nfor i in range(len(p_values)):\n    z_scores[i] = scipy.stats.norm.ppf(p_values[i])\n",
        "\ndist = stats.lognorm.cdf(x, mu, stddev)\nresult = dist ** 2\n",
        "\n# Calculate the expected value and median of the lognormal distribution\nexpected_value = mu + stddev * np.sqrt(np.log(total))\nmedian = mu + stddev * np.sqrt(np.log(total)) * np.exp(-1)\n",
        "\nresult = sa * sb\n",
        "\n    result = np.dot(sA.toarray(), sB.toarray())\n",
        "\nresult = scipy.interpolate.LinearNDInterpolator(points, V)(request)\n",
        "\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n",
        "\n# Create a translation matrix to account for the origin shift\ntranslation_matrix = np.array([[x0, y0], [0, 0]])\n# Rotate the image using the translation matrix\nrotated_image = np.rot90(data_orig, k=angle, axes=1, precompute=True)\nrotated_image = np.dot(rotated_image, translation_matrix)\n# Get the rotated coordinates (xrot, yrot)\nxrot, yrot = (int(round(val)) for val in np.nonzero(rotated_image)[0])\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\n\n# Get the number of elements in the sparse matrix\nnum_rows = M.shape[0]\nnum_cols = M.shape[1]\n\n# Initialize an array to store the main diagonal elements\nmain_diag = np.zeros(num_rows)\n\n# Loop through the rows and add the main diagonal element to the array\nfor i in range(num_rows):\n    main_diag[i] = M.data[i][i]\n\nprint(main_diag)\n",
        "\nresult = stats.kstest(times, \"uniform\")\n",
        "\n    result = stats.kstest(times, \"uniform\")\n",
        "\nresult = stats.kstest(times, \"uniform\", 0.05)\n",
        "\nFeature = c1.toarray() + c2.toarray()\n",
        "\nFeature = []\nfor i in range(len(c1)):\n    Feature.append([c1[i], c2[i]])\nFeature = sparse.csr_matrix(Feature)\n",
        "\nFeature = c1.copy()\nFeature.shape = (len(c1.rows), len(c2.columns))\nFeature.data = [row + col for row in c1.data for col in c2.data]\nFeature.indices = [c1.indices + c2.indices]\nFeature.indices += [len(c1.indices)] * (len(c2.columns) - len(c1.indices))\nFeature.indices += [c2.indices]\nFeature.indices += [len(c1.indices)] * (len(c2.columns) - len(c1.indices))\nFeature.indices += [c2.indices]\nFeature.indices += [len(c1.indices)] * (len(c2.columns) - len(c1.indices))\nFeature.indices += [c2.indices]\n",
        "\n# create a distance matrix between points1 and points2\ndistances = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        distances[i][j] = np.sqrt((points1[i,0]-points2[j,0])**2 + (points1[i,1]-points2[j,1])**2)\n# use the linear assignment problem to find the optimal matching\nresult = scipy.optimize.linear_sum_assignment(distances)\n",
        "\n# create a distance matrix between points1 and points2\ndistances = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        distances[i][j] = np.sqrt((points1[i,0] - points2[j,0])**2 + (points1[i,1] - points2[j,1])**2)\n# use the linear assignment problem to find the optimal matching\nresult = scipy.optimize.linear_sum_assignment(distances)\n",
        "\n# Set the diagonal elements to zero\nb.setdiag(0)\n",
        "\nresult = ndimage.measurements.label(img > threshold)\n",
        "\nresult = ndimage.measurements.label(img > threshold)\nresult = ndimage.measurements.regionprops(result)\nresult = result[result.mean_intensity < 0.75]\nprint(result.shape)\n",
        "\n    result = 0\n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            if img[i, j] > threshold:\n                result += 1\n                for di in range(-1, 2):\n                    for dj in range(-1, 2):\n                        if i + di < 0 or i + di >= img.shape[0] or j + dj < 0 or j + dj >= img.shape[1]:\n                            continue\n                        if img[i + di, j + dj] > threshold:\n                            result += 1\n",
        "\nresult = []\nfor threshold in range(img.min(), img.max()+1):\n    label, num_labels = ndimage.measure.label(img > threshold)\n    region_centers = ndimage.measure.find_objects(label)\n    center_of_mass = np.mean(region_centers, axis=0)\n    distance = np.sqrt((center_of_mass[0] - 0)**2 + (center_of_mass[1] - 0)**2)\n    result.append(distance)\n",
        "\ndef make_symmetric(M):\n    M_symmetric = M.copy()\n    M_symmetric.make_symmetric()\n    return M_symmetric\n",
        "\n    sA = sA.tocsc()\n    sA = sA.T\n    sA = sA.tocsr()\n    sA.data = sA.data[:, np.newaxis]\n    sA.indices = sA.indices[:, np.newaxis]\n    sA.indptr = sA.indptr[:, np.newaxis]\n    # [Missing Code]\n    sA.data = sA.data / 2\n    sA.indices = sA.indices // 2\n    sA.indptr = sA.indptr // 2\n",
        "\nstructuring_element = np.ones((3, 3), dtype=np.uint8)\nopening = scipy.ndimage.binary_dilation(square, structuring_element)\nopening = scipy.ndimage.binary_erosion(opening, structuring_element)\n",
        "\nlabel = np.zeros_like(square)\nlabel[1:-1, 1:-1] = 1\nbinary = np.where(label, square, 1)\nopenings = scipy.ndimage.binary_opening(binary, structure = np.ones((3, 3), dtype = np.uint8))\nclosings = scipy.ndimage.binary_closing(openings, structure = np.ones((3, 3), dtype = np.uint8))\nresult = np.where(closings == 0, 0, square)\nprint(result)\n",
        "\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Calculate mean\nmean = np.mean(col)\n\n# Calculate standard deviation\nstd_dev = np.std(col)\n\nprint(mean)\nprint(std_dev)\n",
        "\nmax_val = np.max(col)\nmin_val = np.min(col)\n",
        "\nmedian = np.median(col.data)\nmode = np.mode(col.data)\n",
        "\ndef fourier(x, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15):\n    return a1 * np.cos(1 * np.pi / tau * x) + \\\n           a2 * np.cos(2 * np.pi / tau * x) + \\\n           a3 * np.cos(3 * np.pi / tau * x) + \\\n           a4 * np.cos(4 * np.pi / tau * x) + \\\n           a5 * np.cos(5 * np.pi / tau * x) + \\\n           a6 * np.cos(6 * np.pi / tau * x) + \\\n           a7 * np.cos(7 * np.pi / tau * x) + \\\n           a8 * np.cos(8 * np.pi / tau * x) + \\\n           a9 * np.cos(9 * np.pi / tau * x) + \\\n           a10 * np.cos(10 * np.pi / tau * x) + \\\n           a11 * np.cos(11 * np.pi / tau * x) + \\\n           a12 * np.cos(12 * np.pi / tau * x) + \\\n           a13 * np.cos(13 * np.pi / tau * x) + \\\n           a14 * np.cos(14 * np.pi / tau * x) + \\\n           a15 * np.cos(15 * np.pi / tau * x)\n",
        "\n# Calculate pairwise Euclidean distances between all regions\ndistances = np.zeros((len(example_array), len(example_array)))\nfor i in range(len(example_array)):\n    for j in range(len(example_array)):\n        if i == j:\n            distances[i][j] = 0\n        else:\n            distances[i][j] = np.sqrt((example_array[i][0] - example_array[j][0])**2 + (example_array[i][1] - example_array[j][1])**2)\n",
        "\n# Calculate pairwise Manhattan distances between all regions\ndistance_matrix = scipy.spatial.distance.cdist(example_array, example_array, 'm')\n# Convert distance matrix to a list of tuples\ndistances = list(distance_matrix)\n",
        "\n    # Calculate pairwise Euclidean distances between all regions\n    dists = scipy.spatial.distance.cdist(example_array, example_array, 'euclidean')\n    # Convert distances from cells to meters\n    dists = dists * example_array.shape[0]\n",
        "\n# Interpolate the data points\nx_int = np.interp(x_val, x[:, 0], y[:, 0])\ny_int = np.interp(x_val, x[:, 1], y[:, 1])\n# Extrapolate the curves\nx_ext = np.linspace(x_val[0], x_val[-1], 100)\ny_ext = np.interp(x_ext, x[:, 0], y[:, 0])\ny_ext = np.interp(x_ext, x[:, 1], y[:, 1])\n# Combine the results\nresult = np.vstack((x_int, y_int)).T\nresult = np.hstack((result, y_ext))\n",
        "\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x1, x2, x3, x4)\n",
        "\nresult = ss.anderson_ksamp(x1, x2, n_samples=10000, normalize=True)\n",
        "\ndef tau1(x):\n    y = np.array(df['A']) #  keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\ndf['AC'] = pd.rolling_apply(df['C'], 3, lambda x: tau1(x))\ndf['BC'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\n",
        "\ndef is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n# [Missing Code]\nresult = is_csr_matrix_only_zeroes(sa)\n",
        "\ndef is_empty(matrix):\n    return len(matrix.nonzero()) == 0\n# [Missing Code]\nresult = is_empty(sa)\n",
        "\nresult = block_diag(*a)\n",
        "\np_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\n",
        "\n    p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\n",
        "\nkurtosis_result = np.mean((a - np.mean(a))**4) / np.std(a)**4\n",
        "\nkurtosis_result = scipy.stats.kurtosis(a)\n",
        "\nresult = scipy.interpolate.interp2d(x, y, z)(s, t)\n",
        "\n    result = scipy.interpolate.interp2d(x, y, z, (s, t))\n",
        "\nresult = []\nfor point in extraPoints:\n    region = vor.region_from_point(point)\n    result.append(region)\n",
        "\nresult = []\nfor i in range(len(extraPoints)):\n    region = vor.regions[vor.vertices[0].distance(extraPoints[i])]\n    result.append(region)\n",
        "\nsparse_matrix = sparse.csc_matrix(vectors)\nresult = sparse_matrix.toarray()\n",
        "\na = np.random.binomial(n=1, p=1/2, size=(9, 9))\nb = scipy.ndimage.median_filter(a, 3, mode='nearest')\nb = b.roll(1, axis=0)\nprint(b)\n",
        "\nresult = M.data[M.indices[row]][column]\n",
        "\nresult = M.getrow(row).toarray()[column]\n",
        "\ndef interp1d_wrapper(x, array):\n    return interp1d(x, array)(x_new)\n# [Missing Code]\nnew_array = interp1d_wrapper(x, array)\n",
        "\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n",
        "\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n",
        "\ndef dct_matrix(N):\n    result = np.identity(N)\n    for i in range(N):\n        result[i, i] = np.sqrt(1.0 / N)\n    return result\n",
        "\nresult = sparse.diags([-1, 0, 1], (5, 5)).toarray()\nresult[0, 0] = matrix[0, 0]\nresult[1, 1] = matrix[1, 1]\nresult[2, 2] = matrix[2, 2]\nresult[3, 3] = matrix[3, 3]\nresult[4, 4] = matrix[4, 4]\n",
        "\nresult = np.array([scipy.stats.binomial.pmf(i, j, p) for i in range(N+1) for j in range(i+1)])\n",
        "\nresult = df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
        "\nfrom scipy.stats import zscore\nresult = df.apply(lambda x: (x - x.mean()) / x.std(), axis=1)\n",
        "\nfrom scipy.stats import zscore\nresult = df.copy()\nresult['zscore'] = zscore(result['sample1'], scale=1)\nresult['zscore'] = result['zscore'].apply(lambda x: x[0] if x[1] == 0 else x[1])\nresult = result.drop(['data'], axis=1)\nresult = result.reset_index()\nresult.index += 1\nprint(result)\n",
        "\n# Calculate zscore for each column\ndf['zscore'] = np.zeros((len(df), 3))\nfor i in range(len(df)):\n    for j in range(3):\n        df.at[i, 'zscore'][j] = stats.zscore(df.at[i, j])\n",
        "\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\nmid = np.array([[0, 0], [0, 1], [1, 1], [1, 0], [0, 0], [0, 0]])\nresult = distance.cdist(np.dstack((np.random.rand(*shape), np.random.rand(*shape))), mid)\nprint(result)\n",
        "\nmid = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [0, 2], [1, 2]])\n",
        "\nimport numpy as np\nfrom scipy.spatial import distance\ndef get_distance_2(y, x):\n    mid = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # center point\n    result = distance.cdist(np.dstack((y, x)), mid, 'euclidean')\n    return result\n",
        "\nresult = scipy.ndimage.zoom(x, 2, order=1, output_shape=shape)\n",
        "\n# Define the objective function\ndef func(x, a):\n    return a.dot(x ** 2) - y\n# Define the constraints\nconstraints = (\n    {'type': 'eq', 'fun': lambda x, a: np.sum(x) - 5},\n    {'type': 'ineq', 'fun': lambda x, a: np.sum(x) - 10},\n    {'type': 'ineq', 'fun': lambda x, a: np.sum(x) - 15}\n)\n# Minimize the objective function with the constraints\nresult = scipy.optimize.minimize(func, x0, args=(a,), constraints=constraints)\n",
        "\nbounds = (x_lower_bounds, None)\nconstraints = (())\n",
        "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) if 0 < t < 2 * np.pi else 2 * np.pi\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n",
        "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n",
        "\nresult = sa.toarray() + sb.toarray()\n",
        "\nresult = sa.toarray()[:, None] + sb.toarray()[None, :]\n",
        "\nresult = scipy.integrate.quad(lambda x: 2*x*c, low, high)[0]\n",
        "\n    result = scipy.integrate.quad(lambda x: 2*x*c, low, high)[0]\n",
        "\ny = V + x\nprint(y)\n",
        "\nV_plus_x = V + x\nprint(V_plus_x)\n",
        "\nV_plus_x = V + x\nB = V_plus_x + y\n",
        "\n# Normalize each column of the matrix\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    Column = Column / Len\n    sa[:,Col] = Column\n",
        "\n# Normalize each column of the matrix\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    Len = math.sqrt(sum(List))\n    Column = Column / Len\n    sa[:,Col] = Column\n",
        "\na = np.where(a > 0, 1, 0)\n",
        "\na = scipy.sparse.csr_matrix(a)\n# [Missing Code]\n",
        "\nfrom scipy.cluster.hierarchy import linkage, cut_tree\ndef closest_to_centroid(centroids, data):\n    # Find the linkage matrix\n    linkage_matrix = linkage(data, method='ward', metric='euclidean')\n    # Find the cluster assignments\n    cluster_assignments = cut_tree(linkage_matrix, centroids)\n    # Initialize the result array\n    result = np.empty(len(centroids), dtype=int)\n    # Loop through each cluster and find the closest element\n    for i in range(len(centroids)):\n        cluster_data = data[cluster_assignments == i]\n        centroid = centroids[i]\n        closest_index = np.argmin(np.linalg.norm(cluster_data - centroid, axis=1))\n        result[i] = closest_index\n    return result\n",
        "\n# First, we need to find the distance between each data point and the centroids\ndistances = np.zeros((100, 5))\nfor i in range(5):\n    distances[:, i] = np.linalg.norm(centroids[:, i] - data, axis=1)\n# [Missing Code]\n# Now, we need to find the closest distance to each centroid\nclosest_distances = np.min(distances, axis=1)\n",
        "\n# Calculate the distance matrix\ndist_matrix = np.zeros((100, 5))\nfor i in range(100):\n    for j in range(5):\n        dist_matrix[i][j] = np.linalg.norm(data[i] - centroids[j])\n# Find the k-th closest element to each centroid\nresult = np.empty((5, ), dtype=int)\nfor i in range(5):\n    k_closest_idx = scipy.spatial.KDTree(data[:, 0:3]).query(centroids[i], k=k)[0]\n    result[i] = k_closest_idx\n",
        "[Missing Code]\nresult = fsolve(eqn, x0=2, args=(a, b), rtol=1e-6, atol=1e-6, maxiter=1000)\n",
        "\nresult = []\nfor a in adata:\n    b = fsolve(eqn, x0=xdata[adata.index(a)], args=(a,))[0]\n    if b < xdata[adata.index(a)]:\n        result.append([b, a])\n    else:\n        result.append([xdata[adata.index(a)], a])\n",
        "\nresult = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d), crit_level=0.05)\n",
        "\nkstest_result = stats.kstest(sample_data, bekkers, args=(estimated_a, estimated_m, estimated_d), critical_value=0.95)\nresult = kstest_result[1]\n",
        "\nimport pandas as pd\nimport io\nfrom scipy import integrate\ndef rolling_integral(df, col, freq):\n    return df.groupby(pd.Grouper(freq=freq)).apply(lambda x: integrate.trapz(x[col]))\nstring = '''\nTime                      A\n2017-12-18-19:54:40   -50187.0\n2017-12-18-19:54:45   -60890.5\n2017-12-18-19:54:50   -28258.5\n2017-12-18-19:54:55    -8151.0\n2017-12-18-19:55:00    -9108.5\n2017-12-18-19:55:05   -12047.0\n2017-12-18-19:55:10   -19418.0\n2017-12-18-19:55:15   -50686.0\n2017-12-18-19:55:20   -57159.0\n2017-12-18-19:55:25   -42847.0\n'''\ndf = pd.read_csv(io.StringIO(string), sep = '\\s+')\nfreq = '25S'\nintegral_df = rolling_integral(df, df['A'], freq)\nprint(integral_df)\n",
        "\ninterp = scipy.interpolate.LinearNDInterpolator(x, y)\nresult = interp(eval)\n",
        "\n# Define the log-likelihood function\ndef log_likelihood(params, data):\n    return -np.sum(np.log(params) * data)\n# Define the bounds for the optimization\nbounds = [(0.001, 1000000), (0.1, 1000000)]\n# Run the optimization to find the maximum likelihood estimates\nresult = sciopt.minimize(log_likelihood, bounds, method='bounded', jac=True)\n# Extract the optimal weights from the result\nweights = result.x\nprint(weights)\n",
        "\npopt = sciopt.fminbound(e, pmin, pmax, args=(x,y), approx_grad=True)\n",
        "\nresult = []\n# Find the indices of the relative extrema\nfor i in range(len(arr)-2*n+1):\n    if arr[i] <= arr[i+n] and arr[i] <= arr[i-n]:\n        result.append(i)\n",
        "\nresult = []\nfor i in range(len(arr)):\n    for j in range(len(arr[i])):\n        if arr[i][j] <= arr[i][j+n] and arr[i][j] <= arr[i][j-n]:\n            result.append([i, j])\nprint(result)\n",
        "\n# We need to filter out only the numerical columns and then apply the zscore function to remove outliers\ndf_num = df.loc[:, df.columns.isin(['NUM1', 'NUM2', 'NUM3'])]\ndf_num = df_num[(np.abs(stats.zscore(df_num)) < 3).all(axis=1)]\n"
    ],
    "Sklearn": [
        "\ndata1 = pd.DataFrame(data)\n",
        "\ndata1 = pd.DataFrame(data)\n",
        "\nimport numpy as np\nfrom sklearn.datasets import load_boston\nimport pandas as pd\ndata = load_boston()\n# Convert the data to a Pandas DataFrame\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\nprint(data1)\n",
        "\n    iris_df = pd.DataFrame(data.data, columns=data.feature_names)\n    iris_df['target'] = data.target\n    return iris_df\n",
        "\ndf_out = pd.get_dummies(df['Col3'], columns=['Apple', 'Orange', 'Banana', 'Grape'])\n",
        "\ndf = pd.get_dummies(df, columns=['Col3'])\n",
        "\ndf_out = pd.get_dummies(df['Col4'], columns=['Apple', 'Banana', 'Grape', 'Orange', 'Suica'])\n",
        "\ndf_out = pd.get_dummies(df.pop('Col3'), columns=['Apple', 'Orange', 'Banana', 'Grape'])\n",
        "\ndf_out = pd.get_dummies(df.pop('Col3'), columns=['Apple', 'Orange', 'Banana'])\ndf = pd.concat([df, df_out], axis=1)\n",
        "\nsvmmodel.fit(X, y)\npredicted_test= svmmodel.predict(x_test)\npredicted_test_scores= svmmodel.decision_function(x_test)\nproba = np.zeros_like(predicted_test)\nfor i in range(len(predicted_test)):\n    proba[i] = 1 / (1 + np.exp(-predicted_test_scores[i]))\nprint(proba)\n",
        "\nprob_model = svm.LinearSVC(penalty='l1', C=1)\nprob_model.fit(X, y)\nx_predict_proba = prob_model.predict_proba(x_predict)\nproba = x_predict_proba[:, 1]\n",
        "\n# Encode the target column\ny = df_origin['target']\ny_encoded = transform_output.transform(y)\n# Concatenate the encoded target with the original dataframe\ndf = pd.concat([df_origin.drop('target', axis=1), y_encoded], axis=1)\n",
        "\nsparse_matrix = csr_matrix(transform_output)\ndf = pd.concat([df_origin, pd.DataFrame(sparse_matrix, columns=['new_column'])], axis=1)\n",
        "\nsparse_df = pd.DataFrame(sparse_matrix, columns=df_origin.columns, dtype=df_origin.dtypes)\nsparse_df.index = df_origin.index\ndf_result = pd.concat([df_origin.drop(['column_to_be_dropped'], axis=1), sparse_df], axis=1)\nreturn df_result\n",
        "[Missing Code]\nsteps = clf.steps\ndel clf.steps[:2]\nclf.steps = steps\nprint(len(clf.steps))",
        "\ndel clf.steps[1]\n",
        "[Missing Code]\nsteps = clf.named_steps()\ndel steps[1]\nclf = Pipeline(steps)\n",
        "\nprint(len(clf.steps))\n",
        "\n# Insert any step\nestimators.insert(1, ('new_step', LinearRegression()))\n",
        "\nsteps = clf.named_steps()\nif 'pOly' in steps:\n    steps.remove('pOly')\n    steps.insert(steps.index('reduce_dIm') + 1, 'pOly')\n    clf = Pipeline(estimators)\nprint(clf.named_steps)\n",
        "\n# Set up the GridSearchCV object\nparam_grid = {'n_estimators': [100, 1000],\n              'max_depth': [2, 4, 6, 8],\n              'learning_rate': [0.01, 0.1]}\n\n# Set up the TimeSeriesSplit object\ncv = TimeSeriesSplit(n_splits=5, test_size=0.2, random_state=42)\n\n# Set up the GridSearchCV object with the TimeSeriesSplit object\ngridsearch = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=cv, n_jobs=-1, iid=True)\n\n# Fit the model using the GridSearchCV object\ngridsearch.fit(trainX, trainY)\n\n# Get the best score and the corresponding parameters\nbest_score = gridsearch.best_score_\nbest_params = gridsearch.best_params_\n\n# Print the best score and the corresponding parameters\nprint(\"Best score: {:.2f}\".format(best_score))\nprint(\"Best parameters: {}\".format(best_params))\n\n# Make predictions using the best parameters\ny_pred = xgb.XGBRegressor(**best_params).predict(testX)\n\n# Evaluate the predictions using the mean absolute error\nmae = np.mean(np.abs(y_pred - testY))\nprint(\"Mean absolute error: {:.2f}\".format(mae))\n",
        "\n# Set the fit parameters\nfit_params = {\"early_stopping_rounds\": 42,\n              \"eval_metric\": \"mae\",\n              \"eval_set\": [[testX, testY]]}\n# Create the grid search object with fit_params\ngridsearch = xgb.XGBRegressor(**fit_params)\n# Use the grid search object instead of the original one\ngridsearch = GridSearchCV(gridsearch, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\n# Fit the model with the new grid search object\ngridsearch.fit(trainX, trainY)\n",
        "\nlogreg.fit(X, y)\nproba = logreg.predict_proba(X)\n",
        "[Missing Code]\nproba = []\nfor train_idx, test_idx in cv:\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict_proba(X_test)\n    proba.append(y_pred)\nprint(proba)",
        "\ninversed = scaler.inverse_transform(scaled)\n",
        "\n    # [Missing Code]\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nmodel_name = model.__class__.__name__\n",
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\ndata = load_data()\n\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\n\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data)\nprint(tf_idf_out)\n",
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\ndata = load_data()\n\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF())\n])\n\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data)\nnmf_out = pipe.named_steps[\"nmf\"].fit_transform(tf_id_f_out)\n",
        "\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\ndata, target = load_data()\n\npipe = Pipeline(steps=[\n    ('select', SelectKBest(k=2)),\n    ('clf', LogisticRegression())]\n)\n\nselect_out = pipe.steps[0].fit_transform(data, target)\nprint(select_out)\n",
        "\n# Create a pipeline\npipeline = Pipeline([\n    ('base_estimator', dt),\n    ('ensemble', bc)\n])\n# Use GridSearchCV to find the best parameters\nclf = GridSearchCV(pipeline, param_grid, cv=5)\nclf = clf.fit(X_train, y_train)\n# Print the best parameters\nprint(clf.best_params_)\n",
        "\n# Convert X and y to pandas DataFrame\nX = pd.DataFrame(X)\ny = pd.DataFrame(y)\n",
        "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\n# [Missing Code]\n",
        "\ndef preprocess(s):\n    return s.upper()\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n",
        "\ntfidf = TfidfVectorizer(preprocessor=prePro)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns)\n",
        "\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=data.columns)\n",
        "\n# fit the pipeline on the data\ngrid = grid.fit(X, y)\n# get the coefficients of the best model\ncoef = grid.best_estimator_.coef_\nprint(coef)\n",
        "\n# fit the pipeline on the data\ngrid = GridSearchCV(pipe, param_grid={\"model__alpha\": [2e-4, 3e-3, 4e-2, 5e-1]}, cv=7)\ngrid.fit(X, y)\n# get the coefficients of the best model\ncoef = grid.best_estimator_.coef_\nprint(coef)\n",
        "\ncolumn_names = list(X.columns)\n",
        "\nselected_features = model.transform(X)\nselected_features.toarray().columns\n",
        "\ncolumn_names = X.columns\n",
        "\ncolumn_names = list(X.columns)\n",
        "\ndef closest_50_samples(p):\n    idx = km.predict([p])[0]\n    return X[idx][:50]\n# [Missing Code]\n",
        "\ndef get_closest_samples(p, X, k):\n    distances = np.zeros((X.shape[0], k))\n    for i in range(k):\n        distances[:, i] = np.linalg.norm(X - km.cluster_centers_[i], axis=1)\n    sorted_indices = np.argsort(distances[:, p])\n    closest_samples = X.iloc[sorted_indices[:50]]\n    return closest_samples\n",
        "\ndef closest_100_samples(p):\n    idx = km.predict([p])[0]\n    closest_samples = X[idx]\n    closest_samples = closest_samples.iloc[:100]\n    return closest_samples\n",
        "\n    idx = km.labels_[np.argmin(km.distances_.flatten() - km.cluster_centers_[p])]\n    samples = X[idx]\n    return samples\n",
        "\n# Convert categorical variable to matrix\nX_train_dummies = pd.get_dummies(X_train[0], columns=['a', 'b'])\n# Merge back with original training data\nX_train_dummies = pd.merge(X_train_dummies, X_train[1:], on='variable_name')\n# [Missing Code]\n",
        "\n# convert categorical variable to matrix\nX_train_dummies = pd.get_dummies(X_train, columns=['categorical_variable'])\n# merge back with original training data\nX_train_dummies = pd.merge(X_train_dummies, X_train, on=['target', 'categorical_variable'])\n# convert target to numeric\nX_train_dummies[['target_a', 'target_b']] = pd.to_numeric(X_train_dummies[['target_a', 'target_b']])\n# fit the model\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train_dummies, y_train)\n",
        "\nfrom sklearn.svm import SVC\nmodel = SVC(kernel='rbf')\nmodel.fit(X, y)\npredict = model.predict(X)\n",
        "\nsvm = sklearn.svm.SVC(kernel='rbf', gamma='auto')\n",
        "\nfrom sklearn.svm import SVC\nmodel = SVC(kernel='poly', degree=2)\nmodel.fit(X, y)\npredictions = model.predict(X)\nprint(predictions)\n",
        "\n# Create and fit the model\nmodel = SVC(kernel='poly', degree=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel.fit(X_train, y_train)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf.toarray(), tfidf.toarray().T).toarray()\ncosine_similarities_of_queries = np.abs(cosine_similarities_of_queries)\ncosine_similarities_of_queries = np.round(cosine_similarities_of_queries, 2)\n",
        "\nquery_tfidf = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(query_tfidf.toarray(), tfidf.toarray().T).toarray()\ncosine_similarities_of_queries = np.round(cosine_similarities_of_queries, 2)\n",
        "\n    query_tfidf = tfidf.transform(queries)\n",
        "\nnew_features = []\nfor sample in features:\n    sample = np.array(sample)\n    sample = np.reshape(sample, (1, sample.size))\n    new_features.append(sample)\n    # [Missing Code]\n",
        "\n# Convert the features to a 2D-array\nnew_f = np.array([list(row) for row in f])\nnew_f = np.reshape(new_f, (new_f.shape[0], -1))\nnew_f = np.argmax(new_f, axis=1)\n",
        "\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\n# Convert the features to a 2D-array\nnew_features = np.array([list(row) for row in features])\n# Reshape the array to a 2D-array\nnew_features = np.reshape(new_features, (len(features), -1))\n# Convert the array to a pandas dataframe\nnew_features = pd.DataFrame(new_features)\n# Rename the columns\nnew_features.columns = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6']\nprint(new_features)\n",
        "\n# Convert features to a 2D-array\nnew_features = np.array(features).reshape(-1, 7)\n# Convert the 2D-array to a pandas dataframe\nnew_features = pd.DataFrame(new_features, columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n# Return the new features\nreturn new_features\n",
        "\n# Convert the features to a 2D-array\nnew_features = np.array(features).reshape(-1, 8)\n",
        "\nfrom sklearn.cluster import AgglomerativeClustering\ndef get_distance_matrix(data):\n    return np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\ndef get_labels(distance_matrix):\n    labels = AgglomerativeClustering(n_clusters=2, linkage='euclidean', random_state=0).fit_predict(distance_matrix)\n    return labels\ncluster_labels = get_labels(get_distance_matrix(data_matrix))\n",
        "\ndistance_matrix = np.array(data_matrix)\nX = np.array(data_matrix)\n",
        "\nfrom sklearn.cluster import AgglomerativeClustering\ndef linkage(X):\n    return np.min(X, axis=1)\ncluster_labels = AgglomerativeClustering(linkage=linkage, n_clusters=2).fit_predict(simM)\n",
        "\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(data_matrix)\ncluster_labels = kmeans.labels_\n",
        "\ndistance_matrix = np.array(data_matrix)\nlinkage_result = scipy.cluster.hierarchy.linkage(distance_matrix, method='ward')\nclusters = linkage_result.fcluster(2, t=0.5)\ncluster_labels = list(map(str, clusters))\n",
        "\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(simM)\ncluster_labels = kmeans.labels_\n",
        "\n# Scaling and centering the data\nscaler = StandardScaler()\ncentered_data = scaler.fit_transform(data)\nscaled_data = centered_data\n",
        "\nfrom sklearn.preprocessing import StandardScaler\nscaled_data = StandardScaler().fit_transform(data)\ncentered_data = scaled_data + np.mean(scaled_data, axis=0)\ncentered_scaled_data = np.round(centered_data, decimals=2)\n",
        "\nbox_cox_data = PowerTransformer().fit_transform(data)\n",
        "\nbox_cox_data = PowerTransformer().fit_transform(data)\n",
        "\n# Yeo-Johnson transformation function\ndef yeo_johnson(x):\n    return (np.log(x) - np.log(np.sqrt(np.pi * 4))) / 2\n",
        "\nfrom sklearn.preprocessing import YeoJohnson\nyeo_johnson_data = YeoJohnson().fit_transform(data)\n",
        "\n# Preserve punctuation marks by setting the 'preserve_punctuation' parameter to True\nvectorizer = CountVectorizer(stop_words='english', min_df=2, max_df=0.5, max_features=1000, preserve_punctuation=True)\ntransformed_text = vectorizer.fit_transform(text)\n",
        "\n# [Missing Code]\n",
        "\nfrom sklearn.model_selection import train_test_split\nx_train, y_train, x_test, y_test = train_test_split(data[:-1], data[-1], test_size=0.2, random_state=42)\n",
        "\n# Split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.3, random_state=42)\n",
        "\n    x_train, y_train, x_test, y_test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\nX = df['mse'].values.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n",
        "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n",
        "\n# Select the top 10 features using LinearSVC\nmodel = LinearSVC(penalty='l1')\nX_train = vectorizer.fit_transform(corpus)\nX_train_selected = model.fit_transform(X_train)\nselected_feature_indices = model.coef_.argsort()[::-1][:10]\nselected_feature_names = [vectorizer.get_feature_name(i) for i in selected_feature_indices]\nprint(selected_feature_names)\n",
        "\n# Use LinearSVC with penalty='l1' and keep default arguments for others unless necessary\nmodel = LinearSVC(penalty='l1')\nmodel.fit(X, y)\nselected_features = model.coef_.argsort()[:, ::-1]\nselected_feature_names = vectorizer.get_feature_names()[selected_features]\nprint(selected_feature_names)\n",
        "\n    # [Missing Code]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocab)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocab)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\n# [Missing Code]\n",
        "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\n# [Missing Code]\n",
        "\n# Create a list to store the slopes\nslopes = []\n# Loop through all columns\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] #removes NaN values for each column to apply sklearn function\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) # either this or the next line\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\nslopes = []\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\n# ElasticNet Regression\n\nX_train = pd.DataFrame(X_train)\ny_train = pd.DataFrame(y_train)\nX_test = pd.DataFrame(X_test)\ny_test = pd.DataFrame(y_test)\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n",
        "\ntransformed = MinMaxScaler().fit_transform(np_array.reshape(-1, 1))\n",
        "\ntransformed = MinMaxScaler().fit_transform(np_array)\n",
        "\n    # Normalize the entire np array all together\n    new_a = (a - a.min()) / (a.max() - a.min())\n",
        "\n# Create a new dataframe with the last close price and moving averages\nclose_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n\n# Predict using the decision tree model\npredict = clf.predict(b)\n",
        "[Missing Code]\nX = np.array(X, dtype=object)\nclf.fit(X, ['2', '3'])",
        "[Missing Code]\nX = np.array(X, dtype=str)\nnew_X = np.array([['asdf', '1'], ['asdf', '0']], dtype=str)\n",
        "[Missing Code]\nX = np.array(X, dtype=object)\nclf.fit(X, ['4', '5'])",
        "\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\nlogReg = LogisticRegression()\nlogReg.fit(X,y)\npredict = logReg.predict(X)\nprint(predict)\n",
        "\n# Convert the data to numpy array\nX = np.array(dataframe.iloc[:, :-1])\ny = np.array(dataframe.iloc[:, -1])\n# Fit the model\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n# [Missing Code]\n",
        "\ndef split_data(data, train_size):\n    train_data, test_data = train_test_split(data, train_size)\n    train_data = train_data.sort([\"date\"])\n    test_data = test_data.sort([\"date\"])\n    return train_data, test_data\n",
        "\ndef split_data(data, train_size):\n    train_data, test_data = np.split(data, [int(len(data) * train_size)])\n    return train_data, test_data\n# [Missing Code]\ntrain_dataframe, test_dataframe = split_data(features_dataframe, 0.8)\n",
        "\n    # Calculate the size of the train set\n    train_size = 0.2\n    train_size_int = int(train_size * len(features_dataframe))\n    # Split the data into train and test sets\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size_int, random_state=42)\n    # Sort the train and test sets by date\n    train_dataframe = train_dataframe.sort([\"date\"])\n    test_dataframe = test_dataframe.sort([\"date\"])\n",
        "\ncols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].apply(scaler.fit_transform).reshape(-1, len(cols)).T\n",
        "\ncols = myData.columns[2:4]\nmyData['new_' + cols] = myData.groupby('Month')[cols].apply(scaler.fit_transform).reshape(-1, 1)\n",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform(words)\nfeature_names = count.get_feature_names()\nprint(feature_names)\n",
        "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n",
        "\nfull_results = GridSearch_fitted.cv_results_\n",
        "\nfull_results = GridSearch_fitted.cv_results_\nfull_results = full_results.sort_values(by=['mean_fit_time'])\nfull_results = full_results.reset_index()\nfull_results.index += 1\nfull_results = full_results[['params', 'mean_fit_time', 'score']]\nfull_results.columns = ['param_name', 'mean_fit_time', 'score']\nprint(full_results)\n",
        "\nwith open('sklearn_model', 'wb') as f:\n    pickle.dump(fitted_model, f)\n",
        "\ncosine_similarity_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = np.triu(cosine_similarity_matrix, k=1)\ncosine_similarity_matrix = 1 - np.diagonal(cosine_similarity_matrix)\n"
    ],
    "Pytorch": [
        "\n    # [Missing Code]\n",
        "\ndef adjust_learning_rate(optimizer, loss_increased):\n    if loss_increased:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] *= 0.5\n",
        "\n    # If the validation loss is too high, reduce the learning rate\n    if valid_loss > 0.001:\n        scheduler.last_epoch = epoch\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.0005\n",
        "\ndef adjust_learning_rate(optimizer, loss):\n    if loss > 0.005:\n        optimizer.lr *= 0.5\n",
        "\n# Convert the output of Word2Vec to a PyTorch tensor\nembedding_weights = torch.from_numpy(word2vec.wv.vectors_)\n# Create an embedding layer in PyTorch\nembedded_input = torch.nn.Embedding(input_size=word2vec.wv.vector_count, output_size=100)(embedding_weights)\n",
        "\nembedded_input = torch.tensor(word2vec.wv.vectors_)\nreturn embedded_input\n",
        "\n# Convert the torch tensor to numpy array\nx_np = x.numpy()\n# Convert the numpy array to pandas dataframe\npx = pd.DataFrame(x_np)\n",
        "\n# Convert the torch tensor to numpy array\nx_np = x.numpy()\n# Convert the numpy array to pandas DataFrame\npx = pd.DataFrame(x_np)\n",
        "\n# Convert the torch tensor to numpy array\ny = x.numpy()\n# Convert the numpy array to pandas dataframe\npx = pd.DataFrame(y)\n",
        "\nC = B[A_log]\n",
        "\nC = B[A_logical]\n",
        "\nC = B[A_log]\n",
        "\n# Convert the logical index to an integer index\nA_log = np.where(A_log == 0, np.nan, None)\n# Use torch.where() to perform logical indexing\nC = torch.where(A_log, B, torch.tensor([]))\n",
        "\nC = B[A_log]\n",
        "\nC = B[A_log]\n",
        "\nC = B.index_select(0, idx)\n",
        "\nx_array = x_array.apply(lambda x: torch.from_numpy(x))\nx_tensor = torch.stack(x_array)\n",
        "\nx_tensor = torch.from_numpy(x).float()\n",
        "\ndef Convert(a):\n    x = np.array(a, dtype=np.float16)\n    t = torch.from_numpy(x)\n    return t\n",
        "\ndef convert_lengths_to_mask(lens):\n    mask = torch.zeros(len(lens), max(lens) + 1)\n    for i, len in enumerate(lens):\n        mask[i, :len] = 1\n    return mask\n",
        "\ndef convert_lengths_to_mask(lens):\n    mask = torch.zeros(len(lens), max(lens) + 1)\n    for i, len in enumerate(lens):\n        mask[i, :len] = 1\n    return mask\n",
        "\ndef convert_lengths_to_mask(lens):\n    mask = torch.zeros(len(lens), max(lens) + 1)\n    for i, len in enumerate(lens):\n        mask[i, :len] = 1\n    return mask\n",
        "\ndef get_mask(lens):\n    mask = torch.zeros(len(lens), max(lens) + 1)\n    for i, len in enumerate(lens):\n        mask[i, 0:len+1] = torch.ones(len+1)\n    return mask\n",
        "\ndiag_ele = Tensor_2D.diag().numpy()\nmatrix = np.eye(index_in_batch)[None, :, :]\nTensor_3D = torch.cat([matrix.repeat(index_in_batch, axis=0), Tensor_2D.unsqueeze(0)], dim=0)\n",
        "\ndef Convert(t):\n    index_in_batch = t.size(0)\n    diag_ele = t.max().item()\n    drag_ele = np.diag(np.ones(index_in_batch))\n    result = torch.from_numpy(drag_ele).type(torch.FloatTensor)\n    return result\n",
        "\n# Flatten the tensors\na_flat = a.view(-1, 11)\nb_flat = b.view(-1, 11)\n# Concatenate the flattened tensors\nab_flat = torch.cat((a_flat, b_flat), dim=1)\n# Reshape the concatenated tensor\nab = ab_flat.view(3, 11)\n",
        "\n# Flatten the tensors\na_flat = a.view(-1, 514)\nb_flat = b.view(-1, 514)\n# Concatenate the flattened tensors\nab_flat = torch.cat((a_flat, b_flat), 0)\n# Reshape the concatenated tensor\nab = ab_flat.reshape(138, 514)\n# Print the resulting tensor\nprint(ab)\n",
        "\n    # Convert tensors to numpy arrays\n    a_np = a.numpy()\n    b_np = b.numpy()\n    # Concatenate the arrays along the last axis\n    ab_np = np.concatenate((a_np, b_np), axis=2)\n    # Convert the concatenated array back to a tensor\n    ab = torch.tensor(ab_np)\n",
        "\na[ : , : , 0 ] = 0\na[ : , : , -1 ] = 0\na[ : , lengths : , : ] = 0\n",
        "\na[ : , np.arange(len(lengths)), : ] = 2333\n",
        "\na[:, :lengths.unsqueeze(-1), :] = 0\n",
        "\na[ : , : lengths , : ] = 2333\n",
        "\nlist_of_tensors = [torch.tensor(tensor) for tensor in list_of_tensors]\n",
        "\nlist = [torch.tensor(x) for x in list]\n",
        "\ndef Convert(lt):\n    tt = torch.tensor([lt[i] for i in range(len(lt))])\n    return tt\n",
        "\nlist_of_tensors = [torch.tensor(tensor) for tensor in list_of_tensors]\ntensor_of_tensors = torch.stack(list_of_tensors)\n",
        "\nresult = t.index_select(0, idx)\n",
        "\nresult = t.index_select(0, idx)\n",
        "\nresult = t.index_select(0, idx)\n",
        "\nresult = x.gather(1, ids.unsqueeze(-1))\n",
        "\nids = torch.tensor(ids).long()\nresult = x.gather(1, ids)\n",
        "\nselected_indices = np.where(ids == 1)[0]\nselected_x = x[selected_indices]\nresult = selected_x.reshape(-1, 2)\n",
        "\n# Create a new tensor with the same dimensions as the softmax output\ny = torch.zeros_like(softmax_output).long()\n# Find the maximum probability for each input\nmax_probs = softmax_output.max(1, keepdim=True)\n# Set the value of y to the index of the maximum probability for each input\ny = y.scatter(1, max_probs.repeat(y.size(0)), 1)\n",
        "\n# Create a new tensor with the same dimensions as the softmax output\ny = torch.zeros_like(softmax_output).long()\n# Find the maximum probability for each input\nmax_probs = softmax_output.max(1, keepdim=True)\n# Set the value of y to the index of the maximum probability for each input\ny = y.scatter(1, max_probs.repeat(y.size(0)), 1)\n",
        "\n# Create a new tensor with the same dimensions as the softmax output\ny = torch.zeros_like(softmax_output)\n# Find the argmax of each row in the softmax output\ny = softmax_output.max(dim=1)\n",
        "\n    # Initialize an empty tensor to store the class probabilities\n    class_probabilities = torch.zeros(softmax_output.size(0), 1)\n    # Loop through each row of the softmax output\n    for i in range(softmax_output.size(0)):\n        # Get the indices of the highest probability class\n        max_prob_indices = torch.argsort(softmax_output[i], dim=1)[:, -1]\n        # Get the class label corresponding to the highest probability index\n        max_prob_class = torch.unique(softmax_output[i][max_prob_indices])[0]\n        # Set the probability of the highest probability class to 1 and the probabilities of the other classes to 0\n        class_probabilities[i] = torch.tensor([[max_prob_class]])\n",
        "\n    # Initialize an empty tensor to store the class with the lowest probability\n    lowest_prob = torch.zeros(softmax_output.size(0), dtype=torch.long)\n    # Loop through each row of the softmax output\n    for i in range(softmax_output.size(0)):\n        # Get the probabilities for each class\n        probs = softmax_output[i].tolist()\n        # Find the index of the class with the lowest probability\n        lowest_index = np.argmin(probs)\n        # Set the corresponding entry in the lowest_prob tensor to the index of the class with the lowest probability\n        lowest_prob[i] = lowest_index\n",
        "\n# Convert labels to one-hot vectors\nlabels = torch.nn.functional.one_hot(labels.float(), num_classes=3)\n# Calculate cross-entropy loss\nloss = F.nll_loss(F.log_softmax(images, dim=1), labels)\n",
        "\ncnt_equal = np.sum(np.equal(A, B))\n",
        "\ncnt_equal = np.sum(np.equal(A, B))\n",
        "\nnot_equal = (A != B).sum()\ncnt_not_equal = len(A) - not_equal\n",
        "\ncnt_equal = np.sum(np.equal(A, B))\n",
        "\n# Convert the tensors to numpy arrays\nA_np = np.array(A)\nB_np = np.array(B)\n# Get the last x elements of each array\nA_last_x = A_np[-x:]\nB_last_x = B_np[-x:]\n# Count the number of equal elements\ncnt_equal = np.sum(A_last_x == B_last_x)\n",
        "\nnot_equal = np.diff(np.asarray(A[-x:])) != np.diff(np.asarray(B[-x:]))\ncnt_not_equal = np.sum(not_equal)\n",
        "\n# Create an empty list to store the tensors\ntensors_31 = []\n# Loop through the range of the fourth dimension\nfor start in range(0, 40, 10):\n    # Create a new tensor with the desired shape\n    tensor = a[:, :, start:start+10, :, :]\n    # Add the tensor to the list of tensors\n    tensors_31.append(tensor)\n",
        "\n# Create an empty list to store the tensors\ntensors_31 = []\n# Loop through the range of the third dimension\nfor i in range(0, 40, chunk_dim):\n    # Create a new tensor with the same dimensions as the original tensor\n    tensor = a[:, :, i:i+chunk_dim, :, :]\n    # Add the tensor to the list of tensors\n    tensors_31.append(tensor)\n",
        "\n# Apply the mask to the output tensor\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n",
        "\n# Create a boolean mask of 0s and 1s\nmask_bool = mask.to(dtype=torch.bool)\n\n# Set the values of output to clean_input_spectrogram where the mask is 0\noutput[mask_bool] = clean_input_spectrogram[mask_bool]\n",
        "\n# Create a new tensor with the minimum absolute values and their signs\nmin_vals = torch.abs(x) < torch.abs(y)\nmin_vals = min_vals.type_as(x)\nmin_vals = torch.where(min_vals, x, y)\nsigned_min = torch.sign(x) * min_vals\n",
        "\n# Create a new tensor with the same shape as x and y\nsigned_max = torch.where(torch.eq(max, torch.max(torch.abs(x))), sign_x, sign_y)\n",
        "\ndef solve(x, y):\n    min_x = torch.min(torch.abs(x))\n    min_y = torch.min(torch.abs(y))\n    signed_min = torch.where(torch.gt(torch.abs(x), torch.abs(y)), x, y)\n    return signed_min\n",
        "\n# Get the predicted class and its confidence score\nconfidence_score = torch.nn.Softmax(dim=1)(output.reshape(1, 3))\nconfidence_score = confidence_score.item()\n",
        "\nimport numpy as np\nimport pandas as pd\nimport torch\n\na, b = load_data()\n\n# Calculate the average of the last column of a and the first column of b\noverlap_avg = (a.last_col + b.first_col) / 2\n\n# Create a new tensor with the merged data\nresult = torch.cat((a[:, :2], b[:, 2:]), dim=1)\nresult[:, 1] = overlap_avg\n\nprint(result)\n",
        "\n    # Calculate the size of the overlapping region\n    overlap_size = min(a.size(1), b.size(1))\n    \n    # Create a new tensor for the result\n    result = torch.zeros_like(a)\n    \n    # Overlap the tensors and calculate the average\n    for i in range(overlap_size):\n        result[:, i] = (a[:, i] + b[:, i]) / 2\n    \n    # Fill in the remaining columns of the result tensor\n    for i in range(overlapping_size, a.size(1)):\n        result[:, i] = a[:, i]\n    \n",
        "\n# Create a new tensor with the desired dimensions and fill it with zeros\nnew = torch.zeros(2, 2)\n",
        "\n# Create a new tensor with the desired dimensions and fill it with zeros\nnew_tensor = torch.tensor([[0., 0., 0., 0.],\n                          [0., 1., 2., 0.],\n                          [0., 3., 4., 0.],\n                          [0., 0., 0., 0.]])\n",
        "\n# Convert the tensor to a numpy array and then to a pandas dataframe\ndf = pd.DataFrame(np.array(t))\n# Add a new column to the dataframe with the values we want to fill in\ndf['new_col'] = [-1 for _ in range(len(df))]\n",
        "\nresult = torch.bmm(data, W).squeeze()\n"
    ]
}