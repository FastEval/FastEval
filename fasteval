#!/usr/bin/env python3

import signal

# This is just for the child processes that we spawn
# The main process will shortly set up its own signal handler
# that overwrites this.
signal.signal(signal.SIGTERM, signal.SIG_IGN)
signal.signal(signal.SIGINT, signal.SIG_IGN)

import multiprocessing

if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn')

def merge_models_and_benchmarks_to_evaluate(existing_models_and_benchmarks, new_model_type, new_model_name, new_benchmarks, model_args):
    import uuid

    if new_model_name is None:
        return (None, existing_models_and_benchmarks)

    inserted_into_existing_entry = False
    existing_entry_id = None
    for item in existing_models_and_benchmarks:
        if item['model_name'] != new_model_name:
            continue
        if item['model_type'] != new_model_type:
            continue
        if item['model_args'] != model_args:
            continue
        for benchmark in new_benchmarks:
            if benchmark not in item['benchmarks']:
                item['benchmarks'].insert(0, benchmark)
        inserted_into_existing_entry = True
        existing_entry_id = item['id']

    if inserted_into_existing_entry:
        return (existing_entry_id, existing_models_and_benchmarks)

    evaluation_id = str(uuid.uuid4())

    existing_models_and_benchmarks.insert(0, {
        'id': evaluation_id,
        'model_type': new_model_type,
        'model_name': new_model_name,
        'benchmarks': new_benchmarks,
        'model_args': model_args,
    })

    return (evaluation_id, existing_models_and_benchmarks)

def stop(*args):
    import os

    os.killpg(0, signal.SIGKILL)

def main():
    import os

    os.setpgrp()

    signal.signal(signal.SIGTERM, stop)
    signal.signal(signal.SIGINT, stop)

    import argparse
    import json
    import os

    import evaluation.args
    from evaluation import benchmarks
    from evaluation.utils import join_threads
    from evaluation.models.models import unload_model
    from evaluation.inference_correctness import run_inference_backend_correctness_check

    all_benchmarks = ['mt-bench', 'cot', 'human-eval-plus', 'lm-evaluation-harness']

    parser = argparse.ArgumentParser()
    parser.add_argument('-b', '--benchmarks', choices=['all'] + all_benchmarks, nargs='*', default='all',
        help='Benchmark(s) that the model will be evaluated on')
    parser.add_argument('-t', '--model-type',
        help='Type of the model that will be evaluated. Can be an API client name (openai), a prompt template (e.g. chatml) or `fastchat` for the fastchat backend.')
    parser.add_argument('-m', '--model-name',
        help='Name of the model that will be evaluated. Depending on the type, it can be an OpenAI model name or a path to a huggingface model.')
    parser.add_argument('--model-tokenizer',
        help='By default, the tokenizer will be the same as the model, but it can also be overwritten with this argument.')
    parser.add_argument('--model-default-system-message',
        help='The default system message of the model. Only applicable for models that use a system message and only if no other system message has been specified.')
    parser.add_argument('--force-backend', choices=['vllm', 'tgi', 'hf_transformers'], required=False,
        help='Force a specific backend for model inference. By default, the backend will be selected automatically depending on model support, '
            + 'but if you encounter bugs with this you can overwrite the backend with this argument.')
    parser.add_argument('--model-force-dtype', choices=['float16', 'bfloat16', 'float32'], required=False,
        help='By default, the dtype of the model will be taken from the model config.json. However, you can overwrite it with this argument.')
    parser.add_argument('--num-gpus-per-model', type=int, default=0,
        help='This argument controls data parallelism. By default, the model will only be instantiated a single time distributed across all GPUs. '
            + 'This works fine if you have one GPU or if you have a big model and two GPUs, but it is not a fast approach if you e.g. have 8 GPUs. '
            + 'In these cases, it is recommended to instantiate the model multiple times on different GPUs and do data parallel evaluation. '
            + 'It is recommended to set --num-gpus-per-model to the number of GPUs that your model will require. '
            + 'For example, if your model requires 2 GPUs and you have 8 GPUs, setting "--num-gpus-per-model 2" will create the model 4 times on 2 GPUs each.')
    parser.add_argument('--run-correctness-check', action='store_true',
        help='Runs a check to make sure that the outputs of the chosen fast inference backend (vLLM or TGI) are equal to those that HF transformers outputs. '
            + 'This is needed because vLLM & TGI sometimes have incorrect implementations or haven\'t implemented a new feature yet but don\'t even warn about that. ')
    args = parser.parse_args()

    evaluation.args.cmd_arguments = args

    model_args = {
        'tokenizer': args.model_tokenizer,
        'default_system_message': args.model_default_system_message,
        'dtype': args.model_force_dtype,
    }

    model_args = { k: v for k, v in model_args.items() if v is not None }

    if args.run_correctness_check:
        run_inference_backend_correctness_check(args.model_type, args.model_name, model_args)
        return

    if 'all' in args.benchmarks:
        args.benchmarks = all_benchmarks

    if os.path.exists('reports/__index__.json'):
        with open('reports/__index__.json') as f:
            evaluation_id, models_and_benchmarks = merge_models_and_benchmarks_to_evaluate(
                json.load(f), args.model_type, args.model_name, args.benchmarks, model_args)

    with open(os.path.join('reports', '__index__.json'), 'w') as f:
        new_content = ',\n    '.join([json.dumps(entry) for entry in models_and_benchmarks])
        new_content = '[\n    ' + new_content + '\n]\n'
        f.write(new_content)

    evaluation_functions = [
        ('mt-bench', benchmarks.mt_bench.evaluate_model),
        ('human-eval-plus', benchmarks.human_eval_plus.evaluate_model),
        ('cot', benchmarks.cot.evaluate_model),
    ]

    for item in models_and_benchmarks:
        for benchmark_name, evaluation_function in evaluation_functions:
            if benchmark_name in item['benchmarks']:
                evaluation_function(item['model_type'], item['model_name'], item['model_args'], item['id'])

        unload_model()

        if 'lm-evaluation-harness' in item['benchmarks']:
            benchmarks.lm_evaluation_harness.evaluate_model(item['model_type'], item['model_name'], item['model_args'], item['id'])

        benchmarks.total.compute_total_scores(item['model_name'], item['id'])

    if evaluation_id is not None:
        scores = benchmarks.total.get_total_scores(args.model_name, evaluation_id)
        print(json.dumps(scores, indent=4))

    join_threads()

if __name__ == '__main__':
    try:
        main()
    except Exception as error:
        import traceback
        traceback.print_exc()
        stop()
